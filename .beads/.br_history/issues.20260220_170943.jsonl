{"id":"bd-10c","title":"[10.9] Build trust economics dashboard with attacker-ROI deltas.","description":"## [10.9] Build trust economics dashboard with attacker-ROI deltas\n\n### Why This Exists\n\nSection 9A.9 (Economic Trust Layer) and Section 9B.9 (decision-theoretic expected-loss and robust posterior updates) require that franken_node's security value proposition be quantified in economic terms, not just binary pass/fail security assessments. Decision-makers (CISOs, engineering leads) need to see how much harder and more expensive attacks become with franken_node compared to baseline Node.js or Bun. Threshold-based security (\"we block X% of attacks\") is insufficient — what matters is the economic impact: attacker cost amplification, defender cost reduction, and the ROI of trust policy investments. This dashboard makes the economic case with quantified models.\n\n### What It Must Do\n\nBuild a trust economics dashboard that presents attacker-ROI analysis and trust policy optimization recommendations:\n\n- **Attack-cost amplification metrics**: For each attack category (credential exfiltration, privilege escalation, supply-chain compromise, policy evasion, data exfiltration), compute the estimated attacker cost under three scenarios: (a) baseline Node.js (no hardening), (b) Bun (default security), (c) franken_node (full trust verification). Cost is measured in: time (hours of attacker effort), compute (resources required), tooling (sophistication of required tools), and detection risk (probability of being caught). The dashboard shows the amplification factor (franken_node cost / baseline cost) for each category.\n- **Privilege-risk pricing curves**: For each trust privilege level (unrestricted, standard, restricted, quarantined), compute the risk-adjusted cost of granting that privilege. Higher privilege = higher potential damage = higher implicit price. Curves show how trust policy configuration affects the privilege-risk balance.\n- **Trust policy tuning recommendations**: Based on the economic model, recommend trust policy configurations that optimize for specific objectives: minimize expected loss, maximize attacker cost, minimize defender operational overhead, or balance all three. Recommendations include: specific policy parameter values, expected economic impact, and confidence intervals.\n- **Economic models**: All metrics are computed from explicit economic models, not heuristic thresholds. Models are documented, versioned, and parameterizable. Model inputs include: attack frequency estimates (from adversarial campaign data, bd-9is), defense effectiveness measurements (from benchmark data, bd-f5d), and operational cost measurements (from telemetry).\n- **Decision-theoretic framing**: Per Section 9B.9, the dashboard uses expected-loss calculations with robust posterior updates. As new adversarial campaign results and telemetry data arrive, the economic model updates its estimates. The dashboard shows confidence intervals and model uncertainty, not just point estimates.\n- **Comparative visualization**: Side-by-side comparison of economic posture across Node.js, Bun, and franken_node with configurable trust policies. Visualizations include: attack-cost radar charts, privilege-risk curves, expected-loss timelines, and policy sensitivity analysis.\n\n### Acceptance Criteria\n\n1. Attack-cost amplification metrics are computed for at least five attack categories with three-way comparison (Node.js baseline, Bun, franken_node).\n2. Amplification factors are derived from explicit economic models with documented assumptions and parameterizable inputs.\n3. Privilege-risk pricing curves are generated for at least four trust privilege levels with configurable policy parameters.\n4. Trust policy tuning recommendations are generated with specific parameter values, expected economic impact, and confidence intervals.\n5. Economic models use decision-theoretic expected-loss calculations with posterior updates from incoming data.\n6. Dashboard data is available via structured JSON API; a reference visualization is provided.\n7. Model versioning ensures that metric changes are attributable to model updates vs. data updates; model changelog is maintained.\n8. A verification script validates model consistency, data freshness, and recommendation quality against test scenarios.\n\n### Key Dependencies\n\n- Adversarial campaign runner (bd-9is) for attack frequency and defense effectiveness data\n- Benchmark infrastructure (bd-f5d) for performance and resilience measurements\n- Telemetry namespace (10.13) for operational cost measurements\n- Trust policy infrastructure (10.4, 10.5) for privilege level definitions\n\n### Testing & Logging Requirements\n\n- Unit tests for economic model calculations, posterior updates, and recommendation generation.\n- Integration test with synthetic attack/defense data verifying end-to-end metric computation.\n- Verification script (`scripts/check_trust_economics.py`) with `--json` and `self_test()`.\n- Model updates logged at INFO; recommendation changes logged at WARN; data freshness issues logged at ERROR.\n\n### Expected Artifacts\n\n- `docs/specs/section_10_9/bd-10c_contract.md` — trust economics specification and model documentation\n- `scripts/check_trust_economics.py` — verification script\n- `tests/test_check_trust_economics.py` — unit tests\n- `fixtures/economic-models/` — model definitions and test scenarios\n- `artifacts/section_10_9/bd-10c/verification_evidence.json`\n- `artifacts/section_10_9/bd-10c/verification_summary.md`","acceptance_criteria":"1. Dashboard displays attacker-ROI deltas: estimated cost-to-compromise franken_node vs. Node.js across at least 5 attack categories (supply-chain, privilege escalation, resource exhaustion, data exfiltration, denial of service).\n2. Per Section 3 category targets: dashboard highlights where franken_node achieves >= 10x compromise reduction (cost increase for attacker).\n3. Metrics are computed from empirical data: adversarial campaign results (bd-9is), fuzz findings, CVE analysis, and audit results.\n4. Dashboard includes time-series views showing how attacker-ROI evolves across releases.\n5. Data sources are documented and each metric includes a methodology note explaining how the ROI delta was calculated.\n6. Dashboard is generated as a static HTML report (no external runtime dependencies) from a structured JSON data source under artifacts/.\n7. Per Section 9F moonshot bets: dashboard includes a 'trust economics summary' section quantifying the aggregate security value proposition with reproducible calculations.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:36:48.602750013Z","created_by":"ubuntu","updated_at":"2026-02-20T16:59:04.904896056Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-9","self-contained","test-obligations"]}
{"id":"bd-10ee","title":"[16] Contribution: transparent technical reports","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 16\n\nTask Objective:\nPublish transparent reports including failures and corrective actions.\n\nExecution Requirements:\n- Make this requirement machine-verifiable and release-gated where applicable.\n- Keep behavior self-documenting so future contributors do not need to re-open the master plan.\n\nTesting & Logging Requirements:\n- Unit tests for rule/contract logic and error conditions.\n- Integration/E2E tests for workflow-level enforcement.\n- Structured logs with stable codes and trace IDs.\n- Reproducible artifacts that prove contract satisfaction.\n\nAcceptance Criteria:\n- Success and failure invariants for [16] Contribution: transparent technical reports are explicitly quantified and machine-verifiable.\n- Determinism requirements for [16] Contribution: transparent technical reports are validated across repeated runs and adversarial perturbations.\n\n\nExpected Artifacts:\n- Machine-readable verification artifact with explicit canonical path under artifacts/section_16/bd-10ee/verification_evidence.json, suitable for CI/release gating.\n- Human-readable verification summary with explicit canonical path under artifacts/section_16/bd-10ee/verification_summary.md, linking implementation intent to measured validation outcomes.\n\nTask-Specific Clarification:\n- The implementation must deliver the exact capability named in the title, with no scope dilution and no silent feature omission.\n- Validation artifacts must include capability-specific thresholds, pass/fail criteria, and reproducible evidence bundles tied to this bead ID.\n- Program/section verification gates must be able to consume this bead's outputs without manual interpretation.\n\nWhy This Improves User Outcomes:\n- \"[16] Contribution: transparent technical reports\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[16] Contribution: transparent technical reports\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"1. Transparent technical reports are published for all major project milestones, including both successes and failures.\n2. Each report includes: (a) objective and hypothesis, (b) methodology, (c) results (quantitative where possible), (d) failures and what was learned, (e) corrective actions taken, (f) open questions and future work.\n3. Failure transparency: at least 30% of published reports include documentation of approaches that did not work and why.\n4. Reports are published on a regular cadence: at least quarterly, with ad-hoc reports for significant incidents or discoveries.\n5. Reports are written for a technical audience but include an executive summary accessible to non-specialists.\n6. Reports are versioned and hosted in a public, searchable archive with consistent formatting.\n7. Community feedback mechanism: each report has a discussion thread or comment section for external feedback.\n8. Evidence: technical_report_registry.json with per-report: title, date, category (success/failure/mixed), publication URL, and feedback count.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:39:37.131441436Z","created_by":"ubuntu","updated_at":"2026-02-20T15:28:44.620275194Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-16","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-10ee","depends_on_id":"bd-3id1","type":"blocks","created_at":"2026-02-20T07:43:26.724779215Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-10g0","title":"[10.16] Section-wide verification gate: comprehensive unit+e2e+logging","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.16\n\nTask Objective:\nCreate a hard section completion gate that verifies all implemented behavior in this section with comprehensive unit tests, integration/e2e scripts, and detailed structured logging evidence.\n\nAcceptance Criteria:\n- Section test matrix covers happy-path, edge-case, and adversarial/error-path scenarios for all section deliverables.\n- E2E scripts execute representative end-user workflows and policy/control-plane transitions for this section.\n- Structured logs include stable event/error codes, trace correlation IDs, and enough context for deterministic replay triage.\n- Verification report is deterministic and machine-readable for CI/release gating.\n\nExpected Artifacts:\n- Section-level test matrix and coverage summary artifact.\n- E2E script suite with pass/fail outputs and reproducible fixtures/seeds.\n- Structured log validation report and traceability bundle.\n- Gate verdict artifact consumable by release automation.\n\n- Machine-readable verification artifact at `artifacts/section_10_16/bd-10g0/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_16/bd-10g0/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests must validate contracts, invariants, and failure semantics.\n- E2E tests must validate cross-component behavior from user/operator entrypoints to persisted effects.\n- Logging must support root-cause analysis without hidden context requirements.\n\nTask-Specific Clarification:\n- For \"[10.16] Section-wide verification gate: comprehensive unit+e2e+logging\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.16] Section-wide verification gate: comprehensive unit+e2e+logging\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.16] Section-wide verification gate: comprehensive unit+e2e+logging\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.16] Section-wide verification gate: comprehensive unit+e2e+logging\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.16] Section-wide verification gate: comprehensive unit+e2e+logging\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:48:15.835934792Z","created_by":"ubuntu","updated_at":"2026-02-20T08:43:51.657945440Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-16","test-obligations","verification-gate"],"dependencies":[{"issue_id":"bd-10g0","depends_on_id":"bd-159q","type":"blocks","created_at":"2026-02-20T07:48:16.036471071Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-10g0","depends_on_id":"bd-1719","type":"blocks","created_at":"2026-02-20T07:48:16.519026205Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-10g0","depends_on_id":"bd-1a1j","type":"blocks","created_at":"2026-02-20T07:48:16.472312389Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-10g0","depends_on_id":"bd-1dpd","type":"blocks","created_at":"2026-02-20T08:24:26.286693849Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-10g0","depends_on_id":"bd-1v65","type":"blocks","created_at":"2026-02-20T07:48:16.283543557Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-10g0","depends_on_id":"bd-1xtf","type":"blocks","created_at":"2026-02-20T07:48:16.567260334Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-10g0","depends_on_id":"bd-26ux","type":"blocks","created_at":"2026-02-20T07:48:16.378661078Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-10g0","depends_on_id":"bd-28ld","type":"blocks","created_at":"2026-02-20T07:48:16.668054368Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-10g0","depends_on_id":"bd-2f5l","type":"blocks","created_at":"2026-02-20T07:48:16.186051413Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-10g0","depends_on_id":"bd-2ji2","type":"blocks","created_at":"2026-02-20T07:48:15.938293521Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-10g0","depends_on_id":"bd-2owx","type":"blocks","created_at":"2026-02-20T07:48:16.716714059Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-10g0","depends_on_id":"bd-2tua","type":"blocks","created_at":"2026-02-20T07:48:16.425842997Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-10g0","depends_on_id":"bd-2twu","type":"blocks","created_at":"2026-02-20T08:20:52.107783222Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-10g0","depends_on_id":"bd-34ll","type":"blocks","created_at":"2026-02-20T07:48:16.619989725Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-10g0","depends_on_id":"bd-35l5","type":"blocks","created_at":"2026-02-20T07:48:15.986272173Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-10g0","depends_on_id":"bd-3ndj","type":"blocks","created_at":"2026-02-20T07:48:16.236183937Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-10g0","depends_on_id":"bd-3u2o","type":"blocks","created_at":"2026-02-20T07:48:16.089917108Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-10g0","depends_on_id":"bd-8l9k","type":"blocks","created_at":"2026-02-20T07:48:16.137776518Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-10g0","depends_on_id":"bd-bt82","type":"blocks","created_at":"2026-02-20T07:48:16.331555341Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-10zx","title":"[BOOTSTRAP] Foundation integration and legacy-bead convergence","description":"Context:\nThis epic integrates active pre-master-plan beads (charter/doc/bootstrap/transplant/check tasks) into the canonical execution graph so no critical foundation work remains structurally orphaned.\n\nObjective:\n- Preserve all existing active work without duplication or scope loss.\n- Provide a single dependency anchor for bootstrap and migration-prep obligations before full plan closure.\n- Keep this bridge explicit so future agents can reason about why these tasks exist and how they relate to the master roadmap.\n\nIn-Scope Bead Families:\n- Product charter and governance baseline (docs surface).\n- CLI/bootstrap readiness beads currently in progress.\n- Transplant integrity chain (snapshot, lockfile, drift workflow).\n- Baseline verification pass executed via rch-only offload.\n\nExecution Notes:\n- This epic is dependency-only orchestration: it does not replace implementation beads.\n- Existing assignees and ownership remain unchanged.\n- Canonical plan epics stay authoritative for feature scope; this epic ensures bootstrap readiness is not forgotten.\n\nAcceptance Criteria:\n- All mapped bootstrap beads are explicitly linked into this epic via dependencies.\n- No mapped bead loses scope, status history, or ownership metadata.\n- Master execution closure (bd-33v) is gated on this integration epic to prevent silent omission.\n\nExpected Artifacts:\n- Dependency map proving each bootstrap bead is attached to this epic.\n- Rationale note that explains why bootstrap beads are retained and how they map to canonical execution.\n- Machine-readable verification artifact at `artifacts/section_bootstrap/bd-10zx/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_bootstrap/bd-10zx/verification_summary.md` linking dependency orchestration intent to measured outcomes.\n\nTesting & Logging Requirements:\n- Unit tests and unit-level validation for any scripts/helpers used to enforce dependency-map integrity.\n- E2E verification that br ready --json and bv --robot-plan reflect this integration deterministically.\n- Detailed structured logs or command artifacts showing before/after graph state and gating behavior.\n\n## Success Criteria\n- Bootstrap and legacy readiness work is fully represented in the canonical dependency graph with no orphan critical tasks.\n- Graph diagnostics remain healthy (no cycles, lint clean) after integration.\n- Agents can select work without ambiguity about bootstrap prerequisites or ownership boundaries.\n\n## Optimization Notes\n- User-Outcome Lens: \"[BOOTSTRAP] Foundation integration and legacy-bead convergence\" must improve operator confidence, safety posture, and deterministic recovery behavior under both normal and adversarial conditions.\n- Cross-Section Coordination: child beads must encode integration assumptions explicitly to avoid local optimizations that degrade system-wide correctness.\n- Verification Non-Negotiable: completion requires reproducible unit/integration/E2E evidence and structured logs suitable for independent replay.\n- Scope Protection: any simplification must preserve canonical-plan feature intent and be justified with explicit tradeoff documentation.","notes":"Verification spine added in this pass: matrix=bd-jvzc, e2e=bd-3k9t, gate=bd-3ohj. This preserves full scope while forcing deterministic quality evidence before bd-10zx closure.","status":"open","priority":1,"issue_type":"epic","created_at":"2026-02-20T07:55:44.537988268Z","created_by":"ubuntu","updated_at":"2026-02-20T16:06:18.316945720Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["bootstrap","plan-integration","quality-gate"],"dependencies":[{"issue_id":"bd-10zx","depends_on_id":"bd-1pk","type":"blocks","created_at":"2026-02-20T07:56:11.516898289Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-10zx","depends_on_id":"bd-1qz","type":"blocks","created_at":"2026-02-20T07:56:10.429288401Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-10zx","depends_on_id":"bd-29q","type":"blocks","created_at":"2026-02-20T07:56:10.754427901Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-10zx","depends_on_id":"bd-2a3","type":"blocks","created_at":"2026-02-20T07:56:10.262899213Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-10zx","depends_on_id":"bd-2lb","type":"blocks","created_at":"2026-02-20T07:56:10.901576221Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-10zx","depends_on_id":"bd-2nd","type":"blocks","created_at":"2026-02-20T07:56:10.100987764Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-10zx","depends_on_id":"bd-32e","type":"blocks","created_at":"2026-02-20T07:56:11.368787246Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-10zx","depends_on_id":"bd-3ohj","type":"blocks","created_at":"2026-02-20T08:03:12.206594381Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-10zx","depends_on_id":"bd-3vk","type":"blocks","created_at":"2026-02-20T07:56:11.051936044Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-10zx","depends_on_id":"bd-7rt","type":"blocks","created_at":"2026-02-20T07:56:10.597375332Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-10zx","depends_on_id":"bd-n9r","type":"blocks","created_at":"2026-02-20T07:56:11.208335195Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-11j7","title":"Epic: Proof-Carrying + Deterministic Operations [10.14d]","description":"- type: epic","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-20T07:47:58.072163029Z","updated_at":"2026-02-20T07:49:21.215295937Z","closed_at":"2026-02-20T07:49:21.215278134Z","close_reason":"Superseded by canonical detailed master-plan graph (bd-33v) with full 10.N-10.21 and 11-16 coverage, explicit dependencies, and per-section verification gates.","source_repo":".","compaction_level":0,"original_size":0}
{"id":"bd-11oz","title":"Epic: FrankenSQLite-Inspired Runtime Systems [10.11]","description":"- type: epic","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-20T07:47:58.072163029Z","updated_at":"2026-02-20T07:49:21.147604066Z","closed_at":"2026-02-20T07:49:21.147582055Z","close_reason":"Superseded by canonical detailed master-plan graph (bd-33v) with full 10.N-10.21 and 11-16 coverage, explicit dependencies, and per-section verification gates.","source_repo":".","compaction_level":0,"original_size":0}
{"id":"bd-11rz","title":"[10.19] Add release gate requiring ATC-backed evidence for designated ecosystem-level trust claims.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.19 — Adversarial Trust Commons Execution Track (9M)\n\nWhy This Exists:\nATC federated intelligence track for privacy-preserving cross-deployment threat learning, robust aggregation, and verifier-backed trust metrics.\n\nTask Objective:\nAdd release gate requiring ATC-backed evidence for designated ecosystem-level trust claims.\n\nAcceptance Criteria:\n- Release and public claims about collective intelligence are blocked without required ATC coverage/provenance artifacts; gate output is signed and machine-readable.\n\nExpected Artifacts:\n- `.github/workflows/atc-claim-gate.yml`, `docs/conformance/atc_release_claim_gate.md`, `artifacts/10.19/atc_release_gate_report.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_19/bd-11rz/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_19/bd-11rz/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.19] Add release gate requiring ATC-backed evidence for designated ecosystem-level trust claims.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.19] Add release gate requiring ATC-backed evidence for designated ecosystem-level trust claims.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.19] Add release gate requiring ATC-backed evidence for designated ecosystem-level trust claims.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.19] Add release gate requiring ATC-backed evidence for designated ecosystem-level trust claims.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.19] Add release gate requiring ATC-backed evidence for designated ecosystem-level trust claims.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"- Release and public claims about collective intelligence are blocked without required ATC coverage/provenance artifacts; gate output is signed and machine-readable.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:37:06.336242Z","created_by":"ubuntu","updated_at":"2026-02-20T17:00:51.653931346Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-19","self-contained","test-obligations"]}
{"id":"bd-126h","title":"[10.14] Implement append-only marker stream for high-impact control events with dense sequence invariant checks.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.14 — FrankenSQLite Deep-Mined Expansion Execution Track (9J)\n\nWhy This Exists:\nFrankenSQLite deep-mined control integrity track: evidence ledgers, proofs, epochs, remote effects, markers/MMR, and deterministic fault labs.\n\nTask Objective:\nImplement append-only marker stream for high-impact control events with dense sequence invariant checks.\n\nAcceptance Criteria:\n- Marker stream is append-only with dense sequence and hash-chain invariants; torn-tail recovery is deterministic; invariant breaks trigger hard alert.\n\nExpected Artifacts:\n- `src/control_plane/marker_stream.rs`, `tests/conformance/marker_stream_invariants.rs`, `artifacts/10.14/marker_stream_integrity_report.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_14/bd-126h/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_14/bd-126h/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.14] Implement append-only marker stream for high-impact control events with dense sequence invariant checks.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.14] Implement append-only marker stream for high-impact control events with dense sequence invariant checks.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.14] Implement append-only marker stream for high-impact control events with dense sequence invariant checks.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.14] Implement append-only marker stream for high-impact control events with dense sequence invariant checks.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.14] Implement append-only marker stream for high-impact control events with dense sequence invariant checks.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"Marker stream is append-only with dense sequence and hash-chain invariants; torn-tail recovery is deterministic; invariant breaks trigger hard alert.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:36:58.546917975Z","created_by":"ubuntu","updated_at":"2026-02-20T16:22:27.650933758Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-14","self-contained","test-obligations"]}
{"id":"bd-129f","title":"[10.14] Implement O(1) marker lookup by sequence and O(log N) timestamp-to-sequence search.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.14 — FrankenSQLite Deep-Mined Expansion Execution Track (9J)\n\nWhy This Exists:\nFrankenSQLite deep-mined control integrity track: evidence ledgers, proofs, epochs, remote effects, markers/MMR, and deterministic fault labs.\n\nTask Objective:\nImplement O(1) marker lookup by sequence and O(log N) timestamp-to-sequence search.\n\nAcceptance Criteria:\n- Sequence lookup performs O(1) slot math; timestamp lookup uses bounded O(log N) search; performance targets are met on large history sets.\n\nExpected Artifacts:\n- `tests/perf/marker_lookup_complexity.rs`, `docs/specs/marker_lookup_algorithms.md`, `artifacts/10.14/marker_lookup_benchmarks.csv`.\n\n- Machine-readable verification artifact at `artifacts/section_10_14/bd-129f/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_14/bd-129f/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.14] Implement O(1) marker lookup by sequence and O(log N) timestamp-to-sequence search.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.14] Implement O(1) marker lookup by sequence and O(log N) timestamp-to-sequence search.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.14] Implement O(1) marker lookup by sequence and O(log N) timestamp-to-sequence search.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.14] Implement O(1) marker lookup by sequence and O(log N) timestamp-to-sequence search.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.14] Implement O(1) marker lookup by sequence and O(log N) timestamp-to-sequence search.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"Sequence lookup performs O(1) slot math; timestamp lookup uses bounded O(log N) search; performance targets are met on large history sets.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:36:58.627411278Z","created_by":"ubuntu","updated_at":"2026-02-20T15:44:03.694246146Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-14","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-129f","depends_on_id":"bd-126h","type":"blocks","created_at":"2026-02-20T07:43:16.019535697Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-12f","title":"[10.3] Build migration confidence report with uncertainty bands.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.3 — Migration System\n\nWhy This Exists:\nMigration autopilot pipeline from audit to rollout, designed to collapse migration friction with deterministic confidence and replayability.\n\nTask Objective:\nBuild migration confidence report with uncertainty bands.\n\nAcceptance Criteria:\n- Implement with explicit success/failure invariants and deterministic behavior under normal, edge, and fault scenarios.\n- Ensure outcomes are verifiable via automated tests and reproducible artifact outputs.\n\nExpected Artifacts:\n- Section-owned design/spec artifact at `docs/specs/section_10_3/bd-12f_contract.md`, capturing decision rationale, invariants, and interface boundaries.\n- Machine-readable verification artifact at `artifacts/section_10_3/bd-12f/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_3/bd-12f/verification_summary.md` linking implementation intent to measured outcomes.\n\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.3] Build migration confidence report with uncertainty bands.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.3] Build migration confidence report with uncertainty bands.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.3] Build migration confidence report with uncertainty bands.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.3] Build migration confidence report with uncertainty bands.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.3] Build migration confidence report with uncertainty bands.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","status":"closed","priority":1,"issue_type":"task","assignee":"CrimsonCrane","created_at":"2026-02-20T07:36:45.191082355Z","created_by":"ubuntu","updated_at":"2026-02-20T10:18:27.341029979Z","closed_at":"2026-02-20T10:18:27.341003880Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-3","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-12f","depends_on_id":"bd-3dn","type":"blocks","created_at":"2026-02-20T07:43:22.233582034Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-12h8","title":"[10.13] Persist required artifacts (`invoke/response/receipt/approval/revocation/audit`) with deterministic replay hooks.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.13 — FCP Deep-Mined Expansion Execution Track (9I)\n\nWhy This Exists:\nDeep connector/provider contract track: lifecycle FSMs, conformance harnesses, fencing, sandboxing, revocation freshness, and protocol hardening.\n\nTask Objective:\nPersist required artifacts (`invoke/response/receipt/approval/revocation/audit`) with deterministic replay hooks.\n\nAcceptance Criteria:\n- Required artifact families are persisted and indexable; replay hook reconstructs high-impact event sequence deterministically; missing required artifacts fail integrity checks.\n\nExpected Artifacts:\n- `tests/integration/required_artifact_replay.rs`, `docs/specs/replay_hook_contract.md`, `artifacts/10.13/replay_integrity_report.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_13/bd-12h8/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_13/bd-12h8/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.13] Persist required artifacts (`invoke/response/receipt/approval/revocation/audit`) with deterministic replay hooks.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.13] Persist required artifacts (`invoke/response/receipt/approval/revocation/audit`) with deterministic replay hooks.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.13] Persist required artifacts (`invoke/response/receipt/approval/revocation/audit`) with deterministic replay hooks.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.13] Persist required artifacts (`invoke/response/receipt/approval/revocation/audit`) with deterministic replay hooks.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.13] Persist required artifacts (`invoke/response/receipt/approval/revocation/audit`) with deterministic replay hooks.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","status":"closed","priority":1,"issue_type":"task","assignee":"CrimsonCrane","created_at":"2026-02-20T07:36:54.331753829Z","created_by":"ubuntu","updated_at":"2026-02-20T13:00:34.420339433Z","closed_at":"2026-02-20T13:00:34.420311151Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-13","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-12h8","depends_on_id":"bd-1p2b","type":"blocks","created_at":"2026-02-20T07:43:13.769445404Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-12n3","title":"[10.14] Implement idempotency key derivation from request bytes with epoch binding.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.14 — FrankenSQLite Deep-Mined Expansion Execution Track (9J)\n\nWhy This Exists:\nFrankenSQLite deep-mined control integrity track: evidence ledgers, proofs, epochs, remote effects, markers/MMR, and deterministic fault labs.\n\nTask Objective:\nImplement idempotency key derivation from request bytes with epoch binding.\n\nAcceptance Criteria:\n- Key derivation is deterministic, domain-separated, and epoch-bound; collisions on distinct requests are empirically negligible; derivation vectors are published.\n\nExpected Artifacts:\n- `src/remote/idempotency.rs`, `tests/conformance/idempotency_key_derivation.rs`, `artifacts/10.14/idempotency_vectors.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_14/bd-12n3/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_14/bd-12n3/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.14] Implement idempotency key derivation from request bytes with epoch binding.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.14] Implement idempotency key derivation from request bytes with epoch binding.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.14] Implement idempotency key derivation from request bytes with epoch binding.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.14] Implement idempotency key derivation from request bytes with epoch binding.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.14] Implement idempotency key derivation from request bytes with epoch binding.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"Key derivation is deterministic, domain-separated, and epoch-bound; collisions on distinct requests are empirically negligible; derivation vectors are published.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:36:57.812183569Z","created_by":"ubuntu","updated_at":"2026-02-20T15:44:05.824918237Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-14","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-12n3","depends_on_id":"bd-ac83","type":"blocks","created_at":"2026-02-20T07:43:15.570763894Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-12q","title":"[10.4] Integrate revocation propagation with canonical freshness checks (from `10.13`) in extension workflows.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.4 — Extension Ecosystem + Registry\n\nWhy This Exists:\nExtension ecosystem and registry trust foundation: signed artifacts, provenance, trust cards, revocation, and quarantine flows.\n\nTask Objective:\nIntegrate revocation propagation with canonical freshness checks (from `10.13`) in extension workflows.\n\nAcceptance Criteria:\n(See structured acceptance_criteria field for domain-specific criteria.)\n\nExpected Artifacts:\n- Section-owned design/spec artifact at `docs/specs/section_10_4/bd-12q_contract.md`, capturing decision rationale, invariants, and interface boundaries.\n- Machine-readable verification artifact at `artifacts/section_10_4/bd-12q/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_4/bd-12q/verification_summary.md` linking implementation intent to measured outcomes.\n\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.4] Integrate revocation propagation with canonical freshness checks (from `10.13`) in extension workflows.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.4] Integrate revocation propagation with canonical freshness checks (from `10.13`) in extension workflows.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.4] Integrate revocation propagation with canonical freshness checks (from `10.13`) in extension workflows.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.4] Integrate revocation propagation with canonical freshness checks (from `10.13`) in extension workflows.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.4] Integrate revocation propagation with canonical freshness checks (from `10.13`) in extension workflows.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"1. Extension install/update/load workflows query the 10.13 revocation registry (bd-y7lu) for current revocation-head checkpoint before proceeding; stale checks (older than the safety-tier-specific freshness threshold from bd-1m8r) block the operation. 2. Freshness tiers map to extension operations: 'dangerous' tier (real-time check) for install/enable of new extensions, 'risky' tier (<=60s staleness) for extension updates, 'routine' tier (<=300s staleness) for extension capability re-evaluation at runtime. 3. Revocation propagation is push-based where available (webhook/SSE from registry) with pull-based fallback; local cache stores last-known-good revocation-head with monotonic sequence number. 4. When a revocation event arrives mid-operation (e.g., during a staged rollout of an extension update), the operation is atomically aborted and the extension is moved to quarantine state. 5. Offline/partitioned nodes enforce a maximum grace period (configurable, default 24h) before all non-cached extensions are suspended; grace period is per-safety-tier. 6. Revocation status is embedded in the extension trust card (bd-2yh) as a real-time field, not a cached snapshot. 7. Integration tests simulate: (a) revocation arriving during install, (b) freshness check timeout, (c) network partition exceeding grace period, (d) revocation of a key in the attestation chain (not just the extension itself). 8. Structured log events: REVOCATION_CHECK_STARTED, REVOCATION_CHECK_PASSED, REVOCATION_CHECK_STALE, REVOCATION_PROPAGATED, EXTENSION_SUSPENDED_OFFLINE_GRACE, each with extension_id, revocation_head_seq, freshness_age_ms, and safety_tier fields.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:36:45.585688440Z","created_by":"ubuntu","updated_at":"2026-02-20T16:59:00.541948520Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-4","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-12q","depends_on_id":"bd-1m8r","type":"blocks","created_at":"2026-02-20T15:00:22.931690858Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-12q","depends_on_id":"bd-y7lu","type":"blocks","created_at":"2026-02-20T15:00:22.748441618Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-137","title":"[10.5] Implement policy-visible compatibility gate APIs.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md — Section 10.5, cross-ref Section 3.2 #5, 9A.5, 9B.5, 9C.5, 9D.5\n\n## Why This Exists\nThis is one of the 10 Impossible-by-Default capabilities (Section 3.2 #5). Policy-visible compatibility gates are the mechanism by which operators control which compatibility behaviors are active, at what risk level, with full traceability. Without this, compatibility shims operate as hidden magic — the exact opposite of trust-native operations.\n\n## What Policy-Visible Compatibility Gate APIs Must Do\n1. **Typed shim registry exposure:** Every compatibility shim registered in the compatibility behavior registry (10.2) must be queryable via API with full metadata: shim ID, behavior description, risk category, activation policy, and divergence rationale.\n\n2. **Mode selection API:** Operators can select compatibility modes (strict, balanced, legacy-risky) per project, per tenant, or per extension scope. Mode selection is policy-gated and produces signed configuration receipts.\n\n3. **Gate evaluation API:** Before executing a compatibility-shimmed behavior, the gate API checks current policy state, evaluates risk constraints, and returns allow/deny/audit decisions with structured rationale.\n\n4. **Policy-as-data contracts:** Shim activation constraints are expressed as machine-verifiable policy predicates (per 9B.5: policy-as-data signatures with attenuation semantics). This means shim activation is cryptographically constrained, not just configuration-driven.\n\n5. **Non-interference and monotonicity checks:** Per 9C.5, the API must enforce that enabling additional shims does not weaken existing security guarantees (monotonicity) and that shim activation in one scope does not leak side effects into another (non-interference).\n\n6. **Performance-optimized evaluation path:** Per 9D.5, the policy evaluation path must be optimized while preserving deterministic rule order. Precompiled decision DAGs or cached policy compilations where safe.\n\n## Context from Section 9 Enhancement Maps\n- 9A.5: \"Any behavior shim must be typed, auditable, and policy-gated, so operators can choose compatibility level by risk appetite with full traceability.\"\n- 9B.5: \"Use policy-as-data signatures and attenuation semantics so shim activation is cryptographically constrained and auditable.\"\n- 9C.5: \"Encode non-interference and monotonicity checks as machine-verifiable policy compiler outputs.\"\n- 9D.5: \"Optimize policy evaluation path while preserving deterministic rule order.\"\n\n## Acceptance Criteria\n1. API surface exposes all registered compatibility shims with full typed metadata.\n2. Mode selection (strict/balanced/legacy-risky) is per-scope and produces signed receipts.\n3. Gate evaluation returns structured allow/deny/audit decisions with machine-readable rationale.\n4. Policy-as-data contracts are cryptographically verifiable.\n5. Non-interference property: shim activation in scope A has no observable effect in scope B.\n6. Monotonicity property: adding shims never weakens existing security guarantees (formally testable).\n7. Gate evaluation latency meets interactive budget (< 1ms p99 for cached policy).\n8. All gate decisions emit structured audit events with trace correlation IDs.\n\n## Expected Artifacts\n- docs/specs/section_10_5/bd-137_contract.md: API design, policy-as-data contract format, non-interference/monotonicity proof strategy\n- artifacts/section_10_5/bd-137/verification_evidence.json: Machine-readable test results\n- artifacts/section_10_5/bd-137/verification_summary.md: Human-readable outcome summary\n\n## Testing & Logging Requirements\n- Unit tests: shim registry query completeness, mode selection receipt signing, gate evaluation determinism, non-interference property, monotonicity property\n- Integration tests: full mode-selection -> gate-evaluation -> audit-emission pipeline\n- E2E tests: operator workflow from selecting mode through executing shimmed behavior to reviewing audit trail\n- Performance tests: gate evaluation latency under policy load\n- Adversarial tests: policy bypass attempts, malformed gate requests, conflicting mode selections\n- Structured logs: stable event codes for gate decisions, policy receipt generation, and audit emission","acceptance_criteria":"1. Expose a PolicyGateStatus struct containing: gate_name (string), pass (bool), evaluated_at (RFC-3339 timestamp), evidence_hash (SHA-256 hex), policy_version (semver string), and detail (human-readable summary).\n2. Implement at least three concrete gate checks: CompatibilityMatrixGate (validates current capability matrix against minimum compatibility floor), RevocationPrecheckGate (confirms no active revocation entries block the requested action), and VersionFenceGate (ensures target version satisfies the fencing predicate from state_model.rs).\n3. Each gate check must complete within 50 ms on a single core; add a benchmark test that asserts this.\n4. Provide a query_gates(action: &str) -> Vec<PolicyGateStatus> API that evaluates all registered gates for a given action name and returns their statuses.\n5. Gates must be composable: a CompositeGate can combine N child gates with AND/OR/THRESHOLD(k-of-n) logic and itself returns a PolicyGateStatus.\n6. All gate evaluations must emit a structured log event (tracing span) containing the gate name, result, and wall-clock duration.\n7. Verification: scripts/check_policy_gate.py --json produces a JSON report; unit tests in tests/test_check_policy_gate.py cover pass, fail, and composite scenarios; evidence artifact lands in artifacts/section_10_5/bd-137/.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:36:46.061122822Z","created_by":"ubuntu","updated_at":"2026-02-20T16:18:01.919973727Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-5","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-137","depends_on_id":"bd-1ta","type":"blocks","created_at":"2026-02-20T07:46:36.343558606Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-137","depends_on_id":"bd-1xg","type":"blocks","created_at":"2026-02-20T07:46:36.388237131Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-137","depends_on_id":"bd-3il","type":"blocks","created_at":"2026-02-20T07:46:36.433708903Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-137","depends_on_id":"bd-cda","type":"blocks","created_at":"2026-02-20T07:46:36.480340506Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-13q","title":"[10.10] Adopt canonical stable error namespace and compatibility policy (from `10.13`) across product surfaces.","description":"[10.10] Adopt canonical stable error namespace and compatibility policy (from 10.13) across product surfaces.\n\n## Why This Exists\n\nSection 9E.9 mandates a unified, stable error namespace across all product surfaces. The 10.13 work established a stable error code registry (`error_code_registry.json`) and error code contract (`error_code_contract.md`) that define structured error codes with retryable flags, retry_after hints, and recovery hints. However, these are currently only enforced within the connector module. This bead extends enforcement to ALL product surfaces: CLI output, REST API responses, trust protocol messages, migration reports, operator dashboards, and SDK error returns. Every error a user or machine consumer sees must carry a stable, documented code from the canonical namespace.\n\n## What It Must Do\n\n1. **Error namespace adoption audit.** Scan all product surfaces (CLI, API, protocol messages, logs, SDK) and identify every error path. Produce an audit report listing each error path, its current error representation, and whether it maps to a stable code. Unmapped errors are violations.\n\n2. **Surface-specific error formatters.** Implement error formatting adapters for each product surface:\n   - CLI: human-readable message with error code in brackets, e.g., `[FN-CONN-0012] Connection refused: retry in 5s`.\n   - JSON API: `{\"error_code\": \"FN-CONN-0012\", \"message\": \"...\", \"retryable\": true, \"retry_after_ms\": 5000, \"recovery_hint\": \"...\"}`.\n   - Protocol messages: compact binary error frame with code, retryable bit, and optional payload.\n   - Logs: structured log fields `error.code`, `error.retryable`, `error.recovery_hint`.\n\n3. **Compatibility policy enforcement.** Once an error code is published in a stable release, it cannot be removed or have its semantics changed (retryable flag, code number). New codes can be added. Deprecated codes must remain valid for at least 2 major versions. The compatibility policy is enforced by a CI check that compares the current registry against the last released registry.\n\n4. **Error code coverage gate.** A verification script ensures that every `Err(...)` return in the Rust codebase and every `raise` / error return in the Python codebase maps to a registered error code. Unregistered errors fail the gate.\n\n5. **Recovery hint quality.** Every error code must have a non-empty `recovery_hint` field that provides actionable guidance. Hints are validated for minimum length (> 20 chars) and must not be generic (\"an error occurred\").\n\n6. **Cross-surface consistency.** The same error code must produce semantically equivalent messages across all surfaces. A test suite generates errors and checks that CLI, API, and log representations all carry the same code and retryable flag.\n\n## Acceptance Criteria\n\n1. Error namespace audit report exists at `artifacts/section_10_10/bd-13q/error_audit.json` with zero unmapped errors.\n2. Surface-specific formatters implemented in `crates/franken-node/src/connector/error_surface.rs`.\n3. Compatibility policy enforced by `scripts/check_error_compat.py` comparing current vs. baseline registry.\n4. Error code coverage gate (`scripts/check_error_coverage.py`) passes with zero unregistered errors.\n5. All error codes have recovery hints > 20 characters.\n6. Cross-surface consistency test confirms code/retryable agreement across CLI, API, and log outputs.\n7. Verification script `scripts/check_error_namespace.py` with `--json` flag confirms all criteria.\n8. Evidence artifacts written to `artifacts/section_10_10/bd-13q/`.\n\n## Key Dependencies\n\n- 10.13 error code registry (`error_code_registry.json`) — source of truth.\n- 10.13 error code contract (`error_code_contract.md`) — policy definition.\n- bd-1l5 (trust object IDs) — error codes for malformed IDs flow from this namespace.\n- CLI module (`src/cli.rs`) — surface adapter integration point.\n\n## Testing & Logging Requirements\n\n- Unit tests in `tests/test_check_error_namespace.py` covering: format validation, compatibility diff detection, coverage gap detection, recovery hint quality.\n- Integration test that exercises error paths end-to-end across CLI and API surfaces.\n- Self-test mode validates detection of deliberately unregistered error codes.\n- Structured logging: `error_namespace.unmapped_error`, `error_namespace.compat_violation`, `error_namespace.coverage_gap` events.\n\n## Expected Artifacts\n\n- `docs/specs/section_10_10/bd-13q_contract.md` — specification document.\n- `crates/franken-node/src/connector/error_surface.rs` — surface formatters.\n- `scripts/check_error_namespace.py` — verification script.\n- `scripts/check_error_compat.py` — compatibility checker.\n- `scripts/check_error_coverage.py` — coverage gate.\n- `tests/test_check_error_namespace.py` — unit tests.\n- `artifacts/section_10_10/bd-13q/verification_evidence.json` — evidence.\n- `artifacts/section_10_10/bd-13q/verification_summary.md` — summary.\n- `artifacts/section_10_10/bd-13q/error_audit.json` — audit report.","acceptance_criteria":"1. Adopt the canonical error code registry from 10.13 (error_code_registry.rs) as the single source of truth for all product-surface error codes. Every error returned to users or operators MUST reference a registered error code from this registry.\n2. Define product-surface error namespace prefixes: FN-CTRL-* (control plane), FN-MIG-* (migration), FN-AUTH-* (authentication/authorization), FN-POL-* (policy), FN-ZON-* (zone/tenant), FN-TOK-* (token). Each prefix MUST be registered in the canonical registry.\n3. Implement an ErrorCompatibilityPolicy with rules: (a) error codes are append-only (existing codes MUST NOT be removed or renumbered), (b) error message text may be refined but the code and category (TRANSIENT, PERMANENT, CONFIGURATION) MUST NOT change, (c) new error codes MUST include a human-readable description and a machine-readable severity tag.\n4. Implement a compatibility check: given two versions of the error registry (old and new), verify no codes were removed, no categories changed, and all new codes have required metadata. Return a CompatibilityReport with added/unchanged/violated lists.\n5. Provide a product_error! macro or builder that constructs errors with: (a) registered code, (b) message, (c) trace correlation ID, (d) optional structured context map. Reject construction with unregistered codes at compile time or test time.\n6. Integrate with the telemetry_namespace module from 10.13: error metrics MUST use the registered code as a dimension, not free-form strings.\n7. Unit tests: (a) all existing product errors map to registered codes, (b) compatibility check passes for additive changes, (c) compatibility check fails for removal, (d) compatibility check fails for category change, (e) product_error! with valid code succeeds, (f) product_error! with unknown code fails.\n8. CI gate: run the compatibility check between the previous release registry and the current one; fail the build on violations.\n9. Verification: scripts/check_error_namespace.py --json, artifacts at artifacts/section_10_10/bd-13q/.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:36:49.504860647Z","created_by":"ubuntu","updated_at":"2026-02-20T17:01:29.955152456Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-10","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-13q","depends_on_id":"bd-novi","type":"blocks","created_at":"2026-02-20T14:59:52.298263711Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-13yn","title":"[12] Risk control: signal poisoning and Sybil","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 12\n\nTask Objective:\nImplement robust aggregation plus attestation/stake weighting and adversarial federation gates.\n\nExecution Requirements:\n- Make this requirement machine-verifiable and release-gated where applicable.\n- Keep behavior self-documenting so future contributors do not need to re-open the master plan.\n\nTesting & Logging Requirements:\n- Unit tests for rule/contract logic and error conditions.\n- Integration/E2E tests for workflow-level enforcement.\n- Structured logs with stable codes and trace IDs.\n- Reproducible artifacts that prove contract satisfaction.\n\nAcceptance Criteria:\n- Success and failure invariants for [12] Risk control: signal poisoning and Sybil are explicitly quantified and machine-verifiable.\n- Determinism requirements for [12] Risk control: signal poisoning and Sybil are validated across repeated runs and adversarial perturbations.\n\n\nExpected Artifacts:\n- Machine-readable verification artifact with explicit canonical path under artifacts/section_12/bd-13yn/verification_evidence.json, suitable for CI/release gating.\n- Human-readable verification summary with explicit canonical path under artifacts/section_12/bd-13yn/verification_summary.md, linking implementation intent to measured validation outcomes.\n\nTask-Specific Clarification:\n- The implementation must deliver the exact capability named in the title, with no scope dilution and no silent feature omission.\n- Validation artifacts must include capability-specific thresholds, pass/fail criteria, and reproducible evidence bundles tied to this bead ID.\n- Program/section verification gates must be able to consume this bead's outputs without manual interpretation.\n\nWhy This Improves User Outcomes:\n- \"[12] Risk control: signal poisoning and Sybil\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[12] Risk control: signal poisoning and Sybil\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"RISK: Signal poisoning and Sybil attacks — malicious nodes inject false reputation/trust signals or create multiple fake identities to manipulate the trust graph.\nIMPACT: Corrupted trust decisions, malicious extensions gaining undeserved trust, legitimate extensions being suppressed.\nCOUNTERMEASURES:\n  (a) Robust aggregation: trust signal aggregation uses trimmed-mean or median (not simple average) to resist outlier injection.\n  (b) Stake weighting: trust signals are weighted by the contributor's own stake/reputation, making Sybil attacks expensive.\n  (c) Adversarial gates: CI includes adversarial test suites that simulate poisoning and Sybil scenarios; trust system must maintain correct rankings.\nVERIFICATION:\n  1. Robust aggregation: injecting 20% poisoned signals shifts the aggregate by <= 5% from the true value.\n  2. Stake weighting: a newly-created node's signal has <= 1% weight vs an established node's signal.\n  3. Sybil resistance: creating 100 fake identities has less influence than 5 established honest nodes.\n  4. Adversarial test suite with >= 10 attack scenarios passes in CI.\nTEST SCENARIOS:\n  - Scenario A: Inject 20% maximally-adversarial signals; verify trust ranking of honest nodes changes by <= 1 position.\n  - Scenario B: Create 100 Sybil identities all endorsing a malicious extension; verify it does not enter the top-50% trust tier.\n  - Scenario C: Simulate a coordinated signal poisoning campaign over 10 rounds; verify trust system converges back to correct rankings within 3 rounds after attack stops.\n  - Scenario D: Verify that stake-weighting function is monotonically increasing with verified history length.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:39:33.765954329Z","created_by":"ubuntu","updated_at":"2026-02-20T15:19:23.743706916Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-12","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-13yn","depends_on_id":"bd-1nab","type":"blocks","created_at":"2026-02-20T07:43:24.997723150Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-145n","title":"[10.15] Integrate deterministic lab runtime scenarios for all high-impact control protocols.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.15 — Asupersync-First Integration Execution Track (8.4-8.6)\n\nWhy This Exists:\nAsupersync-first control-plane migration track enforcing Cx-first, region ownership, cancellation correctness, and protocol-grade verification gates.\n\nTask Objective:\nIntegrate deterministic lab runtime scenarios for all high-impact control protocols.\n\nAcceptance Criteria:\n- Canonical control scenarios replay identically by seed; protocol invariants are asserted with deterministic failure artifacts.\n\nExpected Artifacts:\n- `tests/lab/control_protocol_scenarios.rs`, `docs/testing/control_lab_scenarios.md`, `artifacts/10.15/control_lab_seed_matrix.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_15/bd-145n/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_15/bd-145n/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.15] Integrate deterministic lab runtime scenarios for all high-impact control protocols.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.15] Integrate deterministic lab runtime scenarios for all high-impact control protocols.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.15] Integrate deterministic lab runtime scenarios for all high-impact control protocols.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.15] Integrate deterministic lab runtime scenarios for all high-impact control protocols.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.15] Integrate deterministic lab runtime scenarios for all high-impact control protocols.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"- Canonical control scenarios replay identically by seed; protocol invariants are asserted with deterministic failure artifacts.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:37:00.709371105Z","created_by":"ubuntu","updated_at":"2026-02-20T17:00:41.693154928Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-15","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-145n","depends_on_id":"bd-2qqu","type":"blocks","created_at":"2026-02-20T14:59:37.557871351Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-145n","depends_on_id":"bd-876n","type":"blocks","created_at":"2026-02-20T14:59:37.729448838Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-159q","title":"[10.16] Add waiver workflow for justified substrate exceptions.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.16 — Adjacent Substrate Integration Execution Track (8.7)\n\nWhy This Exists:\nAdjacent substrate integration track to enforce deep composition with frankentui, frankensqlite, sqlmodel_rust, and fastapi_rust.\n\nTask Objective:\nAdd waiver workflow for justified substrate exceptions.\n\nAcceptance Criteria:\n- Waivers require risk analysis, bounded scope, owner signoff, and expiry date; expired waivers fail compliance gate.\n\nExpected Artifacts:\n- `docs/policy/adjacent_substrate_waiver_process.md`, `artifacts/10.16/waiver_registry.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_16/bd-159q/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_16/bd-159q/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.16] Add waiver workflow for justified substrate exceptions.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.16] Add waiver workflow for justified substrate exceptions.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.16] Add waiver workflow for justified substrate exceptions.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.16] Add waiver workflow for justified substrate exceptions.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.16] Add waiver workflow for justified substrate exceptions.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"- Waivers require risk analysis, bounded scope, owner signoff, and expiry date; expired waivers fail compliance gate.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:37:02.682442447Z","created_by":"ubuntu","updated_at":"2026-02-20T17:00:52.762713533Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-16","self-contained","test-obligations"]}
{"id":"bd-15j6","title":"[10.15] Make canonical evidence-ledger emission (from `10.14`) mandatory for policy-influenced control decisions.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.15 — Asupersync-First Integration Execution Track (8.4-8.6)\n\nWhy This Exists:\nAsupersync-first control-plane migration track enforcing Cx-first, region ownership, cancellation correctness, and protocol-grade verification gates.\n\nTask Objective:\nMake canonical evidence-ledger emission (from `10.14`) mandatory for policy-influenced control decisions.\n\nAcceptance Criteria:\n- Missing evidence entry for policy-influenced decision is a conformance failure; control-plane entries align with canonical schema and ordering.\n\nExpected Artifacts:\n- `tests/conformance/control_policy_evidence_required.rs`, `docs/integration/control_evidence_contract.md`, `artifacts/10.15/control_evidence_samples.jsonl`.\n\n- Machine-readable verification artifact at `artifacts/section_10_15/bd-15j6/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_15/bd-15j6/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.15] Make canonical evidence-ledger emission (from `10.14`) mandatory for policy-influenced control decisions.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.15] Make canonical evidence-ledger emission (from `10.14`) mandatory for policy-influenced control decisions.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.15] Make canonical evidence-ledger emission (from `10.14`) mandatory for policy-influenced control decisions.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.15] Make canonical evidence-ledger emission (from `10.14`) mandatory for policy-influenced control decisions.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.15] Make canonical evidence-ledger emission (from `10.14`) mandatory for policy-influenced control decisions.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"- Missing evidence entry for policy-influenced decision is a conformance failure; control-plane entries align with canonical schema and ordering.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:37:00.547863408Z","created_by":"ubuntu","updated_at":"2026-02-20T17:00:41.833445396Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-15","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-15j6","depends_on_id":"bd-2e73","type":"blocks","created_at":"2026-02-20T14:59:36.873265306Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-15j6","depends_on_id":"bd-nupr","type":"blocks","created_at":"2026-02-20T14:59:36.696941218Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-15j6","depends_on_id":"bd-oolt","type":"blocks","created_at":"2026-02-20T14:59:37.043876343Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-15t","title":"[10.9] Build category-shift reporting pipeline with reproducible artifacts.","description":"## [10.9] Build category-shift reporting pipeline with reproducible artifacts\n\n### Why This Exists\n\nSections 14 and 16 require that franken_node's claims of shifting the competitive landscape for secure extension runtimes be backed by reproducible evidence, not marketing assertions. A category-shift report must demonstrate, with verifiable data, that franken_node changes what is possible in the runtime security space — making previously impractical security guarantees practical, making previously expensive protections affordable, and making previously manual processes automatic. This pipeline generates those reports automatically from real measurements, with every claim backed by a reproducible artifact.\n\n### What It Must Do\n\nBuild an automated pipeline that generates category-shift evidence reports with full reproducibility:\n\n- **Report generation pipeline**: An automated pipeline that aggregates data from multiple sources (benchmarks, adversarial campaigns, migration demos, trust economics, verifier attestations) and produces a structured category-shift report. The pipeline runs on demand and on a configurable schedule (default: monthly).\n- **Report dimensions**:\n  1. **Benchmark comparisons**: How franken_node compares to Node.js and Bun across all benchmark dimensions (from bd-f5d). Focus on categories where franken_node enables capabilities that don't exist in alternatives (e.g., deterministic replay, trust verification, containment).\n  2. **Security posture improvements**: Quantified security improvements from adversarial campaign results (bd-9is): attack categories that are effectively neutralized, defense coverage gaps that are closed, attacker-cost amplification factors.\n  3. **Migration velocity metrics**: Migration success rates, time-to-migrate, and complexity reduction from migration demo results (bd-1e0). Evidence that migration is practical for real-world projects.\n  4. **Adoption trends**: Metrics on verifier registrations, attestation volume, and community engagement (from bd-m8p). Evidence of ecosystem traction.\n  5. **Economic impact**: Trust economics data (bd-10c) showing cost-benefit analysis and attacker-ROI deltas.\n- **Reproducibility requirements**: Every claim in the report must reference a specific artifact (benchmark result, campaign result, migration report, attestation) with an integrity hash. A \"reproduce this claim\" script is generated for each claim, allowing independent verification. The report itself includes a manifest listing all referenced artifacts and their hashes.\n- **Claim verification**: Before including a claim in the report, the pipeline verifies: (a) the underlying artifact exists and passes integrity check, (b) the claim accurately represents the artifact data (no cherry-picking or misrepresentation), (c) the artifact was produced within the configured freshness window (default: 30 days).\n- **Output formats**: Reports are generated in multiple formats: structured JSON (for programmatic consumption), Markdown (for publication), and a summary dashboard view. All formats contain the same data with consistent claim identifiers.\n- **Historical trending**: The pipeline maintains a history of reports and can show trends across report periods: improving/declining metrics, new capabilities demonstrated, and coverage gaps closed.\n\n### Acceptance Criteria\n\n1. Pipeline aggregates data from at least four source systems (benchmarks, adversarial campaigns, migration demos, trust economics or verifier portal).\n2. All five report dimensions are populated with real measurement data (not placeholders).\n3. Every claim in the report references a specific artifact with integrity hash; a verification script confirms all references are valid.\n4. \"Reproduce this claim\" scripts are generated for each claim and successfully reproduce the referenced artifact data.\n5. Claim verification rejects stale artifacts (older than freshness window) and inaccurate representations.\n6. Reports are generated in at least two formats (JSON and Markdown) with consistent claim identifiers across formats.\n7. Historical trending shows metric changes across at least two report periods.\n8. The pipeline is idempotent: running it twice with the same input data produces identical reports.\n\n### Key Dependencies\n\n- Benchmark infrastructure (bd-f5d) for benchmark comparison data\n- Adversarial campaign runner (bd-9is) for security posture data\n- Migration demo pipeline (bd-1e0) for migration velocity data\n- Verifier portal (bd-m8p) for adoption trend data\n- Trust economics dashboard (bd-10c) for economic impact data\n\n### Testing & Logging Requirements\n\n- Unit tests for data aggregation, claim verification, and report generation.\n- Integration test with synthetic data from all source systems verifying end-to-end pipeline.\n- Verification script (`scripts/check_category_shift_reports.py`) with `--json` and `self_test()`.\n- Pipeline execution logged at INFO with per-dimension timing; claim verification failures logged at WARN; data source unavailability logged at ERROR.\n\n### Expected Artifacts\n\n- `docs/specs/section_10_9/bd-15t_contract.md` — category-shift reporting specification\n- `scripts/check_category_shift_reports.py` — verification script\n- `tests/test_check_category_shift_reports.py` — unit tests\n- `fixtures/category-shift/` — sample report data and expected output\n- `artifacts/section_10_9/bd-15t/verification_evidence.json`\n- `artifacts/section_10_9/bd-15t/verification_summary.md`","acceptance_criteria":"1. Reporting pipeline aggregates data from all moonshot beads (benchmark campaigns, adversarial runner, migration demos, verifier portal, trust economics) into a unified category-shift report.\n2. Report includes reproducible artifacts: every claim is backed by a specific evidence file under artifacts/ with a hash and a command to regenerate it.\n3. Per Section 3 category-defining targets: report explicitly scores franken_node against the three category thresholds (>= 95% compat, >= 3x migration velocity, >= 10x compromise reduction) with supporting data.\n4. Pipeline produces both human-readable (Markdown/HTML) and machine-readable (JSON) report formats.\n5. Report is versioned and diffable: each release produces a new report, and a diff tool highlights changes from the previous release.\n6. Per Section 9F moonshot bets: report includes a 'bet status' section for each moonshot initiative showing progress, blockers, and projected timeline.\n7. Pipeline is automated: scripts/generate_category_report.sh produces the full report from current artifacts with no manual data entry.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:36:48.681544863Z","created_by":"ubuntu","updated_at":"2026-02-20T16:59:04.766393164Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-9","self-contained","test-obligations"]}
{"id":"bd-15u3","title":"[10.14] Enforce guardrail precedence: anytime-valid bounds override Bayesian recommendations.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.14 — FrankenSQLite Deep-Mined Expansion Execution Track (9J)\n\nWhy This Exists:\nFrankenSQLite deep-mined control integrity track: evidence ledgers, proofs, epochs, remote effects, markers/MMR, and deterministic fault labs.\n\nTask Objective:\nEnforce guardrail precedence: anytime-valid bounds override Bayesian recommendations.\n\nAcceptance Criteria:\n- Decision engine always checks guardrail before recommendation apply; blocked recommendations emit explicit reason; precedence covered by conformance tests.\n\nExpected Artifacts:\n- `tests/conformance/guardrail_precedence.rs`, `docs/specs/decision_precedence_rules.md`, `artifacts/10.14/guardrail_override_events.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_14/bd-15u3/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_14/bd-15u3/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.14] Enforce guardrail precedence: anytime-valid bounds override Bayesian recommendations.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.14] Enforce guardrail precedence: anytime-valid bounds override Bayesian recommendations.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.14] Enforce guardrail precedence: anytime-valid bounds override Bayesian recommendations.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.14] Enforce guardrail precedence: anytime-valid bounds override Bayesian recommendations.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.14] Enforce guardrail precedence: anytime-valid bounds override Bayesian recommendations.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"Decision engine always checks guardrail before recommendation apply; blocked recommendations emit explicit reason; precedence covered by conformance tests.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:36:55.873233584Z","created_by":"ubuntu","updated_at":"2026-02-20T16:23:28.452801574Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-14","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-15u3","depends_on_id":"bd-2igi","type":"blocks","created_at":"2026-02-20T07:43:14.596796369Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-15u3","depends_on_id":"bd-3a3q","type":"blocks","created_at":"2026-02-20T16:23:28.452752603Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-16fq","title":"[10.18] Define VEF policy-constraint language and compiler contract for high-risk action classes.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.18 — Verifiable Execution Fabric Execution Track (9L)\n\nWhy This Exists:\nVEF track for proof-carrying runtime compliance: receipt chains, commitment checkpoints, proof generation/verification, and claim gating.\n\nTask Objective:\nDefine VEF policy-constraint language and compiler contract for high-risk action classes.\n\nAcceptance Criteria:\n- Constraint language maps runtime policy to proof-checkable predicates for required action classes; compiler outputs are deterministic and versioned.\n\nExpected Artifacts:\n- `docs/specs/vef_policy_constraint_language.md`, `spec/vef_policy_constraints_v1.json`, `artifacts/10.18/vef_constraint_compiler_report.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_18/bd-16fq/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_18/bd-16fq/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.18] Define VEF policy-constraint language and compiler contract for high-risk action classes.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.18] Define VEF policy-constraint language and compiler contract for high-risk action classes.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.18] Define VEF policy-constraint language and compiler contract for high-risk action classes.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.18] Define VEF policy-constraint language and compiler contract for high-risk action classes.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.18] Define VEF policy-constraint language and compiler contract for high-risk action classes.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"- Constraint language maps runtime policy to proof-checkable predicates for required action classes; compiler outputs are deterministic and versioned.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:37:04.172496302Z","created_by":"ubuntu","updated_at":"2026-02-20T15:45:57.604181565Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-18","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-16fq","depends_on_id":"bd-1wz","type":"blocks","created_at":"2026-02-20T07:46:34.267565808Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-16fq","depends_on_id":"bd-3qo","type":"blocks","created_at":"2026-02-20T07:46:34.315268576Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-16fq","depends_on_id":"bd-5rh","type":"blocks","created_at":"2026-02-20T07:46:34.359792293Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-16fq","depends_on_id":"bd-cda","type":"blocks","created_at":"2026-02-20T07:46:34.404808737Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-16sk","title":"[10.1] Section-wide verification gate: comprehensive unit+e2e+logging","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.1\n\nTask Objective:\nCreate a hard section completion gate that verifies all implemented behavior in this section with comprehensive unit tests, integration/e2e scripts, and detailed structured logging evidence.\n\nAcceptance Criteria:\n- Section test matrix covers happy-path, edge-case, and adversarial/error-path scenarios for all section deliverables.\n- E2E scripts execute representative end-user workflows and policy/control-plane transitions for this section.\n- Structured logs include stable event/error codes, trace correlation IDs, and enough context for deterministic replay triage.\n- Verification report is deterministic and machine-readable for CI/release gating.\n\nExpected Artifacts:\n- Section-level test matrix and coverage summary artifact.\n- E2E script suite with pass/fail outputs and reproducible fixtures/seeds.\n- Structured log validation report and traceability bundle.\n- Gate verdict artifact consumable by release automation.\n\n- Machine-readable verification artifact at `artifacts/section_10_1/bd-16sk/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_1/bd-16sk/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests must validate contracts, invariants, and failure semantics.\n- E2E tests must validate cross-component behavior from user/operator entrypoints to persisted effects.\n- Logging must support root-cause analysis without hidden context requirements.\n\nTask-Specific Clarification:\n- For \"[10.1] Section-wide verification gate: comprehensive unit+e2e+logging\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.1] Section-wide verification gate: comprehensive unit+e2e+logging\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.1] Section-wide verification gate: comprehensive unit+e2e+logging\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.1] Section-wide verification gate: comprehensive unit+e2e+logging\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.1] Section-wide verification gate: comprehensive unit+e2e+logging\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","status":"closed","priority":1,"issue_type":"task","assignee":"CrimsonCrane","created_at":"2026-02-20T07:48:06.068063125Z","created_by":"ubuntu","updated_at":"2026-02-20T09:28:00.706957898Z","closed_at":"2026-02-20T09:28:00.706931408Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-1","test-obligations","verification-gate"],"dependencies":[{"issue_id":"bd-16sk","depends_on_id":"bd-1dpd","type":"blocks","created_at":"2026-02-20T08:24:27.234048787Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-16sk","depends_on_id":"bd-1j2","type":"blocks","created_at":"2026-02-20T07:48:06.406663715Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-16sk","depends_on_id":"bd-1mj","type":"blocks","created_at":"2026-02-20T07:48:06.267590054Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-16sk","depends_on_id":"bd-1pc","type":"blocks","created_at":"2026-02-20T07:48:06.172035118Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-16sk","depends_on_id":"bd-20l","type":"blocks","created_at":"2026-02-20T07:48:06.217196934Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-16sk","depends_on_id":"bd-2twu","type":"blocks","created_at":"2026-02-20T08:20:53.255202556Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-16sk","depends_on_id":"bd-2zz","type":"blocks","created_at":"2026-02-20T07:48:06.360111910Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-16sk","depends_on_id":"bd-4yv","type":"blocks","created_at":"2026-02-20T07:48:06.314127252Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-16sk","depends_on_id":"bd-vjq","type":"blocks","created_at":"2026-02-20T07:48:06.453547368Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-1719","title":"[10.16] Add deterministic visual/snapshot and interaction tests for `frankentui`-backed surfaces.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.16 — Adjacent Substrate Integration Execution Track (8.7)\n\nWhy This Exists:\nAdjacent substrate integration track to enforce deep composition with frankentui, frankensqlite, sqlmodel_rust, and fastapi_rust.\n\nTask Objective:\nAdd deterministic visual/snapshot and interaction tests for `frankentui`-backed surfaces.\n\nAcceptance Criteria:\n- Snapshot suite runs in CI and catches visual regressions; keyboard-interaction paths are replayable and stable.\n\nExpected Artifacts:\n- `tests/tui/frankentui_snapshots.rs`, `artifacts/10.16/frankentui_snapshot_report.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_16/bd-1719/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_16/bd-1719/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.16] Add deterministic visual/snapshot and interaction tests for `frankentui`-backed surfaces.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.16] Add deterministic visual/snapshot and interaction tests for `frankentui`-backed surfaces.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.16] Add deterministic visual/snapshot and interaction tests for `frankentui`-backed surfaces.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.16] Add deterministic visual/snapshot and interaction tests for `frankentui`-backed surfaces.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.16] Add deterministic visual/snapshot and interaction tests for `frankentui`-backed surfaces.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"- Snapshot suite runs in CI and catches visual regressions; keyboard-interaction paths are replayable and stable.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:37:01.854475439Z","created_by":"ubuntu","updated_at":"2026-02-20T17:05:14.265984109Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-16","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-1719","depends_on_id":"bd-1xtf","type":"blocks","created_at":"2026-02-20T17:05:14.265921492Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-174","title":"[10.10] Implement policy checkpoint chain for product release channels.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md — Section 10.10 (FCP-Inspired Hardening), cross-ref Section 9E.3\n\n## Why This Exists\n\nEnhancement Map 9E.3 requires a checkpointed policy frontier for release channels and rollback resistance. Without an immutable, hash-chained sequence of policy checkpoints, product release channels (stable, beta, canary) lack a verifiable audit trail — operators cannot prove which policy was active at any point in time, and rollback attacks can silently revert security-critical policy changes. This bead implements the product-level policy checkpoint chain that anchors every release channel transition to a signed, sequenced, tamper-evident record. Combined with the rollback/fork detection in bd-2ms, this forms the foundation of the epoch-scoped trust model where no policy state can be rewound without detection.\n\n## What This Must Do\n\n1. Define a `PolicyCheckpoint` struct containing: monotonic sequence number, epoch ID, channel identifier (stable/beta/canary/custom), policy hash (over canonically-serialized policy document), parent checkpoint hash, timestamp, and signer identity.\n2. Implement a `PolicyCheckpointChain` that enforces append-only semantics: each new checkpoint must reference the previous checkpoint's hash, sequence numbers must be strictly monotonic, and epoch boundaries must be explicitly marked.\n3. Provide `create_checkpoint()`, `verify_chain()`, and `latest_for_channel()` APIs that product code uses to gate release operations — no release artifact can be published without a valid checkpoint covering the active policy.\n4. Serialize all checkpoints using the canonical serializer from bd-jjm, ensuring signature preimage stability and cross-kernel verifiability.\n5. Implement checkpoint persistence to a local append-only store with crash-recovery guarantees (write-ahead or fsync discipline).\n6. Expose a `policy_frontier()` query that returns the latest verified checkpoint per channel, used by downstream beads (bd-2ms) for divergence detection.\n\n## Context from Enhancement Maps\n\n- 9E.3: \"Checkpointed policy frontier for release channels and rollback resistance\"\n- 9E.2 (cross-ref): Policy checkpoints depend on deterministic serialization for signature stability.\n- 9B.2 (Migration): Release channel gating ensures migration operations respect the active policy epoch.\n- 9A.5 (Observability): Checkpoint creation/verification events feed into the structured audit log.\n\n## Dependencies\n\n- Upstream: bd-jjm ([10.10] Enforce product-level adoption of canonical deterministic serialization and signature preimage rules) — provides the canonical serializer used for checkpoint serialization and signing.\n- Downstream: bd-2ms ([10.10] Implement rollback/fork detection in control-plane state propagation) — consumes the policy frontier to detect divergence and rollback.\n- Downstream: bd-1jjq ([10.10] Section-wide verification gate) — aggregates verification evidence.\n\n## Acceptance Criteria\n\n1. `PolicyCheckpoint` struct includes all required fields (sequence, epoch, channel, policy_hash, parent_hash, timestamp, signer) with documented invariants for each.\n2. Appending a checkpoint with a non-monotonic sequence number is rejected with error code `CHECKPOINT_SEQ_VIOLATION`.\n3. Appending a checkpoint whose parent_hash does not match the current chain head is rejected with `CHECKPOINT_PARENT_MISMATCH`.\n4. `verify_chain()` detects any gap, reorder, or hash-chain break in O(n) time and returns the first violation index.\n5. `latest_for_channel()` returns the correct checkpoint for each of at least 3 channels (stable, beta, canary) in a multi-channel scenario.\n6. Checkpoint persistence survives process crash (verified by kill-during-write test) with zero data corruption.\n7. All checkpoint serialization routes through bd-jjm's canonical serializer — no ad-hoc encoding paths.\n8. Verification evidence JSON includes chain length, channels covered, and sample checkpoint hashes.\n\n## Testing & Logging Requirements\n\n- Unit tests: Create chains of 100+ checkpoints and verify chain integrity. Test rejection of out-of-order sequence numbers, duplicate sequence numbers, wrong parent hashes, and epoch boundary violations. Test empty chain edge case. Test multi-channel interleaving.\n- Integration tests: Persist a chain, kill the process mid-write, restart, and verify chain integrity and recovery. Verify that checkpoint signatures produced by franken_node can be verified by an independent verifier using golden vectors.\n- Adversarial tests: Attempt to insert a checkpoint that skips a sequence number. Attempt to replace a checkpoint in the middle of the chain (hash-chain should detect). Attempt to create a checkpoint with a forged parent_hash. Feed a chain with a single bit-flip in one checkpoint and verify detection.\n- Structured logs: `CHECKPOINT_CREATED` (sequence, epoch, channel, policy_hash_prefix, signer). `CHECKPOINT_VERIFIED` (chain_length, channels, verification_duration_ms). `CHECKPOINT_REJECTED` (reason, attempted_sequence, expected_sequence). All events include `trace_id` and `epoch_id`.\n\n## Expected Artifacts\n\n- docs/specs/section_10_10/bd-174_contract.md\n- crates/franken-node/src/connector/policy_checkpoint.rs (or similar module path)\n- scripts/check_policy_checkpoint.py with --json flag and self_test()\n- tests/test_check_policy_checkpoint.py\n- artifacts/section_10_10/bd-174/verification_evidence.json\n- artifacts/section_10_10/bd-174/verification_summary.md","acceptance_criteria":"1. Define a PolicyCheckpoint struct containing: (a) checkpoint_id (TrustObjectId with POLICY domain), (b) parent_checkpoint_id (Option, None only for genesis), (c) channel enum (CANARY, BETA, STABLE, LTS), (d) policy_hash (SHA-256 of the canonical-serialized policy document), (e) sequence_number (u64, strictly monotonic per channel), (f) timestamp (UTC, second precision), (g) signer_key_id (TrustObjectId with KEY domain).\n2. Implement a PolicyCheckpointChain that maintains an append-only ordered list of checkpoints per channel. Enforce: (a) each new checkpoint's parent_checkpoint_id equals the previous checkpoint's id, (b) sequence_number == previous + 1, (c) timestamp >= previous timestamp.\n3. Reject any checkpoint whose parent_checkpoint_id does not match the chain tip (fork prevention). Return a typed error PolicyForkDetected with both the expected and received parent IDs.\n4. Implement chain validation: given a full chain, verify the parent linkage, monotonic sequence, and monotonic timestamps from genesis to tip. Return first violation index on failure.\n5. Implement channel promotion: a checkpoint may reference a checkpoint from a less-stable channel (CANARY -> BETA -> STABLE -> LTS) via a promotion_source_id field. Validate that the source checkpoint exists and belongs to a strictly less-stable channel.\n6. Provide a genesis checkpoint constructor that enforces parent=None, sequence=0.\n7. Unit tests: (a) valid chain append, (b) fork rejection, (c) sequence gap rejection, (d) timestamp regression rejection, (e) cross-channel promotion valid/invalid, (f) genesis invariants.\n8. Golden fixture: a 5-checkpoint chain across CANARY->BETA promotion in vectors/policy_checkpoint_chain.json.\n9. Verification script scripts/check_policy_checkpoint.py with --json, artifacts at artifacts/section_10_10/bd-174/.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:36:48.919772635Z","created_by":"ubuntu","updated_at":"2026-02-20T17:01:30.960636704Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-10","self-contained","test-obligations"]}
{"id":"bd-17mb","title":"[10.13] Implement fail-closed manifest negotiation (SemVer-aware version checks, required-feature resolution, transport cap checks).","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.13 — FCP Deep-Mined Expansion Execution Track (9I)\n\nWhy This Exists:\nDeep connector/provider contract track: lifecycle FSMs, conformance harnesses, fencing, sandboxing, revocation freshness, and protocol hardening.\n\nTask Objective:\nImplement fail-closed manifest negotiation (SemVer-aware version checks, required-feature resolution, transport cap checks).\n\nAcceptance Criteria:\n- Unsupported major versions and missing required features hard-fail activation; version comparisons are semantic, not lexical; negotiation decisions are logged.\n\nExpected Artifacts:\n- `docs/specs/manifest_negotiation.md`, `tests/conformance/manifest_negotiation_fail_closed.rs`, `artifacts/10.13/manifest_negotiation_trace.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_13/bd-17mb/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_13/bd-17mb/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.13] Implement fail-closed manifest negotiation (SemVer-aware version checks, required-feature resolution, transport cap checks).\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.13] Implement fail-closed manifest negotiation (SemVer-aware version checks, required-feature resolution, transport cap checks).\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.13] Implement fail-closed manifest negotiation (SemVer-aware version checks, required-feature resolution, transport cap checks).\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.13] Implement fail-closed manifest negotiation (SemVer-aware version checks, required-feature resolution, transport cap checks).\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.13] Implement fail-closed manifest negotiation (SemVer-aware version checks, required-feature resolution, transport cap checks).\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","status":"closed","priority":1,"issue_type":"task","assignee":"CrimsonCrane","created_at":"2026-02-20T07:36:52.450176581Z","created_by":"ubuntu","updated_at":"2026-02-20T11:32:31.228575187Z","closed_at":"2026-02-20T11:32:31.228546493Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-13","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-17mb","depends_on_id":"bd-1nk5","type":"blocks","created_at":"2026-02-20T07:43:12.801157459Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-181w","title":"[10.15] Integrate canonical epoch-scoped validity windows (from `10.14`) for control artifacts and remote contracts.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.15 — Asupersync-First Integration Execution Track (8.4-8.6)\n\nWhy This Exists:\nAsupersync-first control-plane migration track enforcing Cx-first, region ownership, cancellation correctness, and protocol-grade verification gates.\n\nTask Objective:\nIntegrate canonical epoch-scoped validity windows (from `10.14`) for control artifacts and remote contracts.\n\nAcceptance Criteria:\n- Control-plane operations use canonical epoch-validity semantics; future-epoch artifacts are rejected fail-closed; epoch scope is logged for accepted high-impact operations.\n\nExpected Artifacts:\n- `tests/security/control_epoch_validity.rs`, `docs/integration/control_epoch_validity_adoption.md`, `artifacts/10.15/epoch_validity_decisions.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_15/bd-181w/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_15/bd-181w/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.15] Integrate canonical epoch-scoped validity windows (from `10.14`) for control artifacts and remote contracts.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.15] Integrate canonical epoch-scoped validity windows (from `10.14`) for control artifacts and remote contracts.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.15] Integrate canonical epoch-scoped validity windows (from `10.14`) for control artifacts and remote contracts.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.15] Integrate canonical epoch-scoped validity windows (from `10.14`) for control artifacts and remote contracts.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.15] Integrate canonical epoch-scoped validity windows (from `10.14`) for control artifacts and remote contracts.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"- Control-plane operations use canonical epoch-validity semantics; future-epoch artifacts are rejected fail-closed; epoch scope is logged for accepted high-impact operations.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:37:00.383685717Z","created_by":"ubuntu","updated_at":"2026-02-20T17:00:41.980097005Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-15","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-181w","depends_on_id":"bd-2xv8","type":"blocks","created_at":"2026-02-20T14:59:36.467655154Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-181w","depends_on_id":"bd-3hdv","type":"blocks","created_at":"2026-02-20T14:59:36.299215652Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-18ie","title":"[14] Metric family: compatibility correctness by API/risk band","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 14\n\nTask Objective:\nInstrument compatibility correctness metric family by API family and risk band.\n\nExecution Requirements:\n- Make this requirement machine-verifiable and release-gated where applicable.\n- Keep behavior self-documenting so future contributors do not need to re-open the master plan.\n\nTesting & Logging Requirements:\n- Unit tests for rule/contract logic and error conditions.\n- Integration/E2E tests for workflow-level enforcement.\n- Structured logs with stable codes and trace IDs.\n- Reproducible artifacts that prove contract satisfaction.\n\nAcceptance Criteria:\n- Success and failure invariants for [14] Metric family: compatibility correctness by API/risk band are explicitly quantified and machine-verifiable.\n- Determinism requirements for [14] Metric family: compatibility correctness by API/risk band are validated across repeated runs and adversarial perturbations.\n\n\nExpected Artifacts:\n- Machine-readable verification artifact with explicit canonical path under artifacts/section_14/bd-18ie/verification_evidence.json, suitable for CI/release gating.\n- Human-readable verification summary with explicit canonical path under artifacts/section_14/bd-18ie/verification_summary.md, linking implementation intent to measured validation outcomes.\n\nTask-Specific Clarification:\n- The implementation must deliver the exact capability named in the title, with no scope dilution and no silent feature omission.\n- Validation artifacts must include capability-specific thresholds, pass/fail criteria, and reproducible evidence bundles tied to this bead ID.\n- Program/section verification gates must be able to consume this bead's outputs without manual interpretation.\n\nWhy This Improves User Outcomes:\n- \"[14] Metric family: compatibility correctness by API/risk band\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[14] Metric family: compatibility correctness by API/risk band\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"METRIC FAMILY: Compatibility correctness by API family and risk band.\n1. Metrics measured: pass rate, failure rate, skip rate, flaky rate — each broken down by API family (fs, http, net, crypto, stream, buffer, path, child_process, cluster, events, timers, url, zlib, tls, os, querystring).\n2. Risk band classification: critical (security-sensitive APIs: crypto, tls, child_process), high (I/O APIs: fs, net, http), medium (data APIs: stream, buffer, zlib), low (utility APIs: path, url, os, querystring, events, timers).\n3. Reporting format: matrix of API family x risk band with pass/fail counts and percentages.\n4. Threshold gates: critical-band APIs must have >= 99% pass rate; high-band >= 95%; medium-band >= 90%; low-band >= 85%.\n5. Trend tracking: compatibility metrics are tracked over time with automated regression detection (> 2% drop triggers alert).\n6. Publication: metric results are included in every benchmark report with methodology description.\n7. Evidence: compatibility_by_family.json with per-family, per-band metrics and threshold compliance.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:39:35.642921950Z","created_by":"ubuntu","updated_at":"2026-02-20T15:24:28.383154417Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-14","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-18ie","depends_on_id":"bd-3v8g","type":"blocks","created_at":"2026-02-20T07:43:25.936571189Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-18o","title":"[10.13] Implement canonical connector state root/object model with explicit state model tagging.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.13 — FCP Deep-Mined Expansion Execution Track (9I)\n\nWhy This Exists:\nDeep connector/provider contract track: lifecycle FSMs, conformance harnesses, fencing, sandboxing, revocation freshness, and protocol hardening.\n\nTask Objective:\nImplement canonical connector state root/object model with explicit state model tagging.\n\nAcceptance Criteria:\n- All connectors declare state model type; canonical root/head objects are persisted; local cache divergence is detectable and repairable.\n\nExpected Artifacts:\n- `docs/specs/connector_state_model.md`, `tests/integration/connector_state_persistence.rs`, `artifacts/10.13/state_model_samples.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_13/bd-18o/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_13/bd-18o/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.13] Implement canonical connector state root/object model with explicit state model tagging.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.13] Implement canonical connector state root/object model with explicit state model tagging.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.13] Implement canonical connector state root/object model with explicit state model tagging.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.13] Implement canonical connector state root/object model with explicit state model tagging.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.13] Implement canonical connector state root/object model with explicit state model tagging.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","status":"closed","priority":1,"issue_type":"task","assignee":"CrimsonCrane","created_at":"2026-02-20T07:36:51.717873925Z","created_by":"ubuntu","updated_at":"2026-02-20T10:48:39.930887680Z","closed_at":"2026-02-20T10:48:39.930861190Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-13","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-18o","depends_on_id":"bd-3en","type":"blocks","created_at":"2026-02-20T07:43:12.398935263Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-18ud","title":"[10.14] Implement `durability=local` and `durability=quorum(M)` semantics for control/trust artifacts.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.14 — FrankenSQLite Deep-Mined Expansion Execution Track (9J)\n\nWhy This Exists:\nFrankenSQLite deep-mined control integrity track: evidence ledgers, proofs, epochs, remote effects, markers/MMR, and deterministic fault labs.\n\nTask Objective:\nImplement `durability=local` and `durability=quorum(M)` semantics for control/trust artifacts.\n\nAcceptance Criteria:\n- Mode semantics are enforced end-to-end; mode switches are auditable and policy-gated; claim language mapping is deterministic.\n\nExpected Artifacts:\n- `docs/specs/durability_modes.md`, `tests/conformance/durability_mode_semantics.rs`, `artifacts/10.14/durability_mode_claim_matrix.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_14/bd-18ud/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_14/bd-18ud/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.14] Implement `durability=local` and `durability=quorum(M)` semantics for control/trust artifacts.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.14] Implement `durability=local` and `durability=quorum(M)` semantics for control/trust artifacts.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.14] Implement `durability=local` and `durability=quorum(M)` semantics for control/trust artifacts.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.14] Implement `durability=local` and `durability=quorum(M)` semantics for control/trust artifacts.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.14] Implement `durability=local` and `durability=quorum(M)` semantics for control/trust artifacts.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"Mode semantics are enforced end-to-end; mode switches are auditable and policy-gated; claim language mapping is deterministic.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:36:57.406100907Z","created_by":"ubuntu","updated_at":"2026-02-20T15:44:06.885747541Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-14","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-18ud","depends_on_id":"bd-okqy","type":"blocks","created_at":"2026-02-20T07:43:15.358560932Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-1961","title":"[15] Pillar: reputation graph APIs","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 15\n\nTask Objective:\nImplement reputation graph API pillar for ecosystem trust and incident response.\n\nExecution Requirements:\n- Make this requirement machine-verifiable and release-gated where applicable.\n- Keep behavior self-documenting so future contributors do not need to re-open the master plan.\n\nTesting & Logging Requirements:\n- Unit tests for rule/contract logic and error conditions.\n- Integration/E2E tests for workflow-level enforcement.\n- Structured logs with stable codes and trace IDs.\n- Reproducible artifacts that prove contract satisfaction.\n\nAcceptance Criteria:\n- Success and failure invariants for [15] Pillar: reputation graph APIs are explicitly quantified and machine-verifiable.\n- Determinism requirements for [15] Pillar: reputation graph APIs are validated across repeated runs and adversarial perturbations.\n\n\nExpected Artifacts:\n- Machine-readable verification artifact with explicit canonical path under artifacts/section_15/bd-1961/verification_evidence.json, suitable for CI/release gating.\n- Human-readable verification summary with explicit canonical path under artifacts/section_15/bd-1961/verification_summary.md, linking implementation intent to measured validation outcomes.\n\nTask-Specific Clarification:\n- The implementation must deliver the exact capability named in the title, with no scope dilution and no silent feature omission.\n- Validation artifacts must include capability-specific thresholds, pass/fail criteria, and reproducible evidence bundles tied to this bead ID.\n- Program/section verification gates must be able to consume this bead's outputs without manual interpretation.\n\nWhy This Improves User Outcomes:\n- \"[15] Pillar: reputation graph APIs\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[15] Pillar: reputation graph APIs\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"1. Reputation graph API is published with OpenAPI/Swagger specification.\n2. API endpoints include: (a) query node reputation score, (b) query extension trust level, (c) query trust graph neighborhood (1-hop and 2-hop), (d) submit trust signal (authenticated), (e) query reputation history for a node/extension.\n3. API enforces rate limiting: <= 100 requests/minute per API key for free tier.\n4. API responses include: reputation score (0-100), confidence level (low/medium/high), contributing signal count, last-updated timestamp.\n5. Privacy: API never exposes individual trust signals, only aggregates. Minimum cohort size >= 5 for any aggregation.\n6. API authentication via API key with role-based access (read-only, contributor, admin).\n7. API has >= 95% uptime SLA (measured over 30-day window) and p99 latency <= 200ms.\n8. SDK clients available for >= 2 languages (JavaScript, Python).\n9. Evidence: reputation_api_spec.json (OpenAPI) and api_health_report.json with uptime and latency metrics.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:39:36.442226068Z","created_by":"ubuntu","updated_at":"2026-02-20T15:26:40.061464657Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-15","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-1961","depends_on_id":"bd-3mj9","type":"blocks","created_at":"2026-02-20T07:43:26.342166549Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-19j5","title":"Epic: Operational Readiness [10.8]","description":"- type: epic","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-20T07:47:58.072163029Z","updated_at":"2026-02-20T07:49:21.129961907Z","closed_at":"2026-02-20T07:49:21.129941899Z","close_reason":"Superseded by canonical detailed master-plan graph (bd-33v) with full 10.N-10.21 and 11-16 coverage, explicit dependencies, and per-section verification gates.","source_repo":".","compaction_level":0,"original_size":0}
{"id":"bd-19k2","title":"[10.20] Implement expected-loss cascade economics and ROI-aware mitigation ranking.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.20 — Dependency Graph Immune System Execution Track (9N)\n\nWhy This Exists:\nDGIS topological immune system track for dependency contagion modeling, choke-point immunization, and graph-aware containment economics.\n\nTask Objective:\nImplement expected-loss cascade economics and ROI-aware mitigation ranking.\n\nAcceptance Criteria:\n- Each candidate mitigation includes expected-loss delta, residual risk, and operational cost estimates; rankings are stable under fixed assumptions and sensitivity-tested.\n\nExpected Artifacts:\n- `src/security/dgis/cascade_economics.rs`, `docs/specs/dgis_expected_loss_model.md`, `artifacts/10.20/dgis_economic_rankings.csv`.\n\n- Machine-readable verification artifact at `artifacts/section_10_20/bd-19k2/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_20/bd-19k2/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.20] Implement expected-loss cascade economics and ROI-aware mitigation ranking.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.20] Implement expected-loss cascade economics and ROI-aware mitigation ranking.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.20] Implement expected-loss cascade economics and ROI-aware mitigation ranking.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.20] Implement expected-loss cascade economics and ROI-aware mitigation ranking.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.20] Implement expected-loss cascade economics and ROI-aware mitigation ranking.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"- Each candidate mitigation includes expected-loss delta, residual risk, and operational cost estimates; rankings are stable under fixed assumptions and sensitivity-tested.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:37:07.247484594Z","created_by":"ubuntu","updated_at":"2026-02-20T17:05:10.692798219Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-20","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-19k2","depends_on_id":"bd-t89w","type":"blocks","created_at":"2026-02-20T17:05:10.692747354Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-19u","title":"[10.13] Add CRDT state mode scaffolding (lww-map/or-set/gcounter/pncounter) with merge conformance fixtures.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.13 — FCP Deep-Mined Expansion Execution Track (9I)\n\nWhy This Exists:\nDeep connector/provider contract track: lifecycle FSMs, conformance harnesses, fencing, sandboxing, revocation freshness, and protocol hardening.\n\nTask Objective:\nAdd CRDT state mode scaffolding (lww-map/or-set/gcounter/pncounter) with merge conformance fixtures.\n\nAcceptance Criteria:\n- Each CRDT type has merge laws covered by fixtures; merge output is deterministic across replicas; schema tags prevent type confusion.\n\nExpected Artifacts:\n- `tests/conformance/crdt_merge_fixtures.rs`, `fixtures/crdt/*.json`, `docs/specs/crdt_state_mode.md`.\n\n- Machine-readable verification artifact at `artifacts/section_10_13/bd-19u/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_13/bd-19u/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.13] Add CRDT state mode scaffolding (lww-map/or-set/gcounter/pncounter) with merge conformance fixtures.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.13] Add CRDT state mode scaffolding (lww-map/or-set/gcounter/pncounter) with merge conformance fixtures.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.13] Add CRDT state mode scaffolding (lww-map/or-set/gcounter/pncounter) with merge conformance fixtures.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.13] Add CRDT state mode scaffolding (lww-map/or-set/gcounter/pncounter) with merge conformance fixtures.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.13] Add CRDT state mode scaffolding (lww-map/or-set/gcounter/pncounter) with merge conformance fixtures.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","status":"closed","priority":1,"issue_type":"task","assignee":"CrimsonCrane","created_at":"2026-02-20T07:36:51.880209184Z","created_by":"ubuntu","updated_at":"2026-02-20T10:59:51.381242793Z","closed_at":"2026-02-20T10:59:51.381215272Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-13","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-19u","depends_on_id":"bd-1cm","type":"blocks","created_at":"2026-02-20T07:43:12.482525717Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-1a1j","title":"[10.16] Define `frankensqlite` persistence integration contract for control/audit/replay state.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.16 — Adjacent Substrate Integration Execution Track (8.7)\n\nWhy This Exists:\nAdjacent substrate integration track to enforce deep composition with frankentui, frankensqlite, sqlmodel_rust, and fastapi_rust.\n\nTask Objective:\nDefine `frankensqlite` persistence integration contract for control/audit/replay state.\n\nAcceptance Criteria:\n- Contract enumerates required persistence classes and durability modes; storage semantics map to product safety tiers.\n\nExpected Artifacts:\n- `docs/specs/frankensqlite_persistence_contract.md`, `artifacts/10.16/frankensqlite_persistence_matrix.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_16/bd-1a1j/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_16/bd-1a1j/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.16] Define `frankensqlite` persistence integration contract for control/audit/replay state.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.16] Define `frankensqlite` persistence integration contract for control/audit/replay state.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.16] Define `frankensqlite` persistence integration contract for control/audit/replay state.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.16] Define `frankensqlite` persistence integration contract for control/audit/replay state.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.16] Define `frankensqlite` persistence integration contract for control/audit/replay state.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"- Contract enumerates required persistence classes and durability modes; storage semantics map to product safety tiers.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:37:01.935868538Z","created_by":"ubuntu","updated_at":"2026-02-20T17:00:53.615701919Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-16","self-contained","test-obligations"]}
{"id":"bd-1a4a","title":"Epic: Correctness + Policy Boundaries [10.14b]","description":"- type: epic","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-20T07:47:58.072163029Z","updated_at":"2026-02-20T07:49:21.204056603Z","closed_at":"2026-02-20T07:49:21.204038810Z","close_reason":"Superseded by canonical detailed master-plan graph (bd-33v) with full 10.N-10.21 and 11-16 coverage, explicit dependencies, and per-section verification gates.","source_repo":".","compaction_level":0,"original_size":0}
{"id":"bd-1ah","title":"[10.4] Define provenance attestation requirements and verification chain.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.4 — Extension Ecosystem + Registry\n\nWhy This Exists:\nExtension ecosystem and registry trust foundation: signed artifacts, provenance, trust cards, revocation, and quarantine flows.\n\nTask Objective:\nDefine provenance attestation requirements and verification chain.\n\nAcceptance Criteria:\n(See structured acceptance_criteria field for domain-specific criteria.)\n\nExpected Artifacts:\n- Section-owned design/spec artifact at `docs/specs/section_10_4/bd-1ah_contract.md`, capturing decision rationale, invariants, and interface boundaries.\n- Machine-readable verification artifact at `artifacts/section_10_4/bd-1ah/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_4/bd-1ah/verification_summary.md` linking implementation intent to measured outcomes.\n\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.4] Define provenance attestation requirements and verification chain.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.4] Define provenance attestation requirements and verification chain.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.4] Define provenance attestation requirements and verification chain.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.4] Define provenance attestation requirements and verification chain.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.4] Define provenance attestation requirements and verification chain.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"1. Define attestation document format using in-toto or SLSA v1.0 provenance predicate schema, specifying required fields: builder_id, build_type, source_uri, source_digest (SHA-256 minimum), build_config, materials[], and invocation parameters. 2. Verification chain requires at minimum two attestation layers: (a) source-to-build attestation linking git commit to build artifact, and (b) build-to-publish attestation linking build artifact to registry entry. 3. Each attestation is signed using the publisher's registered key from the key-transparency log; signatures use domain-separated signing (context string: 'franken-ext-provenance-v1'). 4. Verification logic validates the full chain from source commit through to published artifact, rejecting any chain with gaps, expired signatures, or revoked keys. 5. Reproducibility markers are required for certification levels >= 'verified': build must produce identical artifact hash from same source+config inputs. 6. Attestation documents reference the manifest schema from bd-1gx via provenance_attestation_refs[] field. 7. Key rotation is supported: old attestations remain valid if the signing key was valid at attestation time (verified against key-transparency log timestamps). 8. Verification errors produce structured rejection codes: CHAIN_GAP, SIGNATURE_EXPIRED, KEY_REVOKED, DIGEST_MISMATCH, MISSING_ATTESTATION_LAYER, REPRODUCIBILITY_FAILED. 9. CLI command 'franken-node ext verify-provenance <artifact>' performs full chain verification and outputs pass/fail with detailed chain trace. 10. Attestation storage format supports both embedded (in manifest) and detached (separate .attestation.json sidecar) modes.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:36:45.507060902Z","created_by":"ubuntu","updated_at":"2026-02-20T16:59:00.723702052Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-4","self-contained","test-obligations"]}
{"id":"bd-1ayu","title":"[10.14] Implement overhead/rate clamp policy for hardening escalations with configured ceilings.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.14 — FrankenSQLite Deep-Mined Expansion Execution Track (9J)\n\nWhy This Exists:\nFrankenSQLite deep-mined control integrity track: evidence ledgers, proofs, epochs, remote effects, markers/MMR, and deterministic fault labs.\n\nTask Objective:\nImplement overhead/rate clamp policy for hardening escalations with configured ceilings.\n\nAcceptance Criteria:\n- Escalation clamps respect min/max bounds and policy budget; clamp calculations are deterministic; clamp hits are visible in telemetry.\n\nExpected Artifacts:\n- `src/policy/hardening_clamps.rs`, `tests/conformance/hardening_clamp_bounds.rs`, `artifacts/10.14/hardening_clamp_metrics.csv`.\n\n- Machine-readable verification artifact at `artifacts/section_10_14/bd-1ayu/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_14/bd-1ayu/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.14] Implement overhead/rate clamp policy for hardening escalations with configured ceilings.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.14] Implement overhead/rate clamp policy for hardening escalations with configured ceilings.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.14] Implement overhead/rate clamp policy for hardening escalations with configured ceilings.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.14] Implement overhead/rate clamp policy for hardening escalations with configured ceilings.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.14] Implement overhead/rate clamp policy for hardening escalations with configured ceilings.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"Escalation clamps respect min/max bounds and policy budget; clamp calculations are deterministic; clamp hits are visible in telemetry.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:36:56.219100534Z","created_by":"ubuntu","updated_at":"2026-02-20T16:23:52.976005681Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-14","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-1ayu","depends_on_id":"bd-3rya","type":"blocks","created_at":"2026-02-20T16:23:52.975925001Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-1b0g","title":"Epic: Remote Effects + Distributed Control [10.14f]","description":"- type: epic","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-20T07:47:58.072163029Z","updated_at":"2026-02-20T07:49:21.239212854Z","closed_at":"2026-02-20T07:49:21.239195321Z","close_reason":"Superseded by canonical detailed master-plan graph (bd-33v) with full 10.N-10.21 and 11-16 coverage, explicit dependencies, and per-section verification gates.","source_repo":".","compaction_level":0,"original_size":0}
{"id":"bd-1b9x","title":"[10.21] Implement survival/hazard model for compromise-propensity progression under observed trajectory patterns.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.21 — Behavioral Phenotype Evolution Tracker Execution Track (9O)\n\nWhy This Exists:\nBPET longitudinal phenotype intelligence track for pre-compromise trajectory detection and integration with DGIS/ATC/migration/economic policy.\n\nTask Objective:\nImplement survival/hazard model for compromise-propensity progression under observed trajectory patterns.\n\nAcceptance Criteria:\n- Hazard outputs are calibrated and monotonic under defined risk assumptions; censoring handling and covariate drift strategy are explicit and test-covered.\n\nExpected Artifacts:\n- `src/security/bpet/hazard_model.rs`, `docs/specs/bpet_time_to_compromise_model.md`, `artifacts/10.21/bpet_hazard_calibration_report.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_21/bd-1b9x/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_21/bd-1b9x/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.21] Implement survival/hazard model for compromise-propensity progression under observed trajectory patterns.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.21] Implement survival/hazard model for compromise-propensity progression under observed trajectory patterns.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.21] Implement survival/hazard model for compromise-propensity progression under observed trajectory patterns.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.21] Implement survival/hazard model for compromise-propensity progression under observed trajectory patterns.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.21] Implement survival/hazard model for compromise-propensity progression under observed trajectory patterns.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"- Hazard outputs are calibrated and monotonic under defined risk assumptions; censoring handling and covariate drift strategy are explicit and test-covered.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:37:08.207596236Z","created_by":"ubuntu","updated_at":"2026-02-20T17:05:37.543951349Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-21","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-1b9x","depends_on_id":"bd-2lll","type":"blocks","created_at":"2026-02-20T17:05:37.543904782Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-1ck","title":"[10.2] Implement L2 engine-boundary semantic oracle integration policy and release gate linkage.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.2 — Compatibility Core\n\nWhy This Exists:\nCompatibility core as strategic wedge: policy-visible behavior, divergence receipts, L1/L2 lockstep, and spec-first fixture governance.\n\nTask Objective:\nImplement L2 engine-boundary semantic oracle integration policy and release gate linkage.\n\nAcceptance Criteria:\n- Implement with explicit success/failure invariants and deterministic behavior under normal, edge, and fault scenarios.\n- Ensure outcomes are verifiable via automated tests and reproducible artifact outputs.\n\nExpected Artifacts:\n- Section-owned design/spec artifact at `docs/specs/section_10_2/bd-1ck_contract.md`, capturing decision rationale, invariants, and interface boundaries.\n- Machine-readable verification artifact at `artifacts/section_10_2/bd-1ck/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_2/bd-1ck/verification_summary.md` linking implementation intent to measured outcomes.\n\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.2] Implement L2 engine-boundary semantic oracle integration policy and release gate linkage.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.2] Implement L2 engine-boundary semantic oracle integration policy and release gate linkage.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.2] Implement L2 engine-boundary semantic oracle integration policy and release gate linkage.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.2] Implement L2 engine-boundary semantic oracle integration policy and release gate linkage.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.2] Implement L2 engine-boundary semantic oracle integration policy and release gate linkage.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","status":"closed","priority":1,"issue_type":"task","assignee":"CrimsonCrane","created_at":"2026-02-20T07:36:44.401871436Z","created_by":"ubuntu","updated_at":"2026-02-20T09:46:37.698578291Z","closed_at":"2026-02-20T09:46:37.698550399Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-2","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-1ck","depends_on_id":"bd-32v","type":"blocks","created_at":"2026-02-20T07:43:20.349163419Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-1clg","title":"Epic: Sandbox + Network Security [10.13b]","description":"- type: epic","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-20T07:47:58.072163029Z","updated_at":"2026-02-20T07:49:21.164744401Z","closed_at":"2026-02-20T07:49:21.164724294Z","close_reason":"Superseded by canonical detailed master-plan graph (bd-33v) with full 10.N-10.21 and 11-16 coverage, explicit dependencies, and per-section verification gates.","source_repo":".","compaction_level":0,"original_size":0}
{"id":"bd-1cm","title":"[10.13] Add singleton-writer fencing validation using `lease_seq` + lease-object linkage.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.13 — FCP Deep-Mined Expansion Execution Track (9I)\n\nWhy This Exists:\nDeep connector/provider contract track: lifecycle FSMs, conformance harnesses, fencing, sandboxing, revocation freshness, and protocol hardening.\n\nTask Objective:\nAdd singleton-writer fencing validation using `lease_seq` + lease-object linkage.\n\nAcceptance Criteria:\n- Unfenced or stale-fenced writes are rejected; fence checks are monotonic; stale writer test cases fail deterministically.\n\nExpected Artifacts:\n- `tests/conformance/singleton_writer_fencing.rs`, `docs/specs/fencing_rules.md`, `artifacts/10.13/fencing_rejection_receipts.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_13/bd-1cm/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_13/bd-1cm/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.13] Add singleton-writer fencing validation using `lease_seq` + lease-object linkage.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.13] Add singleton-writer fencing validation using `lease_seq` + lease-object linkage.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.13] Add singleton-writer fencing validation using `lease_seq` + lease-object linkage.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.13] Add singleton-writer fencing validation using `lease_seq` + lease-object linkage.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.13] Add singleton-writer fencing validation using `lease_seq` + lease-object linkage.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","status":"closed","priority":1,"issue_type":"task","assignee":"CrimsonCrane","created_at":"2026-02-20T07:36:51.799537829Z","created_by":"ubuntu","updated_at":"2026-02-20T10:52:03.912554166Z","closed_at":"2026-02-20T10:52:03.912492521Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-13","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-1cm","depends_on_id":"bd-18o","type":"blocks","created_at":"2026-02-20T07:43:12.440631511Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-1cs7","title":"[10.15] Implement request -> drain -> finalize cancellation protocol across high-impact workflows.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.15 — Asupersync-First Integration Execution Track (8.4-8.6)\n\nWhy This Exists:\nAsupersync-first control-plane migration track enforcing Cx-first, region ownership, cancellation correctness, and protocol-grade verification gates.\n\nTask Objective:\nImplement request -> drain -> finalize cancellation protocol across high-impact workflows.\n\nAcceptance Criteria:\n- Cancellation transitions are explicit and deterministic; cleanup budget bounds are documented and tested.\n\nExpected Artifacts:\n- `docs/specs/cancellation_protocol_contract.md`, `tests/conformance/cancel_drain_finalize.rs`, `artifacts/10.15/cancel_protocol_timing.csv`.\n\n- Machine-readable verification artifact at `artifacts/section_10_15/bd-1cs7/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_15/bd-1cs7/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.15] Implement request -> drain -> finalize cancellation protocol across high-impact workflows.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.15] Implement request -> drain -> finalize cancellation protocol across high-impact workflows.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.15] Implement request -> drain -> finalize cancellation protocol across high-impact workflows.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.15] Implement request -> drain -> finalize cancellation protocol across high-impact workflows.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.15] Implement request -> drain -> finalize cancellation protocol across high-impact workflows.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"- Cancellation transitions are explicit and deterministic; cleanup budget bounds are documented and tested.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:36:59.890626565Z","created_by":"ubuntu","updated_at":"2026-02-20T17:04:34.522014570Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-15","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-1cs7","depends_on_id":"bd-2177","type":"blocks","created_at":"2026-02-20T17:04:34.521963956Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-1cwp","title":"[10.15] Enforce canonical idempotency-key contracts (from `10.14`) on all retryable remote control requests.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.15 — Asupersync-First Integration Execution Track (8.4-8.6)\n\nWhy This Exists:\nAsupersync-first control-plane migration track enforcing Cx-first, region ownership, cancellation correctness, and protocol-grade verification gates.\n\nTask Objective:\nEnforce canonical idempotency-key contracts (from `10.14`) on all retryable remote control requests.\n\nAcceptance Criteria:\n- Control-plane requests inherit canonical idempotency semantics; duplicate same-payload requests dedupe safely; same-key/payload-mismatch hard-fails.\n\nExpected Artifacts:\n- `tests/integration/control_remote_idempotency.rs`, `docs/integration/control_idempotency_adoption.md`, `artifacts/10.15/control_idempotency_report.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_15/bd-1cwp/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_15/bd-1cwp/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.15] Enforce canonical idempotency-key contracts (from `10.14`) on all retryable remote control requests.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.15] Enforce canonical idempotency-key contracts (from `10.14`) on all retryable remote control requests.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.15] Enforce canonical idempotency-key contracts (from `10.14`) on all retryable remote control requests.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.15] Enforce canonical idempotency-key contracts (from `10.14`) on all retryable remote control requests.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.15] Enforce canonical idempotency-key contracts (from `10.14`) on all retryable remote control requests.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"- Control-plane requests inherit canonical idempotency semantics; duplicate same-payload requests dedupe safely; same-key/payload-mismatch hard-fails.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:37:00.220597315Z","created_by":"ubuntu","updated_at":"2026-02-20T17:00:42.263320956Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-15","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-1cwp","depends_on_id":"bd-12n3","type":"blocks","created_at":"2026-02-20T14:59:47.718915217Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1cwp","depends_on_id":"bd-206h","type":"blocks","created_at":"2026-02-20T14:59:47.851186754Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-1d6x","title":"[10.12] Section-wide verification gate: comprehensive unit+e2e+logging","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.12\n\nTask Objective:\nCreate a hard section completion gate that verifies all implemented behavior in this section with comprehensive unit tests, integration/e2e scripts, and detailed structured logging evidence.\n\nAcceptance Criteria:\n- Section test matrix covers happy-path, edge-case, and adversarial/error-path scenarios for all section deliverables.\n- E2E scripts execute representative end-user workflows and policy/control-plane transitions for this section.\n- Structured logs include stable event/error codes, trace correlation IDs, and enough context for deterministic replay triage.\n- Verification report is deterministic and machine-readable for CI/release gating.\n\nExpected Artifacts:\n- Section-level test matrix and coverage summary artifact.\n- E2E script suite with pass/fail outputs and reproducible fixtures/seeds.\n- Structured log validation report and traceability bundle.\n- Gate verdict artifact consumable by release automation.\n\n- Machine-readable verification artifact at `artifacts/section_10_12/bd-1d6x/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_12/bd-1d6x/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests must validate contracts, invariants, and failure semantics.\n- E2E tests must validate cross-component behavior from user/operator entrypoints to persisted effects.\n- Logging must support root-cause analysis without hidden context requirements.\n\nTask-Specific Clarification:\n- For \"[10.12] Section-wide verification gate: comprehensive unit+e2e+logging\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.12] Section-wide verification gate: comprehensive unit+e2e+logging\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.12] Section-wide verification gate: comprehensive unit+e2e+logging\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.12] Section-wide verification gate: comprehensive unit+e2e+logging\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.12] Section-wide verification gate: comprehensive unit+e2e+logging\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:48:08.411696239Z","created_by":"ubuntu","updated_at":"2026-02-20T08:43:52.618878648Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-12","test-obligations","verification-gate"],"dependencies":[{"issue_id":"bd-1d6x","depends_on_id":"bd-1dpd","type":"blocks","created_at":"2026-02-20T08:24:26.820714360Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1d6x","depends_on_id":"bd-2aj","type":"blocks","created_at":"2026-02-20T07:48:08.584401751Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1d6x","depends_on_id":"bd-2twu","type":"blocks","created_at":"2026-02-20T08:20:52.757269754Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1d6x","depends_on_id":"bd-3c2","type":"blocks","created_at":"2026-02-20T07:48:08.704933979Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1d6x","depends_on_id":"bd-3hm","type":"blocks","created_at":"2026-02-20T07:48:08.846372575Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1d6x","depends_on_id":"bd-3j4","type":"blocks","created_at":"2026-02-20T07:48:08.797919960Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1d6x","depends_on_id":"bd-5si","type":"blocks","created_at":"2026-02-20T07:48:08.751513606Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1d6x","depends_on_id":"bd-n1w","type":"blocks","created_at":"2026-02-20T07:48:08.535912979Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1d6x","depends_on_id":"bd-y0v","type":"blocks","created_at":"2026-02-20T07:48:08.654357157Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-1d7n","title":"[10.13] Implement deterministic activation pipeline: sandbox -> ephemeral secret mount -> capability issue -> health-ready transition.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.13 — FCP Deep-Mined Expansion Execution Track (9I)\n\nWhy This Exists:\nDeep connector/provider contract track: lifecycle FSMs, conformance harnesses, fencing, sandboxing, revocation freshness, and protocol hardening.\n\nTask Objective:\nImplement deterministic activation pipeline: sandbox -> ephemeral secret mount -> capability issue -> health-ready transition.\n\nAcceptance Criteria:\n- Stage order is fixed and enforced; partial activation cannot leak persistent secrets; restart replay reproduces identical activation transcript.\n\nExpected Artifacts:\n- `docs/specs/activation_pipeline.md`, `tests/integration/activation_pipeline_determinism.rs`, `artifacts/10.13/activation_stage_transcript.jsonl`.\n\n- Machine-readable verification artifact at `artifacts/section_10_13/bd-1d7n/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_13/bd-1d7n/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.13] Implement deterministic activation pipeline: sandbox -> ephemeral secret mount -> capability issue -> health-ready transition.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.13] Implement deterministic activation pipeline: sandbox -> ephemeral secret mount -> capability issue -> health-ready transition.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.13] Implement deterministic activation pipeline: sandbox -> ephemeral secret mount -> capability issue -> health-ready transition.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.13] Implement deterministic activation pipeline: sandbox -> ephemeral secret mount -> capability issue -> health-ready transition.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.13] Implement deterministic activation pipeline: sandbox -> ephemeral secret mount -> capability issue -> health-ready transition.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","status":"closed","priority":1,"issue_type":"task","assignee":"CrimsonCrane","created_at":"2026-02-20T07:36:52.865799544Z","created_by":"ubuntu","updated_at":"2026-02-20T11:52:50.417718334Z","closed_at":"2026-02-20T11:52:50.417692176Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-13","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-1d7n","depends_on_id":"bd-3i9o","type":"blocks","created_at":"2026-02-20T07:43:13.012177086Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-1dar","title":"[10.14] Implement optional MMR checkpoints and inclusion/prefix proof APIs for external verifiers.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.14 — FrankenSQLite Deep-Mined Expansion Execution Track (9J)\n\nWhy This Exists:\nFrankenSQLite deep-mined control integrity track: evidence ledgers, proofs, epochs, remote effects, markers/MMR, and deterministic fault labs.\n\nTask Objective:\nImplement optional MMR checkpoints and inclusion/prefix proof APIs for external verifiers.\n\nAcceptance Criteria:\n- MMR checkpoints can be enabled/disabled without corrupting marker truth; proof APIs verify inclusion and prefix claims; verifier examples pass.\n\nExpected Artifacts:\n- `src/control_plane/mmr_proofs.rs`, `tests/conformance/mmr_proof_verification.rs`, `artifacts/10.14/mmr_proof_vectors.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_14/bd-1dar/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_14/bd-1dar/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.14] Implement optional MMR checkpoints and inclusion/prefix proof APIs for external verifiers.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.14] Implement optional MMR checkpoints and inclusion/prefix proof APIs for external verifiers.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.14] Implement optional MMR checkpoints and inclusion/prefix proof APIs for external verifiers.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.14] Implement optional MMR checkpoints and inclusion/prefix proof APIs for external verifiers.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.14] Implement optional MMR checkpoints and inclusion/prefix proof APIs for external verifiers.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"MMR checkpoints can be enabled/disabled without corrupting marker truth; proof APIs verify inclusion and prefix claims; verifier examples pass.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:36:58.792991832Z","created_by":"ubuntu","updated_at":"2026-02-20T16:24:19.037147491Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-14","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-1dar","depends_on_id":"bd-126h","type":"blocks","created_at":"2026-02-20T16:24:19.037094863Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-1daz","title":"[10.14] Implement retroactive hardening pipeline that appends additional protection artifacts without rewriting canonical objects.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.14 — FrankenSQLite Deep-Mined Expansion Execution Track (9J)\n\nWhy This Exists:\nFrankenSQLite deep-mined control integrity track: evidence ledgers, proofs, epochs, remote effects, markers/MMR, and deterministic fault labs.\n\nTask Objective:\nImplement retroactive hardening pipeline that appends additional protection artifacts without rewriting canonical objects.\n\nAcceptance Criteria:\n- Retroactive hardening adds union-only artifacts; canonical object identity remains stable; repairability improvement is measurable on target corpus.\n\nExpected Artifacts:\n- `docs/specs/retroactive_hardening.md`, `tests/integration/retroactive_hardening_union_only.rs`, `artifacts/10.14/retroactive_hardening_report.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_14/bd-1daz/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_14/bd-1daz/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.14] Implement retroactive hardening pipeline that appends additional protection artifacts without rewriting canonical objects.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.14] Implement retroactive hardening pipeline that appends additional protection artifacts without rewriting canonical objects.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.14] Implement retroactive hardening pipeline that appends additional protection artifacts without rewriting canonical objects.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.14] Implement retroactive hardening pipeline that appends additional protection artifacts without rewriting canonical objects.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.14] Implement retroactive hardening pipeline that appends additional protection artifacts without rewriting canonical objects.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"Retroactive hardening adds union-only artifacts; canonical object identity remains stable; repairability improvement is measurable on target corpus.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:36:56.299367956Z","created_by":"ubuntu","updated_at":"2026-02-20T16:23:53.258803383Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-14","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-1daz","depends_on_id":"bd-2e73","type":"blocks","created_at":"2026-02-20T16:23:53.258744062Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1daz","depends_on_id":"bd-3rya","type":"blocks","created_at":"2026-02-20T16:23:53.116082624Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-1dpd","title":"[PROGRAM] Enforce rch-only offload contract for CPU-intensive build/test/benchmark workflows","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md (Cross-cutting execution policy across Sections 10.0–10.21 and 11–16)\n\nTask Objective:\nDefine and enforce a fail-closed execution policy requiring rch offload for CPU-intensive build/test/benchmark/coverage/e2e workloads so verification remains reproducible, resource-safe, and audit-traceable under parallel multi-agent execution.\n\nWhy This Exists:\nThe project explicitly requires CPU-intensive operations to run via rch. Without a dedicated policy bead and automated gate, agents may accidentally run heavy local cargo jobs, causing noisy contention, nondeterministic timing artifacts, and verification drift.\n\nAcceptance Criteria:\n- Publish canonical command-class policy that marks CPU-intensive workloads as rch-required (for example: cargo check, cargo clippy, cargo test, cargo nextest, cargo llvm-cov, benchmark suites, broad E2E sweeps).\n- Define and enforce allowlist/exception process for commands that may run locally without violating policy.\n- Implement fail-closed detector that flags local execution of rch-required workloads in CI/local verification gates.\n- Emit machine-readable evidence proving each heavy verification run used rch and include worker provenance (worker id/host/queue/wait/run durations).\n- Integrate policy checks into bootstrap, section, and program-wide verification gates.\n\nExpected Artifacts:\n- docs/verification/RCH_EXECUTION_POLICY.md\n- schemas/rch_execution_provenance.schema.json\n- scripts/verify_rch_required_workloads.sh\n- artifacts/program/rch_execution_policy_report.json\n\nTesting & Logging Requirements:\n- Unit tests for command classification logic, exception resolution, and policy parser behavior.\n- E2E tests that run representative heavy workflows and assert fail-closed behavior when rch is bypassed.\n- Detailed structured logs with stable event codes for classification decisions, policy violations, worker provenance, and remediation hints.\n- Deterministic replay artifacts capturing command input, policy decision, and provenance bundle.\n\nTask-Specific Clarification:\n- Preserve full feature scope: this bead strengthens execution discipline only and must not weaken any existing verification/test requirements.\n- This policy is additive to section/program gates and must be consumable without manual interpretation.\n- Any bypass path must be explicit, logged, and policy-approved; silent bypass is forbidden.\n\nWhy This Improves User Outcomes:\n- Prevents flaky or misleading verification results caused by uncontrolled local resource contention.\n- Improves trust by making heavy verification runs auditable, reproducible, and attributable to specific offload workers.\n- Reduces incident MTTR by pairing failures with deterministic provenance and clear policy-violation diagnostics.","status":"closed","priority":1,"issue_type":"task","assignee":"ubuntu","created_at":"2026-02-20T08:23:55.611747103Z","created_by":"ubuntu","updated_at":"2026-02-20T08:32:49.126193776Z","closed_at":"2026-02-20T08:32:49.126102376Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","program-integration","test-obligations","verification"]}
{"id":"bd-1e0","title":"[10.9] Build migration singularity demo pipeline for flagship repositories.","description":"## [10.9] Build migration singularity demo pipeline for flagship repositories\n\n### Why This Exists\n\nSection 9H.1 (Migration Singularity Engine) and Section 9F.1 envision one-command migration from Node.js to franken_node as the ultimate adoption accelerator. If migration is painful, adoption stalls regardless of technical merit. This bead builds a demo pipeline that proves the concept on well-known, flagship Node.js repositories — the projects that every Node.js developer recognizes. A compelling demo showing Express, Fastify, or Next.js examples migrating with a single command (and producing verifiable before/after evidence) is the strongest possible category marketing asset.\n\n### What It Must Do\n\nBuild an end-to-end migration demo pipeline that demonstrates one-command migration on flagship Node.js projects:\n\n- **Target repositories**: The demo must work on at least three well-known Node.js projects spanning different categories: a web framework (Express or Fastify), a full-stack framework (Next.js starter or Remix example), and a utility library (lodash, date-fns, or equivalent). Repositories are pinned to specific versions for reproducibility.\n- **Pipeline stages** (all automated, single command triggers full pipeline):\n  1. **Audit**: Static analysis of the source repository identifying: API usage by compatibility band (safe/conditional/unsafe), dependency graph with risk annotations, platform-specific code patterns, and estimated migration complexity score.\n  2. **Risk map**: Visual and structured output showing which code paths are safe for automatic migration, which require conditional handling, and which need manual intervention. Risk is quantified per file and per API call site.\n  3. **Rewrite suggestions**: Automated code transformation suggestions for conditional and unsafe API usage. Suggestions include: the original code, the proposed replacement, confidence grade (high/medium/low), and rationale. High-confidence suggestions can be auto-applied; low-confidence suggestions are flagged for human review.\n  4. **Validation**: After applying transformations, run the project's existing test suite under franken_node. Capture: tests passed, tests failed (with root cause classification), tests skipped (with reason).\n  5. **Rollout plan**: A structured plan for deploying the migrated project, including: recommended rollout strategy (canary/blue-green/progressive), confidence grades per migration step, monitoring checkpoints, and rollback triggers.\n- **Before/after evidence**: The pipeline produces side-by-side comparison artifacts: test pass rates before and after, performance benchmarks before and after, security posture before and after (trust verification, containment capabilities gained). Evidence is structured JSON with integrity hashes.\n- **Reproducibility**: The entire pipeline runs in a hermetic container with pinned dependencies. An external party can execute `./migrate-demo.sh <repo-url> <version>` and reproduce the exact same results.\n- **Category marketing output**: The pipeline produces a human-readable migration report (Markdown) suitable for publication, including: executive summary, detailed findings, before/after comparisons, and confidence assessment.\n\n### Acceptance Criteria\n\n1. Pipeline executes end-to-end on at least three flagship Node.js repositories with a single command.\n2. All five pipeline stages (audit, risk map, rewrite suggestions, validation, rollout plan) produce structured output.\n3. Rewrite suggestions include confidence grades; high-confidence suggestions are verifiably correct (validated by test suite).\n4. Before/after evidence includes test pass rates, performance comparison, and security posture improvement.\n5. Pipeline runs in a hermetic container and produces reproducible results across runs.\n6. Migration report (Markdown) is generated automatically and is suitable for publication without manual editing.\n7. The pipeline handles migration failures gracefully: partial migration produces a clear report of what succeeded, what failed, and recommended manual steps.\n8. Execution time for the full pipeline on a medium-sized project (Express) is under 10 minutes on standard CI hardware.\n\n### Key Dependencies\n\n- Compatibility matrix from 10.2 for API risk band classification\n- Migration system from 10.3 for rewrite transformation engine\n- Benchmark infrastructure (bd-f5d) for performance comparison\n- Trust verification infrastructure for security posture comparison\n\n### Testing & Logging Requirements\n\n- Unit tests for each pipeline stage (audit, risk map, rewrite, validation, rollout plan).\n- Integration test that runs the full pipeline on a small synthetic Node.js project.\n- Verification script (`scripts/check_migration_demo.py`) with `--json` and `self_test()`.\n- Pipeline execution logged at INFO with per-stage timing; failures logged at ERROR with root cause.\n\n### Expected Artifacts\n\n- `docs/specs/section_10_9/bd-1e0_contract.md` — migration demo specification\n- `scripts/check_migration_demo.py` — verification script\n- `tests/test_check_migration_demo.py` — unit tests\n- `fixtures/migration-demos/` — pinned repository configurations\n- `artifacts/section_10_9/bd-1e0/verification_evidence.json`\n- `artifacts/section_10_9/bd-1e0/verification_summary.md`","acceptance_criteria":"1. Demo pipeline migrates at least 3 flagship open-source repositories (e.g., Express, Fastify, Next.js-like projects) from Node.js to franken_node with zero manual intervention.\n2. Pipeline produces a migration singularity report: complete migration log, before/after test results, compatibility coverage percentage, and time-to-migrate.\n3. Per Section 3 category targets: demo achieves >= 3x migration velocity compared to manual migration baseline for each flagship repo.\n4. Pipeline is fully scripted and reproducible: scripts/run_singularity_demo.sh <repo-url> executes the entire migration and validation cycle.\n5. Each demo includes a compatibility proof: full test suite of the migrated project passes under franken_node with results compared against Node.js baseline.\n6. Pipeline handles failure gracefully: partial migrations produce a diagnostic report listing blocked modules, unsupported APIs, and suggested workarounds.\n7. Demo artifacts (migration logs, test results, compatibility reports) are published under artifacts/ with a summary suitable for external stakeholders.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:36:48.445017898Z","created_by":"ubuntu","updated_at":"2026-02-20T16:59:05.189045017Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-9","self-contained","test-obligations"]}
{"id":"bd-1eot","title":"[10.19] Integrate privacy-preserving urgent routing for revocation/quarantine signals.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.19 — Adversarial Trust Commons Execution Track (9M)\n\nWhy This Exists:\nATC federated intelligence track for privacy-preserving cross-deployment threat learning, robust aggregation, and verifier-backed trust metrics.\n\nTask Objective:\nIntegrate privacy-preserving urgent routing for revocation/quarantine signals.\n\nAcceptance Criteria:\n- High-severity signals propagate within policy SLO while preserving privacy envelopes; urgent route decisions are signed and replayable.\n\nExpected Artifacts:\n- `docs/specs/atc_urgent_signal_routing.md`, `tests/integration/atc_urgent_routing_latency.rs`, `artifacts/10.19/atc_urgent_routing_telemetry.csv`.\n\n- Machine-readable verification artifact at `artifacts/section_10_19/bd-1eot/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_19/bd-1eot/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.19] Integrate privacy-preserving urgent routing for revocation/quarantine signals.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.19] Integrate privacy-preserving urgent routing for revocation/quarantine signals.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.19] Integrate privacy-preserving urgent routing for revocation/quarantine signals.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.19] Integrate privacy-preserving urgent routing for revocation/quarantine signals.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.19] Integrate privacy-preserving urgent routing for revocation/quarantine signals.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"- High-severity signals propagate within policy SLO while preserving privacy envelopes; urgent route decisions are signed and replayable.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:37:06.087676863Z","created_by":"ubuntu","updated_at":"2026-02-20T17:00:52.353134312Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-19","self-contained","test-obligations"]}
{"id":"bd-1f8m","title":"[10.15] Add invariant-breach runbooks for region-quiescence failure, obligation leak, and cancel-timeout incidents.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.15 — Asupersync-First Integration Execution Track (8.4-8.6)\n\nWhy This Exists:\nAsupersync-first control-plane migration track enforcing Cx-first, region ownership, cancellation correctness, and protocol-grade verification gates.\n\nTask Objective:\nAdd invariant-breach runbooks for region-quiescence failure, obligation leak, and cancel-timeout incidents.\n\nAcceptance Criteria:\n- Runbooks include detection signature, immediate containment steps, replay procedure, and rollback procedure.\n\nExpected Artifacts:\n- `docs/runbooks/region_quiescence_breach.md`, `docs/runbooks/obligation_leak_incident.md`, `docs/runbooks/cancel_timeout_incident.md`.\n\n- Machine-readable verification artifact at `artifacts/section_10_15/bd-1f8m/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_15/bd-1f8m/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.15] Add invariant-breach runbooks for region-quiescence failure, obligation leak, and cancel-timeout incidents.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.15] Add invariant-breach runbooks for region-quiescence failure, obligation leak, and cancel-timeout incidents.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.15] Add invariant-breach runbooks for region-quiescence failure, obligation leak, and cancel-timeout incidents.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.15] Add invariant-breach runbooks for region-quiescence failure, obligation leak, and cancel-timeout incidents.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.15] Add invariant-breach runbooks for region-quiescence failure, obligation leak, and cancel-timeout incidents.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"- Runbooks include detection signature, immediate containment steps, replay procedure, and rollback procedure.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:37:01.200576053Z","created_by":"ubuntu","updated_at":"2026-02-20T17:00:42.411226221Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-15","self-contained","test-obligations"]}
{"id":"bd-1f8v","title":"[10.20] Add operator copilot guidance for dependency updates with topology-aware risk deltas and mitigation playbooks.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.20 — Dependency Graph Immune System Execution Track (9N)\n\nWhy This Exists:\nDGIS topological immune system track for dependency contagion modeling, choke-point immunization, and graph-aware containment economics.\n\nTask Objective:\nAdd operator copilot guidance for dependency updates with topology-aware risk deltas and mitigation playbooks.\n\nAcceptance Criteria:\n- Update proposals include pre/post topology risk scores, containment recommendations, and verifier-backed confidence outputs; high-risk updates require explicit policy acknowledgements.\n\nExpected Artifacts:\n- `src/ops/dgis_update_copilot.rs`, `tests/integration/dgis_update_recommendations.rs`, `artifacts/10.20/dgis_operator_recommendation_log.jsonl`.\n\n- Machine-readable verification artifact at `artifacts/section_10_20/bd-1f8v/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_20/bd-1f8v/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.20] Add operator copilot guidance for dependency updates with topology-aware risk deltas and mitigation playbooks.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.20] Add operator copilot guidance for dependency updates with topology-aware risk deltas and mitigation playbooks.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.20] Add operator copilot guidance for dependency updates with topology-aware risk deltas and mitigation playbooks.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.20] Add operator copilot guidance for dependency updates with topology-aware risk deltas and mitigation playbooks.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.20] Add operator copilot guidance for dependency updates with topology-aware risk deltas and mitigation playbooks.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"- Update proposals include pre/post topology risk scores, containment recommendations, and verifier-backed confidence outputs; high-risk updates require explicit policy acknowledgements.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:37:07.359001655Z","created_by":"ubuntu","updated_at":"2026-02-20T17:00:54.162686204Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-20","self-contained","test-obligations"]}
{"id":"bd-1fck","title":"[10.14] Implement retrievability-before-eviction proofs for L2->L3 lifecycle transitions.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.14 — FrankenSQLite Deep-Mined Expansion Execution Track (9J)\n\nWhy This Exists:\nFrankenSQLite deep-mined control integrity track: evidence ledgers, proofs, epochs, remote effects, markers/MMR, and deterministic fault labs.\n\nTask Objective:\nImplement retrievability-before-eviction proofs for L2->L3 lifecycle transitions.\n\nAcceptance Criteria:\n- Eviction requires successful retrievability proof check; failed proofs block eviction; proof records tie to retired segment IDs.\n\nExpected Artifacts:\n- `src/storage/retrievability_gate.rs`, `tests/integration/retrievability_before_eviction.rs`, `artifacts/10.14/retrievability_proof_receipts.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_14/bd-1fck/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_14/bd-1fck/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.14] Implement retrievability-before-eviction proofs for L2->L3 lifecycle transitions.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.14] Implement retrievability-before-eviction proofs for L2->L3 lifecycle transitions.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.14] Implement retrievability-before-eviction proofs for L2->L3 lifecycle transitions.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.14] Implement retrievability-before-eviction proofs for L2->L3 lifecycle transitions.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.14] Implement retrievability-before-eviction proofs for L2->L3 lifecycle transitions.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"Eviction requires successful retrievability proof check; failed proofs block eviction; proof records tie to retired segment IDs.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:36:57.487589775Z","created_by":"ubuntu","updated_at":"2026-02-20T16:24:04.593643064Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-14","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-1fck","depends_on_id":"bd-okqy","type":"blocks","created_at":"2026-02-20T16:24:04.593580628Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-1fi2","title":"[10.8] Section-wide verification gate: comprehensive unit+e2e+logging","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.8\n\nTask Objective:\nCreate a hard section completion gate that verifies all implemented behavior in this section with comprehensive unit tests, integration/e2e scripts, and detailed structured logging evidence.\n\nAcceptance Criteria:\n- Section test matrix covers happy-path, edge-case, and adversarial/error-path scenarios for all section deliverables.\n- E2E scripts execute representative end-user workflows and policy/control-plane transitions for this section.\n- Structured logs include stable event/error codes, trace correlation IDs, and enough context for deterministic replay triage.\n- Verification report is deterministic and machine-readable for CI/release gating.\n\nExpected Artifacts:\n- Section-level test matrix and coverage summary artifact.\n- E2E script suite with pass/fail outputs and reproducible fixtures/seeds.\n- Structured log validation report and traceability bundle.\n- Gate verdict artifact consumable by release automation.\n\n- Machine-readable verification artifact at `artifacts/section_10_8/bd-1fi2/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_8/bd-1fi2/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests must validate contracts, invariants, and failure semantics.\n- E2E tests must validate cross-component behavior from user/operator entrypoints to persisted effects.\n- Logging must support root-cause analysis without hidden context requirements.\n\nTask-Specific Clarification:\n- For \"[10.8] Section-wide verification gate: comprehensive unit+e2e+logging\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.8] Section-wide verification gate: comprehensive unit+e2e+logging\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.8] Section-wide verification gate: comprehensive unit+e2e+logging\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.8] Section-wide verification gate: comprehensive unit+e2e+logging\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.8] Section-wide verification gate: comprehensive unit+e2e+logging\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"1. Section 10.8 gate aggregates pass/fail status from all sibling beads (bd-tg2, bd-3o6, bd-k6o, bd-f2y, bd-nr4, bd-3m6).\n2. Gate script (scripts/check_section_10_8_gate.py) runs all section verification scripts and produces a unified JSON report.\n3. Gate fails if any sibling bead's verification evidence is missing or shows a failure.\n4. Unit tests cover gate logic: all-pass, single-fail, and missing-evidence scenarios.\n5. Gate produces a section-level summary under artifacts/ with per-bead status, timestamps, and links to individual evidence files.\n6. E2E logging: gate execution emits structured log lines for each sub-check with bead ID, result, and duration.\n7. Gate is idempotent: running it twice in succession with no changes produces identical output.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:48:26.005618085Z","created_by":"ubuntu","updated_at":"2026-02-20T15:19:26.061592607Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-8","test-obligations","verification-gate"],"dependencies":[{"issue_id":"bd-1fi2","depends_on_id":"bd-1dpd","type":"blocks","created_at":"2026-02-20T08:24:24.596033499Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1fi2","depends_on_id":"bd-2twu","type":"blocks","created_at":"2026-02-20T08:20:50.133799070Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1fi2","depends_on_id":"bd-3m6","type":"blocks","created_at":"2026-02-20T07:48:26.101845374Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1fi2","depends_on_id":"bd-3o6","type":"blocks","created_at":"2026-02-20T07:48:26.310271429Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1fi2","depends_on_id":"bd-f2y","type":"blocks","created_at":"2026-02-20T07:48:26.195563990Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1fi2","depends_on_id":"bd-k6o","type":"blocks","created_at":"2026-02-20T07:48:26.255849585Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1fi2","depends_on_id":"bd-nr4","type":"blocks","created_at":"2026-02-20T07:48:26.148262809Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1fi2","depends_on_id":"bd-tg2","type":"blocks","created_at":"2026-02-20T07:48:26.360765476Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-1fp4","title":"[10.14] Implement integrity sweep escalation/de-escalation policy driven by evidence trajectories.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.14 — FrankenSQLite Deep-Mined Expansion Execution Track (9J)\n\nWhy This Exists:\nFrankenSQLite deep-mined control integrity track: evidence ledgers, proofs, epochs, remote effects, markers/MMR, and deterministic fault labs.\n\nTask Objective:\nImplement integrity sweep escalation/de-escalation policy driven by evidence trajectories.\n\nAcceptance Criteria:\n- Sweep cadence adjusts according to policy evidence bands; escalation/de-escalation hysteresis prevents oscillation; decisions are ledgered.\n\nExpected Artifacts:\n- `src/policy/integrity_sweep_scheduler.rs`, `tests/perf/integrity_sweep_adaptation.rs`, `artifacts/10.14/sweep_policy_trajectory.csv`.\n\n- Machine-readable verification artifact at `artifacts/section_10_14/bd-1fp4/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_14/bd-1fp4/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.14] Implement integrity sweep escalation/de-escalation policy driven by evidence trajectories.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.14] Implement integrity sweep escalation/de-escalation policy driven by evidence trajectories.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.14] Implement integrity sweep escalation/de-escalation policy driven by evidence trajectories.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.14] Implement integrity sweep escalation/de-escalation policy driven by evidence trajectories.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.14] Implement integrity sweep escalation/de-escalation policy driven by evidence trajectories.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"Sweep cadence adjusts according to policy evidence bands; escalation/de-escalation hysteresis prevents oscillation; decisions are ledgered.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:36:56.380775372Z","created_by":"ubuntu","updated_at":"2026-02-20T16:23:53.401986152Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-14","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-1fp4","depends_on_id":"bd-1ayu","type":"blocks","created_at":"2026-02-20T16:23:53.401927623Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1fp4","depends_on_id":"bd-1daz","type":"blocks","created_at":"2026-02-20T07:43:14.852197185Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-1fr7","title":"Epic: FCP-Inspired Hardening + Interop [10.10]","description":"- type: epic","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-20T07:47:58.072163029Z","updated_at":"2026-02-20T07:49:21.141916313Z","closed_at":"2026-02-20T07:49:21.141894963Z","close_reason":"Superseded by canonical detailed master-plan graph (bd-33v) with full 10.N-10.21 and 11-16 coverage, explicit dependencies, and per-section verification gates.","source_repo":".","compaction_level":0,"original_size":0}
{"id":"bd-1ga5","title":"[10.21] Implement cohort-aware baseline modeling for expected phenotype evolution patterns.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.21 — Behavioral Phenotype Evolution Tracker Execution Track (9O)\n\nWhy This Exists:\nBPET longitudinal phenotype intelligence track for pre-compromise trajectory detection and integration with DGIS/ATC/migration/economic policy.\n\nTask Objective:\nImplement cohort-aware baseline modeling for expected phenotype evolution patterns.\n\nAcceptance Criteria:\n- Baselines are generated for comparable extension cohorts (domain, maturity, release cadence, dependency class) and include confidence envelopes; model recalibration is deterministic and versioned.\n\nExpected Artifacts:\n- `src/security/bpet/cohort_baselines.rs`, `tests/security/bpet_baseline_calibration.rs`, `artifacts/10.21/bpet_cohort_baseline_report.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_21/bd-1ga5/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_21/bd-1ga5/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.21] Implement cohort-aware baseline modeling for expected phenotype evolution patterns.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.21] Implement cohort-aware baseline modeling for expected phenotype evolution patterns.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.21] Implement cohort-aware baseline modeling for expected phenotype evolution patterns.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.21] Implement cohort-aware baseline modeling for expected phenotype evolution patterns.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.21] Implement cohort-aware baseline modeling for expected phenotype evolution patterns.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"- Baselines are generated for comparable extension cohorts (domain, maturity, release cadence, dependency class) and include confidence envelopes; model recalibration is deterministic and versioned.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:37:07.941121959Z","created_by":"ubuntu","updated_at":"2026-02-20T17:05:27.575327971Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-21","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-1ga5","depends_on_id":"bd-2xgs","type":"blocks","created_at":"2026-02-20T17:05:27.575255456Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-1gnb","title":"[10.13] Require distributed trace correlation IDs across connector execution and control-plane artifacts.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.13 — FCP Deep-Mined Expansion Execution Track (9I)\n\nWhy This Exists:\nDeep connector/provider contract track: lifecycle FSMs, conformance harnesses, fencing, sandboxing, revocation freshness, and protocol hardening.\n\nTask Objective:\nRequire distributed trace correlation IDs across connector execution and control-plane artifacts.\n\nAcceptance Criteria:\n- All high-impact flows carry trace correlation fields end-to-end; missing trace context is surfaced as conformance failure; traces can be stitched across services.\n\nExpected Artifacts:\n- `tests/integration/trace_correlation_end_to_end.rs`, `docs/specs/trace_context_contract.md`, `artifacts/10.13/distributed_trace_sample.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_13/bd-1gnb/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_13/bd-1gnb/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.13] Require distributed trace correlation IDs across connector execution and control-plane artifacts.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.13] Require distributed trace correlation IDs across connector execution and control-plane artifacts.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.13] Require distributed trace correlation IDs across connector execution and control-plane artifacts.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.13] Require distributed trace correlation IDs across connector execution and control-plane artifacts.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.13] Require distributed trace correlation IDs across connector execution and control-plane artifacts.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","status":"closed","priority":1,"issue_type":"task","assignee":"CrimsonCrane","created_at":"2026-02-20T07:36:54.738650207Z","created_by":"ubuntu","updated_at":"2026-02-20T13:26:06.415136147Z","closed_at":"2026-02-20T13:26:06.415109107Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-13","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-1gnb","depends_on_id":"bd-novi","type":"blocks","created_at":"2026-02-20T07:43:13.978818786Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-1gx","title":"[10.4] Define signed extension package manifest schema.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.4 — Extension Ecosystem + Registry (Item 1 of 8)\n\nWhy This Exists:\nThe signed extension package manifest schema is the foundation for the entire extension trust ecosystem. Every extension must have a machine-readable, cryptographically signed manifest that declares its capabilities, provenance, and behavioral profile. This is the first step in building the trust-native extension distribution network.\n\nTask Objective:\nDefine the canonical schema for signed extension package manifests that will be used across the registry, trust card system, and policy evaluation pipeline.\n\nDetailed Acceptance Criteria:\n1. Schema defines required fields: package identity, version, author/publisher, capability declarations (FsRead, FsWrite, NetworkEgress, ProcessSpawn, EnvRead per franken_engine ExtensionManifest), behavioral profile, minimum runtime version.\n2. Schema includes provenance fields: build system, source repository, reproducibility markers, attestation chain.\n3. Schema includes trust metadata: certification level, revocation status pointer, trust card reference.\n4. Manifest is cryptographically signed with publisher key; signature scheme supports threshold signing for high-impact operations (9B.7).\n5. Schema versioning strategy with backward-compatible evolution path.\n6. Schema is published as both human-readable spec and JSON Schema/machine-readable format.\n7. Integration with franken_engine ExtensionManifest validation (from frankenengine-extension-host crate).\n\nKey Dependencies:\n- Depends on 10.1 (Charter) for governance boundaries on what extensions can declare.\n- Depends on 10.N (Normalization) for canonical ownership rules.\n- Consumed by 10.4 remaining items (provenance, trust cards, registry).\n- Consumed by 10.13 (FCP) for manifest negotiation and admission.\n\nExpected Artifacts:\n- Schema definition document at docs/specs/section_10_4/extension_manifest_schema.md\n- JSON Schema at schemas/extension_manifest.schema.json\n- Rust types in src/supply_chain/manifest.rs implementing the schema\n- artifacts/section_10_4/bd-1gx/verification_evidence.json\n\nTesting and Logging Requirements:\n- Unit tests: schema validation (valid manifests, invalid manifests, missing required fields, malformed signatures).\n- Integration tests: manifest creation -> signing -> validation -> registry admission pipeline.\n- Fuzz tests: malformed manifest payloads.\n- Structured logs: MANIFEST_CREATED, MANIFEST_SIGNED, MANIFEST_VALIDATED, MANIFEST_REJECTED with rejection reason codes.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:36:45.428676958Z","created_by":"ubuntu","updated_at":"2026-02-20T13:09:26.543605354Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-4","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-1gx","depends_on_id":"bd-1ta","type":"blocks","created_at":"2026-02-20T07:46:36.052669446Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1gx","depends_on_id":"bd-3il","type":"blocks","created_at":"2026-02-20T07:46:36.098543578Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1gx","depends_on_id":"bd-cda","type":"blocks","created_at":"2026-02-20T07:46:36.144056928Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-1h6","title":"[10.13] Implement standard connector method contract validator (`handshake/describe/introspect/capabilities/configure/simulate/invoke/health/shutdown`).","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.13 — FCP Deep-Mined Expansion Execution Track (9I)\n\nWhy This Exists:\nDeep connector/provider contract track: lifecycle FSMs, conformance harnesses, fencing, sandboxing, revocation freshness, and protocol hardening.\n\nTask Objective:\nImplement standard connector method contract validator (`handshake/describe/introspect/capabilities/configure/simulate/invoke/health/shutdown`).\n\nAcceptance Criteria:\n- Validator rejects missing or schema-invalid methods; method schemas are versioned and pinned; contract report is machine-readable.\n\nExpected Artifacts:\n- `src/conformance/connector_method_validator.rs`, `docs/specs/connector_method_contract.md`, `artifacts/10.13/connector_method_contract_report.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_13/bd-1h6/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_13/bd-1h6/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.13] Implement standard connector method contract validator (`handshake/describe/introspect/capabilities/configure/simulate/invoke/health/shutdown`).\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.13] Implement standard connector method contract validator (`handshake/describe/introspect/capabilities/configure/simulate/invoke/health/shutdown`).\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.13] Implement standard connector method contract validator (`handshake/describe/introspect/capabilities/configure/simulate/invoke/health/shutdown`).\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.13] Implement standard connector method contract validator (`handshake/describe/introspect/capabilities/configure/simulate/invoke/health/shutdown`).\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.13] Implement standard connector method contract validator (`handshake/describe/introspect/capabilities/configure/simulate/invoke/health/shutdown`).\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","status":"closed","priority":1,"issue_type":"task","assignee":"CrimsonCrane","created_at":"2026-02-20T07:36:51.557602070Z","created_by":"ubuntu","updated_at":"2026-02-20T10:40:22.765101588Z","closed_at":"2026-02-20T10:40:22.765075921Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-13","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-1h6","depends_on_id":"bd-1rk","type":"blocks","created_at":"2026-02-20T07:43:12.315822708Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-1h64","title":"Epic: Radical Expansion - Speculative Execution + Security [10.17a]","description":"- type: epic","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-20T07:47:58.072163029Z","updated_at":"2026-02-20T07:49:21.283695075Z","closed_at":"2026-02-20T07:49:21.283675228Z","close_reason":"Superseded by canonical detailed master-plan graph (bd-33v) with full 10.N-10.21 and 11-16 coverage, explicit dependencies, and per-section verification gates.","source_repo":".","compaction_level":0,"original_size":0}
{"id":"bd-1hbw","title":"[10.15] Integrate canonical epoch transition barriers (from `10.14`) across control services with explicit abort semantics.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.15 — Asupersync-First Integration Execution Track (8.4-8.6)\n\nWhy This Exists:\nAsupersync-first control-plane migration track enforcing Cx-first, region ownership, cancellation correctness, and protocol-grade verification gates.\n\nTask Objective:\nIntegrate canonical epoch transition barriers (from `10.14`) across control services with explicit abort semantics.\n\nAcceptance Criteria:\n- Control transitions use canonical barrier protocol; transition commits only with full participant arrival/drain; timeout/cancel abort behavior remains deterministic.\n\nExpected Artifacts:\n- `docs/integration/control_epoch_barrier_adoption.md`, `tests/integration/control_epoch_barrier.rs`, `artifacts/10.15/control_epoch_barrier_transcript.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_15/bd-1hbw/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_15/bd-1hbw/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.15] Integrate canonical epoch transition barriers (from `10.14`) across control services with explicit abort semantics.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.15] Integrate canonical epoch transition barriers (from `10.14`) across control services with explicit abort semantics.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.15] Integrate canonical epoch transition barriers (from `10.14`) across control services with explicit abort semantics.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.15] Integrate canonical epoch transition barriers (from `10.14`) across control services with explicit abort semantics.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.15] Integrate canonical epoch transition barriers (from `10.14`) across control services with explicit abort semantics.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"- Control transitions use canonical barrier protocol; transition commits only with full participant arrival/drain; timeout/cancel abort behavior remains deterministic.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:37:00.465325967Z","created_by":"ubuntu","updated_at":"2026-02-20T17:00:42.556733848Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-15","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-1hbw","depends_on_id":"bd-2wsm","type":"blocks","created_at":"2026-02-20T14:59:37.214208972Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-1hd","title":"[10.10] Adopt canonical trust protocol vectors/golden fixtures (from `10.13` + `10.14`) as product publication and release gates.","description":"[10.10] Adopt canonical trust protocol vectors/golden fixtures (from 10.13 + 10.14) as product publication and release gates.\n\n## Why This Exists\n\nSection 9E.10 requires that no product release ships without passing all canonical trust protocol vectors. The golden vectors defined in 10.13 (schema validation vectors, serialization round-trip vectors, signature verification vectors) and 10.14 (idempotency vectors, epoch key derivation vectors, seed derivation vectors) represent the ground truth for protocol correctness. Today these vectors exist as test fixtures but are not enforced as hard release gates. This bead promotes them from optional test suites to mandatory CI/CD release blockers, ensuring that every build artifact that reaches production has been verified against the full vector corpus.\n\n## What It Must Do\n\n1. **Vector manifest registry.** Create a manifest file (`vectors/release_gate_manifest.json`) that enumerates every golden vector suite required for release. Each entry specifies: suite name, source section (10.13 or 10.14), vector file path, minimum pass count, and versioned schema reference.\n\n2. **Release gate CI job.** Implement a CI gate script (`scripts/check_release_vectors.py`) that loads the manifest, executes each vector suite, collects pass/fail/skip counts, and emits a structured JSON verdict. The gate MUST fail the build if any vector suite has failures or if the pass count drops below the manifest minimum.\n\n3. **Vector suite versioning.** Each vector suite must carry a version number. When vectors are added or modified, the version increments. The release gate records which vector version was validated, creating an auditable chain from build artifact to exact vector set.\n\n4. **Regression detection.** If a previously passing vector starts failing, the gate must emit a specific regression diagnostic with the vector ID, expected output, actual output, and diff. Regressions are always hard failures — no skip or ignore mechanism.\n\n5. **Coverage reporting.** The gate must report vector coverage: how many protocol features have at least one golden vector, and which features lack coverage. Coverage gaps are warnings (not failures) but are logged prominently.\n\n6. **Integration with existing 10.13 gate.** This bead extends the existing `scripts/check_section_10_13_gate.py` by adding 10.14 vectors and promoting the combined suite to a release-blocking gate. The 10.13 gate remains as a subset check; this bead is the superset.\n\n## Acceptance Criteria\n\n1. `vectors/release_gate_manifest.json` exists and lists all vector suites from 10.13 and 10.14.\n2. `scripts/check_release_vectors.py` with `--json` flag runs all suites and returns structured pass/fail verdict.\n3. CI configuration (`.github/workflows/`) includes the release vector gate as a required check.\n4. No release artifact can be produced if any vector suite fails (enforced by CI, not just convention).\n5. Regression diagnostics include vector ID, expected vs actual, and diff output.\n6. Coverage report is generated and stored in `artifacts/section_10_10/bd-1hd/vector_coverage.json`.\n7. Verification evidence written to `artifacts/section_10_10/bd-1hd/`.\n\n## Key Dependencies\n\n- 10.13 golden vectors (bd-novi or equivalent) — source vector suites.\n- 10.14 idempotency/epoch/seed vectors — source vector suites.\n- Existing `scripts/check_section_10_13_gate.py` — extended by this bead.\n- CI workflow infrastructure from 10.1.\n\n## Testing & Logging Requirements\n\n- Unit tests in `tests/test_check_release_vectors.py` covering: manifest loading, suite execution, regression detection, coverage calculation.\n- Integration test that runs the full gate against the current vector corpus and confirms a green verdict.\n- Self-test mode (`self_test()`) that validates the script can detect a deliberately injected vector failure.\n- Structured logging: `release_gate.suite_started`, `release_gate.suite_passed`, `release_gate.suite_failed`, `release_gate.regression_detected` events.\n\n## Expected Artifacts\n\n- `docs/specs/section_10_10/bd-1hd_contract.md` — specification document.\n- `vectors/release_gate_manifest.json` — vector suite manifest.\n- `scripts/check_release_vectors.py` — release gate script.\n- `tests/test_check_release_vectors.py` — unit tests.\n- `artifacts/section_10_10/bd-1hd/verification_evidence.json` — evidence.\n- `artifacts/section_10_10/bd-1hd/verification_summary.md` — summary.\n- `artifacts/section_10_10/bd-1hd/vector_coverage.json` — coverage report.","acceptance_criteria":"1. Integrate the canonical golden vector suite from 10.13 (golden_vectors.rs) and the conformance suite from 10.14 as mandatory release gates. No product build may be published to any release channel (CANARY, BETA, STABLE, LTS) without passing all golden vector tests.\n2. Define a ReleaseGateManifest struct containing: (a) release_id (TrustObjectId with RELEASE domain), (b) channel (release channel enum), (c) golden_vector_suite_version (semver string), (d) golden_vector_results (list of {vector_name, pass/fail, actual_hash, expected_hash}), (e) conformance_profile_results (from 10.13 conformance_profile.rs), (f) gate_timestamp, (g) gate_signer_key_id.\n3. Enforce pass/fail logic: (a) ALL golden vectors must pass for any channel, (b) conformance profile coverage must be >= 100% for STABLE and LTS, >= 90% for BETA, >= 80% for CANARY, (c) any single golden vector failure blocks the release with a GateFailure error listing all failing vectors.\n4. Implement a gate evaluation function: evaluate_release_gate(release_id, channel, vector_results, conformance_results) -> Result<ReleaseGateManifest, GateFailure>.\n5. Implement vector freshness check: the golden vector suite version used in the gate MUST be within 1 minor version of the latest published suite. Reject stale suites with VectorSuiteOutdated error.\n6. Implement a publication receipt: on successful gate evaluation, produce a signed receipt (using an ATTESTATION-role key from bd-364) containing the ReleaseGateManifest. This receipt is the artifact that authorizes publication.\n7. Store the gate manifest and receipt as machine-readable JSON artifacts alongside the release.\n8. Unit tests: (a) all vectors pass => gate passes, (b) one vector fails => gate fails with details, (c) conformance below threshold => gate fails, (d) stale vector suite => rejected, (e) receipt signature verification round-trip.\n9. Integration test: build a mock release pipeline, run full gate evaluation, verify receipt is produced and verifiable.\n10. Verification: scripts/check_release_gates.py --json, artifacts at artifacts/section_10_10/bd-1hd/.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:36:49.585429852Z","created_by":"ubuntu","updated_at":"2026-02-20T17:01:29.812983545Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-10","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-1hd","depends_on_id":"bd-3n2u","type":"blocks","created_at":"2026-02-20T14:59:52.490667600Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-1hf","title":"[PLAN 10.10] FCP-Inspired Hardening + Interop Integration Track","description":"Section: 10.10 — FCP-Inspired Hardening + Interop Integration Track\n\nStrategic Context:\nFCP-inspired hardening/interop contract for IDs, serialization, control-channel auth, revocation freshness, and publication gating.\n\nExecution Requirements:\n- Preserve all scoped capabilities from the canonical plan.\n- Keep contracts self-contained so future contributors can execute without reopening the master plan.\n- Require explicit testing strategy (unit + integration/e2e) and structured logging/telemetry for every child bead.\n- Require artifact-backed validation for performance, security, and correctness claims.\n\nDependency Semantics:\n- This epic is blocked by its child implementation beads.\n- Cross-epic dependencies encode strategic sequencing and canonical ownership boundaries.\n\n## Success Criteria\n- All child beads for this section are completed with acceptance artifacts attached and no unresolved blockers.\n- Section-level verification gate for comprehensive unit tests, integration/e2e workflows, and detailed structured logging evidence is green.\n- Scope-to-plan traceability is explicit, with no feature loss versus the canonical plan section.\n\n## Optimization Notes\n- User-Outcome Lens: \"[PLAN 10.10] FCP-Inspired Hardening + Interop Integration Track\" must improve operator confidence, safety posture, and deterministic recovery behavior under both normal and adversarial conditions.\n- Cross-Section Coordination: child beads must encode integration assumptions explicitly to avoid local optimizations that degrade system-wide correctness.\n- Verification Non-Negotiable: completion requires reproducible unit/integration/E2E evidence and structured logs suitable for independent replay.\n- Scope Protection: any simplification must preserve canonical-plan feature intent and be justified with explicit tradeoff documentation.","status":"open","priority":2,"issue_type":"epic","created_at":"2026-02-20T07:36:41.031586341Z","created_by":"ubuntu","updated_at":"2026-02-20T08:16:47.555101501Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-10"],"dependencies":[{"issue_id":"bd-1hf","depends_on_id":"bd-13q","type":"blocks","created_at":"2026-02-20T07:36:49.542162178Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1hf","depends_on_id":"bd-174","type":"blocks","created_at":"2026-02-20T07:36:48.957857014Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1hf","depends_on_id":"bd-1hd","type":"blocks","created_at":"2026-02-20T07:36:49.621886409Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1hf","depends_on_id":"bd-1jjq","type":"blocks","created_at":"2026-02-20T07:48:07.288639479Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1hf","depends_on_id":"bd-1l5","type":"blocks","created_at":"2026-02-20T07:36:48.795731716Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1hf","depends_on_id":"bd-1r2","type":"blocks","created_at":"2026-02-20T07:36:49.124527458Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1hf","depends_on_id":"bd-1ta","type":"blocks","created_at":"2026-02-20T07:37:10.706149914Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1hf","depends_on_id":"bd-1vp","type":"blocks","created_at":"2026-02-20T07:36:49.462726845Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1hf","depends_on_id":"bd-2ms","type":"blocks","created_at":"2026-02-20T07:36:49.038641299Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1hf","depends_on_id":"bd-2sx","type":"blocks","created_at":"2026-02-20T07:36:49.365138059Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1hf","depends_on_id":"bd-364","type":"blocks","created_at":"2026-02-20T07:36:49.204073066Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1hf","depends_on_id":"bd-5rh","type":"blocks","created_at":"2026-02-20T07:37:10.744067643Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1hf","depends_on_id":"bd-cda","type":"blocks","created_at":"2026-02-20T07:37:09.427896947Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1hf","depends_on_id":"bd-jjm","type":"blocks","created_at":"2026-02-20T07:36:48.874439003Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1hf","depends_on_id":"bd-oty","type":"blocks","created_at":"2026-02-20T07:36:49.284219864Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-1hj3","title":"[10.19] Implement local signal extraction pipeline from trust cards, adversary graph, and control-plane events.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.19 — Adversarial Trust Commons Execution Track (9M)\n\nWhy This Exists:\nATC federated intelligence track for privacy-preserving cross-deployment threat learning, robust aggregation, and verifier-backed trust metrics.\n\nTask Objective:\nImplement local signal extraction pipeline from trust cards, adversary graph, and control-plane events.\n\nAcceptance Criteria:\n- Extraction is deterministic for identical inputs; sensitive raw payloads are excluded by policy; extraction outputs are replay-auditable.\n\nExpected Artifacts:\n- `src/federation/atc_signal_extractor.rs`, `tests/conformance/atc_signal_extraction.rs`, `artifacts/10.19/atc_local_signal_samples.jsonl`.\n\n- Machine-readable verification artifact at `artifacts/section_10_19/bd-1hj3/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_19/bd-1hj3/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.19] Implement local signal extraction pipeline from trust cards, adversary graph, and control-plane events.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.19] Implement local signal extraction pipeline from trust cards, adversary graph, and control-plane events.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.19] Implement local signal extraction pipeline from trust cards, adversary graph, and control-plane events.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.19] Implement local signal extraction pipeline from trust cards, adversary graph, and control-plane events.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.19] Implement local signal extraction pipeline from trust cards, adversary graph, and control-plane events.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"- Extraction is deterministic for identical inputs; sensitive raw payloads are excluded by policy; extraction outputs are replay-auditable.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:37:05.501718880Z","created_by":"ubuntu","updated_at":"2026-02-20T17:00:55.194964402Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-19","self-contained","test-obligations"]}
{"id":"bd-1id0","title":"[10.15] Publish tri-kernel ownership contract (`franken_engine`, `asupersync`, `franken_node`) with explicit interface boundaries.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.15 — Asupersync-First Integration Execution Track (8.4-8.6)\n\nWhy This Exists:\nAsupersync-first control-plane migration track enforcing Cx-first, region ownership, cancellation correctness, and protocol-grade verification gates.\n\nTask Objective:\nPublish tri-kernel ownership contract (`franken_engine`, `asupersync`, `franken_node`) with explicit interface boundaries.\n\nAcceptance Criteria:\n- Contract names owners for execution, correctness, and product planes; boundary violations have deterministic CI failures; exceptions require signed waiver artifact.\n\nExpected Artifacts:\n- `docs/architecture/tri_kernel_ownership_contract.md`, `tests/conformance/ownership_boundary_checks.rs`, `artifacts/10.15/ownership_boundary_report.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_15/bd-1id0/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_15/bd-1id0/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.15] Publish tri-kernel ownership contract (`franken_engine`, `asupersync`, `franken_node`) with explicit interface boundaries.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.15] Publish tri-kernel ownership contract (`franken_engine`, `asupersync`, `franken_node`) with explicit interface boundaries.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.15] Publish tri-kernel ownership contract (`franken_engine`, `asupersync`, `franken_node`) with explicit interface boundaries.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.15] Publish tri-kernel ownership contract (`franken_engine`, `asupersync`, `franken_node`) with explicit interface boundaries.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.15] Publish tri-kernel ownership contract (`franken_engine`, `asupersync`, `franken_node`) with explicit interface boundaries.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"- Contract names owners for execution, correctness, and product planes; boundary violations have deterministic CI failures; exceptions require signed waiver artifact.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:36:59.474000123Z","created_by":"ubuntu","updated_at":"2026-02-20T15:45:54.414589132Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-15","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-1id0","depends_on_id":"bd-1ow","type":"blocks","created_at":"2026-02-20T07:46:33.223681854Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1id0","depends_on_id":"bd-5rh","type":"blocks","created_at":"2026-02-20T07:46:33.274270988Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1id0","depends_on_id":"bd-cda","type":"blocks","created_at":"2026-02-20T07:46:33.319097779Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-1iyx","title":"[10.14] Add determinism conformance tests ensuring identical artifacts across replicas for identical content/config.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.14 — FrankenSQLite Deep-Mined Expansion Execution Track (9J)\n\nWhy This Exists:\nFrankenSQLite deep-mined control integrity track: evidence ledgers, proofs, epochs, remote effects, markers/MMR, and deterministic fault labs.\n\nTask Objective:\nAdd determinism conformance tests ensuring identical artifacts across replicas for identical content/config.\n\nAcceptance Criteria:\n- Multi-replica fixture run yields byte-identical artifact sets; divergence test reports first mismatch and root cause; tests run in CI.\n\nExpected Artifacts:\n- `tests/conformance/replica_artifact_determinism.rs`, `fixtures/determinism/*`, `artifacts/10.14/determinism_conformance_results.csv`.\n\n- Machine-readable verification artifact at `artifacts/section_10_14/bd-1iyx/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_14/bd-1iyx/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.14] Add determinism conformance tests ensuring identical artifacts across replicas for identical content/config.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.14] Add determinism conformance tests ensuring identical artifacts across replicas for identical content/config.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.14] Add determinism conformance tests ensuring identical artifacts across replicas for identical content/config.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.14] Add determinism conformance tests ensuring identical artifacts across replicas for identical content/config.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.14] Add determinism conformance tests ensuring identical artifacts across replicas for identical content/config.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"Multi-replica fixture run yields byte-identical artifact sets; divergence test reports first mismatch and root cause; tests run in CI.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:36:56.970468633Z","created_by":"ubuntu","updated_at":"2026-02-20T15:44:07.992451145Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-14","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-1iyx","depends_on_id":"bd-29r6","type":"blocks","created_at":"2026-02-20T07:43:15.147930610Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-1j2","title":"[10.1] Enforce repository split contract checks in CI.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.1 — Charter + Split Governance\n\nWhy This Exists:\nProgram charter and governance baseline: split contracts, reproducibility, and anti-regression rules for strategic integrity.\n\nTask Objective:\nEnforce repository split contract checks in CI.\n\nAcceptance Criteria:\n- Implement with explicit success/failure invariants and deterministic behavior under normal, edge, and fault scenarios.\n- Ensure outcomes are verifiable via automated tests and reproducible artifact outputs.\n\nExpected Artifacts:\n- Section-owned design/spec artifact at `docs/specs/section_10_1/bd-1j2_contract.md`, capturing decision rationale, invariants, and interface boundaries.\n- Machine-readable verification artifact at `artifacts/section_10_1/bd-1j2/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_1/bd-1j2/verification_summary.md` linking implementation intent to measured outcomes.\n\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.1] Enforce repository split contract checks in CI.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.1] Enforce repository split contract checks in CI.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.1] Enforce repository split contract checks in CI.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.1] Enforce repository split contract checks in CI.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.1] Enforce repository split contract checks in CI.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","status":"closed","priority":1,"issue_type":"task","assignee":"CrimsonCrane","created_at":"2026-02-20T07:36:43.368738359Z","created_by":"ubuntu","updated_at":"2026-02-20T09:06:34.403407547Z","closed_at":"2026-02-20T09:06:34.403381529Z","close_reason":"Split contract CI enforcement implemented. 4 checks (no-local-crates, path-deps, no-internals, governance-docs) all PASS. 8 unit tests all pass. Spec document, enforcement script, and verification artifacts created.","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-1","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-1j2","depends_on_id":"bd-vjq","type":"blocks","created_at":"2026-02-20T07:43:10.574942355Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-1jjq","title":"[10.10] Section-wide verification gate: comprehensive unit+e2e+logging","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.10\n\nTask Objective:\nCreate a hard section completion gate that verifies all implemented behavior in this section with comprehensive unit tests, integration/e2e scripts, and detailed structured logging evidence.\n\nAcceptance Criteria:\n- Section test matrix covers happy-path, edge-case, and adversarial/error-path scenarios for all section deliverables.\n- E2E scripts execute representative end-user workflows and policy/control-plane transitions for this section.\n- Structured logs include stable event/error codes, trace correlation IDs, and enough context for deterministic replay triage.\n- Verification report is deterministic and machine-readable for CI/release gating.\n\nExpected Artifacts:\n- Section-level test matrix and coverage summary artifact.\n- E2E script suite with pass/fail outputs and reproducible fixtures/seeds.\n- Structured log validation report and traceability bundle.\n- Gate verdict artifact consumable by release automation.\n\n- Machine-readable verification artifact at `artifacts/section_10_10/bd-1jjq/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_10/bd-1jjq/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests must validate contracts, invariants, and failure semantics.\n- E2E tests must validate cross-component behavior from user/operator entrypoints to persisted effects.\n- Logging must support root-cause analysis without hidden context requirements.\n\nTask-Specific Clarification:\n- For \"[10.10] Section-wide verification gate: comprehensive unit+e2e+logging\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.10] Section-wide verification gate: comprehensive unit+e2e+logging\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.10] Section-wide verification gate: comprehensive unit+e2e+logging\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.10] Section-wide verification gate: comprehensive unit+e2e+logging\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.10] Section-wide verification gate: comprehensive unit+e2e+logging\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:48:06.665766167Z","created_by":"ubuntu","updated_at":"2026-02-20T08:43:53.090758918Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-10","test-obligations","verification-gate"],"dependencies":[{"issue_id":"bd-1jjq","depends_on_id":"bd-13q","type":"blocks","created_at":"2026-02-20T07:48:06.812416049Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1jjq","depends_on_id":"bd-174","type":"blocks","created_at":"2026-02-20T07:48:07.147997446Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1jjq","depends_on_id":"bd-1dpd","type":"blocks","created_at":"2026-02-20T08:24:27.097219827Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1jjq","depends_on_id":"bd-1hd","type":"blocks","created_at":"2026-02-20T07:48:06.764993302Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1jjq","depends_on_id":"bd-1l5","type":"blocks","created_at":"2026-02-20T07:48:07.242258071Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1jjq","depends_on_id":"bd-1r2","type":"blocks","created_at":"2026-02-20T07:48:07.050557169Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1jjq","depends_on_id":"bd-1vp","type":"blocks","created_at":"2026-02-20T07:48:06.859933272Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1jjq","depends_on_id":"bd-2ms","type":"blocks","created_at":"2026-02-20T07:48:07.098143731Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1jjq","depends_on_id":"bd-2sx","type":"blocks","created_at":"2026-02-20T07:48:06.906505546Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1jjq","depends_on_id":"bd-2twu","type":"blocks","created_at":"2026-02-20T08:20:53.088912456Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1jjq","depends_on_id":"bd-364","type":"blocks","created_at":"2026-02-20T07:48:07.002927075Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1jjq","depends_on_id":"bd-jjm","type":"blocks","created_at":"2026-02-20T07:48:07.195302414Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1jjq","depends_on_id":"bd-oty","type":"blocks","created_at":"2026-02-20T07:48:06.954404720Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-1jmq","title":"[11] Contract field: EV score and tier","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 11\n\nTask Objective:\nRequire EV scoring metadata and tier classification for each major subsystem change.\n\nExecution Requirements:\n- Make this requirement machine-verifiable and release-gated where applicable.\n- Keep behavior self-documenting so future contributors do not need to re-open the master plan.\n\nTesting & Logging Requirements:\n- Unit tests for rule/contract logic and error conditions.\n- Integration/E2E tests for workflow-level enforcement.\n- Structured logs with stable codes and trace IDs.\n- Reproducible artifacts that prove contract satisfaction.\n\nAcceptance Criteria:\n- Success and failure invariants for [11] Contract field: EV score and tier are explicitly quantified and machine-verifiable.\n- Determinism requirements for [11] Contract field: EV score and tier are validated across repeated runs and adversarial perturbations.\n\n\nExpected Artifacts:\n- Machine-readable verification artifact with explicit canonical path under artifacts/section_11/bd-1jmq/verification_evidence.json, suitable for CI/release gating.\n- Human-readable verification summary with explicit canonical path under artifacts/section_11/bd-1jmq/verification_summary.md, linking implementation intent to measured validation outcomes.\n\nTask-Specific Clarification:\n- The implementation must deliver the exact capability named in the title, with no scope dilution and no silent feature omission.\n- Validation artifacts must include capability-specific thresholds, pass/fail criteria, and reproducible evidence bundles tied to this bead ID.\n- Program/section verification gates must be able to consume this bead's outputs without manual interpretation.\n\nWhy This Improves User Outcomes:\n- \"[11] Contract field: EV score and tier\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[11] Contract field: EV score and tier\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"1. Every evidence contract includes an EV-score field computed as: EV = P(success) * benefit - P(failure) * cost, with explicit numeric values for each term.\n2. Tier classification follows: Tier-1 (EV >= 10, critical path), Tier-2 (EV >= 1, important), Tier-3 (EV < 1, nice-to-have).\n3. The EV calculation references concrete data: measured test pass rates, estimated blast radius, historical incident cost.\n4. CI rejects contracts where EV score is missing, non-numeric, or tier is inconsistent with the computed EV value.\n5. Unit test: contracts with correct EV and tier pass; contracts with mismatched tier/EV or missing fields fail validation.\n6. A helper script or library function exists to compute EV from input parameters and assign tier automatically.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:39:32.649250887Z","created_by":"ubuntu","updated_at":"2026-02-20T15:16:05.695603050Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-11","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-1jmq","depends_on_id":"bd-36wa","type":"blocks","created_at":"2026-02-20T07:43:24.372861472Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-1jpc","title":"[10.21] Implement unified evolution-risk scorer with explainability contract and confidence decomposition.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.21 — Behavioral Phenotype Evolution Tracker Execution Track (9O)\n\nWhy This Exists:\nBPET longitudinal phenotype intelligence track for pre-compromise trajectory detection and integration with DGIS/ATC/migration/economic policy.\n\nTask Objective:\nImplement unified evolution-risk scorer with explainability contract and confidence decomposition.\n\nAcceptance Criteria:\n- Scorer combines drift, regime, hazard, and provenance features under a documented weighting policy; output includes stable explanation vectors and confidence intervals.\n\nExpected Artifacts:\n- `src/security/bpet/evolution_risk_scorer.rs`, `tests/conformance/bpet_risk_score_explainability.rs`, `artifacts/10.21/bpet_risk_score_catalog.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_21/bd-1jpc/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_21/bd-1jpc/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.21] Implement unified evolution-risk scorer with explainability contract and confidence decomposition.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.21] Implement unified evolution-risk scorer with explainability contract and confidence decomposition.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.21] Implement unified evolution-risk scorer with explainability contract and confidence decomposition.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.21] Implement unified evolution-risk scorer with explainability contract and confidence decomposition.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.21] Implement unified evolution-risk scorer with explainability contract and confidence decomposition.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"- Scorer combines drift, regime, hazard, and provenance features under a documented weighting policy; output includes stable explanation vectors and confidence intervals.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:37:08.291306712Z","created_by":"ubuntu","updated_at":"2026-02-20T17:05:44.238291290Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-21","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-1jpc","depends_on_id":"bd-1b9x","type":"blocks","created_at":"2026-02-20T17:05:40.888038780Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1jpc","depends_on_id":"bd-2ao3","type":"blocks","created_at":"2026-02-20T17:05:44.238242809Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-1jpo","title":"[10.11] Section-wide verification gate: comprehensive unit+e2e+logging","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.11\n\nTask Objective:\nCreate a hard section completion gate that verifies all implemented behavior in this section with comprehensive unit tests, integration/e2e scripts, and detailed structured logging evidence.\n\nAcceptance Criteria:\n- Section test matrix covers happy-path, edge-case, and adversarial/error-path scenarios for all section deliverables.\n- E2E scripts execute representative end-user workflows and policy/control-plane transitions for this section.\n- Structured logs include stable event/error codes, trace correlation IDs, and enough context for deterministic replay triage.\n- Verification report is deterministic and machine-readable for CI/release gating.\n\nExpected Artifacts:\n- Section-level test matrix and coverage summary artifact.\n- E2E script suite with pass/fail outputs and reproducible fixtures/seeds.\n- Structured log validation report and traceability bundle.\n- Gate verdict artifact consumable by release automation.\n\n- Machine-readable verification artifact at `artifacts/section_10_11/bd-1jpo/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_11/bd-1jpo/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests must validate contracts, invariants, and failure semantics.\n- E2E tests must validate cross-component behavior from user/operator entrypoints to persisted effects.\n- Logging must support root-cause analysis without hidden context requirements.\n\nTask-Specific Clarification:\n- For \"[10.11] Section-wide verification gate: comprehensive unit+e2e+logging\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.11] Section-wide verification gate: comprehensive unit+e2e+logging\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.11] Section-wide verification gate: comprehensive unit+e2e+logging\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.11] Section-wide verification gate: comprehensive unit+e2e+logging\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.11] Section-wide verification gate: comprehensive unit+e2e+logging\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"AC for bd-1jpo (Section 10.11 Verification Gate):\n1. A section-level test matrix covers every 10.11 deliverable (bd-cvt through bd-390) with happy-path, edge-case, and adversarial/error-path scenarios; the matrix is encoded as a machine-readable JSON mapping bead_id -> [test_ids].\n2. E2E scripts exercise representative cross-cutting workflows: (a) full lifecycle from capability profile definition through anti-entropy convergence, (b) cancellation cascade through supervision tree with epoch transition mid-drain, (c) saga compensation under regime-change budget boost.\n3. The test suite runs under the DeterministicRuntime (bd-2ko) with fixed seeds; CI runs each scenario twice and diffs outputs to verify bit-for-bit reproducibility.\n4. Structured log validation: a log-schema checker verifies that every stable event code defined across all 10.11 beads (CAPABILITY_*, AMBIENT_*, CHECKPOINT_*, CANCEL_*, MASK_*, OBLIGATION_*, CHILD_*, SCENARIO_*, BOCPD_*, VOI_*, EPOCH_*, IDEMPOTENCY_*, SAGA_*, LANE_*, BULKHEAD_*, RECONCILIATION_*) appears in at least one test trace and conforms to the declared schema.\n5. Coverage report: all Rust modules under the 10.11 scope achieve >= 80% line coverage; any module below threshold blocks the gate with COVERAGE_BELOW_THRESHOLD.\n6. Gate verdict artifact at artifacts/section_10_11/bd-1jpo/verification_evidence.json encodes: per-bead pass/fail, coverage percentages, reproducibility check result, and log-schema conformance result.\n7. The gate returns exit code 0 only when all sub-checks pass; any failure returns exit code 1 with a structured summary of which checks failed and why.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:48:07.460302460Z","created_by":"ubuntu","updated_at":"2026-02-20T15:19:56.354902072Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-11","test-obligations","verification-gate"],"dependencies":[{"issue_id":"bd-1jpo","depends_on_id":"bd-1dpd","type":"blocks","created_at":"2026-02-20T08:24:26.954868176Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1jpo","depends_on_id":"bd-24k","type":"blocks","created_at":"2026-02-20T07:48:08.001172427Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1jpo","depends_on_id":"bd-2ah","type":"blocks","created_at":"2026-02-20T07:48:07.946131581Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1jpo","depends_on_id":"bd-2gr","type":"blocks","created_at":"2026-02-20T07:48:07.702066458Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1jpo","depends_on_id":"bd-2ko","type":"blocks","created_at":"2026-02-20T07:48:07.847142629Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1jpo","depends_on_id":"bd-2nt","type":"blocks","created_at":"2026-02-20T07:48:07.752435112Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1jpo","depends_on_id":"bd-2twu","type":"blocks","created_at":"2026-02-20T08:20:52.923905878Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1jpo","depends_on_id":"bd-390","type":"blocks","created_at":"2026-02-20T07:48:07.561848996Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1jpo","depends_on_id":"bd-3he","type":"blocks","created_at":"2026-02-20T07:48:07.894550930Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1jpo","depends_on_id":"bd-3hw","type":"blocks","created_at":"2026-02-20T07:48:07.655763125Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1jpo","depends_on_id":"bd-3u4","type":"blocks","created_at":"2026-02-20T07:48:07.799758374Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1jpo","depends_on_id":"bd-3vm","type":"blocks","created_at":"2026-02-20T07:48:08.145815102Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1jpo","depends_on_id":"bd-7om","type":"blocks","created_at":"2026-02-20T07:48:08.051212930Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1jpo","depends_on_id":"bd-93k","type":"blocks","created_at":"2026-02-20T07:48:08.097099947Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1jpo","depends_on_id":"bd-cvt","type":"blocks","created_at":"2026-02-20T07:48:08.193048396Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1jpo","depends_on_id":"bd-lus","type":"blocks","created_at":"2026-02-20T07:48:07.609422944Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-1kfq","title":"[10.9] Section-wide verification gate: comprehensive unit+e2e+logging","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.9\n\nTask Objective:\nCreate a hard section completion gate that verifies all implemented behavior in this section with comprehensive unit tests, integration/e2e scripts, and detailed structured logging evidence.\n\nAcceptance Criteria:\n- Section test matrix covers happy-path, edge-case, and adversarial/error-path scenarios for all section deliverables.\n- E2E scripts execute representative end-user workflows and policy/control-plane transitions for this section.\n- Structured logs include stable event/error codes, trace correlation IDs, and enough context for deterministic replay triage.\n- Verification report is deterministic and machine-readable for CI/release gating.\n\nExpected Artifacts:\n- Section-level test matrix and coverage summary artifact.\n- E2E script suite with pass/fail outputs and reproducible fixtures/seeds.\n- Structured log validation report and traceability bundle.\n- Gate verdict artifact consumable by release automation.\n\n- Machine-readable verification artifact at `artifacts/section_10_9/bd-1kfq/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_9/bd-1kfq/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests must validate contracts, invariants, and failure semantics.\n- E2E tests must validate cross-component behavior from user/operator entrypoints to persisted effects.\n- Logging must support root-cause analysis without hidden context requirements.\n\nTask-Specific Clarification:\n- For \"[10.9] Section-wide verification gate: comprehensive unit+e2e+logging\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.9] Section-wide verification gate: comprehensive unit+e2e+logging\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.9] Section-wide verification gate: comprehensive unit+e2e+logging\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.9] Section-wide verification gate: comprehensive unit+e2e+logging\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.9] Section-wide verification gate: comprehensive unit+e2e+logging\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"1. Section 10.9 gate aggregates pass/fail status from all sibling beads (bd-f5d, bd-9is, bd-1e0, bd-m8p, bd-10c, bd-15t).\n2. Gate script (scripts/check_section_10_9_gate.py) runs all section verification scripts and produces a unified JSON report.\n3. Gate fails if any sibling bead's verification evidence is missing or shows a failure.\n4. Unit tests cover gate logic: all-pass, single-fail, and missing-evidence scenarios.\n5. Gate produces a section-level summary under artifacts/ with per-bead status, timestamps, and links to individual evidence files.\n6. E2E logging: gate execution emits structured log lines for each sub-check with bead ID, result, and duration.\n7. Gate is idempotent: running it twice in succession with no changes produces identical output.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:48:26.577180281Z","created_by":"ubuntu","updated_at":"2026-02-20T15:20:50.794139233Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-9","test-obligations","verification-gate"],"dependencies":[{"issue_id":"bd-1kfq","depends_on_id":"bd-10c","type":"blocks","created_at":"2026-02-20T07:48:26.722244440Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1kfq","depends_on_id":"bd-15t","type":"blocks","created_at":"2026-02-20T07:48:26.674981871Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1kfq","depends_on_id":"bd-1dpd","type":"blocks","created_at":"2026-02-20T08:24:24.451624955Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1kfq","depends_on_id":"bd-1e0","type":"blocks","created_at":"2026-02-20T07:48:26.816221738Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1kfq","depends_on_id":"bd-2twu","type":"blocks","created_at":"2026-02-20T08:20:49.965117653Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1kfq","depends_on_id":"bd-9is","type":"blocks","created_at":"2026-02-20T07:48:26.862838044Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1kfq","depends_on_id":"bd-f5d","type":"blocks","created_at":"2026-02-20T07:48:26.909731235Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1kfq","depends_on_id":"bd-m8p","type":"blocks","created_at":"2026-02-20T07:48:26.769135107Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-1koz","title":"[10.5] Section-wide verification gate: comprehensive unit+e2e+logging","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.5\n\nTask Objective:\nCreate a hard section completion gate that verifies all implemented behavior in this section with comprehensive unit tests, integration/e2e scripts, and detailed structured logging evidence.\n\nAcceptance Criteria:\n- Section test matrix covers happy-path, edge-case, and adversarial/error-path scenarios for all section deliverables.\n- E2E scripts execute representative end-user workflows and policy/control-plane transitions for this section.\n- Structured logs include stable event/error codes, trace correlation IDs, and enough context for deterministic replay triage.\n- Verification report is deterministic and machine-readable for CI/release gating.\n\nExpected Artifacts:\n- Section-level test matrix and coverage summary artifact.\n- E2E script suite with pass/fail outputs and reproducible fixtures/seeds.\n- Structured log validation report and traceability bundle.\n- Gate verdict artifact consumable by release automation.\n\n- Machine-readable verification artifact at `artifacts/section_10_5/bd-1koz/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_5/bd-1koz/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests must validate contracts, invariants, and failure semantics.\n- E2E tests must validate cross-component behavior from user/operator entrypoints to persisted effects.\n- Logging must support root-cause analysis without hidden context requirements.\n\nTask-Specific Clarification:\n- For \"[10.5] Section-wide verification gate: comprehensive unit+e2e+logging\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.5] Section-wide verification gate: comprehensive unit+e2e+logging\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.5] Section-wide verification gate: comprehensive unit+e2e+logging\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.5] Section-wide verification gate: comprehensive unit+e2e+logging\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.5] Section-wide verification gate: comprehensive unit+e2e+logging\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"1. All eight 10.5 implementation beads (bd-137, bd-21z, bd-vll, bd-2fa, bd-2yc, bd-33b, bd-3nr, bd-sh3) must be closed with passing verification evidence before this gate can close.\n2. Every verification script (check_policy_gate, check_signed_receipt, check_replay_bundle, check_counterfactual, check_copilot_api, check_loss_scoring, check_degraded_mode, check_policy_approval) must pass with --json flag and produce valid JSON output with no failures.\n3. All unit test files (tests/test_check_*.py for each of the eight beads) must pass under pytest with zero failures and zero errors.\n4. Integration coverage: at least one end-to-end scenario must chain policy gate evaluation (bd-137) -> signed receipt generation (bd-21z) -> replay bundle inclusion (bd-vll) -> counterfactual replay (bd-2fa), proving the four components compose correctly.\n5. All structured log events (tracing spans) from gate evaluations, receipt signing, degraded mode, and policy approvals must conform to a shared LogEventSchema with required fields: timestamp, event_type, component, and correlation_id.\n6. Evidence artifacts for all eight beads must exist under artifacts/section_10_5/ with both verification_evidence.json and verification_summary.md files present and non-empty.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:48:24.123275190Z","created_by":"ubuntu","updated_at":"2026-02-20T15:16:35.649350840Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-5","test-obligations","verification-gate"],"dependencies":[{"issue_id":"bd-1koz","depends_on_id":"bd-137","type":"blocks","created_at":"2026-02-20T07:48:24.565477706Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1koz","depends_on_id":"bd-1dpd","type":"blocks","created_at":"2026-02-20T08:24:25.016750352Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1koz","depends_on_id":"bd-21z","type":"blocks","created_at":"2026-02-20T07:48:24.518405732Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1koz","depends_on_id":"bd-2fa","type":"blocks","created_at":"2026-02-20T07:48:24.419370113Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1koz","depends_on_id":"bd-2twu","type":"blocks","created_at":"2026-02-20T08:20:50.628953105Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1koz","depends_on_id":"bd-2yc","type":"blocks","created_at":"2026-02-20T07:48:24.371651485Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1koz","depends_on_id":"bd-33b","type":"blocks","created_at":"2026-02-20T07:48:24.320815871Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1koz","depends_on_id":"bd-3nr","type":"blocks","created_at":"2026-02-20T07:48:24.271693007Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1koz","depends_on_id":"bd-sh3","type":"blocks","created_at":"2026-02-20T07:48:24.223879361Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1koz","depends_on_id":"bd-vll","type":"blocks","created_at":"2026-02-20T07:48:24.467720098Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-1l5","title":"[10.10] Define canonical product trust object IDs with domain separation.","description":"[10.10] Define canonical product trust object IDs with domain separation.\n\n## Why This Exists\n\nSection 9E.1 of the franken_node plan mandates stable, collision-free identifiers for every trust object in the system. Trust objects span multiple domains — extensions, trust cards, receipts, policy checkpoints, migration artifacts, and verifier claims. Without domain separation, an ID that is valid in one domain could collide with or be confused for an ID in another, creating subtle but catastrophic trust-boundary violations. This bead establishes the canonical ID scheme that every downstream bead (trust fabric, verifier economy, migration singularity) depends on for unambiguous object reference.\n\n## What It Must Do\n\n1. **Domain-separated prefix scheme.** Define a fixed set of domain prefixes (e.g., `ext:`, `tcard:`, `rcpt:`, `pchk:`, `migr:`, `vclaim:`) that are prepended to every trust object ID. The prefix set is enumerated in a registry file and must be extensible via a versioned schema.\n\n2. **Deterministic ID derivation.** Each trust object ID must be deterministically derivable from its content and context. For content-addressed objects, the ID is `<prefix><hash_algorithm>:<digest>`. For context-addressed objects (e.g., policy checkpoints bound to an epoch), the ID is `<prefix><epoch>:<sequence>:<digest>`. The derivation function must be pure — same inputs always produce the same ID.\n\n3. **Collision resistance guarantees.** The hash algorithm must provide at least 128 bits of collision resistance (SHA-256 truncated to 32 bytes minimum). Cross-domain collisions are structurally impossible due to the prefix scheme. Within-domain collisions must be statistically negligible (< 2^-128 probability).\n\n4. **Canonical serialization for hashing.** Define the canonical byte serialization used as hash input. This must be deterministic (sorted keys, no optional whitespace, fixed endianness). The canonical form must be documented and have a reference implementation in Rust.\n\n5. **Validation and parsing utilities.** Provide `TrustObjectId::parse(s: &str) -> Result<Self, IdError>` and `TrustObjectId::validate(s: &str) -> bool` in Rust. Parsing must reject malformed IDs with specific error codes from the stable error namespace (bd-13q / 10.13).\n\n6. **Human-readable short forms.** Define a short-form representation (first 8 hex chars after prefix) for logging and operator display, with full-form available on demand.\n\n## Acceptance Criteria\n\n1. A registry file (`trust_object_id_registry.json`) enumerates all valid domain prefixes with version metadata.\n2. Rust module `src/connector/trust_object_id.rs` implements `TrustObjectId` with `parse`, `validate`, `derive_content_addressed`, and `derive_context_addressed` methods.\n3. All existing trust objects in the codebase use the new ID scheme (no raw strings or ad-hoc IDs).\n4. Property-based tests demonstrate zero cross-domain collisions over 10M random inputs.\n5. Golden vector file (`vectors/trust_object_ids.json`) contains at least 20 test vectors covering every domain prefix and both derivation modes.\n6. Verification script `scripts/check_trust_object_ids.py` with `--json` flag confirms all criteria pass.\n7. Evidence artifacts written to `artifacts/section_10_10/bd-1l5/`.\n\n## Key Dependencies\n\n- 10.13 stable error namespace (bd-13q) for error codes on malformed IDs.\n- 10.13 golden vectors (bd-1hd) for vector format conventions.\n- Rust `sha2` crate for hash derivation.\n\n## Testing & Logging Requirements\n\n- Unit tests in `tests/test_check_trust_object_ids.py` covering parse/validate round-trips, rejection of malformed IDs, and cross-domain collision checks.\n- Integration test confirming all trust objects in a simulated workload carry valid canonical IDs.\n- Structured logging: every ID derivation logs `trust_object_id.derived` event with domain, algorithm, and truncated digest at DEBUG level.\n\n## Expected Artifacts\n\n- `docs/specs/section_10_10/bd-1l5_contract.md` — specification document.\n- `crates/franken-node/src/connector/trust_object_id.rs` — Rust implementation.\n- `vectors/trust_object_ids.json` — golden test vectors.\n- `scripts/check_trust_object_ids.py` — verification script.\n- `tests/test_check_trust_object_ids.py` — unit tests.\n- `artifacts/section_10_10/bd-1l5/verification_evidence.json` — evidence.\n- `artifacts/section_10_10/bd-1l5/verification_summary.md` — summary.","acceptance_criteria":"1. Define a TrustObjectId type containing: (a) a 4-byte domain tag drawn from an enum of canonical domains (POLICY, TOKEN, KEY, SESSION, ZONE, RELEASE, REVOCATION, MARKER), (b) a 16-byte random component, (c) a 4-byte version field. Total fixed size = 24 bytes.\n2. Implement TrustObjectId::new(domain) that generates a cryptographically random ID with the correct domain tag and version=1.\n3. Implement TrustObjectId::parse(bytes) that validates the domain tag against the canonical enum and rejects unknown domains with a typed error (UnknownDomain).\n4. Enforce domain separation: two IDs with identical random bytes but different domain tags MUST NOT compare as equal. Equality requires all three fields to match.\n5. Implement Display and FromStr for a human-readable form: `{DOMAIN}-{hex(random)}-v{version}` (e.g., `POLICY-a1b2c3...f0-v1`).\n6. Provide a `domain()` accessor returning the enum variant, and a `is_domain(expected)` predicate.\n7. Reject construction or parsing of zero-filled random components (all zeros) as invalid.\n8. Unit tests: (a) round-trip serialize/parse for every domain tag, (b) cross-domain inequality, (c) zero-rejection, (d) unknown-domain rejection, (e) Display/FromStr round-trip.\n9. Golden fixture: generate one canonical ID per domain, store as JSON in vectors/trust_object_ids.json, and verify parse stability across builds.\n10. Verification script scripts/check_trust_object_ids.py emits pass/fail JSON to artifacts/section_10_10/bd-1l5/verification_evidence.json.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:36:48.759819874Z","created_by":"ubuntu","updated_at":"2026-02-20T16:24:04.214299054Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-10","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-1l5","depends_on_id":"bd-1ta","type":"blocks","created_at":"2026-02-20T07:46:31.225935104Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1l5","depends_on_id":"bd-5rh","type":"blocks","created_at":"2026-02-20T07:46:31.287396287Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1l5","depends_on_id":"bd-cda","type":"blocks","created_at":"2026-02-20T07:46:31.342269640Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-1l62","title":"[10.14] Gate durable-claiming operations on verifiable marker/proof availability.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.14 — FrankenSQLite Deep-Mined Expansion Execution Track (9J)\n\nWhy This Exists:\nFrankenSQLite deep-mined control integrity track: evidence ledgers, proofs, epochs, remote effects, markers/MMR, and deterministic fault labs.\n\nTask Objective:\nGate durable-claiming operations on verifiable marker/proof availability.\n\nAcceptance Criteria:\n- Durable claims fail closed when marker/proof verification is incomplete; claim API exposes reason codes; false-claim path is blocked in tests.\n\nExpected Artifacts:\n- `tests/security/durable_claim_gate.rs`, `docs/specs/durable_claim_requirements.md`, `artifacts/10.14/durable_claim_gate_results.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_14/bd-1l62/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_14/bd-1l62/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.14] Gate durable-claiming operations on verifiable marker/proof availability.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.14] Gate durable-claiming operations on verifiable marker/proof availability.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.14] Gate durable-claiming operations on verifiable marker/proof availability.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.14] Gate durable-claiming operations on verifiable marker/proof availability.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.14] Gate durable-claiming operations on verifiable marker/proof availability.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"Durable claims fail closed when marker/proof verification is incomplete; claim API exposes reason codes; false-claim path is blocked in tests.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:36:56.544196093Z","created_by":"ubuntu","updated_at":"2026-02-20T16:23:09.737419284Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-14","self-contained","test-obligations"]}
{"id":"bd-1m7","title":"Add transplant re-sync and drift detection workflow","description":"Add repeatable process/script/docs to re-sync from pi_agent_rust and detect inventory/hash drift.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-20T07:26:05.443350039Z","created_by":"ubuntu","updated_at":"2026-02-20T07:27:13.152239970Z","closed_at":"2026-02-20T07:27:13.152217177Z","close_reason":"Duplicate scope; superseded by bd-29q","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-1m7","depends_on_id":"bd-1qz","type":"blocks","created_at":"2026-02-20T07:26:09.673595026Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1m7","depends_on_id":"bd-2zl","type":"blocks","created_at":"2026-02-20T07:26:09.739001938Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-1m8r","title":"[10.13] Enforce revocation freshness per safety tier before risky and dangerous actions.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.13 — FCP Deep-Mined Expansion Execution Track (9I)\n\nWhy This Exists:\nDeep connector/provider contract track: lifecycle FSMs, conformance harnesses, fencing, sandboxing, revocation freshness, and protocol hardening.\n\nTask Objective:\nEnforce revocation freshness per safety tier before risky and dangerous actions.\n\nAcceptance Criteria:\n- Safety-tier gate denies stale-frontier risky/dangerous actions; override behavior follows policy and is receipt-backed; gate latency meets SLO.\n\nExpected Artifacts:\n- `tests/security/revocation_freshness_gate.rs`, `docs/specs/safety_tier_freshness.md`, `artifacts/10.13/revocation_freshness_decisions.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_13/bd-1m8r/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_13/bd-1m8r/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.13] Enforce revocation freshness per safety tier before risky and dangerous actions.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.13] Enforce revocation freshness per safety tier before risky and dangerous actions.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.13] Enforce revocation freshness per safety tier before risky and dangerous actions.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.13] Enforce revocation freshness per safety tier before risky and dangerous actions.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.13] Enforce revocation freshness per safety tier before risky and dangerous actions.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","status":"closed","priority":1,"issue_type":"task","assignee":"CrimsonCrane","created_at":"2026-02-20T07:36:53.111923915Z","created_by":"ubuntu","updated_at":"2026-02-20T12:03:50.587386394Z","closed_at":"2026-02-20T12:03:50.587354675Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-13","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-1m8r","depends_on_id":"bd-y7lu","type":"blocks","created_at":"2026-02-20T07:43:13.137488665Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-1mj","title":"[10.1] Add claim-language policy requiring verifier artifacts for external claims.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.1 — Charter + Split Governance\n\nWhy This Exists:\nProgram charter and governance baseline: split contracts, reproducibility, and anti-regression rules for strategic integrity.\n\nTask Objective:\nAdd claim-language policy requiring verifier artifacts for external claims.\n\nAcceptance Criteria:\n- Implement with explicit success/failure invariants and deterministic behavior under normal, edge, and fault scenarios.\n- Ensure outcomes are verifiable via automated tests and reproducible artifact outputs.\n\nExpected Artifacts:\n- Section-owned design/spec artifact at `docs/specs/section_10_1/bd-1mj_contract.md`, capturing decision rationale, invariants, and interface boundaries.\n- Machine-readable verification artifact at `artifacts/section_10_1/bd-1mj/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_1/bd-1mj/verification_summary.md` linking implementation intent to measured outcomes.\n\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.1] Add claim-language policy requiring verifier artifacts for external claims.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.1] Add claim-language policy requiring verifier artifacts for external claims.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.1] Add claim-language policy requiring verifier artifacts for external claims.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.1] Add claim-language policy requiring verifier artifacts for external claims.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.1] Add claim-language policy requiring verifier artifacts for external claims.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","status":"closed","priority":1,"issue_type":"task","assignee":"CrimsonCrane","created_at":"2026-02-20T07:36:43.603346339Z","created_by":"ubuntu","updated_at":"2026-02-20T09:20:59.077852855Z","closed_at":"2026-02-20T09:20:59.077814303Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-1","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-1mj","depends_on_id":"bd-4yv","type":"blocks","created_at":"2026-02-20T07:43:10.702211510Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-1n1t","title":"[12] Risk control: topology blind spots","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 12\n\nTask Objective:\nEnforce mandatory graph coverage, topology baselines, and unresolved-edge escalation.\n\nExecution Requirements:\n- Make this requirement machine-verifiable and release-gated where applicable.\n- Keep behavior self-documenting so future contributors do not need to re-open the master plan.\n\nTesting & Logging Requirements:\n- Unit tests for rule/contract logic and error conditions.\n- Integration/E2E tests for workflow-level enforcement.\n- Structured logs with stable codes and trace IDs.\n- Reproducible artifacts that prove contract satisfaction.\n\nAcceptance Criteria:\n- Success and failure invariants for [12] Risk control: topology blind spots are explicitly quantified and machine-verifiable.\n- Determinism requirements for [12] Risk control: topology blind spots are validated across repeated runs and adversarial perturbations.\n\n\nExpected Artifacts:\n- Machine-readable verification artifact with explicit canonical path under artifacts/section_12/bd-1n1t/verification_evidence.json, suitable for CI/release gating.\n- Human-readable verification summary with explicit canonical path under artifacts/section_12/bd-1n1t/verification_summary.md, linking implementation intent to measured validation outcomes.\n\nTask-Specific Clarification:\n- The implementation must deliver the exact capability named in the title, with no scope dilution and no silent feature omission.\n- Validation artifacts must include capability-specific thresholds, pass/fail criteria, and reproducible evidence bundles tied to this bead ID.\n- Program/section verification gates must be able to consume this bead's outputs without manual interpretation.\n\nWhy This Improves User Outcomes:\n- \"[12] Risk control: topology blind spots\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[12] Risk control: topology blind spots\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"RISK: Dependency topology blind spots — unknown or untracked dependency relationships create hidden attack surfaces and cascading failure paths.\nIMPACT: Supply-chain attacks via unmonitored transitive dependencies, cascading failures from topology choke-points, inability to assess blast radius.\nCOUNTERMEASURES:\n  (a) Mandatory graph ingestion: every build ingests the full dependency graph (direct + transitive) into a queryable data structure.\n  (b) Topology-metric baselines: key topology metrics (max depth, fan-out, centrality of critical nodes) are baselined and monitored for drift.\n  (c) Choke-point alerts: dependencies that appear in > 50% of dependency paths are flagged as choke-points with heightened review.\nVERIFICATION:\n  1. Dependency graph is generated on every build and includes all transitive dependencies (verified against cargo metadata or equivalent).\n  2. Topology metrics are computed: max depth, average fan-out, betweenness centrality of top-10 nodes.\n  3. Baselines exist for all topology metrics; drift > 20% from baseline triggers a review alert.\n  4. Choke-point detection correctly identifies dependencies used by > 50% of paths.\nTEST SCENARIOS:\n  - Scenario A: Add a new deep transitive dependency; verify graph ingestion captures it and depth metric increases.\n  - Scenario B: Introduce a dependency that becomes a choke-point (used by > 50% of paths); verify alert is triggered.\n  - Scenario C: Remove a choke-point dependency; verify topology metrics improve and alert clears.\n  - Scenario D: Verify graph ingestion handles cyclic dependencies gracefully (detects and reports them).","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:39:33.852996881Z","created_by":"ubuntu","updated_at":"2026-02-20T15:19:38.990028973Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-12","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-1n1t","depends_on_id":"bd-13yn","type":"blocks","created_at":"2026-02-20T07:43:25.041118773Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-1n5p","title":"[10.15] Replace critical ad hoc messaging with obligation-tracked two-phase channels.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.15 — Asupersync-First Integration Execution Track (8.4-8.6)\n\nWhy This Exists:\nAsupersync-first control-plane migration track enforcing Cx-first, region ownership, cancellation correctness, and protocol-grade verification gates.\n\nTask Objective:\nReplace critical ad hoc messaging with obligation-tracked two-phase channels.\n\nAcceptance Criteria:\n- Publish/revoke/quarantine/migration critical paths use reserve/commit semantics; leak oracle remains green under cancellation injection.\n\nExpected Artifacts:\n- `tests/security/obligation_tracked_channels.rs`, `docs/specs/two_phase_effects.md`, `artifacts/10.15/obligation_leak_oracle_report.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_15/bd-1n5p/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_15/bd-1n5p/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.15] Replace critical ad hoc messaging with obligation-tracked two-phase channels.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.15] Replace critical ad hoc messaging with obligation-tracked two-phase channels.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.15] Replace critical ad hoc messaging with obligation-tracked two-phase channels.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.15] Replace critical ad hoc messaging with obligation-tracked two-phase channels.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.15] Replace critical ad hoc messaging with obligation-tracked two-phase channels.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"- Publish/revoke/quarantine/migration critical paths use reserve/commit semantics; leak oracle remains green under cancellation injection.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:36:59.972242209Z","created_by":"ubuntu","updated_at":"2026-02-20T17:04:37.660221654Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-15","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-1n5p","depends_on_id":"bd-2177","type":"blocks","created_at":"2026-02-20T17:04:37.660172433Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-1nab","title":"[12] Risk control: federated privacy leakage","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 12\n\nTask Objective:\nEnforce strict privacy budgets, secure aggregation, and verifier checks for federation outputs.\n\nExecution Requirements:\n- Make this requirement machine-verifiable and release-gated where applicable.\n- Keep behavior self-documenting so future contributors do not need to re-open the master plan.\n\nTesting & Logging Requirements:\n- Unit tests for rule/contract logic and error conditions.\n- Integration/E2E tests for workflow-level enforcement.\n- Structured logs with stable codes and trace IDs.\n- Reproducible artifacts that prove contract satisfaction.\n\nAcceptance Criteria:\n- Success and failure invariants for [12] Risk control: federated privacy leakage are explicitly quantified and machine-verifiable.\n- Determinism requirements for [12] Risk control: federated privacy leakage are validated across repeated runs and adversarial perturbations.\n\n\nExpected Artifacts:\n- Machine-readable verification artifact with explicit canonical path under artifacts/section_12/bd-1nab/verification_evidence.json, suitable for CI/release gating.\n- Human-readable verification summary with explicit canonical path under artifacts/section_12/bd-1nab/verification_summary.md, linking implementation intent to measured validation outcomes.\n\nTask-Specific Clarification:\n- The implementation must deliver the exact capability named in the title, with no scope dilution and no silent feature omission.\n- Validation artifacts must include capability-specific thresholds, pass/fail criteria, and reproducible evidence bundles tied to this bead ID.\n- Program/section verification gates must be able to consume this bead's outputs without manual interpretation.\n\nWhy This Improves User Outcomes:\n- \"[12] Risk control: federated privacy leakage\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[12] Risk control: federated privacy leakage\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"RISK: Federated privacy leakage — aggregated telemetry or federated learning signals inadvertently reveal individual node/user information.\nIMPACT: Privacy violations, regulatory non-compliance (GDPR/CCPA), loss of user trust, potential legal liability.\nCOUNTERMEASURES:\n  (a) Strict privacy budgets: each telemetry channel has a defined epsilon (differential privacy) budget; once exhausted, no more data is emitted.\n  (b) Secure aggregation: individual contributions are encrypted; only the aggregate is visible to the coordinator.\n  (c) External verifier checks: an independent verifier can audit that privacy budgets are respected without seeing raw data.\nVERIFICATION:\n  1. Every telemetry channel has a configured epsilon budget; default epsilon <= 1.0.\n  2. Privacy budget accounting is tested: after N emissions that exhaust the budget, the (N+1)th emission is blocked.\n  3. Secure aggregation protocol passes: individual contributions are not recoverable from aggregated output (tested with >= 10 participants).\n  4. External verifier API exists and can confirm budget compliance given only aggregate data and budget parameters.\nTEST SCENARIOS:\n  - Scenario A: Emit telemetry until epsilon budget is exhausted; verify subsequent emissions are blocked with clear error.\n  - Scenario B: Run secure aggregation with 10 participants; attempt to recover individual contribution from aggregate; verify failure.\n  - Scenario C: External verifier audits a fully-consumed budget; verify it correctly reports budget exhausted.\n  - Scenario D: Attempt to reset privacy budget without authorization; verify it is denied and logged.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:39:33.672761330Z","created_by":"ubuntu","updated_at":"2026-02-20T15:19:05.872443066Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-12","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-1nab","depends_on_id":"bd-2w4u","type":"blocks","created_at":"2026-02-20T07:43:24.951040084Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-1naf","title":"[10.21] Define BPET governance policy for thresholding, appeals, and evidence-backed override workflows.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.21 — Behavioral Phenotype Evolution Tracker Execution Track (9O)\n\nWhy This Exists:\nBPET longitudinal phenotype intelligence track for pre-compromise trajectory detection and integration with DGIS/ATC/migration/economic policy.\n\nTask Objective:\nDefine BPET governance policy for thresholding, appeals, and evidence-backed override workflows.\n\nAcceptance Criteria:\n- False-positive handling, human override, and appeal lifecycle are explicit, auditable, and bounded by safety constraints; every override emits signed rationale.\n\nExpected Artifacts:\n- `docs/policy/bpet_governance_policy.md`, `tests/policy/bpet_override_audit.rs`, `artifacts/10.21/bpet_governance_audit_log.jsonl`.\n\n- Machine-readable verification artifact at `artifacts/section_10_21/bd-1naf/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_21/bd-1naf/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.21] Define BPET governance policy for thresholding, appeals, and evidence-backed override workflows.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.21] Define BPET governance policy for thresholding, appeals, and evidence-backed override workflows.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.21] Define BPET governance policy for thresholding, appeals, and evidence-backed override workflows.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.21] Define BPET governance policy for thresholding, appeals, and evidence-backed override workflows.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.21] Define BPET governance policy for thresholding, appeals, and evidence-backed override workflows.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"- False-positive handling, human override, and appeal lifecycle are explicit, auditable, and bounded by safety constraints; every override emits signed rationale.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:37:08.878626130Z","created_by":"ubuntu","updated_at":"2026-02-20T17:01:06.074295872Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-21","self-contained","test-obligations"]}
{"id":"bd-1neb","title":"[10.N] Section-wide verification gate: comprehensive unit+e2e+logging","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.N\n\nTask Objective:\nCreate a hard section completion gate that verifies all implemented behavior in this section with comprehensive unit tests, integration/e2e scripts, and detailed structured logging evidence.\n\nAcceptance Criteria:\n- Section test matrix covers happy-path, edge-case, and adversarial/error-path scenarios for all section deliverables.\n- E2E scripts execute representative end-user workflows and policy/control-plane transitions for this section.\n- Structured logs include stable event/error codes, trace correlation IDs, and enough context for deterministic replay triage.\n- Verification report is deterministic and machine-readable for CI/release gating.\n\nExpected Artifacts:\n- Section-level test matrix and coverage summary artifact.\n- E2E script suite with pass/fail outputs and reproducible fixtures/seeds.\n- Structured log validation report and traceability bundle.\n- Gate verdict artifact consumable by release automation.\n\nTesting & Logging Requirements:\n- Unit tests must validate contracts, invariants, and failure semantics.\n- E2E tests must validate cross-component behavior from user/operator entrypoints to persisted effects.\n- Logging must support root-cause analysis without hidden context requirements.\n\nTask-Specific Clarification:\n- For \"[10.N] Section-wide verification gate: comprehensive unit+e2e+logging\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.N] Section-wide verification gate: comprehensive unit+e2e+logging\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.N] Section-wide verification gate: comprehensive unit+e2e+logging\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.N] Section-wide verification gate: comprehensive unit+e2e+logging\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.N] Section-wide verification gate: comprehensive unit+e2e+logging\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","status":"closed","priority":1,"issue_type":"task","assignee":"ubuntu","created_at":"2026-02-20T07:48:27.123913150Z","created_by":"ubuntu","updated_at":"2026-02-20T08:41:08.718496708Z","closed_at":"2026-02-20T08:41:08.718407452Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-N","test-obligations","verification-gate"],"dependencies":[{"issue_id":"bd-1neb","depends_on_id":"bd-1dpd","type":"blocks","created_at":"2026-02-20T08:24:24.312939926Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1neb","depends_on_id":"bd-1oyt","type":"blocks","created_at":"2026-02-20T07:50:04.617165933Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1neb","depends_on_id":"bd-1v2c","type":"blocks","created_at":"2026-02-20T07:50:04.711756785Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1neb","depends_on_id":"bd-2twu","type":"blocks","created_at":"2026-02-20T08:20:49.800136753Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1neb","depends_on_id":"bd-2yhs","type":"blocks","created_at":"2026-02-20T07:50:04.495179795Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1neb","depends_on_id":"bd-zxk8","type":"blocks","created_at":"2026-02-20T07:50:04.399857520Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-1nf","title":"[10.0] Implement operator safety copilot.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.0 — Top 10 Initiative Tracking (Initiative #8)\nCross-references: 9A.8, 9B.8, 9C.8, 9D.8\n\nWhy This Exists:\nOperator safety copilot is the #8 strategic initiative. It offers live recommended actions with expected-loss rationale, confidence context, and deterministic rollback commands. This transforms security incident response from reactive guesswork into informed, evidence-backed decision-making.\n\nTask Objective:\nBuild the operator safety copilot that recommends actions during security incidents, trust decisions, and policy changes — each recommendation backed by expected-loss scoring, confidence intervals, and pre-computed rollback commands.\n\nDetailed Acceptance Criteria:\n1. Action recommendation API: given current state + incident context, returns ranked action list with expected-loss vectors.\n2. Each recommendation includes: action description, expected loss (quantified), confidence interval, uncertainty bands (9C.8), and deterministic rollback command.\n3. VOI-based ranking for recommendations so operator attention goes to highest expected impact actions (9B.8).\n4. Counterfactual replay mode: simulate what-if scenarios for policy changes before committing (10.5).\n5. Recommendation latency optimized for interactive operation budgets — sub-second response (9D.8).\n6. CLI surface: franken-node doctor with copilot recommendations, franken-node incident commands.\n7. Copilot integrates with trust cards (10.0.3), fleet quarantine (10.0.6), and economic trust layer (10.0.9).\n8. All copilot recommendations logged with signed decision receipts for audit trail.\n\nKey Dependencies:\n- Depends on 10.5 (Security + Policy) for expected-loss action scoring and decision receipt export.\n- Depends on trust cards (10.0.3) for extension risk context.\n- Depends on fleet quarantine (10.0.6) for containment action options.\n- Consumed by 10.8 (Operational Readiness) for operator runbooks.\n\nExpected Artifacts:\n- src/copilot/ module with recommendation_engine.rs, expected_loss.rs, rollback_commands.rs.\n- CLI integration for doctor and incident commands.\n- docs/specs/section_10_0/bd-1nf_contract.md\n- artifacts/section_10_0/bd-1nf/verification_evidence.json\n\nTesting and Logging Requirements:\n- Unit tests: action scoring, VOI ranking, rollback command generation, confidence interval computation.\n- Integration tests: copilot recommendation generation from mock incident + trust state.\n- E2E tests: franken-node doctor producing ranked recommendations, franken-node incident counterfactual simulating policy changes.\n- Performance tests: recommendation latency under various state sizes.\n- Structured logs: COPILOT_RECOMMENDATION_GENERATED, ACTION_SCORED, VOI_RANKED, ROLLBACK_COMPUTED, DECISION_RECEIPT_SIGNED with trace IDs.","acceptance_criteria":"1. Live recommended actions: copilot surfaces top-3 recommended actions ranked by expected-loss reduction; updates within <= 10 seconds of state change.\n2. Each recommendation includes: action description, expected-loss rationale (quantified in risk-units), confidence level (high/medium/low with percentage), deterministic rollback command.\n3. Rollback commands are executable: `franken-node copilot execute <recommendation-id>` runs the recommended action; `franken-node copilot rollback <recommendation-id>` deterministically reverses it.\n4. Expected-loss model: recommendations grounded in quantitative risk model considering: blast radius (affected nodes/workloads), severity (critical/high/medium/low), time-to-impact, and historical incident data.\n5. Confidence calibration: copilot confidence predictions are calibrated — actions labeled \"high confidence (>= 90%)\" succeed >= 90% of the time in validation suite.\n6. Safety guardrails: copilot never auto-executes actions above \"medium\" blast radius without operator confirmation; all actions logged before execution.\n7. Integration with quarantine system (bd-yqz): copilot recommends quarantine actions when anomaly detected; pre-computes blast-radius view for operator.\n8. Integration with economic trust layer (bd-2g0): recommendations incorporate privilege-risk pricing signals for cost-aware decision support.\n9. Audit trail: every recommendation (accepted/rejected/deferred), execution, and rollback logged with timestamp, operator identity, and outcome; append-only.\n10. Verification evidence includes: recommendation accuracy test (>= 80% of high-confidence recommendations validated as correct), rollback determinism test, latency measurement for recommendation generation.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:36:43.042426134Z","created_by":"ubuntu","updated_at":"2026-02-20T15:36:24.221629995Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-0","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-1nf","depends_on_id":"bd-2ac","type":"blocks","created_at":"2026-02-20T07:43:10.394615492Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-1nfu","title":"[10.14] Require `RemoteCap` (or equivalent) for all network-bound trust/control operations.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.14 — FrankenSQLite Deep-Mined Expansion Execution Track (9J)\n\nWhy This Exists:\nFrankenSQLite deep-mined control integrity track: evidence ledgers, proofs, epochs, remote effects, markers/MMR, and deterministic fault labs.\n\nTask Objective:\nRequire `RemoteCap` (or equivalent) for all network-bound trust/control operations.\n\nAcceptance Criteria:\n- Network-bound operations fail without capability token; capability checks are centralized and auditable; local-only mode remains functional.\n\nExpected Artifacts:\n- `tests/security/remote_cap_enforcement.rs`, `docs/specs/remote_cap_contract.md`, `artifacts/10.14/remote_cap_denials.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_14/bd-1nfu/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_14/bd-1nfu/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.14] Require `RemoteCap` (or equivalent) for all network-bound trust/control operations.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.14] Require `RemoteCap` (or equivalent) for all network-bound trust/control operations.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.14] Require `RemoteCap` (or equivalent) for all network-bound trust/control operations.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.14] Require `RemoteCap` (or equivalent) for all network-bound trust/control operations.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.14] Require `RemoteCap` (or equivalent) for all network-bound trust/control operations.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"Network-bound operations fail without capability token; capability checks are centralized and auditable; local-only mode remains functional.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:36:57.650595572Z","created_by":"ubuntu","updated_at":"2026-02-20T16:22:36.552062785Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-14","self-contained","test-obligations"]}
{"id":"bd-1nk5","title":"[10.13] Add SSRF-deny default policy template (localhost/tailnet/private CIDR denied unless explicitly allowed).","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.13 — FCP Deep-Mined Expansion Execution Track (9I)\n\nWhy This Exists:\nDeep connector/provider contract track: lifecycle FSMs, conformance harnesses, fencing, sandboxing, revocation freshness, and protocol hardening.\n\nTask Objective:\nAdd SSRF-deny default policy template (localhost/tailnet/private CIDR denied unless explicitly allowed).\n\nAcceptance Criteria:\n- Default templates block unsafe internal destinations; explicit allowlist exceptions require policy receipts; regression tests cover common SSRF patterns.\n\nExpected Artifacts:\n- `config/policies/network_guard_default.toml`, `tests/security/ssrf_default_deny.rs`, `artifacts/10.13/ssrf_policy_test_report.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_13/bd-1nk5/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_13/bd-1nk5/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.13] Add SSRF-deny default policy template (localhost/tailnet/private CIDR denied unless explicitly allowed).\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.13] Add SSRF-deny default policy template (localhost/tailnet/private CIDR denied unless explicitly allowed).\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.13] Add SSRF-deny default policy template (localhost/tailnet/private CIDR denied unless explicitly allowed).\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.13] Add SSRF-deny default policy template (localhost/tailnet/private CIDR denied unless explicitly allowed).\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.13] Add SSRF-deny default policy template (localhost/tailnet/private CIDR denied unless explicitly allowed).\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","status":"closed","priority":1,"issue_type":"task","assignee":"CrimsonCrane","created_at":"2026-02-20T07:36:52.366422143Z","created_by":"ubuntu","updated_at":"2026-02-20T11:28:37.880582192Z","closed_at":"2026-02-20T11:28:37.880556835Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-13","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-1nk5","depends_on_id":"bd-2m2b","type":"blocks","created_at":"2026-02-20T07:43:12.756576197Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-1nl1","title":"[10.17] Build proof-carrying speculative execution governance framework for extension-host hot paths.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.17 — Radical Expansion Execution Track (9K)\n\nWhy This Exists:\nRadical expansion track for proof-carrying speculation, Bayesian adversary control, time-travel replay, ZK attestations, and claim compiler systems.\n\nTask Objective:\nBuild proof-carrying speculative execution governance framework for extension-host hot paths.\n\nAcceptance Criteria:\n- Speculative transforms cannot activate without proof receipts and guard checks; guard failure always degrades to deterministic safe baseline with no correctness regression; activation occurs only via approved franken_engine interfaces.\n\nExpected Artifacts:\n- `docs/specs/proof_carrying_speculation.md`, `src/runtime/speculation/proof_executor.rs`, `tests/conformance/proof_speculation_guards.rs`, `artifacts/10.17/speculation_proof_report.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_17/bd-1nl1/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_17/bd-1nl1/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.17] Build proof-carrying speculative execution governance framework for extension-host hot paths.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.17] Build proof-carrying speculative execution governance framework for extension-host hot paths.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.17] Build proof-carrying speculative execution governance framework for extension-host hot paths.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.17] Build proof-carrying speculative execution governance framework for extension-host hot paths.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.17] Build proof-carrying speculative execution governance framework for extension-host hot paths.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"- Speculative transforms cannot activate without proof receipts and guard checks; guard failure always degrades to deterministic safe baseline with no correctness regression; activation occurs only via approved franken_engine interfaces.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:37:02.927493339Z","created_by":"ubuntu","updated_at":"2026-02-20T15:46:01.006955147Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-17","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-1nl1","depends_on_id":"bd-1ta","type":"blocks","created_at":"2026-02-20T07:46:33.798018723Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1nl1","depends_on_id":"bd-229","type":"blocks","created_at":"2026-02-20T07:46:33.843240390Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1nl1","depends_on_id":"bd-3il","type":"blocks","created_at":"2026-02-20T07:46:33.888249500Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1nl1","depends_on_id":"bd-3qo","type":"blocks","created_at":"2026-02-20T07:46:33.933893504Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1nl1","depends_on_id":"bd-5rh","type":"blocks","created_at":"2026-02-20T07:46:33.978872949Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1nl1","depends_on_id":"bd-cda","type":"blocks","created_at":"2026-02-20T07:46:34.023526217Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1nl1","depends_on_id":"bd-n71","type":"blocks","created_at":"2026-02-20T07:46:34.070551974Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-1nmg","title":"Epic: CI/CD Pipeline Setup","description":"- type: task","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-20T07:47:58.072163029Z","updated_at":"2026-02-20T07:49:21.083126433Z","closed_at":"2026-02-20T07:49:21.083109191Z","close_reason":"Superseded by canonical detailed master-plan graph (bd-33v) with full 10.N-10.21 and 11-16 coverage, explicit dependencies, and per-section verification gates.","source_repo":".","compaction_level":0,"original_size":0}
{"id":"bd-1o4v","title":"[10.18] Implement proof-verification gate API for control-plane trust decisions.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.18 — Verifiable Execution Fabric Execution Track (9L)\n\nWhy This Exists:\nVEF track for proof-carrying runtime compliance: receipt chains, commitment checkpoints, proof generation/verification, and claim gating.\n\nTask Objective:\nImplement proof-verification gate API for control-plane trust decisions.\n\nAcceptance Criteria:\n- Verification gate validates proof, receipt-window commitment, and policy hash binding; invalid/missing proofs return stable fail-closed verdict classes.\n\nExpected Artifacts:\n- `src/trust/vef_verification_gate.rs`, `tests/security/vef_verification_gate.rs`, `artifacts/10.18/vef_verification_gate_report.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_18/bd-1o4v/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_18/bd-1o4v/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.18] Implement proof-verification gate API for control-plane trust decisions.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.18] Implement proof-verification gate API for control-plane trust decisions.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.18] Implement proof-verification gate API for control-plane trust decisions.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.18] Implement proof-verification gate API for control-plane trust decisions.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.18] Implement proof-verification gate API for control-plane trust decisions.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"- Verification gate validates proof, receipt-window commitment, and policy hash binding; invalid/missing proofs return stable fail-closed verdict classes.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:37:04.625740872Z","created_by":"ubuntu","updated_at":"2026-02-20T17:05:51.460115289Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-18","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-1o4v","depends_on_id":"bd-1u8m","type":"blocks","created_at":"2026-02-20T17:05:51.460063573Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-1oof","title":"[10.14] Attach trace-witness references to every high-impact ledger entry.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.14 — FrankenSQLite Deep-Mined Expansion Execution Track (9J)\n\nWhy This Exists:\nFrankenSQLite deep-mined control integrity track: evidence ledgers, proofs, epochs, remote effects, markers/MMR, and deterministic fault labs.\n\nTask Objective:\nAttach trace-witness references to every high-impact ledger entry.\n\nAcceptance Criteria:\n- High-impact evidence entries include stable trace witness IDs; witness references resolve in replay bundles; broken references fail integrity check.\n\nExpected Artifacts:\n- `tests/integration/evidence_trace_witness_linking.rs`, `docs/specs/witness_reference_contract.md`, `artifacts/10.14/witness_link_audit.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_14/bd-1oof/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_14/bd-1oof/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.14] Attach trace-witness references to every high-impact ledger entry.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.14] Attach trace-witness references to every high-impact ledger entry.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.14] Attach trace-witness references to every high-impact ledger entry.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.14] Attach trace-witness references to every high-impact ledger entry.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.14] Attach trace-witness references to every high-impact ledger entry.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"High-impact evidence entries include stable trace witness IDs; witness references resolve in replay bundles; broken references fail integrity check.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:36:55.386366656Z","created_by":"ubuntu","updated_at":"2026-02-20T15:44:12.103719246Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-14","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-1oof","depends_on_id":"bd-oolt","type":"blocks","created_at":"2026-02-20T07:43:14.345863254Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-1ow","title":"[PLAN 10.1] Charter + Split Governance","description":"Section: 10.1 — Charter + Split Governance\n\nStrategic Context:\nProgram charter and governance baseline: split contracts, reproducibility, and anti-regression rules for strategic integrity.\n\nExecution Requirements:\n- Preserve all scoped capabilities from the canonical plan.\n- Keep contracts self-contained so future contributors can execute without reopening the master plan.\n- Require explicit testing strategy (unit + integration/e2e) and structured logging/telemetry for every child bead.\n- Require artifact-backed validation for performance, security, and correctness claims.\n\nDependency Semantics:\n- This epic is blocked by its child implementation beads.\n- Cross-epic dependencies encode strategic sequencing and canonical ownership boundaries.\n\n## Success Criteria\n- All child beads for this section are completed with acceptance artifacts attached and no unresolved blockers.\n- Section-level verification gate for comprehensive unit tests, integration/e2e workflows, and detailed structured logging evidence is green.\n- Scope-to-plan traceability is explicit, with no feature loss versus the canonical plan section.\n\n## Optimization Notes\n- User-Outcome Lens: \"[PLAN 10.1] Charter + Split Governance\" must improve operator confidence, safety posture, and deterministic recovery behavior under both normal and adversarial conditions.\n- Cross-Section Coordination: child beads must encode integration assumptions explicitly to avoid local optimizations that degrade system-wide correctness.\n- Verification Non-Negotiable: completion requires reproducible unit/integration/E2E evidence and structured logs suitable for independent replay.\n- Scope Protection: any simplification must preserve canonical-plan feature intent and be justified with explicit tradeoff documentation.","status":"closed","priority":2,"issue_type":"epic","assignee":"CrimsonCrane","created_at":"2026-02-20T07:36:40.297308976Z","created_by":"ubuntu","updated_at":"2026-02-20T09:28:23.626442305Z","closed_at":"2026-02-20T09:28:23.626404394Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-1"],"dependencies":[{"issue_id":"bd-1ow","depends_on_id":"bd-16sk","type":"blocks","created_at":"2026-02-20T07:48:06.500281934Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1ow","depends_on_id":"bd-1j2","type":"blocks","created_at":"2026-02-20T07:36:43.404919513Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1ow","depends_on_id":"bd-1mj","type":"blocks","created_at":"2026-02-20T07:36:43.639150210Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1ow","depends_on_id":"bd-1pc","type":"blocks","created_at":"2026-02-20T07:36:43.794691404Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1ow","depends_on_id":"bd-20l","type":"blocks","created_at":"2026-02-20T07:36:43.716039320Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1ow","depends_on_id":"bd-2nd","type":"blocks","created_at":"2026-02-20T07:56:25.059990167Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1ow","depends_on_id":"bd-2zz","type":"blocks","created_at":"2026-02-20T07:36:43.484114548Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1ow","depends_on_id":"bd-4yv","type":"blocks","created_at":"2026-02-20T07:36:43.561726795Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1ow","depends_on_id":"bd-cda","type":"blocks","created_at":"2026-02-20T07:37:09.075949892Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1ow","depends_on_id":"bd-vjq","type":"blocks","created_at":"2026-02-20T07:36:43.325531699Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-1oyt","title":"[10.N] Implement dual-oracle completion close-condition gate","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.N — Execution Normalization Contract\n\nTask Objective:\nEncode the dual-oracle completion close condition as a machine-enforced gate: L1 product oracle (10.2), L2 engine-boundary oracle (10.17), and release-policy linkage must all be green.\n\nAcceptance Criteria:\n- Gate consumes L1, L2, and release-policy verdict artifacts.\n- Section/program close condition fails if any required oracle dimension is missing or red.\n- Gate output is deterministic and machine-readable for release automation.\n\nExpected Artifacts:\n- Dual-oracle close-condition policy contract.\n- Gate verdict artifact samples for pass/fail conditions.\n\nTesting & Logging Requirements:\n- Unit tests for gate logic and edge-state handling.\n- E2E tests simulating partial oracle success/failure combinations.\n- Structured gate logs with explicit failing dimension tags.\n\nTask-Specific Clarification:\n- For \"[10.N] Implement dual-oracle completion close-condition gate\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.N] Implement dual-oracle completion close-condition gate\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.N] Implement dual-oracle completion close-condition gate\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.N] Implement dual-oracle completion close-condition gate\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.N] Implement dual-oracle completion close-condition gate\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","status":"closed","priority":1,"issue_type":"task","assignee":"ubuntu","created_at":"2026-02-20T07:50:04.174660975Z","created_by":"ubuntu","updated_at":"2026-02-20T08:20:29.847950311Z","closed_at":"2026-02-20T08:20:29.847853451Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-N","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-1oyt","depends_on_id":"bd-2yhs","type":"blocks","created_at":"2026-02-20T07:50:04.306017356Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-1p2b","title":"[10.13] Implement control-plane retention policy (`required` vs `ephemeral`) and storage enforcement.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.13 — FCP Deep-Mined Expansion Execution Track (9I)\n\nWhy This Exists:\nDeep connector/provider contract track: lifecycle FSMs, conformance harnesses, fencing, sandboxing, revocation freshness, and protocol hardening.\n\nTask Objective:\nImplement control-plane retention policy (`required` vs `ephemeral`) and storage enforcement.\n\nAcceptance Criteria:\n- Retention class is mandatory per control-plane message type; required objects are durably stored; ephemeral objects can be dropped only under policy.\n\nExpected Artifacts:\n- `docs/specs/control_plane_retention.md`, `tests/conformance/retention_class_enforcement.rs`, `artifacts/10.13/retention_policy_matrix.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_13/bd-1p2b/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_13/bd-1p2b/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.13] Implement control-plane retention policy (`required` vs `ephemeral`) and storage enforcement.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.13] Implement control-plane retention policy (`required` vs `ephemeral`) and storage enforcement.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.13] Implement control-plane retention policy (`required` vs `ephemeral`) and storage enforcement.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.13] Implement control-plane retention policy (`required` vs `ephemeral`) and storage enforcement.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.13] Implement control-plane retention policy (`required` vs `ephemeral`) and storage enforcement.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","status":"closed","priority":1,"issue_type":"task","assignee":"CrimsonCrane","created_at":"2026-02-20T07:36:54.252038585Z","created_by":"ubuntu","updated_at":"2026-02-20T12:57:47.283310215Z","closed_at":"2026-02-20T12:57:47.283272364Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-13","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-1p2b","depends_on_id":"bd-3cm3","type":"blocks","created_at":"2026-02-20T07:43:13.725180262Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-1pc","title":"[10.1] Add implementation-governance policy that forbids line-by-line legacy translation and requires spec+fixture references in compatibility PRs.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.1 — Charter + Split Governance\n\nWhy This Exists:\nProgram charter and governance baseline: split contracts, reproducibility, and anti-regression rules for strategic integrity.\n\nTask Objective:\nAdd implementation-governance policy that forbids line-by-line legacy translation and requires spec+fixture references in compatibility PRs.\n\nAcceptance Criteria:\n- Implement with explicit success/failure invariants and deterministic behavior under normal, edge, and fault scenarios.\n- Ensure outcomes are verifiable via automated tests and reproducible artifact outputs.\n\nExpected Artifacts:\n- Section-owned design/spec artifact at `docs/specs/section_10_1/bd-1pc_contract.md`, capturing decision rationale, invariants, and interface boundaries.\n- Machine-readable verification artifact at `artifacts/section_10_1/bd-1pc/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_1/bd-1pc/verification_summary.md` linking implementation intent to measured outcomes.\n\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.1] Add implementation-governance policy that forbids line-by-line legacy translation and requires spec+fixture references in compatibility PRs.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.1] Add implementation-governance policy that forbids line-by-line legacy translation and requires spec+fixture references in compatibility PRs.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.1] Add implementation-governance policy that forbids line-by-line legacy translation and requires spec+fixture references in compatibility PRs.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.1] Add implementation-governance policy that forbids line-by-line legacy translation and requires spec+fixture references in compatibility PRs.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.1] Add implementation-governance policy that forbids line-by-line legacy translation and requires spec+fixture references in compatibility PRs.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","status":"closed","priority":1,"issue_type":"task","assignee":"CrimsonCrane","created_at":"2026-02-20T07:36:43.758632999Z","created_by":"ubuntu","updated_at":"2026-02-20T09:26:08.444045199Z","closed_at":"2026-02-20T09:26:08.444018219Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-1","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-1pc","depends_on_id":"bd-20l","type":"blocks","created_at":"2026-02-20T07:43:10.784299165Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-1pe0","title":"Epic: Dependency Graph Immune System (DGIS) [10.20]","description":"- type: epic","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-20T07:47:58.072163029Z","updated_at":"2026-02-20T07:49:21.316891385Z","closed_at":"2026-02-20T07:49:21.316873822Z","close_reason":"Superseded by canonical detailed master-plan graph (bd-33v) with full 10.N-10.21 and 11-16 coverage, explicit dependencies, and per-section verification gates.","source_repo":".","compaction_level":0,"original_size":0}
{"id":"bd-1pk","title":"Implement doctor command for environment diagnostics","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md (Bootstrap bridge for Sections 10.0, 10.8 operational readiness)\nSection: BOOTSTRAP (Operational readiness diagnostics bridge)\n\nTask Objective:\nImplement `franken-node doctor` as a deterministic environment diagnostics command that surfaces readiness/blockers across runtime, config, and extension-host prerequisites.\n\nIn Scope:\n- Environment/system prerequisite checks relevant to current bootstrap scope.\n- Snapshot/config/path integrity checks with actionable remediation hints.\n- Deterministic machine-readable doctor report output for CI and support workflows.\n\nAcceptance Criteria:\n- Doctor output clearly distinguishes pass/warn/fail states with stable codes.\n- Failure diagnostics include actionable remediation guidance and affected scope.\n- Command behavior is deterministic for equivalent environment/config states.\n\nExpected Artifacts:\n- Doctor checks matrix mapping each check to status codes and remediation text.\n- Sample doctor reports for healthy/degraded/failure environments.\n- CI-consumable diagnostic artifact format documentation.\n\n- Machine-readable verification artifact at `artifacts/section_bootstrap/bd-1pk/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_bootstrap/bd-1pk/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for each check module and status-code mapping.\n- Integration tests validating full report assembly and deterministic ordering.\n- E2E tests for representative degraded-environment scenarios.\n- Structured logs with check IDs, durations, verdicts, and trace correlation IDs.\n\nTask-Specific Clarification:\n- For \"Implement doctor command for environment diagnostics\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"Implement doctor command for environment diagnostics\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"Implement doctor command for environment diagnostics\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"Implement doctor command for environment diagnostics\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"Implement doctor command for environment diagnostics\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:29:22.294182970Z","created_by":"ubuntu","updated_at":"2026-02-20T08:46:43.632159515Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["cli","diagnostics"],"dependencies":[{"issue_id":"bd-1pk","depends_on_id":"bd-2lb","type":"blocks","created_at":"2026-02-20T08:04:16.423814955Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1pk","depends_on_id":"bd-3rp","type":"blocks","created_at":"2026-02-20T07:29:36.464286329Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1pk","depends_on_id":"bd-n9r","type":"blocks","created_at":"2026-02-20T07:29:36.528606668Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-1ps","title":"[PLAN 13] Program Success Criteria Instrumentation","description":"Section 13 success-metrics epic. Implement KPI instrumentation and claim gating for compatibility>=95%, migration>=3x, compromise reduction>=10x, replay coverage=100%, and external replication goals.\n\n## Success Criteria\n- All child beads for this section are completed with acceptance artifacts attached and no unresolved blockers.\n- Section-level verification gate for comprehensive unit tests, integration/e2e workflows, and detailed structured logging evidence is green.\n- Scope-to-plan traceability is explicit, with no feature loss versus the canonical plan section.\n\n## Optimization Notes\n- User-Outcome Lens: \"[PLAN 13] Program Success Criteria Instrumentation\" must improve real operator and end-user reliability, explainability, and time-to-safe-outcome under both normal and degraded conditions.\n- Cross-Section Coordination: this epic's child beads must explicitly encode integration expectations with neighboring sections to prevent local optimizations that break global behavior.\n- Verification Non-Negotiable: completion requires deterministic unit coverage, integration/E2E replay evidence, and structured logs sufficient for independent third-party reproduction and triage.\n- Scope Protection: any proposed simplification must prove feature parity against plan intent and document tradeoffs explicitly; silent scope reduction is forbidden.","status":"open","priority":1,"issue_type":"epic","created_at":"2026-02-20T07:36:42.164001900Z","created_by":"ubuntu","updated_at":"2026-02-20T08:12:47.234822646Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-13"],"dependencies":[{"issue_id":"bd-1ps","depends_on_id":"bd-1w78","type":"blocks","created_at":"2026-02-20T07:39:34.452259797Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1ps","depends_on_id":"bd-1xao","type":"blocks","created_at":"2026-02-20T07:39:34.707834911Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1ps","depends_on_id":"bd-20a","type":"blocks","created_at":"2026-02-20T07:38:35.593712559Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1ps","depends_on_id":"bd-229","type":"blocks","created_at":"2026-02-20T07:38:35.541405788Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1ps","depends_on_id":"bd-26k","type":"blocks","created_at":"2026-02-20T07:38:35.727766589Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1ps","depends_on_id":"bd-28sz","type":"blocks","created_at":"2026-02-20T07:39:34.911708132Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1ps","depends_on_id":"bd-2a4l","type":"blocks","created_at":"2026-02-20T07:39:34.540256587Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1ps","depends_on_id":"bd-2f43","type":"blocks","created_at":"2026-02-20T07:39:34.343763322Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1ps","depends_on_id":"bd-2l1k","type":"blocks","created_at":"2026-02-20T07:39:35.168572649Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1ps","depends_on_id":"bd-32p","type":"blocks","created_at":"2026-02-20T07:38:35.772665862Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1ps","depends_on_id":"bd-34d5","type":"blocks","created_at":"2026-02-20T08:02:26.065583281Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1ps","depends_on_id":"bd-37i","type":"blocks","created_at":"2026-02-20T07:38:35.900694378Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1ps","depends_on_id":"bd-39a","type":"blocks","created_at":"2026-02-20T07:38:35.816400385Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1ps","depends_on_id":"bd-3agp","type":"blocks","created_at":"2026-02-20T07:39:34.998996192Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1ps","depends_on_id":"bd-3cpa","type":"blocks","created_at":"2026-02-20T07:39:35.083603108Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1ps","depends_on_id":"bd-3e74","type":"blocks","created_at":"2026-02-20T07:39:34.816794629Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1ps","depends_on_id":"bd-3fo","type":"blocks","created_at":"2026-02-20T07:38:35.445801167Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1ps","depends_on_id":"bd-3il","type":"blocks","created_at":"2026-02-20T07:38:35.493770228Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1ps","depends_on_id":"bd-3rc","type":"blocks","created_at":"2026-02-20T07:38:35.642745913Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1ps","depends_on_id":"bd-c4f","type":"blocks","created_at":"2026-02-20T07:38:35.684597479Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1ps","depends_on_id":"bd-pga7","type":"blocks","created_at":"2026-02-20T07:39:34.624056279Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1ps","depends_on_id":"bd-whxp","type":"blocks","created_at":"2026-02-20T07:39:35.253274261Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1ps","depends_on_id":"bd-ybe","type":"blocks","created_at":"2026-02-20T07:38:35.858599369Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1ps","depends_on_id":"bd-z7bt","type":"blocks","created_at":"2026-02-20T07:48:29.681207780Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-1q38","title":"[10.20] Implement adversarial contagion simulator for xz-style and multi-stage supply-chain campaigns.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.20 — Dependency Graph Immune System Execution Track (9N)\n\nWhy This Exists:\nDGIS topological immune system track for dependency contagion modeling, choke-point immunization, and graph-aware containment economics.\n\nTask Objective:\nImplement adversarial contagion simulator for xz-style and multi-stage supply-chain campaigns.\n\nAcceptance Criteria:\n- Simulator supports campaign templates, probabilistic branching, and policy-conditioned propagation; runs are reproducible via fixed seeds and canonical scenario descriptors.\n\nExpected Artifacts:\n- `src/security/dgis/contagion_simulator.rs`, `tests/security/dgis_contagion_scenarios.rs`, `artifacts/10.20/dgis_contagion_simulation_report.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_20/bd-1q38/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_20/bd-1q38/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.20] Implement adversarial contagion simulator for xz-style and multi-stage supply-chain campaigns.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.20] Implement adversarial contagion simulator for xz-style and multi-stage supply-chain campaigns.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.20] Implement adversarial contagion simulator for xz-style and multi-stage supply-chain campaigns.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.20] Implement adversarial contagion simulator for xz-style and multi-stage supply-chain campaigns.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.20] Implement adversarial contagion simulator for xz-style and multi-stage supply-chain campaigns.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"- Simulator supports campaign templates, probabilistic branching, and policy-conditioned propagation; runs are reproducible via fixed seeds and canonical scenario descriptors.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:37:06.748790626Z","created_by":"ubuntu","updated_at":"2026-02-20T17:04:58.498124972Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-20","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-1q38","depends_on_id":"bd-t89w","type":"blocks","created_at":"2026-02-20T17:04:58.498075991Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-1qag","title":"Comprehensive Unit Test Suite","description":"- type: task","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-20T07:47:58.072163029Z","updated_at":"2026-02-20T07:49:21.327992592Z","closed_at":"2026-02-20T07:49:21.327971542Z","close_reason":"Superseded by canonical detailed master-plan graph (bd-33v) with full 10.N-10.21 and 11-16 coverage, explicit dependencies, and per-section verification gates.","source_repo":".","compaction_level":0,"original_size":0}
{"id":"bd-1qp","title":"[10.0] Implement compatibility envelope + divergence ledger.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.0 — Top 10 Initiative Tracking (Initiative #1, highest priority)\nCross-references: 9A.1, 9B.1, 9C.1, 9D.1\n\nWhy This Exists:\nThe compatibility envelope is the #1 strategic initiative. It creates a deterministic compatibility layer covering high-value Node/Bun behavior while making intentional divergences first-class, policy-visible, and signed. Without this, users cannot trust that franken_node faithfully reproduces expected behavior or understand where/why it intentionally differs.\n\nTask Objective:\nImplement the compatibility envelope (the boundary defining which Node/Bun APIs are covered and at what fidelity) together with the divergence ledger (an append-only, signed record of intentional behavioral differences with rationale).\n\nDetailed Acceptance Criteria:\n1. Compatibility envelope covers the four bands defined in 10.2: core, high-value, edge, unsafe, each with explicit policy defaults.\n2. Divergence ledger stores signed rationale entries for every intentional behavioral difference, with author, timestamp, affected API, severity, and justification fields.\n3. Each divergence entry is append-only and tamper-evident (hash-chained or Merkle-backed).\n4. Policy-visible: operators can query the ledger to understand all divergences for a given API surface, risk band, or profile.\n5. Integration with compatibility modes (strict, balanced, legacy-risky) — divergence handling varies by mode.\n6. Shim dispatch overhead profiled and reduced with precompiled decision DAGs where safe (9D.1).\n7. Typed-state transition primitives and session-type protocol checks applied to compatibility pathways (9B.1).\n8. Proof-carrying compatibility claims: each shim publishes invariance evidence and explicit divergence rationale (9C.1).\n\nKey Dependencies:\n- Depends on 10.2 (Compatibility Core) for band definitions and fixture runners.\n- Depends on 10.1 (Charter) for split-governance boundaries.\n- Depends on 10.N (Normalization) for canonical ownership rules.\n- Consumed by 10.3 (Migration) for risk assessment and 10.7 (Verification) for golden corpus.\n\nExpected Artifacts:\n- src/conformance/compatibility_envelope.rs — envelope definition and query API.\n- src/conformance/divergence_ledger.rs — signed divergence entry storage.\n- docs/specs/section_10_0/bd-1qp_contract.md — design rationale, invariants, interface boundaries.\n- artifacts/section_10_0/bd-1qp/verification_evidence.json — machine-readable CI gate.\n- artifacts/section_10_0/bd-1qp/verification_summary.md — human-readable summary.\n\nTesting and Logging Requirements:\n- Unit tests: envelope boundary queries, divergence entry creation/validation, hash chain integrity, mode-dependent divergence handling, band membership queries.\n- Integration tests: full workflow from API call -> envelope check -> divergence recording -> ledger query -> policy-visible output.\n- E2E tests: operator scenario exercising franken-node verify lockstep with divergence report generation.\n- Fuzz tests: malformed divergence entries, hash chain corruption detection.\n- Structured logs: stable event codes for COMPAT_ENVELOPE_QUERY, DIVERGENCE_RECORDED, DIVERGENCE_QUERIED, HASH_CHAIN_VERIFIED with trace correlation IDs.\n\nWhy This Improves User Outcomes:\n- Operators get deterministic, explainable answers about what behavior they can expect from franken_node vs Node/Bun.\n- Signed divergence rationale prevents silent behavior drift — every difference is intentional and auditable.\n- Policy-visible divergence data enables informed risk decisions during migration and rollout.\n- Proof-carrying claims (9C.1) enable external verification of compatibility assertions.","acceptance_criteria":"1. Envelope definition covers >= 95% of high-value Node.js API surface (fs, net, http, crypto, stream, child_process, path, os, events, buffer, url, util, assert, timers) measured by weighted API call frequency in npm-top-1000.\n2. Divergence ledger schema captures: API name, divergence type (semantic/signature/missing), severity (breaking/degraded/cosmetic), policy disposition (accepted/mitigated/blocked), signature of approver.\n3. Every intentional divergence has a signed entry with rationale, linked mitigation shim (if any), and policy-gate flag.\n4. Compatibility score computation is deterministic: same input manifest produces identical score across runs (no floating-point drift, no ordering sensitivity).\n5. Ledger entries are cryptographically signed and append-only; tampering detection via hash chain verification.\n6. CI gate rejects PRs that reduce overall compatibility score below the 95% category target threshold (Section 3).\n7. Divergence ledger is queryable via CLI (`franken-node compat divergences --format json`) and produces machine-readable output for downstream tooling.\n8. Replay coverage for envelope validation reaches 100% of catalogued divergences (Section 3 target).\n9. Cross-references to enhancement maps 9A.1, 9B.1, 9C.1, 9D.1 verified: each sub-deliverable traceable.\n10. Verification evidence artifact contains: compatibility score, divergence count by severity, ledger integrity hash, timestamp.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:36:42.485738572Z","created_by":"ubuntu","updated_at":"2026-02-20T15:35:17.412591453Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-0","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-1qp","depends_on_id":"bd-1hf","type":"blocks","created_at":"2026-02-20T07:46:29.385108753Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1qp","depends_on_id":"bd-1ow","type":"blocks","created_at":"2026-02-20T07:46:29.445817264Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1qp","depends_on_id":"bd-1ta","type":"blocks","created_at":"2026-02-20T07:46:29.507098101Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1qp","depends_on_id":"bd-1u9","type":"blocks","created_at":"2026-02-20T07:46:29.575562889Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1qp","depends_on_id":"bd-1wz","type":"blocks","created_at":"2026-02-20T07:46:29.631360965Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1qp","depends_on_id":"bd-1xg","type":"blocks","created_at":"2026-02-20T07:46:29.699988185Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1qp","depends_on_id":"bd-20a","type":"blocks","created_at":"2026-02-20T07:46:29.759040892Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1qp","depends_on_id":"bd-20z","type":"blocks","created_at":"2026-02-20T07:46:29.814011967Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1qp","depends_on_id":"bd-229","type":"blocks","created_at":"2026-02-20T07:46:29.882495770Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1qp","depends_on_id":"bd-26k","type":"blocks","created_at":"2026-02-20T07:46:29.948869653Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1qp","depends_on_id":"bd-32p","type":"blocks","created_at":"2026-02-20T07:46:30.005348757Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1qp","depends_on_id":"bd-37i","type":"blocks","created_at":"2026-02-20T07:46:30.076732061Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1qp","depends_on_id":"bd-39a","type":"blocks","created_at":"2026-02-20T07:46:30.142011806Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1qp","depends_on_id":"bd-3il","type":"blocks","created_at":"2026-02-20T07:46:30.207349830Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1qp","depends_on_id":"bd-3qo","type":"blocks","created_at":"2026-02-20T07:46:30.267897552Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1qp","depends_on_id":"bd-3rc","type":"blocks","created_at":"2026-02-20T07:46:30.337547547Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1qp","depends_on_id":"bd-5rh","type":"blocks","created_at":"2026-02-20T07:46:30.396154233Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1qp","depends_on_id":"bd-c4f","type":"blocks","created_at":"2026-02-20T07:46:30.456287573Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1qp","depends_on_id":"bd-cda","type":"blocks","created_at":"2026-02-20T07:46:30.521352479Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1qp","depends_on_id":"bd-go4","type":"blocks","created_at":"2026-02-20T07:46:30.584715574Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1qp","depends_on_id":"bd-n71","type":"blocks","created_at":"2026-02-20T07:46:30.640613455Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1qp","depends_on_id":"bd-ybe","type":"blocks","created_at":"2026-02-20T07:46:30.696473096Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-1qz","title":"Restore transplant snapshot files under transplant/pi_agent_rust","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md (Bootstrap bridge for migration/transplant integrity readiness)\n\nTask Objective:\nRestore the documented transplant snapshot under `transplant/pi_agent_rust` as the canonical input set for lockfile generation, drift detection, and provenance audits.\n\nIn Scope:\n- Rehydrate required snapshot files/directories exactly per documented inventory.\n- Validate inventory completeness and deterministic file layout.\n- Establish provenance notes (source revision/reference) for auditability.\n\nAcceptance Criteria:\n- Snapshot inventory is complete and matches documented expectations.\n- File layout and metadata are deterministic and reproducible across environments.\n- Downstream integrity beads (`bd-7rt`, `bd-29q`) can run without manual patching.\n\nExpected Artifacts:\n- Restored snapshot inventory report with counts and source references.\n- Canonical manifest of restored paths used by downstream integrity tooling.\n- Reproducibility note describing restoration procedure.\n\nTesting & Logging Requirements:\n- Unit tests (or deterministic validators) for inventory completeness checks.\n- Integration tests that validate downstream lockfile generation and drift detection consume restored snapshot correctly.\n- E2E test path that exercises restore -> lockfile -> drift workflow.\n- Structured logs recording restored path IDs, provenance refs, and validation outcomes.","notes":"Legacy transplant integrity prerequisite retained for continuity; upstream/master-plan workstreams should reference this when touching transplant provenance.","status":"closed","priority":1,"issue_type":"task","assignee":"ubuntu","created_at":"2026-02-20T07:26:05.311794442Z","created_by":"ubuntu","updated_at":"2026-02-20T08:08:46.889348412Z","closed_at":"2026-02-20T08:08:46.889259687Z","source_repo":".","compaction_level":0,"original_size":0}
{"id":"bd-1r2","title":"[10.10] Implement audience-bound token chains for control actions.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md — Section 10.10 (FCP-Inspired Hardening), cross-ref Section 9E.4\n\n## Why This Exists\n\nEnhancement Map 9E.4 mandates capability token delegation chains for migration and control-plane actions. In the three-kernel architecture, control actions (policy updates, migration triggers, zone reconfiguration) must carry cryptographic proof of authorization that is audience-bound — a token issued for one service/kernel cannot be replayed against another. Without audience-bound token chains, the no-ambient-authority invariant (8.5) is violated: any entity with a valid token could escalate its reach across kernel boundaries. This bead implements the product-level token chain system where each delegation step narrows scope, binds to a specific audience, and carries a verifiable chain of custody back to the original authority.\n\n## What This Must Do\n\n1. Define an `AudienceBoundToken` struct containing: issuer identity, audience (target service/kernel/zone), delegated capabilities (explicit allowlist), expiry (epoch-scoped), nonce, parent token hash (for chain verification), and signature over canonical preimage.\n2. Implement `TokenChain` — an ordered sequence of delegation tokens where each token's `parent_token_hash` links to its predecessor, capabilities are monotonically narrowing (each delegation can only reduce scope, never widen), and audience fields are non-empty.\n3. Provide `issue_token()`, `delegate_token()`, `verify_chain()`, and `check_audience()` APIs for product control-plane code to use when authorizing actions.\n4. Enforce that `delegate_token()` rejects any attempt to widen capabilities beyond what the parent token grants — strict attenuation invariant.\n5. Integrate audience checking into all control-plane action dispatch points: before executing any control action, the dispatcher verifies that the token chain's terminal audience matches the executing service identity.\n6. All token serialization uses the canonical serializer from bd-jjm; all chain verification checks divergence-free state from bd-2ms before accepting tokens from remote nodes.\n\n## Context from Enhancement Maps\n\n- 9E.4: \"Capability token delegation chains for migration and control-plane actions\"\n- 9E.5 (cross-ref): Key-role separation (bd-364) ensures that signing keys used for token issuance are distinct from encryption and operational keys.\n- 9E.1 (cross-ref): Canonical object identity (bd-1l5) provides the domain-separation tags used in token type identification.\n- 9B.3 (Migration): Migration operations require delegation tokens that prove the migration controller has authority scoped to the specific migration epoch and target.\n\n## Dependencies\n\n- Upstream: bd-2ms ([10.10] Implement rollback/fork detection in control-plane state propagation) — token acceptance requires divergence-free state verification.\n- Downstream: bd-364 ([10.10] Implement key-role separation for control-plane signing/encryption/issuance) — key-role separation builds on the token chain to ensure issuance keys are properly scoped.\n- Downstream: bd-1jjq ([10.10] Section-wide verification gate) — aggregates verification evidence.\n\n## Acceptance Criteria\n\n1. `AudienceBoundToken` includes all required fields (issuer, audience, capabilities, expiry, nonce, parent_hash, signature) with documented invariants.\n2. Token delegation strictly attenuates: any attempt to delegate a wider capability set than the parent token grants is rejected with `TOKEN_ATTENUATION_VIOLATION`.\n3. Audience mismatch is detected and rejected with `TOKEN_AUDIENCE_MISMATCH` before any control action executes — zero bypass paths verified by code audit.\n4. Token chains of depth 10+ verify correctly with sub-millisecond verification time per chain link.\n5. Expired tokens (past epoch boundary) are rejected with `TOKEN_EXPIRED` regardless of chain validity.\n6. Nonce uniqueness is enforced within an epoch — replaying a token with the same nonce is rejected with `TOKEN_REPLAY_DETECTED`.\n7. All token serialization routes through bd-jjm's canonical serializer — verified by golden vector cross-check.\n8. Verification evidence JSON includes chain depths tested, attenuation scenarios, and audience mismatch rejection counts.\n\n## Testing & Logging Requirements\n\n- Unit tests: Create token chains of depth 1, 5, and 20 and verify correct chain validation. Test strict attenuation: parent grants {read, write, admin}, child attempts {read, write, admin, superadmin} — must reject. Test audience binding: token for \"kernel-A\" presented to \"kernel-B\" — must reject. Test expiry at epoch boundary. Test nonce replay detection. Test empty capability set (should be valid — a fully attenuated token grants nothing).\n- Integration tests: Simulate a migration workflow where a migration controller issues a scoped token to a worker, the worker delegates a further-narrowed token to a sub-task, and the final action is authorized against the full chain. Verify that token verification checks divergence state (bd-2ms) before accepting remote tokens.\n- Adversarial tests: Attempt to forge a token chain by inserting a token with a fake parent_hash. Attempt to widen capabilities in the middle of a chain. Attempt cross-audience token replay. Attempt to use a token after its epoch expires but before garbage collection. Test with corrupted signature bytes.\n- Structured logs: `TOKEN_ISSUED` (issuer, audience, capability_count, expiry_epoch, chain_depth). `TOKEN_DELEGATED` (delegator, new_audience, attenuated_capabilities, new_chain_depth). `TOKEN_VERIFIED` (chain_depth, audience_match, verification_duration_us). `TOKEN_REJECTED` (reason, attempted_audience, actual_audience, chain_depth). All events include `trace_id`, `epoch_id`, and `action_id`.\n\n## Expected Artifacts\n\n- docs/specs/section_10_10/bd-1r2_contract.md\n- crates/franken-node/src/connector/token_chain.rs (or similar module path)\n- scripts/check_token_chain.py with --json flag and self_test()\n- tests/test_check_token_chain.py\n- artifacts/section_10_10/bd-1r2/verification_evidence.json\n- artifacts/section_10_10/bd-1r2/verification_summary.md","acceptance_criteria":"1. Define an AudienceBoundToken struct containing: (a) token_id (TrustObjectId with TOKEN domain), (b) issuer_key_id (TrustObjectId with KEY domain), (c) audience (a list of 1+ TrustObjectId values identifying the intended recipients/zones), (d) action_scope (enum: MIGRATE, ROLLBACK, PROMOTE, REVOKE, CONFIGURE), (e) issued_at (UTC timestamp), (f) expires_at (UTC timestamp), (g) delegation_parent (Option<token_id> for chained delegation, None for root tokens), (h) max_delegation_depth (u8, default 0 = no further delegation).\n2. Implement token chain validation: given a chain of tokens [root, delegate1, delegate2, ...], verify: (a) each delegate's delegation_parent matches the previous token_id, (b) chain length <= root's max_delegation_depth + 1, (c) each delegate's audience is a subset of its parent's audience (no audience escalation), (d) each delegate's action_scope is a subset of its parent's action_scope, (e) no token in the chain is expired at the evaluation timestamp.\n3. Implement audience binding check: given a token and a requester identity (TrustObjectId), verify the requester appears in the token's audience list. Reject with AudienceMismatch error.\n4. Implement scope narrowing: a delegated token MUST NOT grant action_scopes not present in the parent. Return ScopeEscalation error on violation.\n5. Reject tokens where issued_at >= expires_at (zero or negative validity window).\n6. Reject delegation chains where any intermediate token is expired even if the leaf is not.\n7. Unit tests: (a) valid root token creation, (b) valid single-hop delegation, (c) valid multi-hop delegation, (d) audience escalation rejection, (e) scope escalation rejection, (f) depth limit exceeded, (g) expired intermediate rejection, (h) zero-validity rejection, (i) audience mismatch.\n8. Golden fixture: a 3-level delegation chain in vectors/audience_bound_tokens.json with known pass/fail cases.\n9. Verification: scripts/check_token_chains.py --json, artifacts at artifacts/section_10_10/bd-1r2/.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:36:49.081452553Z","created_by":"ubuntu","updated_at":"2026-02-20T17:01:30.685447225Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-10","self-contained","test-obligations"]}
{"id":"bd-1rbl","title":"Epic: Asupersync Ownership + Boundaries [10.15a]","description":"- type: epic","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-20T07:47:58.072163029Z","updated_at":"2026-02-20T07:49:21.256050204Z","closed_at":"2026-02-20T07:49:21.256031920Z","close_reason":"Superseded by canonical detailed master-plan graph (bd-33v) with full 10.N-10.21 and 11-16 coverage, explicit dependencies, and per-section verification gates.","source_repo":".","compaction_level":0,"original_size":0}
{"id":"bd-1rff","title":"[12] Risk control: longitudinal privacy/re-identification","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 12\n\nTask Objective:\nImplement privacy-preserving trajectory sketching and cohort-size publication thresholds with federated temporal checks.\n\nExecution Requirements:\n- Make this requirement machine-verifiable and release-gated where applicable.\n- Keep behavior self-documenting so future contributors do not need to re-open the master plan.\n\nTesting & Logging Requirements:\n- Unit tests for rule/contract logic and error conditions.\n- Integration/E2E tests for workflow-level enforcement.\n- Structured logs with stable codes and trace IDs.\n- Reproducible artifacts that prove contract satisfaction.\n\nAcceptance Criteria:\n- Success and failure invariants for [12] Risk control: longitudinal privacy/re-identification are explicitly quantified and machine-verifiable.\n- Determinism requirements for [12] Risk control: longitudinal privacy/re-identification are validated across repeated runs and adversarial perturbations.\n\n\nExpected Artifacts:\n- Machine-readable verification artifact with explicit canonical path under artifacts/section_12/bd-1rff/verification_evidence.json, suitable for CI/release gating.\n- Human-readable verification summary with explicit canonical path under artifacts/section_12/bd-1rff/verification_summary.md, linking implementation intent to measured validation outcomes.\n\nTask-Specific Clarification:\n- The implementation must deliver the exact capability named in the title, with no scope dilution and no silent feature omission.\n- Validation artifacts must include capability-specific thresholds, pass/fail criteria, and reproducible evidence bundles tied to this bead ID.\n- Program/section verification gates must be able to consume this bead's outputs without manual interpretation.\n\nWhy This Improves User Outcomes:\n- \"[12] Risk control: longitudinal privacy/re-identification\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[12] Risk control: longitudinal privacy/re-identification\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"RISK: Longitudinal trajectory privacy/re-identification — accumulated behavioral data over time enables re-identification of individual nodes or users.\nIMPACT: Privacy violation through trajectory analysis, de-anonymization of supposedly anonymous participants, regulatory exposure.\nCOUNTERMEASURES:\n  (a) Privacy-preserving sketching: behavioral trajectories are represented as lossy sketches (e.g., count-min sketch, HyperLogLog) that prevent exact reconstruction.\n  (b) Minimum cohort-size thresholds: no query or aggregation is served if the underlying cohort has fewer than k participants (k >= 50).\n  (c) Temporal aggregation: individual time-series data is aggregated into epoch buckets (minimum 1-hour granularity) to prevent fine-grained tracking.\nVERIFICATION:\n  1. Sketch-based representation: raw trajectories are not stored; only sketches are persisted. Verified by attempting to reconstruct exact trajectory from sketch and confirming failure.\n  2. k-anonymity: all query results are filtered by cohort size >= 50; queries on smaller cohorts return empty/error.\n  3. Temporal granularity: no stored data has resolution finer than 1-hour epochs.\n  4. Re-identification test: given 1000 sketches, an adversary with auxiliary data cannot link > 1% of sketches to individuals.\nTEST SCENARIOS:\n  - Scenario A: Store 100 trajectories as sketches; attempt to reconstruct any single trajectory; verify failure.\n  - Scenario B: Query a cohort of 30 participants; verify response is blocked with 'insufficient cohort size' error.\n  - Scenario C: Attempt to store sub-hour-granularity data; verify it is automatically bucketed to 1-hour epochs.\n  - Scenario D: Run linkage attack with full auxiliary data on 1000 sketches; verify success rate < 1%.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:39:34.126779671Z","created_by":"ubuntu","updated_at":"2026-02-20T15:20:24.510712927Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-12","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-1rff","depends_on_id":"bd-v4ps","type":"blocks","created_at":"2026-02-20T07:43:25.171374050Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-1rk","title":"[10.13] Add lifecycle-aware health gating and rollout-state persistence for every connector instance.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.13 — FCP Deep-Mined Expansion Execution Track (9I)\n\nWhy This Exists:\nDeep connector/provider contract track: lifecycle FSMs, conformance harnesses, fencing, sandboxing, revocation freshness, and protocol hardening.\n\nTask Objective:\nAdd lifecycle-aware health gating and rollout-state persistence for every connector instance.\n\nAcceptance Criteria:\n- Activation requires lifecycle + health gate satisfaction; rollout state survives restart and failover; recovery replay reproduces same state.\n\nExpected Artifacts:\n- `docs/specs/rollout_state_machine.md`, `tests/integration/lifecycle_health_gate.rs`, `artifacts/10.13/rollout_state_replay.log`.\n\n- Machine-readable verification artifact at `artifacts/section_10_13/bd-1rk/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_13/bd-1rk/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.13] Add lifecycle-aware health gating and rollout-state persistence for every connector instance.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.13] Add lifecycle-aware health gating and rollout-state persistence for every connector instance.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.13] Add lifecycle-aware health gating and rollout-state persistence for every connector instance.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.13] Add lifecycle-aware health gating and rollout-state persistence for every connector instance.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.13] Add lifecycle-aware health gating and rollout-state persistence for every connector instance.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","status":"closed","priority":1,"issue_type":"task","assignee":"CrimsonCrane","created_at":"2026-02-20T07:36:51.474592690Z","created_by":"ubuntu","updated_at":"2026-02-20T10:36:52.823494220Z","closed_at":"2026-02-20T10:36:52.823465837Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-13","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-1rk","depends_on_id":"bd-2gh","type":"blocks","created_at":"2026-02-20T07:43:12.274386354Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-1ru2","title":"[10.14] Implement cancel-safe eviction saga (upload -> verify -> retire) with deterministic compensations.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.14 — FrankenSQLite Deep-Mined Expansion Execution Track (9J)\n\nWhy This Exists:\nFrankenSQLite deep-mined control integrity track: evidence ledgers, proofs, epochs, remote effects, markers/MMR, and deterministic fault labs.\n\nTask Objective:\nImplement cancel-safe eviction saga (upload -> verify -> retire) with deterministic compensations.\n\nAcceptance Criteria:\n- Saga guarantees no partial retire on cancellation/crash; compensation path is deterministic; leak tests confirm zero orphan states.\n\nExpected Artifacts:\n- `docs/specs/eviction_saga.md`, `tests/integration/eviction_saga_cancel_safety.rs`, `artifacts/10.14/eviction_saga_trace.jsonl`.\n\n- Machine-readable verification artifact at `artifacts/section_10_14/bd-1ru2/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_14/bd-1ru2/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.14] Implement cancel-safe eviction saga (upload -> verify -> retire) with deterministic compensations.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.14] Implement cancel-safe eviction saga (upload -> verify -> retire) with deterministic compensations.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.14] Implement cancel-safe eviction saga (upload -> verify -> retire) with deterministic compensations.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.14] Implement cancel-safe eviction saga (upload -> verify -> retire) with deterministic compensations.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.14] Implement cancel-safe eviction saga (upload -> verify -> retire) with deterministic compensations.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"Saga guarantees no partial retire on cancellation/crash; compensation path is deterministic; leak tests confirm zero orphan states.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:36:57.569198947Z","created_by":"ubuntu","updated_at":"2026-02-20T16:24:04.735805813Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-14","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-1ru2","depends_on_id":"bd-1fck","type":"blocks","created_at":"2026-02-20T07:43:15.442435676Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1ru2","depends_on_id":"bd-1nfu","type":"blocks","created_at":"2026-02-20T16:24:04.735728519Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-1rwq","title":"[10.7] Section-wide verification gate: comprehensive unit+e2e+logging","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.7\n\nTask Objective:\nCreate a hard section completion gate that verifies all implemented behavior in this section with comprehensive unit tests, integration/e2e scripts, and detailed structured logging evidence.\n\nAcceptance Criteria:\n- Section test matrix covers happy-path, edge-case, and adversarial/error-path scenarios for all section deliverables.\n- E2E scripts execute representative end-user workflows and policy/control-plane transitions for this section.\n- Structured logs include stable event/error codes, trace correlation IDs, and enough context for deterministic replay triage.\n- Verification report is deterministic and machine-readable for CI/release gating.\n\nExpected Artifacts:\n- Section-level test matrix and coverage summary artifact.\n- E2E script suite with pass/fail outputs and reproducible fixtures/seeds.\n- Structured log validation report and traceability bundle.\n- Gate verdict artifact consumable by release automation.\n\n- Machine-readable verification artifact at `artifacts/section_10_7/bd-1rwq/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_7/bd-1rwq/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests must validate contracts, invariants, and failure semantics.\n- E2E tests must validate cross-component behavior from user/operator entrypoints to persisted effects.\n- Logging must support root-cause analysis without hidden context requirements.\n\nTask-Specific Clarification:\n- For \"[10.7] Section-wide verification gate: comprehensive unit+e2e+logging\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.7] Section-wide verification gate: comprehensive unit+e2e+logging\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.7] Section-wide verification gate: comprehensive unit+e2e+logging\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.7] Section-wide verification gate: comprehensive unit+e2e+logging\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.7] Section-wide verification gate: comprehensive unit+e2e+logging\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"1. Section 10.7 gate aggregates pass/fail status from all sibling beads (bd-2ja, bd-s6y, bd-1ul, bd-1u4, bd-3ex, bd-2pu).\n2. Gate script (scripts/check_section_10_7_gate.py) runs all section verification scripts and produces a unified JSON report.\n3. Gate fails if any sibling bead's verification evidence is missing or shows a failure.\n4. Unit tests cover gate logic: all-pass, single-fail, and missing-evidence scenarios.\n5. Gate produces a section-level summary under artifacts/ with per-bead status, timestamps, and links to individual evidence files.\n6. E2E logging: gate execution emits structured log lines for each sub-check with bead ID, result, and duration.\n7. Gate is idempotent: running it twice in succession with no changes produces identical output.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:48:25.395020464Z","created_by":"ubuntu","updated_at":"2026-02-20T15:17:48.141256904Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-7","test-obligations","verification-gate"],"dependencies":[{"issue_id":"bd-1rwq","depends_on_id":"bd-1dpd","type":"blocks","created_at":"2026-02-20T08:24:24.738723630Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1rwq","depends_on_id":"bd-1u4","type":"blocks","created_at":"2026-02-20T07:48:25.600772869Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1rwq","depends_on_id":"bd-1ul","type":"blocks","created_at":"2026-02-20T07:48:25.666133146Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1rwq","depends_on_id":"bd-2ja","type":"blocks","created_at":"2026-02-20T07:48:25.766621101Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1rwq","depends_on_id":"bd-2pu","type":"blocks","created_at":"2026-02-20T07:48:25.495331970Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1rwq","depends_on_id":"bd-2twu","type":"blocks","created_at":"2026-02-20T08:20:50.298599714Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1rwq","depends_on_id":"bd-3ex","type":"blocks","created_at":"2026-02-20T07:48:25.545846777Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1rwq","depends_on_id":"bd-s6y","type":"blocks","created_at":"2026-02-20T07:48:25.718349612Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-1sgr","title":"[16] Output contract: multiple reproducible technical reports","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 16\n\nTask Objective:\nDeliver multiple publishable reports with reproducible artifact bundles.\n\nExecution Requirements:\n- Make this requirement machine-verifiable and release-gated where applicable.\n- Keep behavior self-documenting so future contributors do not need to re-open the master plan.\n\nTesting & Logging Requirements:\n- Unit tests for rule/contract logic and error conditions.\n- Integration/E2E tests for workflow-level enforcement.\n- Structured logs with stable codes and trace IDs.\n- Reproducible artifacts that prove contract satisfaction.\n\nAcceptance Criteria:\n- Success and failure invariants for [16] Output contract: multiple reproducible technical reports are explicitly quantified and machine-verifiable.\n- Determinism requirements for [16] Output contract: multiple reproducible technical reports are validated across repeated runs and adversarial perturbations.\n\n\nExpected Artifacts:\n- Machine-readable verification artifact with explicit canonical path under artifacts/section_16/bd-1sgr/verification_evidence.json, suitable for CI/release gating.\n- Human-readable verification summary with explicit canonical path under artifacts/section_16/bd-1sgr/verification_summary.md, linking implementation intent to measured validation outcomes.\n\nTask-Specific Clarification:\n- The implementation must deliver the exact capability named in the title, with no scope dilution and no silent feature omission.\n- Validation artifacts must include capability-specific thresholds, pass/fail criteria, and reproducible evidence bundles tied to this bead ID.\n- Program/section verification gates must be able to consume this bead's outputs without manual interpretation.\n\nWhy This Improves User Outcomes:\n- \"[16] Output contract: multiple reproducible technical reports\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[16] Output contract: multiple reproducible technical reports\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"1. At least 3 reproducible technical reports published covering distinct aspects: (a) compatibility/migration system, (b) trust/security system, (c) benchmark/verification methodology.\n2. Each report includes: (a) complete methodology section enabling independent replication, (b) all data and scripts required to reproduce results (published alongside report), (c) reproduction instructions tested on a clean environment, (d) expected results with tolerance bounds.\n3. Reproducibility verified: at least 1 report has been independently reproduced by an external party with results within 10% of original.\n4. Reports are formatted for academic/industry publication: abstract, introduction, related work, methodology, results, discussion, conclusion, references.\n5. Reports are submitted to at least 1 venue: academic conference, industry journal, or technical blog with peer review.\n6. All reports carry a reproducibility badge or statement indicating level of reproducibility achieved.\n7. Evidence: reproducible_report_registry.json with per-report: title, topic, reproduction status, venue, and external reproduction results.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:39:37.216637449Z","created_by":"ubuntu","updated_at":"2026-02-20T15:28:57.921783116Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-16","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-1sgr","depends_on_id":"bd-10ee","type":"blocks","created_at":"2026-02-20T07:43:26.769758348Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-1ta","title":"[PLAN 10.13] FCP Deep-Mined Expansion Execution Track (9I)","description":"Section: 10.13 — FCP Deep-Mined Expansion Execution Track (9I)\n\nStrategic Context:\nDeep connector/provider contract track: lifecycle FSMs, conformance harnesses, fencing, sandboxing, revocation freshness, and protocol hardening.\n\nExecution Requirements:\n- Preserve all scoped capabilities from the canonical plan.\n- Keep contracts self-contained so future contributors can execute without reopening the master plan.\n- Require explicit testing strategy (unit + integration/e2e) and structured logging/telemetry for every child bead.\n- Require artifact-backed validation for performance, security, and correctness claims.\n\nDependency Semantics:\n- This epic is blocked by its child implementation beads.\n- Cross-epic dependencies encode strategic sequencing and canonical ownership boundaries.\n\n## Success Criteria\n- All child beads for this section are completed with acceptance artifacts attached and no unresolved blockers.\n- Section-level verification gate for comprehensive unit tests, integration/e2e workflows, and detailed structured logging evidence is green.\n- Scope-to-plan traceability is explicit, with no feature loss versus the canonical plan section.\n\n## Optimization Notes\n- User-Outcome Lens: \"[PLAN 10.13] FCP Deep-Mined Expansion Execution Track (9I)\" must improve operator confidence, safety posture, and deterministic recovery behavior under both normal and adversarial conditions.\n- Cross-Section Coordination: child beads must encode integration assumptions explicitly to avoid local optimizations that degrade system-wide correctness.\n- Verification Non-Negotiable: completion requires reproducible unit/integration/E2E evidence and structured logs suitable for independent replay.\n- Scope Protection: any simplification must preserve canonical-plan feature intent and be justified with explicit tradeoff documentation.","status":"closed","priority":1,"issue_type":"epic","created_at":"2026-02-20T07:36:41.285268229Z","created_by":"ubuntu","updated_at":"2026-02-20T14:58:29.693108668Z","closed_at":"2026-02-20T14:58:29.693081257Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-13"],"dependencies":[{"issue_id":"bd-1ta","depends_on_id":"bd-12h8","type":"blocks","created_at":"2026-02-20T07:36:54.368428342Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1ta","depends_on_id":"bd-17mb","type":"blocks","created_at":"2026-02-20T07:36:52.489455475Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1ta","depends_on_id":"bd-18o","type":"blocks","created_at":"2026-02-20T07:36:51.755584378Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1ta","depends_on_id":"bd-19u","type":"blocks","created_at":"2026-02-20T07:36:51.917227447Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1ta","depends_on_id":"bd-1cm","type":"blocks","created_at":"2026-02-20T07:36:51.836601837Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1ta","depends_on_id":"bd-1d7n","type":"blocks","created_at":"2026-02-20T07:36:52.906227178Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1ta","depends_on_id":"bd-1gnb","type":"blocks","created_at":"2026-02-20T07:36:54.775278144Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1ta","depends_on_id":"bd-1h6","type":"blocks","created_at":"2026-02-20T07:36:51.594085677Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1ta","depends_on_id":"bd-1m8r","type":"blocks","created_at":"2026-02-20T07:36:53.149339088Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1ta","depends_on_id":"bd-1nk5","type":"blocks","created_at":"2026-02-20T07:36:52.405095099Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1ta","depends_on_id":"bd-1ow","type":"blocks","created_at":"2026-02-20T07:37:11.011553523Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1ta","depends_on_id":"bd-1p2b","type":"blocks","created_at":"2026-02-20T07:36:54.288673444Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1ta","depends_on_id":"bd-1rk","type":"blocks","created_at":"2026-02-20T07:36:51.514993544Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1ta","depends_on_id":"bd-1ugy","type":"blocks","created_at":"2026-02-20T07:36:54.614852151Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1ta","depends_on_id":"bd-1vvs","type":"blocks","created_at":"2026-02-20T07:36:52.240693691Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1ta","depends_on_id":"bd-1z9s","type":"blocks","created_at":"2026-02-20T07:36:52.736543932Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1ta","depends_on_id":"bd-24s","type":"blocks","created_at":"2026-02-20T07:36:51.997880568Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1ta","depends_on_id":"bd-29ct","type":"blocks","created_at":"2026-02-20T07:36:55.016580493Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1ta","depends_on_id":"bd-29w6","type":"blocks","created_at":"2026-02-20T07:36:53.799851866Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1ta","depends_on_id":"bd-2eun","type":"blocks","created_at":"2026-02-20T07:36:54.124335315Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1ta","depends_on_id":"bd-2gh","type":"blocks","created_at":"2026-02-20T07:36:51.430546205Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1ta","depends_on_id":"bd-2k74","type":"blocks","created_at":"2026-02-20T07:36:53.960331227Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1ta","depends_on_id":"bd-2m2b","type":"blocks","created_at":"2026-02-20T07:36:52.322919772Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1ta","depends_on_id":"bd-2t5u","type":"blocks","created_at":"2026-02-20T07:36:53.716832437Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1ta","depends_on_id":"bd-2vs4","type":"blocks","created_at":"2026-02-20T07:36:53.392506551Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1ta","depends_on_id":"bd-2yc4","type":"blocks","created_at":"2026-02-20T07:36:52.987816072Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1ta","depends_on_id":"bd-35by","type":"blocks","created_at":"2026-02-20T07:36:54.935988516Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1ta","depends_on_id":"bd-35q1","type":"blocks","created_at":"2026-02-20T07:36:52.655405537Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1ta","depends_on_id":"bd-3b8m","type":"blocks","created_at":"2026-02-20T07:36:54.040955064Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1ta","depends_on_id":"bd-3cm3","type":"blocks","created_at":"2026-02-20T07:36:54.208054036Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1ta","depends_on_id":"bd-3en","type":"blocks","created_at":"2026-02-20T07:36:51.674458095Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1ta","depends_on_id":"bd-3i9o","type":"blocks","created_at":"2026-02-20T07:36:52.820856339Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1ta","depends_on_id":"bd-3n2u","type":"blocks","created_at":"2026-02-20T07:36:55.096274577Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1ta","depends_on_id":"bd-3n58","type":"blocks","created_at":"2026-02-20T07:36:52.573199233Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1ta","depends_on_id":"bd-3tzl","type":"blocks","created_at":"2026-02-20T07:36:54.534100497Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1ta","depends_on_id":"bd-3ua7","type":"blocks","created_at":"2026-02-20T07:36:52.160058995Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1ta","depends_on_id":"bd-3uoo","type":"blocks","created_at":"2026-02-20T07:48:11.356690640Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1ta","depends_on_id":"bd-8uvb","type":"blocks","created_at":"2026-02-20T07:36:53.472587026Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1ta","depends_on_id":"bd-8vby","type":"blocks","created_at":"2026-02-20T07:36:53.554148338Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1ta","depends_on_id":"bd-91gg","type":"blocks","created_at":"2026-02-20T07:36:53.879952197Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1ta","depends_on_id":"bd-b44","type":"blocks","created_at":"2026-02-20T07:36:52.077621159Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1ta","depends_on_id":"bd-bq6y","type":"blocks","created_at":"2026-02-20T07:36:53.312328605Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1ta","depends_on_id":"bd-cda","type":"blocks","created_at":"2026-02-20T07:37:09.546802079Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1ta","depends_on_id":"bd-ck2h","type":"blocks","created_at":"2026-02-20T07:36:54.855586673Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1ta","depends_on_id":"bd-jxgt","type":"blocks","created_at":"2026-02-20T07:36:53.637535011Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1ta","depends_on_id":"bd-novi","type":"blocks","created_at":"2026-02-20T07:36:54.695090870Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1ta","depends_on_id":"bd-v97o","type":"blocks","created_at":"2026-02-20T07:36:54.453073599Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1ta","depends_on_id":"bd-w0jq","type":"blocks","created_at":"2026-02-20T07:36:53.231142371Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1ta","depends_on_id":"bd-y7lu","type":"blocks","created_at":"2026-02-20T07:36:53.068068396Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-1tnu","title":"[10.20] Implement trust barrier primitives and policy wiring (behavioral sandbox escalation, composition firewall, verified-fork pinning, staged rollout fences).","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.20 — Dependency Graph Immune System Execution Track (9N)\n\nWhy This Exists:\nDGIS topological immune system track for dependency contagion modeling, choke-point immunization, and graph-aware containment economics.\n\nTask Objective:\nImplement trust barrier primitives and policy wiring (behavioral sandbox escalation, composition firewall, verified-fork pinning, staged rollout fences).\n\nAcceptance Criteria:\n- Barrier primitives are independently testable and composable; policy engine can enforce barrier sets at designated choke points with deterministic overrides and audit receipts.\n\nExpected Artifacts:\n- `src/security/dgis/barrier_primitives.rs`, `tests/integration/dgis_barrier_enforcement.rs`, `artifacts/10.20/dgis_barrier_enforcement_trace.jsonl`.\n\n- Machine-readable verification artifact at `artifacts/section_10_20/bd-1tnu/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_20/bd-1tnu/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.20] Implement trust barrier primitives and policy wiring (behavioral sandbox escalation, composition firewall, verified-fork pinning, staged rollout fences).\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.20] Implement trust barrier primitives and policy wiring (behavioral sandbox escalation, composition firewall, verified-fork pinning, staged rollout fences).\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.20] Implement trust barrier primitives and policy wiring (behavioral sandbox escalation, composition firewall, verified-fork pinning, staged rollout fences).\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.20] Implement trust barrier primitives and policy wiring (behavioral sandbox escalation, composition firewall, verified-fork pinning, staged rollout fences).\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.20] Implement trust barrier primitives and policy wiring (behavioral sandbox escalation, composition firewall, verified-fork pinning, staged rollout fences).\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"- Barrier primitives are independently testable and composable; policy engine can enforce barrier sets at designated choke points with deterministic overrides and audit receipts.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:37:06.914671199Z","created_by":"ubuntu","updated_at":"2026-02-20T17:00:55.935700271Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-20","self-contained","test-obligations"]}
{"id":"bd-1u4","title":"[10.7] Add metamorphic tests for compatibility invariants.","description":"## [10.7] Metamorphic Tests for Compatibility Invariants\n\n### Why This Exists\n\nTraditional testing validates specific input-output pairs, but compatibility invariants are relational properties that hold across families of inputs. Metamorphic testing validates that certain transformations of inputs preserve expected relationships in outputs, catching bugs that point-wise tests miss. For franken_node, the core compatibility promise is behavioral equivalence with Node.js — this is inherently a metamorphic property. This bead introduces metamorphic testing to systematically verify compatibility invariants that cannot be expressed as simple assertion-based tests.\n\n### What It Must Do\n\n**Define metamorphic relations**: Identify and formalize the key metamorphic relations for compatibility:\n\n1. *Equivalence relation*: If API X produces output Y in Node.js, then API X must produce output Y in franken_node (modulo documented divergences). Input transformation: identity. Output relation: equality after normalization.\n2. *Monotonicity relation*: Extending an API call with additional optional parameters must not break backward compatibility. Input transformation: add optional parameters. Output relation: original output fields unchanged.\n3. *Idempotency relation*: Applying a migration twice produces the same result as applying it once. Input transformation: repeat operation. Output relation: equality.\n4. *Commutativity of independent operations*: Independent policy evaluations produce the same results regardless of execution order. Input transformation: permute order. Output relation: set equality.\n\n**Test generator**: A metamorphic test generator takes a base input, applies transformations to produce metamorphic input pairs, executes both inputs, and validates the expected relation holds. The generator supports pluggable relations and transformations so new invariants can be added without modifying the framework.\n\n**Corpus of base inputs**: A curated set of base inputs (at least 100) covering diverse API usage patterns, migration scenarios, and policy configurations. Base inputs are stored in `tests/metamorphic/corpus/`.\n\n**Relation violation reporting**: When a metamorphic relation is violated, the report includes: the base input, the transformation applied, the expected relation, the actual outputs from both executions, and the specific field or value where the relation broke. This makes debugging straightforward.\n\n### Acceptance Criteria\n\n1. At least 4 metamorphic relations are formally defined and implemented: equivalence, monotonicity, idempotency, and commutativity.\n2. A metamorphic test generator produces input pairs from base inputs and validates relations, with pluggable relation/transformation support.\n3. Base input corpus contains at least 100 inputs covering API usage, migration scenarios, and policy configurations.\n4. Relation violation reports include: base input, transformation, expected relation, actual outputs, and specific divergence point.\n5. CI gate runs metamorphic test suite and fails on any relation violation.\n6. New metamorphic relations can be added by implementing a relation interface without modifying the generator framework.\n7. Verification script `scripts/check_metamorphic_tests.py` with `--json` flag validates relation coverage and violation detection.\n8. Unit tests in `tests/test_check_metamorphic_tests.py` cover relation validation logic, generator correctness, corpus loading, and violation report formatting.\n\n### Key Dependencies\n\n- Compatibility verification from 10.2 (for equivalence relation testing against Node.js).\n- Migration scanner from 10.3 (for idempotency relation).\n- Policy evaluation from 10.4 (for commutativity relation).\n- Lockstep harness from 10.2 (for cross-runtime execution).\n\n### Testing & Logging Requirements\n\n- Relation pass test: run each relation with known-compatible inputs, assert all pass.\n- Relation fail test: inject a known violation for each relation, assert detection and correct report.\n- Generator extensibility test: add a new relation via the plugin interface, verify it executes correctly.\n- Structured JSON logs per metamorphic test run: relation name, base inputs tested, transformations applied, violations found, total pass/fail counts.\n\n### Expected Artifacts\n\n- Metamorphic test framework in `tests/metamorphic/` or `crates/franken-node/tests/`.\n- `tests/metamorphic/corpus/` — base input corpus.\n- `tests/metamorphic/relations/` — relation definitions.\n- `scripts/check_metamorphic_tests.py` — verification script.\n- `tests/test_check_metamorphic_tests.py` — unit tests.\n- `artifacts/section_10_7/bd-1u4/verification_evidence.json` — test results.\n- `artifacts/section_10_7/bd-1u4/verification_summary.md` — human-readable summary.\n\n### Additional E2E Requirement\n\n- E2E test scripts must execute full end-to-end workflows from clean fixtures, emit deterministic machine-readable pass/fail evidence, and capture stepwise structured logs for root-cause triage.","acceptance_criteria":"1. At least 5 metamorphic relations are defined and documented (e.g., 'if module M loads successfully, then M with an added no-op wrapper must also load with identical exports').\n2. Each metamorphic relation is encoded as a test generator that produces input pairs (A, f(A)) and asserts output relation (X, f(X)).\n3. Relations cover: module loading invariants, migration plan idempotency, shim composition commutativity, and compatibility-set closure properties.\n4. Tests run against both the Rust implementation and the lockstep oracle to detect divergence.\n5. Metamorphic test suite is integrated into CI and produces a structured JSON report with per-relation pass/fail status.\n6. At least one relation validates the extraction-and-proof discipline from Section 5.4: porting a fixture through the migration pipeline and back yields equivalent output.\n7. False-positive rate is documented: each relation includes a rationale explaining why the invariant must hold and under what conditions it could legitimately break.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:36:47.507200223Z","created_by":"ubuntu","updated_at":"2026-02-20T17:08:35.236132222Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-7","self-contained","test-obligations"]}
{"id":"bd-1u8m","title":"[10.18] Implement proof-generation service interface (backend-agnostic) for receipt-window compliance proofs.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.18 — Verifiable Execution Fabric Execution Track (9L)\n\nWhy This Exists:\nVEF track for proof-carrying runtime compliance: receipt chains, commitment checkpoints, proof generation/verification, and claim gating.\n\nTask Objective:\nImplement proof-generation service interface (backend-agnostic) for receipt-window compliance proofs.\n\nAcceptance Criteria:\n- Proof service supports deterministic input envelope and output proof envelope; backend selection is pluggable without semantic drift.\n\nExpected Artifacts:\n- `docs/specs/vef_proof_service_contract.md`, `src/trust/vef_proof_service.rs`, `artifacts/10.18/vef_proof_service_matrix.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_18/bd-1u8m/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_18/bd-1u8m/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.18] Implement proof-generation service interface (backend-agnostic) for receipt-window compliance proofs.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.18] Implement proof-generation service interface (backend-agnostic) for receipt-window compliance proofs.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.18] Implement proof-generation service interface (backend-agnostic) for receipt-window compliance proofs.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.18] Implement proof-generation service interface (backend-agnostic) for receipt-window compliance proofs.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.18] Implement proof-generation service interface (backend-agnostic) for receipt-window compliance proofs.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"- Proof service supports deterministic input envelope and output proof envelope; backend selection is pluggable without semantic drift.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:37:04.544729684Z","created_by":"ubuntu","updated_at":"2026-02-20T17:05:47.998550027Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-18","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-1u8m","depends_on_id":"bd-28u0","type":"blocks","created_at":"2026-02-20T17:05:47.998484175Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-1u9","title":"[PLAN 10.6] Performance + Packaging","description":"\n\n## Success Criteria\n- All child beads for this section are completed with acceptance artifacts attached and no unresolved blockers.\n- Section-level verification gate for comprehensive unit tests, integration/e2e workflows, and detailed structured logging evidence is green.\n- Scope-to-plan traceability is explicit, with no feature loss versus the canonical plan section.\n\n## Optimization Notes\n- User-Outcome Lens: \"[PLAN 10.6] Performance + Packaging\" must improve real operator and end-user reliability, explainability, and time-to-safe-outcome under both normal and degraded conditions.\n- Cross-Section Coordination: this epic's child beads must explicitly encode integration expectations with neighboring sections to prevent local optimizations that break global behavior.\n- Verification Non-Negotiable: completion requires deterministic unit coverage, integration/E2E replay evidence, and structured logs sufficient for independent third-party reproduction and triage.\n- Scope Protection: any proposed simplification must prove feature parity against plan intent and document tradeoffs explicitly; silent scope reduction is forbidden.","status":"open","priority":2,"issue_type":"epic","created_at":"2026-02-20T07:36:40.705483806Z","created_by":"ubuntu","updated_at":"2026-02-20T16:17:13.712607535Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-6"],"dependencies":[{"issue_id":"bd-1u9","depends_on_id":"bd-20a","type":"blocks","created_at":"2026-02-20T07:37:10.241891429Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1u9","depends_on_id":"bd-229","type":"blocks","created_at":"2026-02-20T07:37:10.203432742Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1u9","depends_on_id":"bd-2pw","type":"blocks","created_at":"2026-02-20T07:36:47.138295371Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1u9","depends_on_id":"bd-2q5","type":"blocks","created_at":"2026-02-20T07:36:46.973090247Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1u9","depends_on_id":"bd-2vl5","type":"blocks","created_at":"2026-02-20T16:17:13.712524781Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1u9","depends_on_id":"bd-38m","type":"blocks","created_at":"2026-02-20T07:36:46.894386777Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1u9","depends_on_id":"bd-3il","type":"blocks","created_at":"2026-02-20T07:37:10.162896506Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1u9","depends_on_id":"bd-3kn","type":"blocks","created_at":"2026-02-20T07:36:47.055417847Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1u9","depends_on_id":"bd-3lh","type":"blocks","created_at":"2026-02-20T07:36:46.815299473Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1u9","depends_on_id":"bd-3p9n","type":"blocks","created_at":"2026-02-20T07:48:25.223542477Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1u9","depends_on_id":"bd-3q9","type":"blocks","created_at":"2026-02-20T07:36:47.220923231Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1u9","depends_on_id":"bd-cda","type":"blocks","created_at":"2026-02-20T07:37:09.271148865Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1u9","depends_on_id":"bd-k4s","type":"blocks","created_at":"2026-02-20T07:36:46.737278545Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-1ugy","title":"[10.13] Define stable telemetry namespace for protocol/capability/egress/security planes.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.13 — FCP Deep-Mined Expansion Execution Track (9I)\n\nWhy This Exists:\nDeep connector/provider contract track: lifecycle FSMs, conformance harnesses, fencing, sandboxing, revocation freshness, and protocol hardening.\n\nTask Objective:\nDefine stable telemetry namespace for protocol/capability/egress/security planes.\n\nAcceptance Criteria:\n- Metric names and labels are versioned and frozen by contract; deprecations follow compatibility policy; schema validator enforces namespace rules.\n\nExpected Artifacts:\n- `docs/observability/telemetry_namespace.md`, `tests/conformance/metric_schema_stability.rs`, `artifacts/10.13/telemetry_schema_catalog.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_13/bd-1ugy/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_13/bd-1ugy/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.13] Define stable telemetry namespace for protocol/capability/egress/security planes.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.13] Define stable telemetry namespace for protocol/capability/egress/security planes.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.13] Define stable telemetry namespace for protocol/capability/egress/security planes.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.13] Define stable telemetry namespace for protocol/capability/egress/security planes.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.13] Define stable telemetry namespace for protocol/capability/egress/security planes.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","status":"closed","priority":1,"issue_type":"task","assignee":"CrimsonCrane","created_at":"2026-02-20T07:36:54.577751044Z","created_by":"ubuntu","updated_at":"2026-02-20T13:17:20.021892265Z","closed_at":"2026-02-20T13:17:20.021867379Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-13","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-1ugy","depends_on_id":"bd-3tzl","type":"blocks","created_at":"2026-02-20T07:43:13.895487393Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-1ul","title":"[10.7] Add fuzz/adversarial tests for migration and shim logic.","description":"## [10.7] Fuzz and Adversarial Tests for Migration and Shim Logic\n\n### Why This Exists\n\nMigration scanning and compatibility shim logic are security-critical attack surfaces. The migration scanner processes untrusted project structures (arbitrary directory trees, malformed package.json files, adversarial dependency graphs), and the compatibility shim translates between runtime APIs where type confusion or unexpected inputs could bypass policy enforcement. Section 10.13's adversarial fuzz corpus pattern establishes the methodology; this bead applies it specifically to migration and shim logic to discover crashes, panics, hangs, and logic errors that unit tests miss.\n\n### What It Must Do\n\n**Migration scanner fuzzing**: Generate adversarial inputs for the migration scanner including: deeply nested directory structures (path traversal attempts), malformed package.json (invalid JSON, unexpected types, circular references in dependency fields), oversized files (memory exhaustion attempts), symlink loops, files with pathological names (null bytes, unicode edge cases), and adversarial dependency trees (diamond dependencies, version conflicts, impossible constraint sets).\n\n**Shim logic fuzzing**: Generate adversarial inputs for compatibility shims including: type confusion attacks (passing objects where primitives are expected and vice versa), boundary values (MAX_SAFE_INTEGER + 1, empty strings, null/undefined), policy bypass attempts (crafted inputs that might skip validation), and encoding edge cases (mixed encodings, BOM markers, surrogate pairs).\n\n**Structured corpus management**: Fuzz inputs are organized in a structured corpus directory (`fuzz/corpus/migration/` and `fuzz/corpus/shim/`). Regression seeds — inputs that previously triggered bugs — are permanently preserved in `fuzz/regression/`. New crash-triggering inputs are automatically added to the regression set.\n\n**CI gate**: A fuzz health budget defines the minimum fuzz execution time per CI run (e.g., 60 seconds per target). The gate verifies that fuzzing ran for at least the budgeted time and that no new crashes were found. If a new crash is discovered, the gate fails and the crashing input is preserved as a regression seed.\n\n**Coverage tracking**: Fuzz runs report code coverage achieved. Coverage should monotonically increase as the corpus grows. A coverage regression (new code added without corresponding corpus expansion) triggers a warning.\n\n### Acceptance Criteria\n\n1. Fuzz targets exist for the migration scanner (at least 3 entry points: directory scan, package.json parse, dependency tree resolution) and compatibility shims (at least 2 entry points: API translation, type coercion).\n2. Structured corpus directories exist at `fuzz/corpus/migration/` and `fuzz/corpus/shim/` with at least 50 seed inputs each.\n3. Regression seeds in `fuzz/regression/` are preserved permanently and run on every CI build.\n4. CI fuzz health gate enforces minimum fuzz time budget (configurable, default 60 seconds per target).\n5. New crash-triggering inputs are automatically added to the regression set.\n6. Coverage reports are generated per fuzz run and stored in artifacts.\n7. Verification script `scripts/check_fuzz_adversarial.py` with `--json` flag validates corpus health, regression seed presence, and budget compliance.\n8. Unit tests in `tests/test_check_fuzz_adversarial.py` cover corpus management, regression seed handling, budget enforcement, and coverage report parsing.\n\n### Key Dependencies\n\n- Migration scanner from 10.3.\n- Compatibility shims from 10.2.\n- Fuzz corpus infrastructure from 10.13 (bd-3n2u or related fuzz corpus bead).\n- Fuzzing engine (cargo-fuzz/libFuzzer for Rust, or structured Python fuzzer).\n\n### Testing & Logging Requirements\n\n- Regression test: ensure all seeds in `fuzz/regression/` are executed on every CI run and none cause crashes.\n- Budget enforcement test: run with artificially low budget, verify gate enforces minimum time.\n- Corpus growth test: add a new fuzz target, verify corpus expands.\n- Structured JSON logs per fuzz session: target name, seeds executed, new paths found, crashes found, coverage percentage, wall-clock time.\n\n### Expected Artifacts\n\n- Fuzz targets in `fuzz/targets/` or `crates/franken-node/fuzz/`.\n- `fuzz/corpus/migration/` and `fuzz/corpus/shim/` — seed corpora.\n- `fuzz/regression/` — permanent regression seeds.\n- `scripts/check_fuzz_adversarial.py` — verification script.\n- `tests/test_check_fuzz_adversarial.py` — unit tests.\n- `artifacts/section_10_7/bd-1ul/verification_evidence.json` — fuzz results.\n- `artifacts/section_10_7/bd-1ul/verification_summary.md` — human-readable summary.\n\n### Additional E2E Requirement\n\n- E2E test scripts must execute full end-to-end workflows from clean fixtures, emit deterministic machine-readable pass/fail evidence, and capture stepwise structured logs for root-cause triage.","acceptance_criteria":"1. Fuzz tests target at minimum: migration plan parser, shim dispatch logic, compatibility mapping engine, and configuration loader.\n2. Fuzz harnesses use cargo-fuzz (libfuzzer) or equivalent and are runnable with a single command.\n3. Each fuzz target runs for at least 10 minutes in CI without panics, hangs, or memory safety violations.\n4. Adversarial tests include: malformed migration manifests, circular dependency graphs, oversized inputs, and type-confused shim arguments.\n5. Any crash or violation found by fuzzing is captured as a regression test in the corpus and added to the golden test suite.\n6. Fuzz corpus seeds are stored under fixtures/fuzz/ and are version-controlled.\n7. Coverage report shows fuzz targets exercise at least 70% of branches in the targeted modules.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:36:47.424296620Z","created_by":"ubuntu","updated_at":"2026-02-20T17:08:35.437807001Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-7","self-contained","test-obligations"]}
{"id":"bd-1v2c","title":"[10.N] Implement cross-track canonical-reference linting","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.N — Execution Normalization Contract\n\nTask Objective:\nAdd a cross-track reference lint gate requiring integration/policy/adoption tasks to reference canonical owner IDs and artifact contracts, preventing silent semantic drift.\n\nAcceptance Criteria:\n- Lint rejects missing or invalid canonical-owner references.\n- Lint enforces artifact-contract linkage for cross-track integration tasks.\n- Findings include precise remediation pointers to canonical owner beads/contracts.\n\nExpected Artifacts:\n- Cross-track lint rules and mapping config.\n- Lint conformance report across current bead graph/tasks.\n\nTesting & Logging Requirements:\n- Unit tests for lint parsing and reference validation behavior.\n- E2E tests for CI enforcement on valid/invalid cross-track references.\n- Structured lint logs with stable finding categories.\n\nTask-Specific Clarification:\n- For \"[10.N] Implement cross-track canonical-reference linting\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.N] Implement cross-track canonical-reference linting\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.N] Implement cross-track canonical-reference linting\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.N] Implement cross-track canonical-reference linting\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.N] Implement cross-track canonical-reference linting\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","status":"closed","priority":1,"issue_type":"task","assignee":"ubuntu","created_at":"2026-02-20T07:50:04.216769836Z","created_by":"ubuntu","updated_at":"2026-02-20T08:26:52.206405612Z","closed_at":"2026-02-20T08:26:52.206316045Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-N","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-1v2c","depends_on_id":"bd-1oyt","type":"blocks","created_at":"2026-02-20T07:50:04.353037955Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-1v65","title":"[10.16] Integrate `sqlmodel_rust` in domains where typed schema/query safety is high-EV.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.16 — Adjacent Substrate Integration Execution Track (8.7)\n\nWhy This Exists:\nAdjacent substrate integration track to enforce deep composition with frankentui, frankensqlite, sqlmodel_rust, and fastapi_rust.\n\nTask Objective:\nIntegrate `sqlmodel_rust` in domains where typed schema/query safety is high-EV.\n\nAcceptance Criteria:\n- Selected domains use typed models and query contracts; schema drift is caught by conformance checks.\n\nExpected Artifacts:\n- `tests/conformance/sqlmodel_contracts.rs`, `artifacts/10.16/sqlmodel_integration_domains.csv`.\n\n- Machine-readable verification artifact at `artifacts/section_10_16/bd-1v65/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_16/bd-1v65/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.16] Integrate `sqlmodel_rust` in domains where typed schema/query safety is high-EV.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.16] Integrate `sqlmodel_rust` in domains where typed schema/query safety is high-EV.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.16] Integrate `sqlmodel_rust` in domains where typed schema/query safety is high-EV.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.16] Integrate `sqlmodel_rust` in domains where typed schema/query safety is high-EV.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.16] Integrate `sqlmodel_rust` in domains where typed schema/query safety is high-EV.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"- Selected domains use typed models and query contracts; schema drift is caught by conformance checks.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:37:02.267781296Z","created_by":"ubuntu","updated_at":"2026-02-20T17:05:27.508387681Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-16","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-1v65","depends_on_id":"bd-bt82","type":"blocks","created_at":"2026-02-20T17:05:27.508306700Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-1vc4","title":"Epic: Admission + Quarantine Controls [10.13f]","description":"- type: epic","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-20T07:47:58.072163029Z","updated_at":"2026-02-20T07:49:21.187290094Z","closed_at":"2026-02-20T07:49:21.187272261Z","close_reason":"Superseded by canonical detailed master-plan graph (bd-33v) with full 10.N-10.21 and 11-16 coverage, explicit dependencies, and per-section verification gates.","source_repo":".","compaction_level":0,"original_size":0}
{"id":"bd-1vm","title":"[10.4] Implement fast quarantine/recall workflow for compromised artifacts.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.4 — Extension Ecosystem + Registry\n\nWhy This Exists:\nExtension ecosystem and registry trust foundation: signed artifacts, provenance, trust cards, revocation, and quarantine flows.\n\nTask Objective:\nImplement fast quarantine/recall workflow for compromised artifacts.\n\nAcceptance Criteria:\n(See structured acceptance_criteria field for domain-specific criteria.)\n\nExpected Artifacts:\n- Section-owned design/spec artifact at `docs/specs/section_10_4/bd-1vm_contract.md`, capturing decision rationale, invariants, and interface boundaries.\n- Machine-readable verification artifact at `artifacts/section_10_4/bd-1vm/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_4/bd-1vm/verification_summary.md` linking implementation intent to measured outcomes.\n\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.4] Implement fast quarantine/recall workflow for compromised artifacts.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.4] Implement fast quarantine/recall workflow for compromised artifacts.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.4] Implement fast quarantine/recall workflow for compromised artifacts.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.4] Implement fast quarantine/recall workflow for compromised artifacts.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.4] Implement fast quarantine/recall workflow for compromised artifacts.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"1. Quarantine workflow has three trigger modes: (a) automated — triggered by revocation event from 10.13 registry, (b) operator-initiated — via CLI 'franken-node ext quarantine <ext-id> --reason <code>', (c) publisher-initiated — publisher issues recall via registry API with signed recall notice. 2. Quarantine is enacted within 30 seconds of trigger on the local node: extension is disabled (no new invocations), in-flight operations are drained with configurable timeout (default 10s), and extension state is preserved for forensic analysis. 3. Recall workflow extends quarantine to fleet-wide: recall notice is propagated to all nodes via the revocation registry (10.13); each node independently enacts quarantine upon receiving the notice. 4. Quarantine state machine: Active -> Quarantined -> {Recalled, Reinstated}. Reinstated requires explicit operator approval plus passing a re-verification check (full provenance chain re-validation). 5. Compromised artifact forensics: upon quarantine, the system captures and preserves extension binary hash, last-known behavioral telemetry snapshot, capability exercise log (last 1000 events), and active session count at quarantine time. Forensic bundle is stored at artifacts/quarantine/<ext-id>/<timestamp>/. 6. Fleet recall progress tracking: operator can query 'franken-node ext recall status <ext-id>' to see per-node quarantine confirmation status (confirmed/pending/unreachable). 7. Rollback support: if the quarantined extension was an update, automatic rollback to the last known-good version is attempted; if no known-good version exists, the extension remains disabled. 8. Quarantine does not cascade to other extensions unless they declare a hard dependency on the quarantined extension; soft-dependency extensions receive a degraded-mode signal. 9. Time budget: full quarantine-to-disabled path must complete in <30s on a single node, and fleet-wide recall propagation must reach 99% of connected nodes within 5 minutes. 10. Structured log events: QUARANTINE_TRIGGERED (trigger_mode, ext_id, reason_code), QUARANTINE_DRAIN_STARTED, QUARANTINE_DRAIN_COMPLETE (drained_sessions_count), RECALL_PROPAGATED, RECALL_NODE_CONFIRMED, EXTENSION_REINSTATED (operator_id, re_verification_result).","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:36:45.825278520Z","created_by":"ubuntu","updated_at":"2026-02-20T16:58:59.975360043Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-4","self-contained","test-obligations"]}
{"id":"bd-1vp","title":"[10.10] Implement zone/tenant trust segmentation policies.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md — Section 10.10 (FCP-Inspired Hardening), cross-ref Section 9E.8\n\n## Why This Exists\n\nEnhancement Map 9E.8 requires zone-style trust segmentation for team/project/tenant boundaries. In a multi-tenant or multi-team deployment of franken_node, different organizational units must operate within cryptographically enforced trust boundaries — a team's control actions, tokens, and policies must not leak into or affect another team's zone. Without zone segmentation, the no-ambient-authority invariant (8.5) is violated at the organizational level: any authenticated entity could potentially affect resources outside its intended scope. This bead implements the product-level zone segmentation policy engine that partitions the trust domain into isolated zones, each with its own policy namespace, key bindings, and token scope — ensuring that cross-zone operations require explicit, auditable authorization.\n\n## What This Must Do\n\n1. Define a `TrustZone` struct containing: zone identifier (globally unique, domain-separated per bd-1l5), zone owner identity, zone-specific policy checkpoint chain reference, zone-scoped key bindings (subset of the global key-role registry from bd-364), and a list of authorized cross-zone bridges.\n2. Implement a `ZoneRegistry` that manages zone lifecycle: `create_zone()`, `delete_zone()` (requires freshness-gated authorization from bd-2sx), `list_zones()`, and `resolve_zone(resource_id) -> ZoneId`.\n3. Implement zone-scoped token validation: `AudienceBoundTokens` (from bd-1r2) must include a zone claim, and the token verifier rejects tokens whose zone claim does not match the zone of the resource being accessed.\n4. Implement cross-zone bridge authorization: operations that span zones (e.g., migrating a resource from zone A to zone B) require a cross-zone bridge token signed by both zone owners, with explicit capability attenuation for the cross-zone scope.\n5. Ensure all zone boundary changes (creation, deletion, bridge establishment) are recorded in the policy checkpoint chain (bd-174) for auditability and rollback detection.\n6. Integrate zone context into all structured log events: every control action log must include the `zone_id` of the acting entity and the `zone_id` of the target resource.\n\n## Context from Enhancement Maps\n\n- 9E.8: \"Zone-style trust segmentation for team/project/tenant boundaries\"\n- 9E.4 (cross-ref): Token delegation chains (bd-1r2) carry zone claims that this bead validates.\n- 9E.7 (cross-ref): Zone deletion and cross-zone bridge creation are Tier-1 critical actions requiring revocation freshness (bd-2sx).\n- 9E.3 (cross-ref): Zone boundary changes are checkpointed in the policy chain (bd-174) for tamper evidence.\n- 9C.1 (Multi-tenancy): Zone segmentation is the primary mechanism for tenant isolation in shared deployments.\n\n## Dependencies\n\n- Upstream: bd-2sx ([10.10] Integrate canonical revocation freshness semantics before risky product actions) — zone deletion and bridge creation require freshness-gated authorization.\n- Upstream (implicit): bd-1r2 (token chains carry zone claims), bd-364 (zone-scoped key bindings), bd-174 (zone changes recorded in checkpoint chain).\n- Downstream: bd-1jjq ([10.10] Section-wide verification gate) — aggregates verification evidence.\n- Downstream: bd-13q ([10.10] Adopt canonical stable error namespace) — zone-related errors use the stable error taxonomy.\n\n## Acceptance Criteria\n\n1. `TrustZone` struct includes all required fields (zone_id, owner, policy_chain_ref, key_bindings, cross_zone_bridges) with documented invariants.\n2. Zone isolation is enforced: a token with zone claim \"zone-A\" cannot authorize actions on resources in \"zone-B\" — rejected with `ZONE_BOUNDARY_VIOLATION`.\n3. Cross-zone bridge requires dual-owner authorization: a bridge token must carry signatures from both zone owners, and a single-owner signature is rejected with `BRIDGE_AUTH_INCOMPLETE`.\n4. Zone deletion is Tier-1 gated: deletion without a valid freshness proof is rejected with `FRESHNESS_STALE`.\n5. All zone boundary changes (create, delete, bridge) produce checkpoints in the policy chain — verified by querying the checkpoint chain after each operation.\n6. Zone-scoped key bindings restrict which keys are valid within a zone: using a key not bound to the target zone is rejected with `KEY_ZONE_MISMATCH`.\n7. Resource-to-zone resolution is deterministic and consistent: `resolve_zone()` returns the same zone for a given resource across all nodes (verified by multi-node simulation).\n8. Verification evidence JSON includes zone count, cross-zone bridge scenarios tested, boundary violation rejection counts, and checkpoint coverage.\n\n## Testing & Logging Requirements\n\n- Unit tests: Create multiple zones and verify isolation — actions in zone A cannot affect zone B. Test cross-zone bridge creation with valid dual-owner auth and invalid single-owner auth. Test zone deletion with and without freshness proof. Test zone-scoped token validation: correct zone accepted, wrong zone rejected. Test resource-to-zone resolution with various resource ID patterns.\n- Integration tests: Multi-zone workflow: create zones A, B, C. Issue zone-scoped tokens. Attempt cross-zone operations without bridge (must fail). Establish bridge between A and B. Perform cross-zone migration. Verify checkpoint chain records all zone changes. Verify that zone deletion triggers divergence detection in bd-2ms if replicated state is inconsistent.\n- Adversarial tests: Attempt to create a zone with a duplicate zone_id. Attempt to forge a cross-zone bridge token with only one signature. Attempt to access a deleted zone's resources. Attempt to re-register a deleted zone's ID to hijack its resources. Test with a zone whose owner key has been revoked — verify freshness gate blocks all operations.\n- Structured logs: `ZONE_CREATED` (zone_id, owner, key_binding_count). `ZONE_DELETED` (zone_id, owner, freshness_proof_epoch). `ZONE_BRIDGE_CREATED` (source_zone, target_zone, bridge_capabilities, dual_auth_verified). `ZONE_BOUNDARY_VIOLATION` (acting_zone, target_zone, action, token_chain_depth). `ZONE_RESOLVED` (resource_id, resolved_zone_id). All events include `trace_id`, `epoch_id`, and `zone_id`.\n\n## Expected Artifacts\n\n- docs/specs/section_10_10/bd-1vp_contract.md\n- crates/franken-node/src/connector/trust_zone.rs (or similar module path)\n- scripts/check_zone_segmentation.py with --json flag and self_test()\n- tests/test_check_zone_segmentation.py\n- artifacts/section_10_10/bd-1vp/verification_evidence.json\n- artifacts/section_10_10/bd-1vp/verification_summary.md","acceptance_criteria":"1. Define a TrustZone struct containing: (a) zone_id (TrustObjectId with ZONE domain), (b) zone_name (human-readable, max 128 chars, alphanumeric + hyphens), (c) parent_zone_id (Option, None for root zone), (d) trust_boundary_policy (enum: STRICT_ISOLATION, CONTROLLED_BRIDGE, OPEN), (e) created_at, (f) owner_key_id (TrustObjectId with KEY domain).\n2. Define a TenantBinding struct: (a) tenant_id (string, max 64 chars), (b) zone_id, (c) role (enum: OWNER, OPERATOR, READER), (d) bound_at, (e) bound_by (authority key_id).\n3. Implement zone hierarchy: zones form a tree rooted at a single root zone. Enforce that parent_zone_id references an existing zone. Reject cycles (a zone cannot be its own ancestor).\n4. Implement trust boundary enforcement: (a) STRICT_ISOLATION: no cross-zone token delegation or key sharing; tokens with audience in zone A are rejected in zone B. (b) CONTROLLED_BRIDGE: cross-zone access requires an explicit bridge policy listing source_zone, target_zone, and allowed action_scopes. (c) OPEN: no cross-zone restrictions (for development/test only, must be flagged in logs).\n5. Implement a ZonePolicyEngine with: (a) create_zone(name, parent, policy, owner_key) -> Result, (b) bind_tenant(tenant_id, zone_id, role, authority) -> Result, (c) check_access(requester_tenant, target_zone, action) -> Result<Allow/Deny>, (d) create_bridge(source_zone, target_zone, scopes, authority) -> Result.\n6. Enforce that bridge creation requires the authority key to be an OWNER in both the source and target zones.\n7. Emit structured log events for: zone creation, tenant binding, access check (with allow/deny result), bridge creation, and cross-zone violation attempts.\n8. Unit tests: (a) root zone creation, (b) child zone creation, (c) cycle rejection, (d) STRICT_ISOLATION cross-zone rejection, (e) CONTROLLED_BRIDGE with valid bridge, (f) CONTROLLED_BRIDGE without bridge (rejected), (g) tenant binding and role check, (h) bridge authority validation.\n9. Integration test: create a 3-level zone hierarchy, bind tenants, attempt cross-zone actions, verify isolation.\n10. Verification: scripts/check_zone_segmentation.py --json, artifacts at artifacts/section_10_10/bd-1vp/.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:36:49.426354415Z","created_by":"ubuntu","updated_at":"2026-02-20T17:01:30.084538774Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-10","self-contained","test-obligations"]}
{"id":"bd-1vsr","title":"[10.14] Implement transition abort semantics on timeout/cancellation unless explicit force policy is provided.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.14 — FrankenSQLite Deep-Mined Expansion Execution Track (9J)\n\nWhy This Exists:\nFrankenSQLite deep-mined control integrity track: evidence ledgers, proofs, epochs, remote effects, markers/MMR, and deterministic fault labs.\n\nTask Objective:\nImplement transition abort semantics on timeout/cancellation unless explicit force policy is provided.\n\nAcceptance Criteria:\n- Default behavior aborts transition on timeout/cancel; force policy is explicit, scoped, and audited; partial transition state is impossible.\n\nExpected Artifacts:\n- `tests/security/epoch_transition_abort_semantics.rs`, `docs/specs/force_transition_policy.md`, `artifacts/10.14/transition_abort_events.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_14/bd-1vsr/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_14/bd-1vsr/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.14] Implement transition abort semantics on timeout/cancellation unless explicit force policy is provided.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.14] Implement transition abort semantics on timeout/cancellation unless explicit force policy is provided.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.14] Implement transition abort semantics on timeout/cancellation unless explicit force policy is provided.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.14] Implement transition abort semantics on timeout/cancellation unless explicit force policy is provided.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.14] Implement transition abort semantics on timeout/cancellation unless explicit force policy is provided.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"Default behavior aborts transition on timeout/cancel; force policy is explicit, scoped, and audited; partial transition state is impossible.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:36:58.465729166Z","created_by":"ubuntu","updated_at":"2026-02-20T15:44:04.117458069Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-14","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-1vsr","depends_on_id":"bd-2wsm","type":"blocks","created_at":"2026-02-20T07:43:15.934925123Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-1vvs","title":"[10.13] Add strict-plus isolation backend (microVM when available, hardened fallback otherwise).","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.13 — FCP Deep-Mined Expansion Execution Track (9I)\n\nWhy This Exists:\nDeep connector/provider contract track: lifecycle FSMs, conformance harnesses, fencing, sandboxing, revocation freshness, and protocol hardening.\n\nTask Objective:\nAdd strict-plus isolation backend (microVM when available, hardened fallback otherwise).\n\nAcceptance Criteria:\n- `strict_plus` maps to microVM isolation where supported; unsupported platforms use hardened fallback with equivalent policy guarantees; compatibility tests pass across OS targets.\n\nExpected Artifacts:\n- `docs/specs/strict_plus_backend_matrix.md`, `tests/integration/strict_plus_isolation.rs`, `artifacts/10.13/strict_plus_runtime_matrix.csv`.\n\n- Machine-readable verification artifact at `artifacts/section_10_13/bd-1vvs/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_13/bd-1vvs/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.13] Add strict-plus isolation backend (microVM when available, hardened fallback otherwise).\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.13] Add strict-plus isolation backend (microVM when available, hardened fallback otherwise).\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.13] Add strict-plus isolation backend (microVM when available, hardened fallback otherwise).\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.13] Add strict-plus isolation backend (microVM when available, hardened fallback otherwise).\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.13] Add strict-plus isolation backend (microVM when available, hardened fallback otherwise).\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","status":"closed","priority":1,"issue_type":"task","assignee":"CrimsonCrane","created_at":"2026-02-20T07:36:52.204383297Z","created_by":"ubuntu","updated_at":"2026-02-20T11:15:35.550490307Z","closed_at":"2026-02-20T11:15:35.550465711Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-13","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-1vvs","depends_on_id":"bd-3ua7","type":"blocks","created_at":"2026-02-20T07:43:12.651901464Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-1w78","title":"[13] Success criterion: continuous lockstep validation","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 13\n\nTask Objective:\nInstrument and enforce continuous lockstep validation for compatibility claims.\n\nExecution Requirements:\n- Make this requirement machine-verifiable and release-gated where applicable.\n- Keep behavior self-documenting so future contributors do not need to re-open the master plan.\n\nTesting & Logging Requirements:\n- Unit tests for rule/contract logic and error conditions.\n- Integration/E2E tests for workflow-level enforcement.\n- Structured logs with stable codes and trace IDs.\n- Reproducible artifacts that prove contract satisfaction.\n\nAcceptance Criteria:\n- Success and failure invariants for [13] Success criterion: continuous lockstep validation are explicitly quantified and machine-verifiable.\n- Determinism requirements for [13] Success criterion: continuous lockstep validation are validated across repeated runs and adversarial perturbations.\n\n\nExpected Artifacts:\n- Machine-readable verification artifact with explicit canonical path under artifacts/section_13/bd-1w78/verification_evidence.json, suitable for CI/release gating.\n- Human-readable verification summary with explicit canonical path under artifacts/section_13/bd-1w78/verification_summary.md, linking implementation intent to measured validation outcomes.\n\nTask-Specific Clarification:\n- The implementation must deliver the exact capability named in the title, with no scope dilution and no silent feature omission.\n- Validation artifacts must include capability-specific thresholds, pass/fail criteria, and reproducible evidence bundles tied to this bead ID.\n- Program/section verification gates must be able to consume this bead's outputs without manual interpretation.\n\nWhy This Improves User Outcomes:\n- \"[13] Success criterion: continuous lockstep validation\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[13] Success criterion: continuous lockstep validation\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"1. Lockstep validation runs continuously: every CI pipeline execution includes a lockstep comparison against reference Node.js on a representative test subset.\n2. The representative subset covers >= 100 API calls across critical API families (fs, http, crypto, stream, net).\n3. Any lockstep divergence fails CI and produces a divergence receipt with: input, expected output, actual output, API family, severity.\n4. Lockstep validation history is tracked: a dashboard or report shows pass/fail trend over the last 30 days.\n5. Divergence receipts are immutable once created (append-only storage).\n6. Lockstep validation can be triggered on-demand for a specific API family (not just full suite).\n7. Evidence: lockstep_validation_status.json with latest run date, pass rate, and list of any open divergences.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:39:34.389711289Z","created_by":"ubuntu","updated_at":"2026-02-20T15:22:28.124905491Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-13","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-1w78","depends_on_id":"bd-2f43","type":"blocks","created_at":"2026-02-20T07:43:25.313179869Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-1wz","title":"[PLAN 10.17] Radical Expansion Execution Track (9K)","description":"Section: 10.17 — Radical Expansion Execution Track (9K)\n\nStrategic Context:\nRadical expansion track for proof-carrying speculation, Bayesian adversary control, time-travel replay, ZK attestations, and claim compiler systems.\n\nExecution Requirements:\n- Preserve all scoped capabilities from the canonical plan.\n- Keep contracts self-contained so future contributors can execute without reopening the master plan.\n- Require explicit testing strategy (unit + integration/e2e) and structured logging/telemetry for every child bead.\n- Require artifact-backed validation for performance, security, and correctness claims.\n\nDependency Semantics:\n- This epic is blocked by its child implementation beads.\n- Cross-epic dependencies encode strategic sequencing and canonical ownership boundaries.\n\n## Success Criteria\n- All child beads for this section are completed with acceptance artifacts attached and no unresolved blockers.\n- Section-level verification gate for comprehensive unit tests, integration/e2e workflows, and detailed structured logging evidence is green.\n- Scope-to-plan traceability is explicit, with no feature loss versus the canonical plan section.\n\n## Optimization Notes\n- User-Outcome Lens: \"[PLAN 10.17] Radical Expansion Execution Track (9K)\" must improve operator confidence, safety posture, and deterministic recovery behavior under both normal and adversarial conditions.\n- Cross-Section Coordination: child beads must encode integration assumptions explicitly to avoid local optimizations that degrade system-wide correctness.\n- Verification Non-Negotiable: completion requires reproducible unit/integration/E2E evidence and structured logs suitable for independent replay.\n- Scope Protection: any simplification must preserve canonical-plan feature intent and be justified with explicit tradeoff documentation.","status":"open","priority":1,"issue_type":"epic","created_at":"2026-02-20T07:36:41.623984119Z","created_by":"ubuntu","updated_at":"2026-02-20T08:41:05.804997206Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-17"],"dependencies":[{"issue_id":"bd-1wz","depends_on_id":"bd-1nl1","type":"blocks","created_at":"2026-02-20T07:37:02.964405495Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1wz","depends_on_id":"bd-1ta","type":"blocks","created_at":"2026-02-20T07:37:11.319185203Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1wz","depends_on_id":"bd-1xbc","type":"blocks","created_at":"2026-02-20T07:37:03.136967152Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1wz","depends_on_id":"bd-21fo","type":"blocks","created_at":"2026-02-20T07:37:03.635743433Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1wz","depends_on_id":"bd-229","type":"blocks","created_at":"2026-02-20T07:37:11.280821353Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1wz","depends_on_id":"bd-26mk","type":"blocks","created_at":"2026-02-20T07:37:03.553966348Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1wz","depends_on_id":"bd-274s","type":"blocks","created_at":"2026-02-20T07:37:03.047856227Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1wz","depends_on_id":"bd-2iyk","type":"blocks","created_at":"2026-02-20T07:37:03.798361148Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1wz","depends_on_id":"bd-2kd9","type":"blocks","created_at":"2026-02-20T07:37:04.127277755Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1wz","depends_on_id":"bd-2o8b","type":"blocks","created_at":"2026-02-20T07:37:03.963175925Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1wz","depends_on_id":"bd-383z","type":"blocks","created_at":"2026-02-20T07:37:04.044673980Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1wz","depends_on_id":"bd-3il","type":"blocks","created_at":"2026-02-20T07:37:11.242903474Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1wz","depends_on_id":"bd-3ku8","type":"blocks","created_at":"2026-02-20T07:37:03.222341417Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1wz","depends_on_id":"bd-3l2p","type":"blocks","created_at":"2026-02-20T07:37:03.717250324Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1wz","depends_on_id":"bd-3qo","type":"blocks","created_at":"2026-02-20T07:37:11.397069457Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1wz","depends_on_id":"bd-3t08","type":"blocks","created_at":"2026-02-20T07:48:17.789556785Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1wz","depends_on_id":"bd-5rh","type":"blocks","created_at":"2026-02-20T07:37:11.358150353Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1wz","depends_on_id":"bd-al8i","type":"blocks","created_at":"2026-02-20T07:37:03.471219888Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1wz","depends_on_id":"bd-cda","type":"blocks","created_at":"2026-02-20T07:37:09.702995447Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1wz","depends_on_id":"bd-gad3","type":"blocks","created_at":"2026-02-20T07:37:03.304005170Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1wz","depends_on_id":"bd-kcg9","type":"blocks","created_at":"2026-02-20T07:37:03.389259753Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1wz","depends_on_id":"bd-n71","type":"blocks","created_at":"2026-02-20T07:37:11.436332251Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1wz","depends_on_id":"bd-nbwo","type":"blocks","created_at":"2026-02-20T07:37:03.879067618Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-1xao","title":"[13] Success criterion: impossible-by-default adoption","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 13\n\nTask Objective:\nTrack and gate production-grade adoption of impossible-by-default capabilities.\n\nExecution Requirements:\n- Make this requirement machine-verifiable and release-gated where applicable.\n- Keep behavior self-documenting so future contributors do not need to re-open the master plan.\n\nTesting & Logging Requirements:\n- Unit tests for rule/contract logic and error conditions.\n- Integration/E2E tests for workflow-level enforcement.\n- Structured logs with stable codes and trace IDs.\n- Reproducible artifacts that prove contract satisfaction.\n\nAcceptance Criteria:\n- Success and failure invariants for [13] Success criterion: impossible-by-default adoption are explicitly quantified and machine-verifiable.\n- Determinism requirements for [13] Success criterion: impossible-by-default adoption are validated across repeated runs and adversarial perturbations.\n\n\nExpected Artifacts:\n- Machine-readable verification artifact with explicit canonical path under artifacts/section_13/bd-1xao/verification_evidence.json, suitable for CI/release gating.\n- Human-readable verification summary with explicit canonical path under artifacts/section_13/bd-1xao/verification_summary.md, linking implementation intent to measured validation outcomes.\n\nTask-Specific Clarification:\n- The implementation must deliver the exact capability named in the title, with no scope dilution and no silent feature omission.\n- Validation artifacts must include capability-specific thresholds, pass/fail criteria, and reproducible evidence bundles tied to this bead ID.\n- Program/section verification gates must be able to consume this bead's outputs without manual interpretation.\n\nWhy This Improves User Outcomes:\n- \"[13] Success criterion: impossible-by-default adoption\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[13] Success criterion: impossible-by-default adoption\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"1. 'Impossible-by-default' capabilities are defined: operations that are blocked unless explicitly enabled with cryptographic authorization.\n2. At minimum, these capabilities are impossible-by-default: (a) arbitrary file system access outside project root, (b) outbound network to non-allowlisted hosts, (c) spawning child processes without sandbox, (d) loading unsigned extensions, (e) disabling hardening profiles.\n3. Each impossible-by-default capability requires explicit opt-in via signed capability token with expiry.\n4. Attempting a blocked operation produces a clear, actionable error message (not a generic permission denied).\n5. Adoption metric: >= 90% of production deployments run with all impossible-by-default capabilities enforced (measured via telemetry).\n6. No impossible-by-default capability can be silently disabled; any disabling is logged and alerted.\n7. Evidence: capability_enforcement_report.json listing each capability, enforcement status, and opt-in rate.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:39:34.668279352Z","created_by":"ubuntu","updated_at":"2026-02-20T15:23:06.909308202Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-13","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-1xao","depends_on_id":"bd-pga7","type":"blocks","created_at":"2026-02-20T07:43:25.443838167Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-1xbc","title":"[10.17] Add deterministic time-travel runtime capture/replay for extension-host workflows.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.17 — Radical Expansion Execution Track (9K)\n\nWhy This Exists:\nRadical expansion track for proof-carrying speculation, Bayesian adversary control, time-travel replay, ZK attestations, and claim compiler systems.\n\nTask Objective:\nAdd deterministic time-travel runtime capture/replay for extension-host workflows.\n\nAcceptance Criteria:\n- Captured executions replay byte-for-byte equivalent control decisions under same seed/input; incident replay includes stepwise state navigation and divergence explanation.\n\nExpected Artifacts:\n- `docs/specs/time_travel_runtime.md`, `src/replay/time_travel_engine.rs`, `tests/lab/time_travel_replay_equivalence.rs`, `artifacts/10.17/time_travel_replay_report.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_17/bd-1xbc/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_17/bd-1xbc/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.17] Add deterministic time-travel runtime capture/replay for extension-host workflows.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.17] Add deterministic time-travel runtime capture/replay for extension-host workflows.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.17] Add deterministic time-travel runtime capture/replay for extension-host workflows.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.17] Add deterministic time-travel runtime capture/replay for extension-host workflows.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.17] Add deterministic time-travel runtime capture/replay for extension-host workflows.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"- Captured executions replay byte-for-byte equivalent control decisions under same seed/input; incident replay includes stepwise state navigation and divergence explanation.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:37:03.099102362Z","created_by":"ubuntu","updated_at":"2026-02-20T15:46:00.589726222Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-17","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-1xbc","depends_on_id":"bd-274s","type":"blocks","created_at":"2026-02-20T07:43:18.349690460Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-1xg","title":"[PLAN 10.4] Extension Ecosystem + Registry","description":"Section: 10.4 — Extension Ecosystem + Registry\n\nStrategic Context:\nExtension ecosystem and registry trust foundation: signed artifacts, provenance, trust cards, revocation, and quarantine flows.\n\nExecution Requirements:\n- Preserve all scoped capabilities from the canonical plan.\n- Keep contracts self-contained so future contributors can execute without reopening the master plan.\n- Require explicit testing strategy (unit + integration/e2e) and structured logging/telemetry for every child bead.\n- Require artifact-backed validation for performance, security, and correctness claims.\n\nDependency Semantics:\n- This epic is blocked by its child implementation beads.\n- Cross-epic dependencies encode strategic sequencing and canonical ownership boundaries.\n\n## Success Criteria\n- All child beads for this section are completed with acceptance artifacts attached and no unresolved blockers.\n- Section-level verification gate for comprehensive unit tests, integration/e2e workflows, and detailed structured logging evidence is green.\n- Scope-to-plan traceability is explicit, with no feature loss versus the canonical plan section.\n\n## Optimization Notes\n- User-Outcome Lens: \"[PLAN 10.4] Extension Ecosystem + Registry\" must improve operator confidence, safety posture, and deterministic recovery behavior under both normal and adversarial conditions.\n- Cross-Section Coordination: child beads must encode integration assumptions explicitly to avoid local optimizations that degrade system-wide correctness.\n- Verification Non-Negotiable: completion requires reproducible unit/integration/E2E evidence and structured logs suitable for independent replay.\n- Scope Protection: any simplification must preserve canonical-plan feature intent and be justified with explicit tradeoff documentation.","status":"open","priority":1,"issue_type":"epic","created_at":"2026-02-20T07:36:40.545082850Z","created_by":"ubuntu","updated_at":"2026-02-20T16:17:13.540706590Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-4"],"dependencies":[{"issue_id":"bd-1xg","depends_on_id":"bd-12q","type":"blocks","created_at":"2026-02-20T07:36:45.622421462Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1xg","depends_on_id":"bd-1ah","type":"blocks","created_at":"2026-02-20T07:36:45.543343715Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1xg","depends_on_id":"bd-1gx","type":"blocks","created_at":"2026-02-20T07:36:45.464548314Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1xg","depends_on_id":"bd-1ta","type":"blocks","created_at":"2026-02-20T07:37:10.009028359Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1xg","depends_on_id":"bd-1vm","type":"blocks","created_at":"2026-02-20T07:36:45.861491603Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1xg","depends_on_id":"bd-261k","type":"blocks","created_at":"2026-02-20T07:48:23.958545463Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1xg","depends_on_id":"bd-273","type":"blocks","created_at":"2026-02-20T07:36:45.940576082Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1xg","depends_on_id":"bd-2yh","type":"blocks","created_at":"2026-02-20T07:36:45.701292433Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1xg","depends_on_id":"bd-3il","type":"blocks","created_at":"2026-02-20T07:37:09.970879510Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1xg","depends_on_id":"bd-cda","type":"blocks","created_at":"2026-02-20T07:37:09.193972901Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1xg","depends_on_id":"bd-ml1","type":"blocks","created_at":"2026-02-20T07:36:45.782176575Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1xg","depends_on_id":"bd-phf","type":"blocks","created_at":"2026-02-20T07:36:46.018692649Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1xg","depends_on_id":"bd-ud5h","type":"blocks","created_at":"2026-02-20T16:17:13.540626220Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-1xtf","title":"[10.16] Migrate existing or planned relevant TUI workflows to `frankentui` primitives.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.16 — Adjacent Substrate Integration Execution Track (8.7)\n\nWhy This Exists:\nAdjacent substrate integration track to enforce deep composition with frankentui, frankensqlite, sqlmodel_rust, and fastapi_rust.\n\nTask Objective:\nMigrate existing or planned relevant TUI workflows to `frankentui` primitives.\n\nAcceptance Criteria:\n- Relevant workflows use `frankentui` abstraction points; no duplicate homegrown TUI stack remains in migrated surfaces.\n\nExpected Artifacts:\n- `tests/integration/frankentui_surface_migration.rs`, `artifacts/10.16/frankentui_surface_inventory.csv`.\n\n- Machine-readable verification artifact at `artifacts/section_10_16/bd-1xtf/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_16/bd-1xtf/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.16] Migrate existing or planned relevant TUI workflows to `frankentui` primitives.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.16] Migrate existing or planned relevant TUI workflows to `frankentui` primitives.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.16] Migrate existing or planned relevant TUI workflows to `frankentui` primitives.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.16] Migrate existing or planned relevant TUI workflows to `frankentui` primitives.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.16] Migrate existing or planned relevant TUI workflows to `frankentui` primitives.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"- Relevant workflows use `frankentui` abstraction points; no duplicate homegrown TUI stack remains in migrated surfaces.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:37:01.770888002Z","created_by":"ubuntu","updated_at":"2026-02-20T17:05:11.175538515Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-16","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-1xtf","depends_on_id":"bd-34ll","type":"blocks","created_at":"2026-02-20T17:05:11.175448186Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-1xwz","title":"[10.15] Add performance budget guard for asupersync integration overhead in control-plane hot paths.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.15 — Asupersync-First Integration Execution Track (8.4-8.6)\n\nWhy This Exists:\nAsupersync-first control-plane migration track enforcing Cx-first, region ownership, cancellation correctness, and protocol-grade verification gates.\n\nTask Objective:\nAdd performance budget guard for asupersync integration overhead in control-plane hot paths.\n\nAcceptance Criteria:\n- Integration overhead remains within agreed p95/p99/cold-start budgets; regressions fail CI and include flamegraph evidence.\n\nExpected Artifacts:\n- `benchmarks/asupersync_integration_overhead/*`, `tests/perf/control_plane_overhead_gate.rs`, `artifacts/10.15/integration_overhead_report.csv`.\n\n- Machine-readable verification artifact at `artifacts/section_10_15/bd-1xwz/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_15/bd-1xwz/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.15] Add performance budget guard for asupersync integration overhead in control-plane hot paths.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.15] Add performance budget guard for asupersync integration overhead in control-plane hot paths.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.15] Add performance budget guard for asupersync integration overhead in control-plane hot paths.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.15] Add performance budget guard for asupersync integration overhead in control-plane hot paths.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.15] Add performance budget guard for asupersync integration overhead in control-plane hot paths.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"- Integration overhead remains within agreed p95/p99/cold-start budgets; regressions fail CI and include flamegraph evidence.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:37:01.363671779Z","created_by":"ubuntu","updated_at":"2026-02-20T17:00:42.843139657Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-15","self-contained","test-obligations"]}
{"id":"bd-1z3","title":"[10.2] Implement deterministic compatibility fixture runner and result canonicalizer.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.2 — Compatibility Core\n\nWhy This Exists:\nCompatibility core as strategic wedge: policy-visible behavior, divergence receipts, L1/L2 lockstep, and spec-first fixture governance.\n\nTask Objective:\nImplement deterministic compatibility fixture runner and result canonicalizer.\n\nAcceptance Criteria:\n- Implement with explicit success/failure invariants and deterministic behavior under normal, edge, and fault scenarios.\n- Ensure outcomes are verifiable via automated tests and reproducible artifact outputs.\n\nExpected Artifacts:\n- Section-owned design/spec artifact at `docs/specs/section_10_2/bd-1z3_contract.md`, capturing decision rationale, invariants, and interface boundaries.\n- Machine-readable verification artifact at `artifacts/section_10_2/bd-1z3/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_2/bd-1z3/verification_summary.md` linking implementation intent to measured outcomes.\n\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.2] Implement deterministic compatibility fixture runner and result canonicalizer.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.2] Implement deterministic compatibility fixture runner and result canonicalizer.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.2] Implement deterministic compatibility fixture runner and result canonicalizer.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.2] Implement deterministic compatibility fixture runner and result canonicalizer.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.2] Implement deterministic compatibility fixture runner and result canonicalizer.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","status":"closed","priority":1,"issue_type":"task","assignee":"CrimsonCrane","created_at":"2026-02-20T07:36:44.163128134Z","created_by":"ubuntu","updated_at":"2026-02-20T09:40:35.689847016Z","closed_at":"2026-02-20T09:40:35.689820326Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-2","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-1z3","depends_on_id":"bd-2kf","type":"blocks","created_at":"2026-02-20T07:43:20.220218653Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-1z9s","title":"[10.13] Implement transparency-log inclusion proof checks in install/update pipelines.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.13 — FCP Deep-Mined Expansion Execution Track (9I)\n\nWhy This Exists:\nDeep connector/provider contract track: lifecycle FSMs, conformance harnesses, fencing, sandboxing, revocation freshness, and protocol hardening.\n\nTask Objective:\nImplement transparency-log inclusion proof checks in install/update pipelines.\n\nAcceptance Criteria:\n- Install/update fails if required inclusion proof is missing/invalid; log roots are pinned per policy; verification path is replayable.\n\nExpected Artifacts:\n- `src/supply_chain/transparency_verifier.rs`, `tests/security/transparency_inclusion.rs`, `artifacts/10.13/transparency_proof_receipts.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_13/bd-1z9s/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_13/bd-1z9s/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.13] Implement transparency-log inclusion proof checks in install/update pipelines.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.13] Implement transparency-log inclusion proof checks in install/update pipelines.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.13] Implement transparency-log inclusion proof checks in install/update pipelines.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.13] Implement transparency-log inclusion proof checks in install/update pipelines.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.13] Implement transparency-log inclusion proof checks in install/update pipelines.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","status":"closed","priority":1,"issue_type":"task","assignee":"CrimsonCrane","created_at":"2026-02-20T07:36:52.699603053Z","created_by":"ubuntu","updated_at":"2026-02-20T11:43:23.636373487Z","closed_at":"2026-02-20T11:43:23.636346347Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-13","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-1z9s","depends_on_id":"bd-35q1","type":"blocks","created_at":"2026-02-20T07:43:12.928750837Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-1zym","title":"[10.14] Implement automatic hardening trigger on guardrail rejection evidence.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.14 — FrankenSQLite Deep-Mined Expansion Execution Track (9J)\n\nWhy This Exists:\nFrankenSQLite deep-mined control integrity track: evidence ledgers, proofs, epochs, remote effects, markers/MMR, and deterministic fault labs.\n\nTask Objective:\nImplement automatic hardening trigger on guardrail rejection evidence.\n\nAcceptance Criteria:\n- Guardrail rejection triggers hardening within configured latency bound; trigger path is idempotent; trigger events include causal evidence pointer.\n\nExpected Artifacts:\n- `tests/integration/hardening_auto_trigger.rs`, `docs/specs/hardening_trigger_policy.md`, `artifacts/10.14/hardening_trigger_events.jsonl`.\n\n- Machine-readable verification artifact at `artifacts/section_10_14/bd-1zym/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_14/bd-1zym/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.14] Implement automatic hardening trigger on guardrail rejection evidence.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.14] Implement automatic hardening trigger on guardrail rejection evidence.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.14] Implement automatic hardening trigger on guardrail rejection evidence.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.14] Implement automatic hardening trigger on guardrail rejection evidence.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.14] Implement automatic hardening trigger on guardrail rejection evidence.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"Guardrail rejection triggers hardening within configured latency bound; trigger path is idempotent; trigger events include causal evidence pointer.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:36:56.138256697Z","created_by":"ubuntu","updated_at":"2026-02-20T16:23:52.829412928Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-14","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-1zym","depends_on_id":"bd-3a3q","type":"blocks","created_at":"2026-02-20T16:23:52.829364638Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1zym","depends_on_id":"bd-3rya","type":"blocks","created_at":"2026-02-20T07:43:14.725796218Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-206h","title":"[10.14] Implement idempotency dedupe store semantics (same key/same payload returns cached outcome; mismatch conflicts).","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.14 — FrankenSQLite Deep-Mined Expansion Execution Track (9J)\n\nWhy This Exists:\nFrankenSQLite deep-mined control integrity track: evidence ledgers, proofs, epochs, remote effects, markers/MMR, and deterministic fault labs.\n\nTask Objective:\nImplement idempotency dedupe store semantics (same key/same payload returns cached outcome; mismatch conflicts).\n\nAcceptance Criteria:\n- Duplicate same-payload requests are safely deduped; same-key different-payload conflicts hard-fail; dedupe state handles restart recovery.\n\nExpected Artifacts:\n- `tests/integration/idempotency_dedupe_store.rs`, `docs/specs/idempotency_store_semantics.md`, `artifacts/10.14/idempotency_conflict_report.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_14/bd-206h/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_14/bd-206h/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.14] Implement idempotency dedupe store semantics (same key/same payload returns cached outcome; mismatch conflicts).\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.14] Implement idempotency dedupe store semantics (same key/same payload returns cached outcome; mismatch conflicts).\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.14] Implement idempotency dedupe store semantics (same key/same payload returns cached outcome; mismatch conflicts).\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.14] Implement idempotency dedupe store semantics (same key/same payload returns cached outcome; mismatch conflicts).\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.14] Implement idempotency dedupe store semantics (same key/same payload returns cached outcome; mismatch conflicts).\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"Duplicate same-payload requests are safely deduped; same-key different-payload conflicts hard-fail; dedupe state handles restart recovery.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:36:57.892308276Z","created_by":"ubuntu","updated_at":"2026-02-20T15:44:05.616764950Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-14","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-206h","depends_on_id":"bd-12n3","type":"blocks","created_at":"2026-02-20T07:43:15.613060159Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-209w","title":"[15] Pillar: signed extension registry with provenance and revocation","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 15\n\nTask Objective:\nImplement signed extension registry pillar with strict provenance and revocation controls.\n\nExecution Requirements:\n- Make this requirement machine-verifiable and release-gated where applicable.\n- Keep behavior self-documenting so future contributors do not need to re-open the master plan.\n\nTesting & Logging Requirements:\n- Unit tests for rule/contract logic and error conditions.\n- Integration/E2E tests for workflow-level enforcement.\n- Structured logs with stable codes and trace IDs.\n- Reproducible artifacts that prove contract satisfaction.\n\nAcceptance Criteria:\n- Success and failure invariants for [15] Pillar: signed extension registry with provenance and revocation are explicitly quantified and machine-verifiable.\n- Determinism requirements for [15] Pillar: signed extension registry with provenance and revocation are validated across repeated runs and adversarial perturbations.\n\n\nExpected Artifacts:\n- Machine-readable verification artifact with explicit canonical path under artifacts/section_15/bd-209w/verification_evidence.json, suitable for CI/release gating.\n- Human-readable verification summary with explicit canonical path under artifacts/section_15/bd-209w/verification_summary.md, linking implementation intent to measured validation outcomes.\n\nTask-Specific Clarification:\n- The implementation must deliver the exact capability named in the title, with no scope dilution and no silent feature omission.\n- Validation artifacts must include capability-specific thresholds, pass/fail criteria, and reproducible evidence bundles tied to this bead ID.\n- Program/section verification gates must be able to consume this bead's outputs without manual interpretation.\n\nWhy This Improves User Outcomes:\n- \"[15] Pillar: signed extension registry with provenance and revocation\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[15] Pillar: signed extension registry with provenance and revocation\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"1. Extension registry exists and enforces cryptographic signing: all published extensions must be signed with a verified identity key.\n2. Provenance tracking: each extension version records build provenance (source repo, commit hash, build environment hash, builder identity).\n3. Revocation support: compromised extensions can be revoked in <= 5 minutes; revocation propagates to all consumers within 1 hour.\n4. Revocation is irrevocable (cannot be undone without re-signing with a new key and new review).\n5. Registry rejects unsigned extensions with clear error message.\n6. Provenance is queryable: users can verify any extension's build chain before installation.\n7. Signing key rotation is supported without breaking existing installations (grace period for old signatures).\n8. CI test: publish an unsigned extension; verify rejection. Publish a signed extension; verify acceptance. Revoke it; verify consumers are notified.\n9. Evidence: extension_registry_status.json with total extensions, signed %, revocation count, and average revocation propagation time.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:39:36.160912260Z","created_by":"ubuntu","updated_at":"2026-02-20T16:09:26.661730374Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-15","self-contained","test-obligations"]}
{"id":"bd-20a","title":"[PLAN 10.5] Security + Policy Product Surfaces","description":"Section: 10.5 — Security + Policy Product Surfaces\n\nStrategic Context:\nSecurity and policy product surfaces: decision receipts, incident replay, expected-loss policying, and auditable degraded-mode behavior.\n\nExecution Requirements:\n- Preserve all scoped capabilities from the canonical plan.\n- Keep contracts self-contained so future contributors can execute without reopening the master plan.\n- Require explicit testing strategy (unit + integration/e2e) and structured logging/telemetry for every child bead.\n- Require artifact-backed validation for performance, security, and correctness claims.\n\nDependency Semantics:\n- This epic is blocked by its child implementation beads.\n- Cross-epic dependencies encode strategic sequencing and canonical ownership boundaries.\n\n## Success Criteria\n- All child beads for this section are completed with acceptance artifacts attached and no unresolved blockers.\n- Section-level verification gate for comprehensive unit tests, integration/e2e workflows, and detailed structured logging evidence is green.\n- Scope-to-plan traceability is explicit, with no feature loss versus the canonical plan section.\n\n## Optimization Notes\n- User-Outcome Lens: \"[PLAN 10.5] Security + Policy Product Surfaces\" must improve operator confidence, safety posture, and deterministic recovery behavior under both normal and adversarial conditions.\n- Cross-Section Coordination: child beads must encode integration assumptions explicitly to avoid local optimizations that degrade system-wide correctness.\n- Verification Non-Negotiable: completion requires reproducible unit/integration/E2E evidence and structured logs suitable for independent replay.\n- Scope Protection: any simplification must preserve canonical-plan feature intent and be justified with explicit tradeoff documentation.","status":"open","priority":1,"issue_type":"epic","created_at":"2026-02-20T07:36:40.625682742Z","created_by":"ubuntu","updated_at":"2026-02-20T16:17:13.374858204Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-5"],"dependencies":[{"issue_id":"bd-20a","depends_on_id":"bd-137","type":"blocks","created_at":"2026-02-20T07:36:46.096981716Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-20a","depends_on_id":"bd-1koz","type":"blocks","created_at":"2026-02-20T07:48:24.613964616Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-20a","depends_on_id":"bd-1ta","type":"blocks","created_at":"2026-02-20T07:37:10.124940476Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-20a","depends_on_id":"bd-1xg","type":"blocks","created_at":"2026-02-20T07:37:10.085765636Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-20a","depends_on_id":"bd-21z","type":"blocks","created_at":"2026-02-20T07:36:46.176895270Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-20a","depends_on_id":"bd-2fa","type":"blocks","created_at":"2026-02-20T07:36:46.337388607Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-20a","depends_on_id":"bd-2yc","type":"blocks","created_at":"2026-02-20T07:36:46.420485430Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-20a","depends_on_id":"bd-33b","type":"blocks","created_at":"2026-02-20T07:36:46.500080260Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-20a","depends_on_id":"bd-3il","type":"blocks","created_at":"2026-02-20T07:37:10.047461638Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-20a","depends_on_id":"bd-3nr","type":"blocks","created_at":"2026-02-20T07:36:46.580197663Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-20a","depends_on_id":"bd-cda","type":"blocks","created_at":"2026-02-20T07:37:09.233010936Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-20a","depends_on_id":"bd-sh3","type":"blocks","created_at":"2026-02-20T07:36:46.658462145Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-20a","depends_on_id":"bd-ud5h","type":"blocks","created_at":"2026-02-20T16:17:13.374770961Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-20a","depends_on_id":"bd-vll","type":"blocks","created_at":"2026-02-20T07:36:46.258189635Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-20eg","title":"[10.15] Section-wide verification gate: comprehensive unit+e2e+logging","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.15\n\nTask Objective:\nCreate a hard section completion gate that verifies all implemented behavior in this section with comprehensive unit tests, integration/e2e scripts, and detailed structured logging evidence.\n\nAcceptance Criteria:\n- Section test matrix covers happy-path, edge-case, and adversarial/error-path scenarios for all section deliverables.\n- E2E scripts execute representative end-user workflows and policy/control-plane transitions for this section.\n- Structured logs include stable event/error codes, trace correlation IDs, and enough context for deterministic replay triage.\n- Verification report is deterministic and machine-readable for CI/release gating.\n\nExpected Artifacts:\n- Section-level test matrix and coverage summary artifact.\n- E2E script suite with pass/fail outputs and reproducible fixtures/seeds.\n- Structured log validation report and traceability bundle.\n- Gate verdict artifact consumable by release automation.\n\n- Machine-readable verification artifact at `artifacts/section_10_15/bd-20eg/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_15/bd-20eg/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests must validate contracts, invariants, and failure semantics.\n- E2E tests must validate cross-component behavior from user/operator entrypoints to persisted effects.\n- Logging must support root-cause analysis without hidden context requirements.\n\nTask-Specific Clarification:\n- For \"[10.15] Section-wide verification gate: comprehensive unit+e2e+logging\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.15] Section-wide verification gate: comprehensive unit+e2e+logging\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.15] Section-wide verification gate: comprehensive unit+e2e+logging\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.15] Section-wide verification gate: comprehensive unit+e2e+logging\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.15] Section-wide verification gate: comprehensive unit+e2e+logging\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:48:14.380295860Z","created_by":"ubuntu","updated_at":"2026-02-20T15:02:21.806383192Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-15","test-obligations","verification-gate"],"dependencies":[{"issue_id":"bd-20eg","depends_on_id":"bd-145n","type":"blocks","created_at":"2026-02-20T07:48:14.902052648Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-20eg","depends_on_id":"bd-15j6","type":"blocks","created_at":"2026-02-20T07:48:15.000448235Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-20eg","depends_on_id":"bd-181w","type":"blocks","created_at":"2026-02-20T07:48:15.096091826Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-20eg","depends_on_id":"bd-1cs7","type":"blocks","created_at":"2026-02-20T07:48:15.383228093Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-20eg","depends_on_id":"bd-1cwp","type":"blocks","created_at":"2026-02-20T07:48:15.191345371Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-20eg","depends_on_id":"bd-1dpd","type":"blocks","created_at":"2026-02-20T08:24:26.419698294Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-20eg","depends_on_id":"bd-1f8m","type":"blocks","created_at":"2026-02-20T07:48:14.620636757Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-20eg","depends_on_id":"bd-1hbw","type":"blocks","created_at":"2026-02-20T07:48:15.047427006Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-20eg","depends_on_id":"bd-1id0","type":"blocks","created_at":"2026-02-20T07:48:15.622846444Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-20eg","depends_on_id":"bd-1n5p","type":"blocks","created_at":"2026-02-20T07:48:15.334337551Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-20eg","depends_on_id":"bd-1xwz","type":"blocks","created_at":"2026-02-20T07:48:14.525913169Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-20eg","depends_on_id":"bd-2177","type":"blocks","created_at":"2026-02-20T07:48:15.570039929Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-20eg","depends_on_id":"bd-25oa","type":"blocks","created_at":"2026-02-20T07:48:14.765281165Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-20eg","depends_on_id":"bd-2g6r","type":"blocks","created_at":"2026-02-20T07:48:15.523039990Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-20eg","depends_on_id":"bd-2h2s","type":"blocks","created_at":"2026-02-20T07:48:14.573322111Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-20eg","depends_on_id":"bd-2tdi","type":"blocks","created_at":"2026-02-20T07:48:15.430358876Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-20eg","depends_on_id":"bd-2twu","type":"blocks","created_at":"2026-02-20T08:20:52.268307086Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-20eg","depends_on_id":"bd-3014","type":"blocks","created_at":"2026-02-20T07:48:15.238341905Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-20eg","depends_on_id":"bd-33kj","type":"blocks","created_at":"2026-02-20T07:48:14.476202260Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-20eg","depends_on_id":"bd-3epz","type":"blocks","created_at":"2026-02-20T15:02:21.806309495Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-20eg","depends_on_id":"bd-3gnh","type":"blocks","created_at":"2026-02-20T07:48:14.666356923Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-20eg","depends_on_id":"bd-3h63","type":"blocks","created_at":"2026-02-20T07:48:15.145068368Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-20eg","depends_on_id":"bd-3tpg","type":"blocks","created_at":"2026-02-20T07:48:14.856468736Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-20eg","depends_on_id":"bd-3u6o","type":"blocks","created_at":"2026-02-20T07:48:14.810956257Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-20eg","depends_on_id":"bd-721z","type":"blocks","created_at":"2026-02-20T07:48:15.476416351Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-20eg","depends_on_id":"bd-cuut","type":"blocks","created_at":"2026-02-20T07:48:15.284850720Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-20eg","depends_on_id":"bd-h93z","type":"blocks","created_at":"2026-02-20T07:48:14.719037824Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-20eg","depends_on_id":"bd-tyr2","type":"blocks","created_at":"2026-02-20T07:48:14.948864107Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-20l","title":"[10.1] Add ADR: \"Hybrid Baseline Strategy\" codifying no Bun-first clone, spec-first compatibility extraction, and native franken architecture from day one.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.1 — Charter + Split Governance\n\nWhy This Exists:\nProgram charter and governance baseline: split contracts, reproducibility, and anti-regression rules for strategic integrity.\n\nTask Objective:\nAdd ADR: \"Hybrid Baseline Strategy\" codifying no Bun-first clone, spec-first compatibility extraction, and native franken architecture from day one.\n\nAcceptance Criteria:\n- Implement with explicit success/failure invariants and deterministic behavior under normal, edge, and fault scenarios.\n- Ensure outcomes are verifiable via automated tests and reproducible artifact outputs.\n\nExpected Artifacts:\n- Section-owned design/spec artifact at `docs/specs/section_10_1/bd-20l_contract.md`, capturing decision rationale, invariants, and interface boundaries.\n- Machine-readable verification artifact at `artifacts/section_10_1/bd-20l/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_1/bd-20l/verification_summary.md` linking implementation intent to measured outcomes.\n\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.1] Add ADR: \"Hybrid Baseline Strategy\" codifying no Bun-first clone, spec-first compatibility extraction, and native franken architecture from day one.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.1] Add ADR: \"Hybrid Baseline Strategy\" codifying no Bun-first clone, spec-first compatibility extraction, and native franken architecture from day one.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.1] Add ADR: \"Hybrid Baseline Strategy\" codifying no Bun-first clone, spec-first compatibility extraction, and native franken architecture from day one.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.1] Add ADR: \"Hybrid Baseline Strategy\" codifying no Bun-first clone, spec-first compatibility extraction, and native franken architecture from day one.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.1] Add ADR: \"Hybrid Baseline Strategy\" codifying no Bun-first clone, spec-first compatibility extraction, and native franken architecture from day one.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","status":"closed","priority":1,"issue_type":"task","assignee":"CrimsonCrane","created_at":"2026-02-20T07:36:43.680535788Z","created_by":"ubuntu","updated_at":"2026-02-20T09:23:59.423500678Z","closed_at":"2026-02-20T09:23:59.423474329Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-1","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-20l","depends_on_id":"bd-1mj","type":"blocks","created_at":"2026-02-20T07:43:10.743693489Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-20uo","title":"[10.14] Integrate proof-carrying repair artifacts into decode/reconstruction paths.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.14 — FrankenSQLite Deep-Mined Expansion Execution Track (9J)\n\nWhy This Exists:\nFrankenSQLite deep-mined control integrity track: evidence ledgers, proofs, epochs, remote effects, markers/MMR, and deterministic fault labs.\n\nTask Objective:\nIntegrate proof-carrying repair artifacts into decode/reconstruction paths.\n\nAcceptance Criteria:\n- Repair operations emit proof metadata in required modes; proof verification API validates emitted artifacts; missing proofs are flagged where mandatory.\n\nExpected Artifacts:\n- `src/repair/proof_carrying_decode.rs`, `tests/conformance/proof_carrying_repair.rs`, `artifacts/10.14/repair_proof_samples.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_14/bd-20uo/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_14/bd-20uo/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.14] Integrate proof-carrying repair artifacts into decode/reconstruction paths.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.14] Integrate proof-carrying repair artifacts into decode/reconstruction paths.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.14] Integrate proof-carrying repair artifacts into decode/reconstruction paths.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.14] Integrate proof-carrying repair artifacts into decode/reconstruction paths.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.14] Integrate proof-carrying repair artifacts into decode/reconstruction paths.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"Repair operations emit proof metadata in required modes; proof verification API validates emitted artifacts; missing proofs are flagged where mandatory.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:36:56.625163980Z","created_by":"ubuntu","updated_at":"2026-02-20T15:44:08.839349643Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-14","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-20uo","depends_on_id":"bd-1l62","type":"blocks","created_at":"2026-02-20T07:43:14.980735544Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-20z","title":"[PLAN 10.11] FrankenSQLite-Inspired Runtime Systems Integration Track","description":"Section: 10.11 — FrankenSQLite-Inspired Runtime Systems Integration Track\n\nStrategic Context:\nFrankenSQLite-inspired systems integration of capabilities, cancellation protocol, obligations, deterministic labs, and anti-entropy.\n\nExecution Requirements:\n- Preserve all scoped capabilities from the canonical plan.\n- Keep contracts self-contained so future contributors can execute without reopening the master plan.\n- Require explicit testing strategy (unit + integration/e2e) and structured logging/telemetry for every child bead.\n- Require artifact-backed validation for performance, security, and correctness claims.\n\nDependency Semantics:\n- This epic is blocked by its child implementation beads.\n- Cross-epic dependencies encode strategic sequencing and canonical ownership boundaries.\n\n## Success Criteria\n- All child beads for this section are completed with acceptance artifacts attached and no unresolved blockers.\n- Section-level verification gate for comprehensive unit tests, integration/e2e workflows, and detailed structured logging evidence is green.\n- Scope-to-plan traceability is explicit, with no feature loss versus the canonical plan section.\n\n## Optimization Notes\n- User-Outcome Lens: \"[PLAN 10.11] FrankenSQLite-Inspired Runtime Systems Integration Track\" must improve operator confidence, safety posture, and deterministic recovery behavior under both normal and adversarial conditions.\n- Cross-Section Coordination: child beads must encode integration assumptions explicitly to avoid local optimizations that degrade system-wide correctness.\n- Verification Non-Negotiable: completion requires reproducible unit/integration/E2E evidence and structured logs suitable for independent replay.\n- Scope Protection: any simplification must preserve canonical-plan feature intent and be justified with explicit tradeoff documentation.","status":"open","priority":2,"issue_type":"epic","created_at":"2026-02-20T07:36:41.112788935Z","created_by":"ubuntu","updated_at":"2026-02-20T08:16:47.109184553Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-11"],"dependencies":[{"issue_id":"bd-20z","depends_on_id":"bd-1jpo","type":"blocks","created_at":"2026-02-20T07:48:08.241412787Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-20z","depends_on_id":"bd-24k","type":"blocks","created_at":"2026-02-20T07:36:50.018349403Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-20z","depends_on_id":"bd-2ah","type":"blocks","created_at":"2026-02-20T07:36:50.097560728Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-20z","depends_on_id":"bd-2gr","type":"blocks","created_at":"2026-02-20T07:36:50.507533577Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-20z","depends_on_id":"bd-2ko","type":"blocks","created_at":"2026-02-20T07:36:50.262374764Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-20z","depends_on_id":"bd-2nt","type":"blocks","created_at":"2026-02-20T07:36:50.423815637Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-20z","depends_on_id":"bd-390","type":"blocks","created_at":"2026-02-20T07:36:50.788797873Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-20z","depends_on_id":"bd-3he","type":"blocks","created_at":"2026-02-20T07:36:50.178419883Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-20z","depends_on_id":"bd-3hw","type":"blocks","created_at":"2026-02-20T07:36:50.592136395Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-20z","depends_on_id":"bd-3qo","type":"blocks","created_at":"2026-02-20T07:37:10.820071904Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-20z","depends_on_id":"bd-3u4","type":"blocks","created_at":"2026-02-20T07:36:50.344097979Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-20z","depends_on_id":"bd-3vm","type":"blocks","created_at":"2026-02-20T07:36:49.780345147Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-20z","depends_on_id":"bd-5rh","type":"blocks","created_at":"2026-02-20T07:37:10.781857082Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-20z","depends_on_id":"bd-7om","type":"blocks","created_at":"2026-02-20T07:36:49.938282063Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-20z","depends_on_id":"bd-93k","type":"blocks","created_at":"2026-02-20T07:36:49.859363223Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-20z","depends_on_id":"bd-cda","type":"blocks","created_at":"2026-02-20T07:37:09.466777348Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-20z","depends_on_id":"bd-cvt","type":"blocks","created_at":"2026-02-20T07:36:49.701076385Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-20z","depends_on_id":"bd-lus","type":"blocks","created_at":"2026-02-20T07:36:50.675525232Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-214l","title":"Epic: Radical Expansion - Verifier SDK + Claims [10.17d]","description":"- type: epic","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-20T07:47:58.072163029Z","updated_at":"2026-02-20T07:49:21.300299832Z","closed_at":"2026-02-20T07:49:21.300278963Z","close_reason":"Superseded by canonical detailed master-plan graph (bd-33v) with full 10.N-10.21 and 11-16 coverage, explicit dependencies, and per-section verification gates.","source_repo":".","compaction_level":0,"original_size":0}
{"id":"bd-2177","title":"[10.15] Define high-impact workflow inventory mapped to required asupersync primitives.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.15 — Asupersync-First Integration Execution Track (8.4-8.6)\n\nWhy This Exists:\nAsupersync-first control-plane migration track enforcing Cx-first, region ownership, cancellation correctness, and protocol-grade verification gates.\n\nTask Objective:\nDefine high-impact workflow inventory mapped to required asupersync primitives.\n\nAcceptance Criteria:\n- Every critical workflow is mapped to `Cx`, region, cancellation, obligation, remote, epoch, and evidence requirements; unmapped workflows fail planning gate.\n\nExpected Artifacts:\n- `docs/architecture/high_impact_workflow_map.md`, `artifacts/10.15/workflow_primitive_matrix.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_15/bd-2177/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_15/bd-2177/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.15] Define high-impact workflow inventory mapped to required asupersync primitives.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.15] Define high-impact workflow inventory mapped to required asupersync primitives.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.15] Define high-impact workflow inventory mapped to required asupersync primitives.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.15] Define high-impact workflow inventory mapped to required asupersync primitives.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.15] Define high-impact workflow inventory mapped to required asupersync primitives.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"- Every critical workflow is mapped to `Cx`, region, cancellation, obligation, remote, epoch, and evidence requirements; unmapped workflows fail planning gate.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:36:59.561049939Z","created_by":"ubuntu","updated_at":"2026-02-20T17:04:28.294265644Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-15","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-2177","depends_on_id":"bd-1id0","type":"blocks","created_at":"2026-02-20T17:04:28.294212996Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-21fo","title":"[10.17] Build self-evolving optimization governor with safety-envelope enforcement.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.17 — Radical Expansion Execution Track (9K)\n\nWhy This Exists:\nRadical expansion track for proof-carrying speculation, Bayesian adversary control, time-travel replay, ZK attestations, and claim compiler systems.\n\nTask Objective:\nBuild self-evolving optimization governor with safety-envelope enforcement.\n\nAcceptance Criteria:\n- Candidate optimizations require shadow evaluation plus anytime-valid safety checks; unsafe or non-beneficial policies auto-reject or auto-revert with evidence; governor can only adjust exposed runtime knobs, not local engine-core internals.\n\nExpected Artifacts:\n- `docs/specs/optimization_governor.md`, `src/perf/optimization_governor.rs`, `tests/perf/governor_safety_envelope.rs`, `artifacts/10.17/governor_decision_log.jsonl`.\n\n- Machine-readable verification artifact at `artifacts/section_10_17/bd-21fo/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_17/bd-21fo/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.17] Build self-evolving optimization governor with safety-envelope enforcement.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.17] Build self-evolving optimization governor with safety-envelope enforcement.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.17] Build self-evolving optimization governor with safety-envelope enforcement.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.17] Build self-evolving optimization governor with safety-envelope enforcement.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.17] Build self-evolving optimization governor with safety-envelope enforcement.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"- Candidate optimizations require shadow evaluation plus anytime-valid safety checks; unsafe or non-beneficial policies auto-reject or auto-revert with evidence; governor can only adjust exposed runtime knobs, not local engine-core internals.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:37:03.598298856Z","created_by":"ubuntu","updated_at":"2026-02-20T15:45:59.308943834Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-17","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-21fo","depends_on_id":"bd-26mk","type":"blocks","created_at":"2026-02-20T07:43:18.603142707Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-21qe","title":"Epic: Charter + Split Governance [10.1]","description":"- type: epic","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-20T07:47:58.072163029Z","updated_at":"2026-02-20T07:49:21.089828756Z","closed_at":"2026-02-20T07:49:21.089805192Z","close_reason":"Superseded by canonical detailed master-plan graph (bd-33v) with full 10.N-10.21 and 11-16 coverage, explicit dependencies, and per-section verification gates.","source_repo":".","compaction_level":0,"original_size":0}
{"id":"bd-21z","title":"[10.5] Implement signed decision receipt export for high-impact actions.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.5 — Security + Policy Product Surfaces (Item 2 of 8)\n\nWhy This Exists:\nSigned decision receipts are the audit backbone of the policy system. Every high-impact action (quarantine, revocation, policy change, deployment promotion) must produce a cryptographically signed receipt that captures the decision context, evidence used, action taken, and rollback path. This enables deterministic post-hoc analysis and external verification.\n\nTask Objective:\nImplement signed decision receipt export for all high-impact actions in the policy/control system.\n\nDetailed Acceptance Criteria:\n1. Decision receipt schema captures: action type, decision timestamp, evidence references (ledger entry IDs), actor identity, policy rule chain that authorized the action, confidence context, and rollback command.\n2. Receipts are cryptographically signed by the acting control-plane identity.\n3. Receipt export in both machine-readable (JSON) and human-readable formats.\n4. Receipts are append-only and hash-chained for tamper evidence.\n5. CLI surface: receipt export accessible via franken-node incident bundle and trust commands.\n6. Receipts integrate with evidence ledger (10.14) and operator copilot (10.0.8).\n7. High-impact action classes requiring receipts: quarantine, revocation, policy change, deployment promotion, trust-level transition.\n\nKey Dependencies:\n- Depends on 10.14 (FrankenSQLite Deep-Mined) for evidence ledger integration.\n- Depends on 10.13 (FCP Deep-Mined) for authenticated control channel.\n- Consumed by 10.8 (Operational Readiness) for incident bundle retention.\n- Consumed by 10.17 (Radical Expansion) for verifier SDK integration.\n\nExpected Artifacts:\n- src/security/decision_receipt.rs — receipt schema, signing, export.\n- CLI integration for receipt export in incident and trust commands.\n- docs/specs/section_10_5/bd-21z_contract.md\n- artifacts/section_10_5/bd-21z/verification_evidence.json\n\nTesting and Logging Requirements:\n- Unit tests: receipt construction, signing, hash chain integrity, schema validation.\n- Integration tests: full action -> receipt generation -> export -> verification pipeline.\n- E2E tests: franken-node incident bundle producing complete receipt chain.\n- Adversarial tests: receipt tampering detection, replay of receipts from different contexts.\n- Structured logs: DECISION_RECEIPT_GENERATED, RECEIPT_SIGNED, RECEIPT_EXPORTED, HASH_CHAIN_VERIFIED with trace IDs.","acceptance_criteria":"1. Define a SignedReceipt struct with fields: receipt_id (UUID v7), action_name (string), actor_identity (string), timestamp (RFC-3339), input_hash (SHA-256 of serialized action input), output_hash (SHA-256 of serialized action output), decision (enum: Approved | Denied | Escalated), rationale (string), and signature (Ed25519 detached signature, base64-encoded).\n2. Implement sign_receipt(receipt: &Receipt, signing_key: &Ed25519PrivateKey) -> SignedReceipt that produces a deterministic canonical JSON serialization before signing (keys sorted, no optional whitespace).\n3. Implement verify_receipt(signed: &SignedReceipt, public_key: &Ed25519PublicKey) -> Result<bool> that returns true only if the signature matches the canonical form.\n4. High-impact actions are tagged via a #[high_impact] attribute macro or a runtime registry; any action so tagged must produce a receipt or the call returns Err.\n5. Receipts must be exportable as both JSON and CBOR; round-trip fidelity test must pass (deserialize(serialize(r)) == r).\n6. Provide an export_receipts(filter: ReceiptQuery) -> Vec<SignedReceipt> query API supporting time-range and action-name filters.\n7. Verification: scripts/check_signed_receipt.py --json validates signature correctness on sample receipts; unit tests cover sign, verify, tamper-detection (flipped bit fails verify), and round-trip; evidence artifact in artifacts/section_10_5/bd-21z/.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:36:46.140259819Z","created_by":"ubuntu","updated_at":"2026-02-20T16:59:01.945684558Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-5","self-contained","test-obligations"]}
{"id":"bd-229","title":"[PLAN 10.3] Migration System","description":"Section: 10.3 — Migration System\n\nStrategic Context:\nMigration autopilot pipeline from audit to rollout, designed to collapse migration friction with deterministic confidence and replayability.\n\nExecution Requirements:\n- Preserve all scoped capabilities from the canonical plan.\n- Keep contracts self-contained so future contributors can execute without reopening the master plan.\n- Require explicit testing strategy (unit + integration/e2e) and structured logging/telemetry for every child bead.\n- Require artifact-backed validation for performance, security, and correctness claims.\n\nDependency Semantics:\n- This epic is blocked by its child implementation beads.\n- Cross-epic dependencies encode strategic sequencing and canonical ownership boundaries.\n\n## Success Criteria\n- All child beads for this section are completed with acceptance artifacts attached and no unresolved blockers.\n- Section-level verification gate for comprehensive unit tests, integration/e2e workflows, and detailed structured logging evidence is green.\n- Scope-to-plan traceability is explicit, with no feature loss versus the canonical plan section.\n\n## Optimization Notes\n- User-Outcome Lens: \"[PLAN 10.3] Migration System\" must improve operator confidence, safety posture, and deterministic recovery behavior under both normal and adversarial conditions.\n- Cross-Section Coordination: child beads must encode integration assumptions explicitly to avoid local optimizations that degrade system-wide correctness.\n- Verification Non-Negotiable: completion requires reproducible unit/integration/E2E evidence and structured logs suitable for independent replay.\n- Scope Protection: any simplification must preserve canonical-plan feature intent and be justified with explicit tradeoff documentation.","status":"closed","priority":1,"issue_type":"epic","created_at":"2026-02-20T07:36:40.464258670Z","created_by":"ubuntu","updated_at":"2026-02-20T10:23:12.513827884Z","closed_at":"2026-02-20T10:23:12.513802988Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-3"],"dependencies":[{"issue_id":"bd-229","depends_on_id":"bd-12f","type":"blocks","created_at":"2026-02-20T07:36:45.226343355Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-229","depends_on_id":"bd-2a0","type":"blocks","created_at":"2026-02-20T07:36:44.833338372Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-229","depends_on_id":"bd-2ew","type":"blocks","created_at":"2026-02-20T07:36:44.993367595Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-229","depends_on_id":"bd-2st","type":"blocks","created_at":"2026-02-20T07:36:45.071858368Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-229","depends_on_id":"bd-33x","type":"blocks","created_at":"2026-02-20T07:36:44.913766113Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-229","depends_on_id":"bd-3dn","type":"blocks","created_at":"2026-02-20T07:36:45.149421614Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-229","depends_on_id":"bd-3enl","type":"blocks","created_at":"2026-02-20T07:48:23.311983086Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-229","depends_on_id":"bd-3f9","type":"blocks","created_at":"2026-02-20T07:36:45.385689926Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-229","depends_on_id":"bd-3il","type":"blocks","created_at":"2026-02-20T07:37:09.932613653Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-229","depends_on_id":"bd-cda","type":"blocks","created_at":"2026-02-20T07:37:09.153087314Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-229","depends_on_id":"bd-hg1","type":"blocks","created_at":"2026-02-20T07:36:45.303263873Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-22e7","title":"[5] Method Stack Compliance — 4 mandatory execution disciplines","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md — Section 5\n\n## Why This Exists\nfranken_node is driven by four complementary methodologies. These are not optional preferences; they are mandatory execution disciplines. Every implementation bead must comply with at least one of these method stacks, and cross-cutting beads must demonstrate compliance with all applicable stacks.\n\n## Four Required Method Stacks\n\n### 5.1 extreme-software-optimization (Execution Discipline)\nMandatory loop for every performance-sensitive change:\n1. Baseline (p50/p95/p99, throughput, memory, cold start)\n2. Profile (top hotspots only)\n3. Prove behavior invariance and compatibility envelopes\n4. Implement one lever\n5. Verify compatibility/security artifacts\n6. Re-profile\nNO optimization lands without artifact-backed regression safety.\n\n### 5.2 alien-artifact-coding (Mathematical Decision Core)\nUse formal decision systems for product control surfaces:\n- Expected-loss rollout choices\n- Posterior trust state updates\n- Confidence-aware migration recommendations\n- Explainable policy decisions and receipts\n\n### 5.3 alien-graveyard (High-EV Primitive Selection)\nAdopt only high-EV disruptive primitives with fallback contracts:\n- EV thresholding (EV >= 2.0)\n- Failure-mode predesign\n- Deterministic degraded operation pathways\n\n### 5.4 porting-to-rust (Spec-First Essence Extraction Protocol)\nFor compatibility surfaces, apply the porting discipline as extraction-and-proof:\n- Extract behavior into explicit specs (data shapes, invariants, defaults, errors, edge cases)\n- Capture Node/Bun fixture outputs as conformance baselines\n- Implement from spec and fixture contracts, NOT legacy source structure\n- Enforce parity and divergence visibility via lockstep oracle + artifact gates\nRULE: Legacy code is input to specification and oracle generation, NOT implementation blueprint.\n\n## Compliance Mapping to Execution Tracks\n- 5.1 (perf discipline): Required for 10.6, 10.14, 10.15, 10.17, 10.18\n- 5.2 (math decision): Required for 10.5, 10.17, 10.19, 10.20, 10.21\n- 5.3 (high-EV primitives): Required for all 10.x tracks adopting advanced primitives\n- 5.4 (spec-first extraction): Required for 10.2, 10.3, 10.7\n\n## Acceptance Criteria\n- Each implementation PR cites which method stack(s) it follows\n- Performance PRs include before/after baseline artifacts (5.1)\n- Decision-surface PRs include formal decision rationale (5.2)\n- New primitive adoption PRs include EV analysis and fallback contract (5.3)\n- Compatibility PRs include spec reference and fixture IDs (5.4)\n\n## Testing Requirements\n- CI gate checking for method stack citation in PR descriptions\n- Audit script validating artifact presence for performance changes\n- Review checklist enforcing method stack compliance\n\n\n## Additional Verification Requirements\n- Unit tests for method-stack compliance validators (citation checks, artifact-rule checks, stack-to-change classification).\n- E2E compliance scripts that execute representative change flows and verify required method-stack evidence is produced end-to-end.\n- Structured logs for compliance evaluations with deterministic rule IDs, pass/fail status, and remediation guidance.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T16:14:34.097919725Z","created_by":"ubuntu","updated_at":"2026-02-20T17:08:28.667457427Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["cross-cutting","methodology","plan","section-5"]}
{"id":"bd-22yy","title":"[10.14] Add DPOR-style schedule exploration gates for control/epoch/remote protocols.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.14 — FrankenSQLite Deep-Mined Expansion Execution Track (9J)\n\nWhy This Exists:\nFrankenSQLite deep-mined control integrity track: evidence ledgers, proofs, epochs, remote effects, markers/MMR, and deterministic fault labs.\n\nTask Objective:\nAdd DPOR-style schedule exploration gates for control/epoch/remote protocols.\n\nAcceptance Criteria:\n- DPOR explorer covers targeted protocol classes; minimal counterexample traces are emitted on failure; gate runs within bounded CI budget.\n\nExpected Artifacts:\n- `tests/lab/dpor_protocol_exploration.rs`, `docs/testing/dpor_gate_scope.md`, `artifacts/10.14/dpor_exploration_summary.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_14/bd-22yy/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_14/bd-22yy/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.14] Add DPOR-style schedule exploration gates for control/epoch/remote protocols.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.14] Add DPOR-style schedule exploration gates for control/epoch/remote protocols.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.14] Add DPOR-style schedule exploration gates for control/epoch/remote protocols.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.14] Add DPOR-style schedule exploration gates for control/epoch/remote protocols.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.14] Add DPOR-style schedule exploration gates for control/epoch/remote protocols.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"DPOR explorer covers targeted protocol classes; minimal counterexample traces are emitted on failure; gate runs within bounded CI budget.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:36:59.312014395Z","created_by":"ubuntu","updated_at":"2026-02-20T16:24:30.501778755Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-14","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-22yy","depends_on_id":"bd-3hdv","type":"blocks","created_at":"2026-02-20T16:24:30.315869111Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-22yy","depends_on_id":"bd-ac83","type":"blocks","created_at":"2026-02-20T16:24:30.501713263Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-232t","title":"[10.21] Integrate BPET trajectory signals into trust cards and adversary graph posterior updates.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.21 — Behavioral Phenotype Evolution Tracker Execution Track (9O)\n\nWhy This Exists:\nBPET longitudinal phenotype intelligence track for pre-compromise trajectory detection and integration with DGIS/ATC/migration/economic policy.\n\nTask Objective:\nIntegrate BPET trajectory signals into trust cards and adversary graph posterior updates.\n\nAcceptance Criteria:\n- Trust surfaces show \"current state + trajectory path\" with interpretable risk deltas; adversary posteriors account for evolution velocity and suspicious sequence motifs.\n\nExpected Artifacts:\n- `src/security/bpet/trust_surface_integration.rs`, `tests/integration/bpet_trust_card_integration.rs`, `artifacts/10.21/bpet_trust_surface_snapshot.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_21/bd-232t/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_21/bd-232t/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.21] Integrate BPET trajectory signals into trust cards and adversary graph posterior updates.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.21] Integrate BPET trajectory signals into trust cards and adversary graph posterior updates.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.21] Integrate BPET trajectory signals into trust cards and adversary graph posterior updates.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.21] Integrate BPET trajectory signals into trust cards and adversary graph posterior updates.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.21] Integrate BPET trajectory signals into trust cards and adversary graph posterior updates.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"- Trust surfaces show \"current state + trajectory path\" with interpretable risk deltas; adversary posteriors account for evolution velocity and suspicious sequence motifs.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:37:08.376285199Z","created_by":"ubuntu","updated_at":"2026-02-20T17:05:47.831466108Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-21","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-232t","depends_on_id":"bd-1jpc","type":"blocks","created_at":"2026-02-20T17:05:47.831418490Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-23ys","title":"[10.2] Section-wide verification gate: comprehensive unit+e2e+logging","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.2\n\nTask Objective:\nCreate a hard section completion gate that verifies all implemented behavior in this section with comprehensive unit tests, integration/e2e scripts, and detailed structured logging evidence.\n\nAcceptance Criteria:\n- Section test matrix covers happy-path, edge-case, and adversarial/error-path scenarios for all section deliverables.\n- E2E scripts execute representative end-user workflows and policy/control-plane transitions for this section.\n- Structured logs include stable event/error codes, trace correlation IDs, and enough context for deterministic replay triage.\n- Verification report is deterministic and machine-readable for CI/release gating.\n\nExpected Artifacts:\n- Section-level test matrix and coverage summary artifact.\n- E2E script suite with pass/fail outputs and reproducible fixtures/seeds.\n- Structured log validation report and traceability bundle.\n- Gate verdict artifact consumable by release automation.\n\n- Machine-readable verification artifact at `artifacts/section_10_2/bd-23ys/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_2/bd-23ys/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests must validate contracts, invariants, and failure semantics.\n- E2E tests must validate cross-component behavior from user/operator entrypoints to persisted effects.\n- Logging must support root-cause analysis without hidden context requirements.\n\nTask-Specific Clarification:\n- For \"[10.2] Section-wide verification gate: comprehensive unit+e2e+logging\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.2] Section-wide verification gate: comprehensive unit+e2e+logging\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.2] Section-wide verification gate: comprehensive unit+e2e+logging\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.2] Section-wide verification gate: comprehensive unit+e2e+logging\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.2] Section-wide verification gate: comprehensive unit+e2e+logging\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","status":"closed","priority":1,"issue_type":"task","assignee":"CrimsonCrane","created_at":"2026-02-20T07:48:19.853287882Z","created_by":"ubuntu","updated_at":"2026-02-20T10:05:13.441850691Z","closed_at":"2026-02-20T10:05:13.441825414Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-2","test-obligations","verification-gate"],"dependencies":[{"issue_id":"bd-23ys","depends_on_id":"bd-1ck","type":"blocks","created_at":"2026-02-20T07:48:20.135370837Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-23ys","depends_on_id":"bd-1dpd","type":"blocks","created_at":"2026-02-20T08:24:25.739671273Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-23ys","depends_on_id":"bd-1z3","type":"blocks","created_at":"2026-02-20T07:48:20.274745139Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-23ys","depends_on_id":"bd-240","type":"blocks","created_at":"2026-02-20T07:48:20.086658337Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-23ys","depends_on_id":"bd-2hs","type":"blocks","created_at":"2026-02-20T07:48:20.040594050Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-23ys","depends_on_id":"bd-2kf","type":"blocks","created_at":"2026-02-20T07:48:20.320334281Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-23ys","depends_on_id":"bd-2qf","type":"blocks","created_at":"2026-02-20T07:48:20.411648378Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-23ys","depends_on_id":"bd-2twu","type":"blocks","created_at":"2026-02-20T08:20:51.476069017Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-23ys","depends_on_id":"bd-2vi","type":"blocks","created_at":"2026-02-20T07:48:20.226260003Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-23ys","depends_on_id":"bd-2wz","type":"blocks","created_at":"2026-02-20T07:48:20.457425080Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-23ys","depends_on_id":"bd-32v","type":"blocks","created_at":"2026-02-20T07:48:20.180919113Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-23ys","depends_on_id":"bd-38l","type":"blocks","created_at":"2026-02-20T07:48:20.365949782Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-23ys","depends_on_id":"bd-7mt","type":"blocks","created_at":"2026-02-20T07:48:19.948593724Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-23ys","depends_on_id":"bd-80g","type":"blocks","created_at":"2026-02-20T07:48:19.994743491Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-240","title":"[10.2] Implement compatibility regression dashboard by API family.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.2 — Compatibility Core\n\nWhy This Exists:\nCompatibility core as strategic wedge: policy-visible behavior, divergence receipts, L1/L2 lockstep, and spec-first fixture governance.\n\nTask Objective:\nImplement compatibility regression dashboard by API family.\n\nAcceptance Criteria:\n- Implement with explicit success/failure invariants and deterministic behavior under normal, edge, and fault scenarios.\n- Ensure outcomes are verifiable via automated tests and reproducible artifact outputs.\n\nExpected Artifacts:\n- Section-owned design/spec artifact at `docs/specs/section_10_2/bd-240_contract.md`, capturing decision rationale, invariants, and interface boundaries.\n- Machine-readable verification artifact at `artifacts/section_10_2/bd-240/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_2/bd-240/verification_summary.md` linking implementation intent to measured outcomes.\n\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.2] Implement compatibility regression dashboard by API family.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.2] Implement compatibility regression dashboard by API family.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.2] Implement compatibility regression dashboard by API family.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.2] Implement compatibility regression dashboard by API family.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.2] Implement compatibility regression dashboard by API family.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","status":"closed","priority":1,"issue_type":"task","assignee":"CrimsonCrane","created_at":"2026-02-20T07:36:44.483262502Z","created_by":"ubuntu","updated_at":"2026-02-20T09:48:27.583939849Z","closed_at":"2026-02-20T09:48:27.583912297Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-2","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-240","depends_on_id":"bd-1ck","type":"blocks","created_at":"2026-02-20T07:43:20.390854638Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2437","title":"Epic: Connector Lifecycle + State Management [10.13a]","description":"- type: epic","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-20T07:47:58.072163029Z","updated_at":"2026-02-20T07:49:21.159047190Z","closed_at":"2026-02-20T07:49:21.159027133Z","close_reason":"Superseded by canonical detailed master-plan graph (bd-33v) with full 10.N-10.21 and 11-16 coverage, explicit dependencies, and per-section verification gates.","source_repo":".","compaction_level":0,"original_size":0}
{"id":"bd-24du","title":"[10.19] Define ATC degraded/offline modes and local-first fallback behavior.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.19 — Adversarial Trust Commons Execution Track (9M)\n\nWhy This Exists:\nATC federated intelligence track for privacy-preserving cross-deployment threat learning, robust aggregation, and verifier-backed trust metrics.\n\nTask Objective:\nDefine ATC degraded/offline modes and local-first fallback behavior.\n\nAcceptance Criteria:\n- Federation outage or partition triggers deterministic fallback policy; local risk controls remain functional; rejoin/reconciliation is audited.\n\nExpected Artifacts:\n- `docs/specs/atc_degraded_mode.md`, `tests/integration/atc_partition_fallback.rs`, `artifacts/10.19/atc_degraded_mode_events.jsonl`.\n\n- Machine-readable verification artifact at `artifacts/section_10_19/bd-24du/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_19/bd-24du/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.19] Define ATC degraded/offline modes and local-first fallback behavior.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.19] Define ATC degraded/offline modes and local-first fallback behavior.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.19] Define ATC degraded/offline modes and local-first fallback behavior.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.19] Define ATC degraded/offline modes and local-first fallback behavior.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.19] Define ATC degraded/offline modes and local-first fallback behavior.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"- Federation outage or partition triggers deterministic fallback policy; local risk controls remain functional; rejoin/reconciliation is audited.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:37:06.252412913Z","created_by":"ubuntu","updated_at":"2026-02-20T17:00:51.827221010Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-19","self-contained","test-obligations"]}
{"id":"bd-24k","title":"[10.11] Implement bounded masking helper for tiny atomic product operations.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md — Section 10.11 (FrankenSQLite-Inspired Runtime Systems), cross-ref Section 9G.1\n\n## Why This Exists\n\nMany product operations in franken_node are tiny and atomic — checking a single capability flag, reading a single trust artifact field, incrementing a monotonic counter, toggling a feature gate. These operations are too small to justify the overhead of a full saga or two-phase channel, but they still need to be safe: they must not be interrupted by cancellation mid-write (leaving partial state), they must respect capability context (Cx-first), and they must produce deterministic results. Enhancement Map 9G.1 (capability-context-first product runtime APIs) implies that even the smallest operations must flow through the capability system. The \"bounded masking\" pattern from FrankenSQLite provides a lightweight primitive for this: a short critical section where cancellation signals are temporarily masked (not ignored — deferred) while the tiny atomic operation completes, with a strict bound on how long masking can last.\n\nThis bead implements the bounded masking helper as a product-layer utility, ensuring that tiny atomic operations can complete without cancellation-induced partial writes while maintaining the overall cancellation discipline required by the three-kernel architecture.\n\n## What This Must Do\n\n1. Implement a `bounded_mask<T, F>(max_duration: Duration, op: F) -> Result<T, MaskError>` helper where `op` is a synchronous or near-synchronous closure that executes with cancellation signals deferred (masked).\n2. Enforce a strict upper bound on mask duration: if `op` exceeds `max_duration`, the mask is forcibly lifted and the operation is aborted with a `MaskTimeoutExceeded` error. The bound must be configurable but default to a conservative value (e.g., 1ms).\n3. Ensure the masking helper requires a valid `CapabilityContext` — it is not callable from ambient (uncapabilitated) code paths. The Cx token is threaded through and available inside the masked closure.\n4. After the masked operation completes (or is aborted), immediately deliver any deferred cancellation signals so that the broader workflow can proceed with its cancel-drain-finalize sequence.\n5. Emit a structured log event for every mask invocation, including: mask duration, whether the operation completed within the bound, and whether deferred cancellation signals were pending.\n6. Provide compile-time or lint-time enforcement that `bounded_mask` closures do not contain `.await` points (masking async operations would defeat the purpose and risk unbounded masking).\n\n## Context from Enhancement Maps\n\n- 9G.1: \"Capability-context-first product runtime APIs\" — even tiny operations must flow through the capability system.\n- 9G.2: \"Cancellation as a strict protocol for all long-running orchestration tasks\" — bounded masking is the complement: it defines the narrow exception for short operations while preserving overall cancellation discipline.\n- 9J.19: \"Cancellation-complete protocol discipline with obligation closure proofs\" — bounded masking must not violate cancellation completeness; deferred signals must be delivered after unmask.\n- Architecture invariant #1 (8.5): Cx-first control — bounded_mask requires CapabilityContext.\n- Architecture invariant #3 (8.5): Cancellation protocol semantics — masking is temporary deferral, not suppression.\n- Architecture invariant #10 (8.5): No ambient authority — bounded_mask rejects uncapabilitated callers.\n\n## Dependencies\n\n- Upstream: bd-7om (cancel-drain-finalize protocol — bounded masking must interoperate with the cancellation protocol), bd-2g6r (10.15 Cx-first signature policy — provides the CapabilityContext requirement pattern)\n- Downstream: bd-93k (checkpoint placement uses bounded_mask for atomic checkpoint writes), bd-3vm (ambient-authority audit gate verifies bounded_mask is not called from ambient paths), bd-1jpo (10.11 section-wide verification gate)\n\n## Acceptance Criteria\n\n1. `bounded_mask` with a compliant operation (< 1ms) completes successfully and returns the operation result.\n2. `bounded_mask` with an operation exceeding `max_duration` returns `MaskTimeoutExceeded` error within 2x the configured bound (to account for scheduling jitter).\n3. A cancellation signal arriving during a masked operation is deferred and delivered within 1 event-loop tick after the mask is lifted.\n4. Calling `bounded_mask` without a valid `CapabilityContext` produces a compile-time error or a runtime `MissingCapabilityContext` panic.\n5. A masked closure containing an `.await` point produces a compile-time error (via procedural macro, `!Send` bound, or equivalent enforcement).\n6. Structured log event `bounded_mask.invocation` includes: `mask_duration_us`, `completed_within_bound` (bool), `deferred_cancel_pending` (bool), `trace_id`.\n7. Under high-frequency invocation (100,000 mask operations per second), overhead per invocation is < 5 microseconds (measured by benchmark).\n8. Verification evidence JSON includes invocations_total, completed_within_bound, mask_timeout_exceeded, deferred_cancels_delivered, and avg_mask_duration_us fields.\n\n## Testing & Logging Requirements\n\n- Unit tests: (a) Successful masked operation returns correct value; (b) Timed-out operation returns MaskTimeoutExceeded; (c) Deferred cancel signal is delivered after unmask; (d) Missing CapabilityContext is rejected; (e) Nested bounded_mask calls are handled correctly (inner mask respects outer deadline).\n- Integration tests: (a) Bounded mask within a cancel-drain-finalize workflow — verify cancel is deferred during mask and delivered after; (b) Bounded mask used for atomic counter increment under concurrent cancellation; (c) Bounded mask used for atomic trust artifact field update.\n- Adversarial tests: (a) Closure that sleeps beyond max_duration — verify timeout enforcement; (b) Concurrent cancellation signals during mask — verify all are delivered after unmask in order; (c) Panic inside masked closure — verify mask is lifted and cancel signals are still delivered; (d) Extremely short max_duration (1us) — verify operation still gets a fair chance.\n- Structured logs: Events use stable codes (FN-BM-001 through FN-BM-006), include `trace_id`, `cx_id`, `mask_duration_us`, `outcome`. JSON-formatted.\n\n## Expected Artifacts\n\n- docs/specs/section_10_11/bd-24k_contract.md\n- crates/franken-node/src/runtime/bounded_mask.rs (or equivalent module path)\n- scripts/check_bounded_masking.py (with --json flag and self_test())\n- tests/test_check_bounded_masking.py\n- artifacts/section_10_11/bd-24k/verification_evidence.json\n- artifacts/section_10_11/bd-24k/verification_summary.md","acceptance_criteria":"AC for bd-24k:\n1. A BoundedMask<T> helper type wraps tiny atomic operations (operations completing in bounded constant time, no I/O, no allocation) and suppresses cancellation signals for their duration.\n2. The masking window has a compile-time upper bound (MAX_MASK_DURATION_NS constant, default 1 microsecond); any operation exceeding this bound in test mode triggers a MASK_BUDGET_EXCEEDED warning.\n3. BoundedMask<T> implements a scoped guard pattern: cancellation tokens are checked before entering the mask and immediately after exiting; cancellation that arrives during the masked window is deferred, not dropped.\n4. The mask is NOT nestable: attempting to create a BoundedMask inside an existing BoundedMask panics with MASK_NESTING_VIOLATION to prevent unbounded masking chains.\n5. Operations inside a BoundedMask must not perform any async .await, heap allocation, or I/O syscall; a debug-mode assertion verifies no .await points exist within the masked scope (or a lint enforces this).\n6. Unit tests verify: (a) cancellation arriving during masked window is deferred and delivered after mask drops, (b) mask nesting panics, (c) operation completing within budget succeeds silently, (d) operation exceeding budget in test mode emits MASK_BUDGET_EXCEEDED, (e) cancellation before mask entry aborts immediately without entering the mask.\n7. Structured log events: MASK_ENTER / MASK_EXIT / MASK_BUDGET_EXCEEDED / MASK_NESTING_VIOLATION with operation name and elapsed nanoseconds.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:36:49.981757845Z","created_by":"ubuntu","updated_at":"2026-02-20T17:01:41.064794081Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-11","self-contained","test-obligations"]}
{"id":"bd-24s","title":"[10.13] Implement snapshot policy (`every_updates`, `every_bytes`) and bounded replay targets for connector state.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.13 — FCP Deep-Mined Expansion Execution Track (9I)\n\nWhy This Exists:\nDeep connector/provider contract track: lifecycle FSMs, conformance harnesses, fencing, sandboxing, revocation freshness, and protocol hardening.\n\nTask Objective:\nImplement snapshot policy (`every_updates`, `every_bytes`) and bounded replay targets for connector state.\n\nAcceptance Criteria:\n- Replay cost is bounded by configured thresholds; snapshots are validated against chain heads; snapshot policy changes are audited.\n\nExpected Artifacts:\n- `docs/specs/state_snapshot_policy.md`, `tests/perf/state_replay_bound.rs`, `artifacts/10.13/snapshot_policy_benchmark.csv`.\n\n- Machine-readable verification artifact at `artifacts/section_10_13/bd-24s/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_13/bd-24s/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.13] Implement snapshot policy (`every_updates`, `every_bytes`) and bounded replay targets for connector state.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.13] Implement snapshot policy (`every_updates`, `every_bytes`) and bounded replay targets for connector state.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.13] Implement snapshot policy (`every_updates`, `every_bytes`) and bounded replay targets for connector state.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.13] Implement snapshot policy (`every_updates`, `every_bytes`) and bounded replay targets for connector state.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.13] Implement snapshot policy (`every_updates`, `every_bytes`) and bounded replay targets for connector state.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","status":"closed","priority":1,"issue_type":"task","assignee":"CrimsonCrane","created_at":"2026-02-20T07:36:51.960580991Z","created_by":"ubuntu","updated_at":"2026-02-20T11:03:46.573161408Z","closed_at":"2026-02-20T11:03:46.573132714Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-13","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-24s","depends_on_id":"bd-19u","type":"blocks","created_at":"2026-02-20T07:43:12.524092544Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-253o","title":"[10.19] Integrate ATC global priors into Bayesian adversary graph and risk scoring pipelines.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.19 — Adversarial Trust Commons Execution Track (9M)\n\nWhy This Exists:\nATC federated intelligence track for privacy-preserving cross-deployment threat learning, robust aggregation, and verifier-backed trust metrics.\n\nTask Objective:\nIntegrate ATC global priors into Bayesian adversary graph and risk scoring pipelines.\n\nAcceptance Criteria:\n- Global priors influence local posterior updates under explicit weighting policy; local-vs-global attribution is explainable in evidence outputs.\n\nExpected Artifacts:\n- `src/security/adversary_graph_federated_priors.rs`, `tests/integration/atc_bayesian_prior_integration.rs`, `artifacts/10.19/atc_prior_influence_report.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_19/bd-253o/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_19/bd-253o/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.19] Integrate ATC global priors into Bayesian adversary graph and risk scoring pipelines.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.19] Integrate ATC global priors into Bayesian adversary graph and risk scoring pipelines.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.19] Integrate ATC global priors into Bayesian adversary graph and risk scoring pipelines.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.19] Integrate ATC global priors into Bayesian adversary graph and risk scoring pipelines.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.19] Integrate ATC global priors into Bayesian adversary graph and risk scoring pipelines.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"- Global priors influence local posterior updates under explicit weighting policy; local-vs-global attribution is explainable in evidence outputs.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:37:06.003325443Z","created_by":"ubuntu","updated_at":"2026-02-20T17:00:53.038487277Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-19","self-contained","test-obligations"]}
{"id":"bd-2573","title":"[10.14] Define object-class profile registry (critical marker, trust receipt, replay bundle, telemetry artifact).","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.14 — FrankenSQLite Deep-Mined Expansion Execution Track (9J)\n\nWhy This Exists:\nFrankenSQLite deep-mined control integrity track: evidence ledgers, proofs, epochs, remote effects, markers/MMR, and deterministic fault labs.\n\nTask Objective:\nDefine object-class profile registry (critical marker, trust receipt, replay bundle, telemetry artifact).\n\nAcceptance Criteria:\n- Registry includes required classes and default policies; unknown class usage fails validation; class definitions are versioned.\n\nExpected Artifacts:\n- `docs/specs/object_class_profiles.md`, `config/object_class_profiles.toml`, `artifacts/10.14/object_class_registry.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_14/bd-2573/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_14/bd-2573/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.14] Define object-class profile registry (critical marker, trust receipt, replay bundle, telemetry artifact).\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.14] Define object-class profile registry (critical marker, trust receipt, replay bundle, telemetry artifact).\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.14] Define object-class profile registry (critical marker, trust receipt, replay bundle, telemetry artifact).\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.14] Define object-class profile registry (critical marker, trust receipt, replay bundle, telemetry artifact).\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.14] Define object-class profile registry (critical marker, trust receipt, replay bundle, telemetry artifact).\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"Registry includes required classes and default policies; unknown class usage fails validation; class definitions are versioned.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:36:57.055364136Z","created_by":"ubuntu","updated_at":"2026-02-20T16:22:36.966490307Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-14","self-contained","test-obligations"]}
{"id":"bd-25nl","title":"[10.14] Implement root-auth fail-closed bootstrap checks before accepting manifest updates.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.14 — FrankenSQLite Deep-Mined Expansion Execution Track (9J)\n\nWhy This Exists:\nFrankenSQLite deep-mined control integrity track: evidence ledgers, proofs, epochs, remote effects, markers/MMR, and deterministic fault labs.\n\nTask Objective:\nImplement root-auth fail-closed bootstrap checks before accepting manifest updates.\n\nAcceptance Criteria:\n- Bootstrap rejects unauthenticated or malformed root pointers; acceptance requires valid auth material and version checks; failures are diagnosable.\n\nExpected Artifacts:\n- `tests/security/root_bootstrap_fail_closed.rs`, `docs/specs/root_bootstrap_auth.md`, `artifacts/10.14/root_bootstrap_validation_report.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_14/bd-25nl/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_14/bd-25nl/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.14] Implement root-auth fail-closed bootstrap checks before accepting manifest updates.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.14] Implement root-auth fail-closed bootstrap checks before accepting manifest updates.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.14] Implement root-auth fail-closed bootstrap checks before accepting manifest updates.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.14] Implement root-auth fail-closed bootstrap checks before accepting manifest updates.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.14] Implement root-auth fail-closed bootstrap checks before accepting manifest updates.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"Bootstrap rejects unauthenticated or malformed root pointers; acceptance requires valid auth material and version checks; failures are diagnosable.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:36:58.955425474Z","created_by":"ubuntu","updated_at":"2026-02-20T16:24:29.258309456Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-14","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-25nl","depends_on_id":"bd-3hdv","type":"blocks","created_at":"2026-02-20T16:24:29.258223265Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-25nl","depends_on_id":"bd-nwhn","type":"blocks","created_at":"2026-02-20T07:43:16.188914520Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-25oa","title":"[10.15] Enforce canonical DPOR-style schedule exploration (from `10.14`) for epoch/lease/remote/evidence interactions.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.15 — Asupersync-First Integration Execution Track (8.4-8.6)\n\nWhy This Exists:\nAsupersync-first control-plane migration track enforcing Cx-first, region ownership, cancellation correctness, and protocol-grade verification gates.\n\nTask Objective:\nEnforce canonical DPOR-style schedule exploration (from `10.14`) for epoch/lease/remote/evidence interactions.\n\nAcceptance Criteria:\n- Canonical explorer covers targeted protocol classes with bounded CI budget; minimal counterexample traces are emitted on violations and consumed by control-plane release gates.\n\nExpected Artifacts:\n- `tests/lab/control_dpor_exploration.rs`, `docs/testing/control_dpor_scope.md`, `artifacts/10.15/control_dpor_results.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_15/bd-25oa/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_15/bd-25oa/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.15] Enforce canonical DPOR-style schedule exploration (from `10.14`) for epoch/lease/remote/evidence interactions.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.15] Enforce canonical DPOR-style schedule exploration (from `10.14`) for epoch/lease/remote/evidence interactions.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.15] Enforce canonical DPOR-style schedule exploration (from `10.14`) for epoch/lease/remote/evidence interactions.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.15] Enforce canonical DPOR-style schedule exploration (from `10.14`) for epoch/lease/remote/evidence interactions.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.15] Enforce canonical DPOR-style schedule exploration (from `10.14`) for epoch/lease/remote/evidence interactions.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"- Canonical explorer covers targeted protocol classes with bounded CI budget; minimal counterexample traces are emitted on violations and consumed by control-plane release gates.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:37:00.955164951Z","created_by":"ubuntu","updated_at":"2026-02-20T17:00:43.130209332Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-15","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-25oa","depends_on_id":"bd-22yy","type":"blocks","created_at":"2026-02-20T14:59:47.448553095Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-261k","title":"[10.4] Section-wide verification gate: comprehensive unit+e2e+logging","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.4\n\nTask Objective:\nCreate a hard section completion gate that verifies all implemented behavior in this section with comprehensive unit tests, integration/e2e scripts, and detailed structured logging evidence.\n\nAcceptance Criteria:\n(See structured acceptance_criteria field for domain-specific criteria.)\n\n- Machine-readable verification artifact at `artifacts/section_10_4/bd-261k/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_4/bd-261k/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests must validate contracts, invariants, and failure semantics.\n- E2E tests must validate cross-component behavior from user/operator entrypoints to persisted effects.\n- Logging must support root-cause analysis without hidden context requirements.\n\nTask-Specific Clarification:\n- For \"[10.4] Section-wide verification gate: comprehensive unit+e2e+logging\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.4] Section-wide verification gate: comprehensive unit+e2e+logging\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.4] Section-wide verification gate: comprehensive unit+e2e+logging\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.4] Section-wide verification gate: comprehensive unit+e2e+logging\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.4] Section-wide verification gate: comprehensive unit+e2e+logging\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"1. Gate script (scripts/gate_section_10_4.py) runs all 8 section bead verification scripts in dependency order; any single bead failure fails the gate. 2. Unit test coverage: each bead's verification script (check_*.py) has a companion test file (test_check_*.py) with >=90% line coverage on the verification logic; coverage is measured and reported in the gate artifact. 3. Integration/E2E test suite covers the following cross-bead workflows: (a) full extension lifecycle: manifest creation -> signing -> provenance attestation -> registry publish -> trust card generation -> install with freshness check, (b) quarantine flow: publish extension -> trigger revocation -> verify quarantine enacted -> verify trust card updated -> verify fleet recall propagation, (c) certification upgrade path: uncertified extension -> add provenance -> reach Community -> add reproducibility -> reach Verified, (d) reputation impact: simulate security incident -> verify publisher reputation downgrade -> verify dependent extension certification downgrade -> verify telemetry anomaly alert. 4. Structured log validation: all 8 bead implementations emit their specified structured log events; gate verifies each event code appears in test logs with required fields present and non-empty. 5. Deterministic replay: each E2E test produces a fixture file that can be replayed to reproduce the exact same test outcome; replay fixtures are stored in fixtures/section_10_4/. 6. Gate produces machine-readable verdict at artifacts/section_10_4/bd-261k/verification_evidence.json with fields: gate_pass (bool), beads_tested[], per_bead_results[] (each with bead_id, unit_pass, integration_pass, log_events_validated), overall_coverage_pct, and timestamp. 7. Gate produces human-readable summary at artifacts/section_10_4/bd-261k/verification_summary.md with pass/fail table, coverage stats, and links to individual bead evidence. 8. All tests run without network access (air-gapped mode) using mock registry and mock revocation endpoints; test fixtures provide deterministic responses. 9. Gate execution completes within 5 minutes on a standard CI runner. 10. Gate is idempotent: running it twice with no code changes produces identical verdict artifacts (same content hash).","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:48:23.478467151Z","created_by":"ubuntu","updated_at":"2026-02-20T15:18:42.307533816Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-4","test-obligations","verification-gate"],"dependencies":[{"issue_id":"bd-261k","depends_on_id":"bd-12q","type":"blocks","created_at":"2026-02-20T07:48:23.815919644Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-261k","depends_on_id":"bd-1ah","type":"blocks","created_at":"2026-02-20T07:48:23.862901270Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-261k","depends_on_id":"bd-1dpd","type":"blocks","created_at":"2026-02-20T08:24:25.152773401Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-261k","depends_on_id":"bd-1gx","type":"blocks","created_at":"2026-02-20T07:48:23.912287455Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-261k","depends_on_id":"bd-1vm","type":"blocks","created_at":"2026-02-20T07:48:23.673400936Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-261k","depends_on_id":"bd-273","type":"blocks","created_at":"2026-02-20T07:48:23.625599543Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-261k","depends_on_id":"bd-2twu","type":"blocks","created_at":"2026-02-20T08:20:50.800068266Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-261k","depends_on_id":"bd-2yh","type":"blocks","created_at":"2026-02-20T07:48:23.766314863Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-261k","depends_on_id":"bd-ml1","type":"blocks","created_at":"2026-02-20T07:48:23.713120646Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-261k","depends_on_id":"bd-phf","type":"blocks","created_at":"2026-02-20T07:48:23.578140488Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-26k","title":"[PLAN 10.9] Moonshot Disruption Track","description":"Section: 10.9 — Moonshot Disruption Track\n\nStrategic Context:\nMoonshot disruption track for public benchmark leadership, adversarial campaigns, verifier economy, and category-shift reporting.\n\nExecution Requirements:\n- Preserve all scoped capabilities from the canonical plan.\n- Keep contracts self-contained so future contributors can execute without reopening the master plan.\n- Require explicit testing strategy (unit + integration/e2e) and structured logging/telemetry for every child bead.\n- Require artifact-backed validation for performance, security, and correctness claims.\n\nDependency Semantics:\n- This epic is blocked by its child implementation beads.\n- Cross-epic dependencies encode strategic sequencing and canonical ownership boundaries.\n\n## Success Criteria\n- All child beads for this section are completed with acceptance artifacts attached and no unresolved blockers.\n- Section-level verification gate for comprehensive unit tests, integration/e2e workflows, and detailed structured logging evidence is green.\n- Scope-to-plan traceability is explicit, with no feature loss versus the canonical plan section.\n\n## Optimization Notes\n- User-Outcome Lens: \"[PLAN 10.9] Moonshot Disruption Track\" must improve operator confidence, safety posture, and deterministic recovery behavior under both normal and adversarial conditions.\n- Cross-Section Coordination: child beads must encode integration assumptions explicitly to avoid local optimizations that degrade system-wide correctness.\n- Verification Non-Negotiable: completion requires reproducible unit/integration/E2E evidence and structured logs suitable for independent replay.\n- Scope Protection: any simplification must preserve canonical-plan feature intent and be justified with explicit tradeoff documentation.","status":"open","priority":2,"issue_type":"epic","created_at":"2026-02-20T07:36:40.950883086Z","created_by":"ubuntu","updated_at":"2026-02-20T08:16:47.979293914Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-9"],"dependencies":[{"issue_id":"bd-26k","depends_on_id":"bd-10c","type":"blocks","created_at":"2026-02-20T07:36:48.638742405Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-26k","depends_on_id":"bd-15t","type":"blocks","created_at":"2026-02-20T07:36:48.717638904Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-26k","depends_on_id":"bd-1e0","type":"blocks","created_at":"2026-02-20T07:36:48.481628402Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-26k","depends_on_id":"bd-1kfq","type":"blocks","created_at":"2026-02-20T07:48:26.958413458Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-26k","depends_on_id":"bd-1xg","type":"blocks","created_at":"2026-02-20T07:37:10.629763520Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-26k","depends_on_id":"bd-20a","type":"blocks","created_at":"2026-02-20T07:37:10.668196839Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-26k","depends_on_id":"bd-229","type":"blocks","created_at":"2026-02-20T07:37:10.588594125Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-26k","depends_on_id":"bd-3il","type":"blocks","created_at":"2026-02-20T07:37:10.550584435Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-26k","depends_on_id":"bd-9is","type":"blocks","created_at":"2026-02-20T07:36:48.401730338Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-26k","depends_on_id":"bd-cda","type":"blocks","created_at":"2026-02-20T07:37:09.388900058Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-26k","depends_on_id":"bd-f5d","type":"blocks","created_at":"2026-02-20T07:36:48.324029836Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-26k","depends_on_id":"bd-m8p","type":"blocks","created_at":"2026-02-20T07:36:48.560611953Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-26mk","title":"[10.17] Implement security staking and slashing framework for publisher trust governance.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.17 — Radical Expansion Execution Track (9K)\n\nWhy This Exists:\nRadical expansion track for proof-carrying speculation, Bayesian adversary control, time-travel replay, ZK attestations, and claim compiler systems.\n\nTask Objective:\nImplement security staking and slashing framework for publisher trust governance.\n\nAcceptance Criteria:\n- High-risk capabilities enforce stake policy gates; validated malicious behavior triggers deterministic slashing workflow with appeal/audit trail artifacts.\n\nExpected Artifacts:\n- `docs/policy/security_staking_and_slashing.md`, `src/registry/staking_governance.rs`, `tests/integration/staking_slashing_flows.rs`, `artifacts/10.17/staking_ledger_snapshot.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_17/bd-26mk/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_17/bd-26mk/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.17] Implement security staking and slashing framework for publisher trust governance.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.17] Implement security staking and slashing framework for publisher trust governance.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.17] Implement security staking and slashing framework for publisher trust governance.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.17] Implement security staking and slashing framework for publisher trust governance.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.17] Implement security staking and slashing framework for publisher trust governance.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"- High-risk capabilities enforce stake policy gates; validated malicious behavior triggers deterministic slashing workflow with appeal/audit trail artifacts.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:37:03.516382462Z","created_by":"ubuntu","updated_at":"2026-02-20T15:45:59.522512515Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-17","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-26mk","depends_on_id":"bd-al8i","type":"blocks","created_at":"2026-02-20T07:43:18.561264270Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-26ux","title":"[10.16] Add migration path from interim/local stores to `frankensqlite` for relevant state domains.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.16 — Adjacent Substrate Integration Execution Track (8.7)\n\nWhy This Exists:\nAdjacent substrate integration track to enforce deep composition with frankentui, frankensqlite, sqlmodel_rust, and fastapi_rust.\n\nTask Objective:\nAdd migration path from interim/local stores to `frankensqlite` for relevant state domains.\n\nAcceptance Criteria:\n- Migration tooling is deterministic and idempotent; rollback path exists; migrated data matches source invariants.\n\nExpected Artifacts:\n- `docs/migration/to_frankensqlite.md`, `tests/migration/frankensqlite_migration_idempotence.rs`, `artifacts/10.16/frankensqlite_migration_report.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_16/bd-26ux/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_16/bd-26ux/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.16] Add migration path from interim/local stores to `frankensqlite` for relevant state domains.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.16] Add migration path from interim/local stores to `frankensqlite` for relevant state domains.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.16] Add migration path from interim/local stores to `frankensqlite` for relevant state domains.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.16] Add migration path from interim/local stores to `frankensqlite` for relevant state domains.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.16] Add migration path from interim/local stores to `frankensqlite` for relevant state domains.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"- Migration tooling is deterministic and idempotent; rollback path exists; migrated data matches source invariants.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:37:02.102470926Z","created_by":"ubuntu","updated_at":"2026-02-20T17:05:20.448815138Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-16","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-26ux","depends_on_id":"bd-2tua","type":"blocks","created_at":"2026-02-20T17:05:20.448754906Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-273","title":"[10.4] Implement extension certification levels tied to policy controls.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.4 — Extension Ecosystem + Registry\n\nWhy This Exists:\nExtension ecosystem and registry trust foundation: signed artifacts, provenance, trust cards, revocation, and quarantine flows.\n\nTask Objective:\nImplement extension certification levels tied to policy controls.\n\nAcceptance Criteria:\n(See structured acceptance_criteria field for domain-specific criteria.)\n\nExpected Artifacts:\n- Section-owned design/spec artifact at `docs/specs/section_10_4/bd-273_contract.md`, capturing decision rationale, invariants, and interface boundaries.\n- Machine-readable verification artifact at `artifacts/section_10_4/bd-273/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_4/bd-273/verification_summary.md` linking implementation intent to measured outcomes.\n\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.4] Implement extension certification levels tied to policy controls.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.4] Implement extension certification levels tied to policy controls.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.4] Implement extension certification levels tied to policy controls.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.4] Implement extension certification levels tied to policy controls.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.4] Implement extension certification levels tied to policy controls.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"1. Define four certification levels in ascending order: Uncertified (no verification beyond valid manifest), Community (passes automated test suite + valid provenance chain), Verified (reproducible build + behavioral telemetry baseline established + publisher reputation >= Established), Certified (all Verified requirements + manual security audit + threshold-signed attestation from >=2 independent reviewers). 2. Each certification level maps to a policy control set: Uncertified extensions are limited to sandbox-only execution with no network/filesystem access; Community extensions may access declared capabilities within resource envelope; Verified extensions may access elevated capabilities (e.g., ProcessSpawn) with audit logging; Certified extensions may access all declared capabilities including cross-extension IPC. 3. Certification level is stored in the extension manifest (certification_level field) and independently verifiable: the trust card (bd-2yh) cross-references the certification evidence chain. 4. Level transitions require explicit evidence: Community->Verified requires provenance chain from bd-1ah with reproducibility proof; Verified->Certified requires signed audit report referencing specific code version hash. 5. Downgrade is automatic: if any certification requirement is violated (e.g., provenance chain broken, publisher reputation drops below threshold, revocation event on a dependency), the extension is downgraded to the highest level still satisfied; policy controls are adjusted immediately. 6. Fleet operators can set minimum certification level per deployment context via policy file: e.g., production requires >= Verified, staging allows >= Community. 7. CLI commands: 'franken-node ext cert show <ext-id>' (current level + evidence summary), 'franken-node ext cert requirements <level>' (list all requirements for a level), 'franken-node ext cert evaluate <ext-id> --target-level <level>' (gap analysis). 8. Certification level changes emit structured events: CERTIFICATION_LEVEL_CHANGED with ext_id, old_level, new_level, trigger (upgrade_evidence | downgrade_violation), and violation_details[] for downgrades. 9. Certification audit trail is append-only and tamper-evident (hash-chained log entries). 10. Integration tests cover: upgrade path through all four levels, automatic downgrade on revocation, policy enforcement blocking uncertified extension in production context, and certification re-evaluation after dependency update.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:36:45.904492560Z","created_by":"ubuntu","updated_at":"2026-02-20T16:58:59.795787905Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-4","self-contained","test-obligations"]}
{"id":"bd-274s","title":"[10.17] Implement Bayesian adversary graph and automated quarantine controller.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.17 — Radical Expansion Execution Track (9K)\n\nWhy This Exists:\nRadical expansion track for proof-carrying speculation, Bayesian adversary control, time-travel replay, ZK attestations, and claim compiler systems.\n\nTask Objective:\nImplement Bayesian adversary graph and automated quarantine controller.\n\nAcceptance Criteria:\n- Risk posterior updates are deterministic from identical evidence; policy thresholds trigger reproducible control actions (throttle/isolate/revoke/quarantine) with signed evidence entries.\n\nExpected Artifacts:\n- `src/security/adversary_graph.rs`, `src/security/quarantine_controller.rs`, `tests/integration/bayesian_risk_quarantine.rs`, `artifacts/10.17/adversary_graph_state.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_17/bd-274s/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_17/bd-274s/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.17] Implement Bayesian adversary graph and automated quarantine controller.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.17] Implement Bayesian adversary graph and automated quarantine controller.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.17] Implement Bayesian adversary graph and automated quarantine controller.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.17] Implement Bayesian adversary graph and automated quarantine controller.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.17] Implement Bayesian adversary graph and automated quarantine controller.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"- Risk posterior updates are deterministic from identical evidence; policy thresholds trigger reproducible control actions (throttle/isolate/revoke/quarantine) with signed evidence entries.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:37:03.008093682Z","created_by":"ubuntu","updated_at":"2026-02-20T15:46:00.799133743Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-17","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-274s","depends_on_id":"bd-1nl1","type":"blocks","created_at":"2026-02-20T07:43:18.307975166Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-27o2","title":"[10.14] Add profile tuning harness and publish benchmark-driven policy updates as signed artifacts.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.14 — FrankenSQLite Deep-Mined Expansion Execution Track (9J)\n\nWhy This Exists:\nFrankenSQLite deep-mined control integrity track: evidence ledgers, proofs, epochs, remote effects, markers/MMR, and deterministic fault labs.\n\nTask Objective:\nAdd profile tuning harness and publish benchmark-driven policy updates as signed artifacts.\n\nAcceptance Criteria:\n- Harness recomputes candidate policy updates reproducibly; updates are signed and linked to benchmark provenance; unsafe regressions are auto-rejected.\n\nExpected Artifacts:\n- `tools/profile_tuning_harness.rs`, `docs/specs/policy_update_signing.md`, `artifacts/10.14/signed_policy_update_bundle.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_14/bd-27o2/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_14/bd-27o2/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.14] Add profile tuning harness and publish benchmark-driven policy updates as signed artifacts.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.14] Add profile tuning harness and publish benchmark-driven policy updates as signed artifacts.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.14] Add profile tuning harness and publish benchmark-driven policy updates as signed artifacts.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.14] Add profile tuning harness and publish benchmark-driven policy updates as signed artifacts.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.14] Add profile tuning harness and publish benchmark-driven policy updates as signed artifacts.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"Harness recomputes candidate policy updates reproducibly; updates are signed and linked to benchmark provenance; unsafe regressions are auto-rejected.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:36:57.222598430Z","created_by":"ubuntu","updated_at":"2026-02-20T15:44:07.315910092Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-14","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-27o2","depends_on_id":"bd-8tvs","type":"blocks","created_at":"2026-02-20T07:43:15.273847616Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2808","title":"[10.14] Implement deterministic repro bundle export for control-plane failures and policy incidents.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.14 — FrankenSQLite Deep-Mined Expansion Execution Track (9J)\n\nWhy This Exists:\nFrankenSQLite deep-mined control integrity track: evidence ledgers, proofs, epochs, remote effects, markers/MMR, and deterministic fault labs.\n\nTask Objective:\nImplement deterministic repro bundle export for control-plane failures and policy incidents.\n\nAcceptance Criteria:\n- Repro bundles include seed, config, event-sequence trace, and evidence references; replay tool re-executes incident deterministically; bundle schema is versioned.\n\nExpected Artifacts:\n- `src/tools/repro_bundle_export.rs`, `tests/integration/repro_bundle_replay.rs`, `artifacts/10.14/repro_bundle_schema_v1.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_14/bd-2808/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_14/bd-2808/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.14] Implement deterministic repro bundle export for control-plane failures and policy incidents.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.14] Implement deterministic repro bundle export for control-plane failures and policy incidents.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.14] Implement deterministic repro bundle export for control-plane failures and policy incidents.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.14] Implement deterministic repro bundle export for control-plane failures and policy incidents.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.14] Implement deterministic repro bundle export for control-plane failures and policy incidents.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"Repro bundles include seed, config, event-sequence trace, and evidence references; replay tool re-executes incident deterministically; bundle schema is versioned.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:36:59.061914429Z","created_by":"ubuntu","updated_at":"2026-02-20T16:24:29.566625805Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-14","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-2808","depends_on_id":"bd-nupr","type":"blocks","created_at":"2026-02-20T16:24:29.566573217Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-28k6","title":"Epic: Monotonic Hardening System [10.14c]","description":"- type: epic","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-20T07:47:58.072163029Z","updated_at":"2026-02-20T07:49:21.209727165Z","closed_at":"2026-02-20T07:49:21.209709522Z","close_reason":"Superseded by canonical detailed master-plan graph (bd-33v) with full 10.N-10.21 and 11-16 coverage, explicit dependencies, and per-section verification gates.","source_repo":".","compaction_level":0,"original_size":0}
{"id":"bd-28ld","title":"[10.16] Add architecture dependency map showing where each adjacent substrate is required in `franken_node`.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.16 — Adjacent Substrate Integration Execution Track (8.7)\n\nWhy This Exists:\nAdjacent substrate integration track to enforce deep composition with frankentui, frankensqlite, sqlmodel_rust, and fastapi_rust.\n\nTask Objective:\nAdd architecture dependency map showing where each adjacent substrate is required in `franken_node`.\n\nAcceptance Criteria:\n- Map covers presentation, persistence, model, and service planes; unmapped relevant modules fail architecture review gate.\n\nExpected Artifacts:\n- `docs/architecture/adjacent_substrate_dependency_map.md`, `artifacts/10.16/substrate_dependency_matrix.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_16/bd-28ld/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_16/bd-28ld/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.16] Add architecture dependency map showing where each adjacent substrate is required in `franken_node`.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.16] Add architecture dependency map showing where each adjacent substrate is required in `franken_node`.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.16] Add architecture dependency map showing where each adjacent substrate is required in `franken_node`.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.16] Add architecture dependency map showing where each adjacent substrate is required in `franken_node`.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.16] Add architecture dependency map showing where each adjacent substrate is required in `franken_node`.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"- Map covers presentation, persistence, model, and service planes; unmapped relevant modules fail architecture review gate.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:37:01.607249937Z","created_by":"ubuntu","updated_at":"2026-02-20T17:00:55.360311400Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-16","self-contained","test-obligations"]}
{"id":"bd-28sz","title":"[13] Concrete target gate: >=95% compatibility corpus pass","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 13 — Program Success Criteria\nConcrete Target: >= 95% compatibility corpus pass\n\nWhy This Exists:\nThis is one of 6 concrete quantitative targets that define program success. The >= 95% compatibility corpus pass target means franken_node must correctly handle at least 95% of the targeted compatibility corpus (spanning core/high-value/edge API bands) as validated by the lockstep oracle.\n\nTask Objective:\nInstrument, measure, and gate on the >= 95% compatibility corpus pass target across the full targeted fixture corpus.\n\nDetailed Acceptance Criteria:\n1. Compatibility corpus defined and version-controlled (from 10.7 golden corpus bead).\n2. Automated measurement: corpus execution produces per-band pass/fail counts and overall percentage.\n3. Release gate: releases blocked when overall pass rate drops below 95%.\n4. Per-band breakdown: core band must have higher threshold (>= 99%), high-value >= 95%, edge >= 90%.\n5. Regression detection: any corpus pass rate decrease from previous release triggers investigation.\n6. Results published as machine-readable artifact for CI/release gating and public benchmark reporting.\n7. External reproducibility: any party with corpus access can independently verify the pass rate.\n\nKey Dependencies:\n- Depends on 10.2 (Compatibility Core) for fixture runner and band definitions.\n- Depends on 10.7 (Conformance) for golden corpus.\n- Depends on 10.0 (lockstep oracle) for measurement infrastructure.\n- Feeds into 14 (Benchmarks) as a key metric family.\n\nExpected Artifacts:\n- CI gate configuration for 95% threshold enforcement.\n- Measurement dashboard with per-band breakdown.\n- artifacts/section_13/bd-28sz/verification_evidence.json\n\nTesting and Logging Requirements:\n- Unit tests: threshold calculation, per-band aggregation, regression detection logic.\n- Integration tests: corpus execution producing accurate pass/fail metrics.\n- E2E tests: release gate blocking when below threshold.\n- Structured logs: CORPUS_PASS_RATE_COMPUTED, THRESHOLD_MET, THRESHOLD_BREACHED, REGRESSION_DETECTED with band breakdowns and trace IDs.","acceptance_criteria":"1. A targeted compatibility corpus exists with >= 500 test cases covering Node.js core APIs (fs, http, net, crypto, stream, buffer, path, os, child_process, cluster, events, timers, url, querystring, zlib, tls).\n2. Each test case is tagged by API family and risk band (critical/high/medium/low).\n3. franken_node achieves >= 95% pass rate across the full corpus, measured by automated CI run.\n4. Pass rate is reported broken down by API family; no single API family has pass rate < 80%.\n5. Failing tests are tracked as individual beads with investigation status.\n6. The corpus is versioned and reproducible: running the same corpus version always produces the same results on the same franken_node version.\n7. Evidence artifact: compatibility_corpus_results.json with per-test pass/fail, overall rate, and per-family breakdown.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:39:34.869993782Z","created_by":"ubuntu","updated_at":"2026-02-20T15:21:02.516217704Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-13","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-28sz","depends_on_id":"bd-3e74","type":"blocks","created_at":"2026-02-20T07:43:25.532140538Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-28u0","title":"[10.18] Implement receipt-window selection and proof-job scheduler with bounded latency budgets.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.18 — Verifiable Execution Fabric Execution Track (9L)\n\nWhy This Exists:\nVEF track for proof-carrying runtime compliance: receipt chains, commitment checkpoints, proof generation/verification, and claim gating.\n\nTask Objective:\nImplement receipt-window selection and proof-job scheduler with bounded latency budgets.\n\nAcceptance Criteria:\n- Proof windows are deterministic by policy and workload class; scheduler respects latency/resource budgets; backlog health is observable.\n\nExpected Artifacts:\n- `src/trust/vef_proof_scheduler.rs`, `tests/perf/vef_scheduler_latency_budget.rs`, `artifacts/10.18/vef_scheduler_metrics.csv`.\n\n- Machine-readable verification artifact at `artifacts/section_10_18/bd-28u0/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_18/bd-28u0/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.18] Implement receipt-window selection and proof-job scheduler with bounded latency budgets.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.18] Implement receipt-window selection and proof-job scheduler with bounded latency budgets.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.18] Implement receipt-window selection and proof-job scheduler with bounded latency budgets.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.18] Implement receipt-window selection and proof-job scheduler with bounded latency budgets.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.18] Implement receipt-window selection and proof-job scheduler with bounded latency budgets.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"- Proof windows are deterministic by policy and workload class; scheduler respects latency/resource budgets; backlog health is observable.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:37:04.458416931Z","created_by":"ubuntu","updated_at":"2026-02-20T17:05:44.579981492Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-18","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-28u0","depends_on_id":"bd-3g4k","type":"blocks","created_at":"2026-02-20T17:05:44.579933933Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-28wj","title":"[4] Non-Negotiable Constraints — 13 hard guardrails for all implementation","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md — Section 4\n\n## Why This Exists\nThese are the hard guardrails that ALL implementation work must respect. Violating any of these constraints is a program-level failure, not a local trade-off. This bead exists to make these constraints explicitly trackable and enforceable without consulting the plan document.\n\n## Non-Negotiable Constraints (Verbatim from Plan)\n\n1. **Engine dependency rule:** franken_node depends on /dp/franken_engine; it does not fork engine internals. No local reintroduction of engine core crates in this repository.\n\n2. **Asupersync dependency rule:** franken_node depends on /dp/asupersync as the control/correctness substrate. High-impact async control paths MUST be Cx-first, region-owned, cancel-correct, and obligation-tracked.\n\n3. **FrankenTUI substrate rule:** Console/TUI surfaces MUST use /dp/frankentui as the canonical presentation substrate.\n\n4. **FrankenSQLite substrate rule:** Any feature needing SQLite persistence MUST use /dp/frankensqlite as the storage substrate.\n\n5. **SQLModel Rust preference:** /dp/sqlmodel_rust SHOULD be used for typed schema/model/query integration.\n\n6. **FastAPI Rust preference:** /dp/fastapi_rust SHOULD be used for service/API control surfaces.\n\n7. **Waiver discipline:** Any deviation from substrate rules requires an explicit, signed waiver artifact with rationale, risk analysis, and expiry.\n\n8. **Compatibility shim visibility:** Compatibility shims must be explicit, typed, and policy-visible.\n\n9. **No line-by-line translation:** Legacy runtimes may be used for spec extraction and conformance fixture capture ONLY. Line-by-line Bun/Node translation is off-charter.\n\n10. **Policy-gated dangerous behavior:** Dangerous compatibility behavior must be gated by policy and auditable receipts.\n\n11. **Evidence-backed claims:** Every major claim ships with reproducible benchmark/security artifacts.\n\n12. **Deterministic migration:** Migration tooling must be deterministic and replayable for high-severity failures.\n\n13. **Safe defaults:** Product defaults prioritize safe operation while preserving practical adoption velocity.\n\n## Enforcement Strategy\n- CI gates check for engine crate reintroduction (implemented in 10.1)\n- Substrate compliance gates enforce TUI/SQLite/API substrate rules (10.16)\n- Compatibility PR review gates require spec section + fixture IDs (10.2)\n- Waiver registry tracks all approved deviations with expiry dates (10.16)\n\n## Acceptance Criteria\n- All 13 constraints are enforceable via automated CI or review gates\n- Constraint violation attempts produce clear, actionable rejection messages\n- Waiver registry is maintained with all active deviations documented\n- Quarterly audit confirms no silent constraint erosion\n\n## Testing Requirements\n- Unit tests for each CI gate enforcement rule\n- E2E test confirming constraint violation rejection\n- Structured logging with constraint-violation event codes","status":"open","priority":0,"issue_type":"task","created_at":"2026-02-20T16:14:07.813632210Z","created_by":"ubuntu","updated_at":"2026-02-20T16:14:07.813632210Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["constraints","governance","plan","section-4"]}
{"id":"bd-293y","title":"[10.19] Define ATC federation trust model, participant identity contracts, and governance boundaries.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.19 — Adversarial Trust Commons Execution Track (9M)\n\nWhy This Exists:\nATC federated intelligence track for privacy-preserving cross-deployment threat learning, robust aggregation, and verifier-backed trust metrics.\n\nTask Objective:\nDefine ATC federation trust model, participant identity contracts, and governance boundaries.\n\nAcceptance Criteria:\n- Participant roles, identity requirements, trust zones, and governance controls are explicit and machine-readable; unauthorized participants fail closed.\n\nExpected Artifacts:\n- `docs/specs/atc_federation_trust_model.md`, `spec/atc_participant_contract_v1.json`, `artifacts/10.19/atc_participant_registry_snapshot.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_19/bd-293y/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_19/bd-293y/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.19] Define ATC federation trust model, participant identity contracts, and governance boundaries.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.19] Define ATC federation trust model, participant identity contracts, and governance boundaries.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.19] Define ATC federation trust model, participant identity contracts, and governance boundaries.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.19] Define ATC federation trust model, participant identity contracts, and governance boundaries.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.19] Define ATC federation trust model, participant identity contracts, and governance boundaries.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"- Participant roles, identity requirements, trust zones, and governance controls are explicit and machine-readable; unauthorized participants fail closed.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:37:05.303535007Z","created_by":"ubuntu","updated_at":"2026-02-20T15:45:20.213332691Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-19","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-293y","depends_on_id":"bd-1wz","type":"blocks","created_at":"2026-02-20T07:46:34.600503314Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-293y","depends_on_id":"bd-32p","type":"blocks","created_at":"2026-02-20T07:46:34.666362629Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-293y","depends_on_id":"bd-cda","type":"blocks","created_at":"2026-02-20T07:46:34.725739941Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-295v","title":"[PROGRAM] Define cross-section integration journey matrix + deterministic fixtures","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md (Cross-cutting across Sections 10.0–10.21 and 11–16)\nSection: PROGRAM (Cross-section integration journey matrix)\n\nTask Objective:\nDefine a canonical program-wide integration journey matrix with deterministic fixture packs that exercise cross-section behavior at real product seams, not just within section-local boundaries.\n\nWhy This Improves User Outcomes:\nSection gates validate local correctness, but users experience the product through multi-section flows. This matrix ensures we validate full journeys where migration, compatibility, trust policy, incident handling, and ecosystem controls intersect.\n\nAcceptance Criteria:\n- Matrix enumerates all critical cross-section user/operator journeys (happy path, edge, adversarial/error).\n- Every journey maps to owning beads, required fixtures, expected outputs, and failure taxonomy.\n- Fixture packs are deterministic, replayable, and machine-indexed for CI and incident forensics.\n- Matrix explicitly identifies seams where section-local guarantees can conflict and defines resolution assertions.\n\nExpected Artifacts:\n- Program-wide journey matrix document with bead traceability and ownership map.\n- Deterministic fixture catalog for full-journey replay.\n- Machine-readable journey-to-evidence mapping artifact used by orchestration/gating tasks.\n\nTesting & Logging Requirements:\n- Unit tests for matrix/fixture schema validators and mapping integrity checks.\n- E2E dry-runs proving each matrix journey can be executed in deterministic order.\n- Detailed structured logs for journey selection, fixture resolution, and assertion mapping with stable event codes and trace IDs.\n\nTask-Specific Clarification:\n- Preserve full plan ambition by validating integrated behavior across sections, not reducing checks to isolated component tests.\n- Do not remove or simplify any section-level verification obligations; this layer is additive and cross-sectional.","status":"closed","priority":1,"issue_type":"task","assignee":"ubuntu","created_at":"2026-02-20T08:08:05.489422547Z","created_by":"ubuntu","updated_at":"2026-02-20T08:35:25.271069991Z","closed_at":"2026-02-20T08:35:25.270980965Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","program-integration","test-obligations","verification"]}
{"id":"bd-29ct","title":"[10.13] Build adversarial fuzz corpus gates, including decode-DoS and replay/splice handshake scenarios.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.13 — FCP Deep-Mined Expansion Execution Track (9I)\n\nWhy This Exists:\nDeep connector/provider contract track: lifecycle FSMs, conformance harnesses, fencing, sandboxing, revocation freshness, and protocol hardening.\n\nTask Objective:\nBuild adversarial fuzz corpus gates, including decode-DoS and replay/splice handshake scenarios.\n\nAcceptance Criteria:\n- Fuzz targets include parser, handshake, token validation, and decode-DoS corpora; CI gate enforces minimum fuzz health budget; regressions are triaged with seeds.\n\nExpected Artifacts:\n- `fuzz/targets/*`, `docs/security/adversarial_fuzzing.md`, `artifacts/10.13/fuzz_campaign_summary.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_13/bd-29ct/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_13/bd-29ct/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.13] Build adversarial fuzz corpus gates, including decode-DoS and replay/splice handshake scenarios.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.13] Build adversarial fuzz corpus gates, including decode-DoS and replay/splice handshake scenarios.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.13] Build adversarial fuzz corpus gates, including decode-DoS and replay/splice handshake scenarios.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.13] Build adversarial fuzz corpus gates, including decode-DoS and replay/splice handshake scenarios.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.13] Build adversarial fuzz corpus gates, including decode-DoS and replay/splice handshake scenarios.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","status":"closed","priority":1,"issue_type":"task","assignee":"CrimsonCrane","created_at":"2026-02-20T07:36:54.979948138Z","created_by":"ubuntu","updated_at":"2026-02-20T13:35:23.094631381Z","closed_at":"2026-02-20T13:35:23.094602778Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-13","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-29ct","depends_on_id":"bd-35by","type":"blocks","created_at":"2026-02-20T07:43:14.107826128Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-29q","title":"Add transplant re-sync + drift detection workflow","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md (Bootstrap bridge for continuous transplant integrity)\n\nTask Objective:\nImplement a reproducible transplant re-sync and drift detection workflow that keeps transplanted assets aligned with upstream source while preserving local auditability.\n\nIn Scope:\n- Re-sync procedure definition and automation entrypoint.\n- Drift detection categories (content drift, missing files, unexpected files, metadata drift).\n- Operator/CI-facing outputs for safe review and decision-making.\n\nAcceptance Criteria:\n- Re-sync workflow is deterministic and documents each transformation step.\n- Drift detection emits categorized, actionable findings with stable IDs.\n- Workflow can run non-destructively for preview and produce reproducible evidence bundles.\n\nExpected Artifacts:\n- Re-sync + drift workflow document and executable script entrypoint.\n- Drift report schema with stable category/error codes.\n- Before/after fixture bundle demonstrating expected workflow behavior.\n\nTesting & Logging Requirements:\n- Unit tests for drift classification and report formatting.\n- Integration tests validating re-sync behavior against controlled fixture changes.\n- E2E tests for full restore -> lock -> drift -> re-sync cycle.\n- Structured logs with workflow stage events, drift categories, and trace correlation IDs.\n\nTask-Specific Clarification:\n- For \"Add transplant re-sync + drift detection workflow\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"Add transplant re-sync + drift detection workflow\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"Add transplant re-sync + drift detection workflow\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"Add transplant re-sync + drift detection workflow\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"Add transplant re-sync + drift detection workflow\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","status":"closed","priority":1,"issue_type":"task","assignee":"ubuntu","created_at":"2026-02-20T07:26:02.925883480Z","created_by":"ubuntu","updated_at":"2026-02-20T08:16:16.700695689Z","closed_at":"2026-02-20T08:16:16.700607174Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["transplant","workflow"],"dependencies":[{"issue_id":"bd-29q","depends_on_id":"bd-7rt","type":"blocks","created_at":"2026-02-20T07:32:08.028944379Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-29r6","title":"[10.14] Implement content-derived deterministic seed derivation for encoding/repair schedules.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.14 — FrankenSQLite Deep-Mined Expansion Execution Track (9J)\n\nWhy This Exists:\nFrankenSQLite deep-mined control integrity track: evidence ledgers, proofs, epochs, remote effects, markers/MMR, and deterministic fault labs.\n\nTask Objective:\nImplement content-derived deterministic seed derivation for encoding/repair schedules.\n\nAcceptance Criteria:\n- Seed derivation is domain-separated and stable; identical content/config produces identical schedule; schedule changes require version bump artifact.\n\nExpected Artifacts:\n- `src/encoding/deterministic_seed.rs`, `tests/conformance/deterministic_seed_derivation.rs`, `artifacts/10.14/seed_derivation_vectors.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_14/bd-29r6/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_14/bd-29r6/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.14] Implement content-derived deterministic seed derivation for encoding/repair schedules.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.14] Implement content-derived deterministic seed derivation for encoding/repair schedules.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.14] Implement content-derived deterministic seed derivation for encoding/repair schedules.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.14] Implement content-derived deterministic seed derivation for encoding/repair schedules.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.14] Implement content-derived deterministic seed derivation for encoding/repair schedules.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"Seed derivation is domain-separated and stable; identical content/config produces identical schedule; schedule changes require version bump artifact.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:36:56.889689537Z","created_by":"ubuntu","updated_at":"2026-02-20T16:23:09.325110403Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-14","self-contained","test-obligations"]}
{"id":"bd-29w6","title":"[10.13] Implement offline coverage tracker and SLO dashboards (`coverage`, `availability`, `repair debt`).","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.13 — FCP Deep-Mined Expansion Execution Track (9I)\n\nWhy This Exists:\nDeep connector/provider contract track: lifecycle FSMs, conformance harnesses, fencing, sandboxing, revocation freshness, and protocol hardening.\n\nTask Objective:\nImplement offline coverage tracker and SLO dashboards (`coverage`, `availability`, `repair debt`).\n\nAcceptance Criteria:\n- Coverage metrics are computed continuously and per policy scope; SLO breach alerts trigger automatically; dashboard values are traceable to raw events.\n\nExpected Artifacts:\n- `docs/observability/offline_slo_metrics.md`, `tests/integration/offline_coverage_metrics.rs`, `artifacts/10.13/offline_slo_dashboard_snapshot.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_13/bd-29w6/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_13/bd-29w6/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.13] Implement offline coverage tracker and SLO dashboards (`coverage`, `availability`, `repair debt`).\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.13] Implement offline coverage tracker and SLO dashboards (`coverage`, `availability`, `repair debt`).\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.13] Implement offline coverage tracker and SLO dashboards (`coverage`, `availability`, `repair debt`).\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.13] Implement offline coverage tracker and SLO dashboards (`coverage`, `availability`, `repair debt`).\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.13] Implement offline coverage tracker and SLO dashboards (`coverage`, `availability`, `repair debt`).\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","status":"closed","priority":1,"issue_type":"task","assignee":"CrimsonCrane","created_at":"2026-02-20T07:36:53.760738770Z","created_by":"ubuntu","updated_at":"2026-02-20T12:36:00.001208050Z","closed_at":"2026-02-20T12:36:00.001180449Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-13","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-29w6","depends_on_id":"bd-2t5u","type":"blocks","created_at":"2026-02-20T07:43:13.474744273Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-29yx","title":"[10.14] Add suspicious-artifact challenge flow that requests proof artifacts before trust promotion.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.14 — FrankenSQLite Deep-Mined Expansion Execution Track (9J)\n\nWhy This Exists:\nFrankenSQLite deep-mined control integrity track: evidence ledgers, proofs, epochs, remote effects, markers/MMR, and deterministic fault labs.\n\nTask Objective:\nAdd suspicious-artifact challenge flow that requests proof artifacts before trust promotion.\n\nAcceptance Criteria:\n- Challenge workflow can defer promotion pending proof response; unresolved challenges timeout to deny by default; challenge states are auditable.\n\nExpected Artifacts:\n- `docs/specs/suspicious_artifact_challenge.md`, `tests/security/challenge_flow_before_promotion.rs`, `artifacts/10.14/challenge_flow_transcript.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_14/bd-29yx/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_14/bd-29yx/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.14] Add suspicious-artifact challenge flow that requests proof artifacts before trust promotion.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.14] Add suspicious-artifact challenge flow that requests proof artifacts before trust promotion.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.14] Add suspicious-artifact challenge flow that requests proof artifacts before trust promotion.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.14] Add suspicious-artifact challenge flow that requests proof artifacts before trust promotion.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.14] Add suspicious-artifact challenge flow that requests proof artifacts before trust promotion.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"Challenge workflow can defer promotion pending proof response; unresolved challenges timeout to deny by default; challenge states are auditable.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:36:56.708172389Z","created_by":"ubuntu","updated_at":"2026-02-20T16:24:04.161265043Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-14","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-29yx","depends_on_id":"bd-1l62","type":"blocks","created_at":"2026-02-20T16:24:04.161180245Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2a0","title":"[10.3] Build project scanner for API/runtime/dependency risk inventory.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.3 — Migration System\n\nWhy This Exists:\nMigration autopilot pipeline from audit to rollout, designed to collapse migration friction with deterministic confidence and replayability.\n\nTask Objective:\nBuild project scanner for API/runtime/dependency risk inventory.\n\nAcceptance Criteria:\n- Implement with explicit success/failure invariants and deterministic behavior under normal, edge, and fault scenarios.\n- Ensure outcomes are verifiable via automated tests and reproducible artifact outputs.\n\nExpected Artifacts:\n- Section-owned design/spec artifact at `docs/specs/section_10_3/bd-2a0_contract.md`, capturing decision rationale, invariants, and interface boundaries.\n- Machine-readable verification artifact at `artifacts/section_10_3/bd-2a0/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_3/bd-2a0/verification_summary.md` linking implementation intent to measured outcomes.\n\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.3] Build project scanner for API/runtime/dependency risk inventory.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.3] Build project scanner for API/runtime/dependency risk inventory.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.3] Build project scanner for API/runtime/dependency risk inventory.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.3] Build project scanner for API/runtime/dependency risk inventory.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.3] Build project scanner for API/runtime/dependency risk inventory.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","status":"closed","priority":1,"issue_type":"task","assignee":"CrimsonCrane","created_at":"2026-02-20T07:36:44.797550931Z","created_by":"ubuntu","updated_at":"2026-02-20T10:08:33.597464974Z","closed_at":"2026-02-20T10:08:33.597440017Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-3","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-2a0","depends_on_id":"bd-3il","type":"blocks","created_at":"2026-02-20T07:46:35.805293080Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2a0","depends_on_id":"bd-cda","type":"blocks","created_at":"2026-02-20T07:46:35.855759255Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2a3","title":"Run baseline workspace checks via rch offload","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md (Bootstrap quality baseline / CI-readiness)\nSection: BOOTSTRAP (Quality baseline and CI readiness)\n\nTask Objective:\nRun baseline workspace quality checks exclusively via `rch` offload to avoid local CPU contention and establish deterministic quality evidence before deeper implementation.\n\nIn Scope:\n- Execute required baseline commands through `rch exec`:\n  - `cargo fmt --check`\n  - `cargo check --all-targets`\n  - `cargo clippy --all-targets -- -D warnings`\n- Capture outputs in reproducible machine/human-readable artifacts.\n- Report failures with actionable remediation context.\n\nAcceptance Criteria:\n- All baseline checks execute via `rch` (no direct local cargo execution for this bead).\n- Results are captured with command, environment, and timestamp provenance.\n- Failure reports include exact failing targets/lints and reproduction hints.\n\nExpected Artifacts:\n- Baseline-check report bundle (command outputs + summarized status table).\n- Machine-readable pass/fail artifact suitable for CI gating.\n- Traceability note linking baseline outcomes to bootstrap readiness status.\n\n- Machine-readable verification artifact at `artifacts/section_bootstrap/bd-2a3/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_bootstrap/bd-2a3/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit-style validation for report parser/formatter (if implemented).\n- E2E execution script that runs the full `rch` baseline sequence end-to-end.\n- Detailed structured logs capturing each offloaded command lifecycle and exit semantics.\n- Stable error/status codes for each baseline check class.\n\nTask-Specific Clarification:\n- For \"Run baseline workspace checks via rch offload\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"Run baseline workspace checks via rch offload\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"Run baseline workspace checks via rch offload\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"Run baseline workspace checks via rch offload\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"Run baseline workspace checks via rch offload\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","status":"in_progress","priority":1,"issue_type":"task","assignee":"TurquoiseHarbor","created_at":"2026-02-20T07:26:05.580672327Z","created_by":"ubuntu","updated_at":"2026-02-20T08:46:44.422808785Z","source_repo":".","compaction_level":0,"original_size":0}
{"id":"bd-2a4l","title":"[13] Success criterion: externally verifiable trust/security claims","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 13\n\nTask Objective:\nEnsure all trust/security claims are externally verifiable and reproducible.\n\nExecution Requirements:\n- Make this requirement machine-verifiable and release-gated where applicable.\n- Keep behavior self-documenting so future contributors do not need to re-open the master plan.\n\nTesting & Logging Requirements:\n- Unit tests for rule/contract logic and error conditions.\n- Integration/E2E tests for workflow-level enforcement.\n- Structured logs with stable codes and trace IDs.\n- Reproducible artifacts that prove contract satisfaction.\n\nAcceptance Criteria:\n- Success and failure invariants for [13] Success criterion: externally verifiable trust/security claims are explicitly quantified and machine-verifiable.\n- Determinism requirements for [13] Success criterion: externally verifiable trust/security claims are validated across repeated runs and adversarial perturbations.\n\n\nExpected Artifacts:\n- Machine-readable verification artifact with explicit canonical path under artifacts/section_13/bd-2a4l/verification_evidence.json, suitable for CI/release gating.\n- Human-readable verification summary with explicit canonical path under artifacts/section_13/bd-2a4l/verification_summary.md, linking implementation intent to measured validation outcomes.\n\nTask-Specific Clarification:\n- The implementation must deliver the exact capability named in the title, with no scope dilution and no silent feature omission.\n- Validation artifacts must include capability-specific thresholds, pass/fail criteria, and reproducible evidence bundles tied to this bead ID.\n- Program/section verification gates must be able to consume this bead's outputs without manual interpretation.\n\nWhy This Improves User Outcomes:\n- \"[13] Success criterion: externally verifiable trust/security claims\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[13] Success criterion: externally verifiable trust/security claims\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"1. Every trust and security claim in the documentation has a corresponding verifiable evidence artifact.\n2. An external verifier (without source code access) can validate claims using the published verifier toolkit.\n3. Verifiable claims include: (a) compromise reduction ratio, (b) trust decision determinism, (c) privacy budget compliance, (d) containment latency.\n4. Each claim has a machine-readable evidence format (JSON) with: claim statement, measurement methodology, raw data reference, computed result, confidence interval.\n5. External verification has been performed by >= 1 independent party and results are published.\n6. Claims are versioned: when the system changes, claims are re-verified and version-stamped.\n7. Evidence: verifiable_claims_registry.json listing each claim, its evidence artifact path, verification status, and last-verified version.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:39:34.500284922Z","created_by":"ubuntu","updated_at":"2026-02-20T15:22:40.765125212Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-13","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-2a4l","depends_on_id":"bd-1w78","type":"blocks","created_at":"2026-02-20T07:43:25.357215995Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2a6g","title":"[14] Metric family: containment/revocation latency and convergence","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 14\n\nTask Objective:\nInstrument containment and revocation latency/convergence metric family.\n\nExecution Requirements:\n- Make this requirement machine-verifiable and release-gated where applicable.\n- Keep behavior self-documenting so future contributors do not need to re-open the master plan.\n\nTesting & Logging Requirements:\n- Unit tests for rule/contract logic and error conditions.\n- Integration/E2E tests for workflow-level enforcement.\n- Structured logs with stable codes and trace IDs.\n- Reproducible artifacts that prove contract satisfaction.\n\nAcceptance Criteria:\n- Success and failure invariants for [14] Metric family: containment/revocation latency and convergence are explicitly quantified and machine-verifiable.\n- Determinism requirements for [14] Metric family: containment/revocation latency and convergence are validated across repeated runs and adversarial perturbations.\n\n\nExpected Artifacts:\n- Machine-readable verification artifact with explicit canonical path under artifacts/section_14/bd-2a6g/verification_evidence.json, suitable for CI/release gating.\n- Human-readable verification summary with explicit canonical path under artifacts/section_14/bd-2a6g/verification_summary.md, linking implementation intent to measured validation outcomes.\n\nTask-Specific Clarification:\n- The implementation must deliver the exact capability named in the title, with no scope dilution and no silent feature omission.\n- Validation artifacts must include capability-specific thresholds, pass/fail criteria, and reproducible evidence bundles tied to this bead ID.\n- Program/section verification gates must be able to consume this bead's outputs without manual interpretation.\n\nWhy This Improves User Outcomes:\n- \"[14] Metric family: containment/revocation latency and convergence\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[14] Metric family: containment/revocation latency and convergence\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"METRIC FAMILY: Containment/revocation latency and convergence.\n1. Metrics measured: (a) detection-to-containment latency (ms), (b) containment-to-full-isolation latency (ms), (c) revocation propagation time (time for all nodes to receive revocation), (d) convergence time (time for system to reach stable state post-incident).\n2. Measured for incident types: malicious extension detected, compromised node detected, trust graph corruption, supply-chain attack detected.\n3. Latency gates: detection-to-containment <= 30 seconds (automated), revocation propagation <= 60 seconds (10-node cluster), convergence <= 5 minutes.\n4. Measured under load conditions: idle, moderate (50% capacity), high (90% capacity).\n5. Revocation completeness: 100% of affected nodes must receive and enforce revocation within the propagation window.\n6. Publication: latency metrics included in benchmark report with percentile distributions.\n7. Evidence: containment_latency_metrics.json with per-incident-type, per-load-condition latency percentiles.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:39:35.815915491Z","created_by":"ubuntu","updated_at":"2026-02-20T15:24:52.780011344Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-14","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-2a6g","depends_on_id":"bd-ka0n","type":"blocks","created_at":"2026-02-20T07:43:26.023257801Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2ac","title":"[10.0] Implement secure extension distribution network.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.0 — Top 10 Initiative Tracking (Initiative #7)\nCross-references: 9A.7, 9B.7, 9C.7, 9D.7\n\nWhy This Exists:\nSecure extension distribution network is the #7 strategic initiative. It builds a signed registry and distribution model with revocation propagation and reputation linkage to reduce supply-chain compromise windows. This is the infrastructure that makes the trust ecosystem viable.\n\nTask Objective:\nBuild the signed extension registry and distribution network with end-to-end integrity guarantees: signed packages, provenance attestation, revocation propagation, and publisher reputation linkage.\n\nDetailed Acceptance Criteria:\n1. Signed extension package format with manifest schema (10.4), provenance attestation chain, and content integrity verification.\n2. Registry supports publish, search, and install with signature verification at every stage.\n3. Revocation propagation with canonical freshness checks (from 10.13) — compromised packages are rapidly recalled.\n4. Publisher reputation linkage: registry entries linked to publisher trust cards with explainable transitions.\n5. Key-transparency and threshold-signing flows for high-impact trust operations (9B.7).\n6. Cryptographic decision receipts and inclusion proofs for trust transitions (9C.7).\n7. Signature/provenance verification optimized at scale with batched pipelines (9D.7).\n8. CLI surface: franken-node registry publish/search commands.\n\nKey Dependencies:\n- Depends on 10.4 (Extension Ecosystem) for manifest schema and provenance requirements.\n- Depends on 10.13 (FCP Deep-Mined) for revocation freshness semantics.\n- Consumed by 10.5 (Security) for policy-gated distribution.\n- Consumed by 10.15 (Ecosystem Capture) for signed extension registry pillar.\n\nExpected Artifacts:\n- src/supply_chain/registry.rs — registry client and verification.\n- src/supply_chain/distribution.rs — package distribution with integrity.\n- CLI integration for registry commands in cli.rs.\n- docs/specs/section_10_0/bd-2ac_contract.md\n- artifacts/section_10_0/bd-2ac/verification_evidence.json\n\nTesting and Logging Requirements:\n- Unit tests: package signing, signature verification, revocation check, reputation query, threshold signing.\n- Integration tests: full publish -> search -> install -> verify pipeline with revocation injection.\n- E2E tests: franken-node registry publish/search CLI workflows.\n- Adversarial tests: tampered packages, stale revocation data, expired signatures, threshold signing failures.\n- Structured logs: PACKAGE_PUBLISHED, SIGNATURE_VERIFIED, REVOCATION_PROPAGATED, REPUTATION_LINKED, INCLUSION_PROOF_GENERATED with trace IDs.","acceptance_criteria":"1. Signed registry: every published extension artifact is signed with publisher key; signature verified on download before installation; unsigned artifacts rejected by default.\n2. Signature scheme uses Ed25519 or equivalent; key management supports rotation with overlap period; compromised key revocation propagates within <= 60 seconds.\n3. Revocation propagation: revocation list updates distributed via push channel to all fleet nodes; node-local cache expires within <= 300 seconds; offline nodes sync on reconnect.\n4. Reputation linkage: extension reputation score derived from trust card data (bd-y4g), download telemetry, vulnerability history, and behavioral anomaly flags.\n5. Registry API supports: publish, search, download, verify, revoke, audit-log query; all endpoints authenticated and rate-limited.\n6. Integrity verification: downloaded artifacts verified against content-addressable hash (SHA-256 minimum) and publisher signature; double-verification before installation.\n7. Compromise reduction: secure distribution network contributes to >= 10x compromise reduction target (Section 3) by preventing supply-chain attacks at distribution layer.\n8. Audit log: every registry operation (publish/download/revoke/key-rotate) logged with timestamp, actor identity, artifact hash, operation result; append-only.\n9. Offline resilience: nodes cache verified artifacts locally; cached artifacts usable without network; cache invalidation triggered by revocation events.\n10. Verification evidence includes: signature verification round-trip test, revocation propagation latency measurement, tampered-artifact rejection test, offline-cache validity test.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:36:42.960936524Z","created_by":"ubuntu","updated_at":"2026-02-20T15:36:15.363919858Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-0","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-2ac","depends_on_id":"bd-1ah","type":"blocks","created_at":"2026-02-20T15:01:24.535795128Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2ac","depends_on_id":"bd-1gx","type":"blocks","created_at":"2026-02-20T15:01:24.359647442Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2ac","depends_on_id":"bd-yqz","type":"blocks","created_at":"2026-02-20T07:43:10.352633662Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2ad0","title":"[16] Contribution: reproducible migration and incident datasets","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 16\n\nTask Objective:\nPublish reproducible migration and incident datasets with artifact bundles.\n\nExecution Requirements:\n- Make this requirement machine-verifiable and release-gated where applicable.\n- Keep behavior self-documenting so future contributors do not need to re-open the master plan.\n\nTesting & Logging Requirements:\n- Unit tests for rule/contract logic and error conditions.\n- Integration/E2E tests for workflow-level enforcement.\n- Structured logs with stable codes and trace IDs.\n- Reproducible artifacts that prove contract satisfaction.\n\nAcceptance Criteria:\n- Success and failure invariants for [16] Contribution: reproducible migration and incident datasets are explicitly quantified and machine-verifiable.\n- Determinism requirements for [16] Contribution: reproducible migration and incident datasets are validated across repeated runs and adversarial perturbations.\n\n\nExpected Artifacts:\n- Machine-readable verification artifact with explicit canonical path under artifacts/section_16/bd-2ad0/verification_evidence.json, suitable for CI/release gating.\n- Human-readable verification summary with explicit canonical path under artifacts/section_16/bd-2ad0/verification_summary.md, linking implementation intent to measured validation outcomes.\n\nTask-Specific Clarification:\n- The implementation must deliver the exact capability named in the title, with no scope dilution and no silent feature omission.\n- Validation artifacts must include capability-specific thresholds, pass/fail criteria, and reproducible evidence bundles tied to this bead ID.\n- Program/section verification gates must be able to consume this bead's outputs without manual interpretation.\n\nWhy This Improves User Outcomes:\n- \"[16] Contribution: reproducible migration and incident datasets\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[16] Contribution: reproducible migration and incident datasets\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"1. Reproducible migration dataset published: >= 10 anonymized migration traces capturing: project structure, migration steps taken, timing per step, blockers encountered, final outcome.\n2. Reproducible incident dataset published: >= 20 incident replay artifacts covering all high-severity incident types, with sanitized inputs and expected outputs.\n3. Datasets are versioned, hosted on a public repository or data archive (e.g., Zenodo, GitHub Releases), and have DOI or permanent identifier.\n4. Each dataset includes: (a) README with schema description, (b) data dictionary for all fields, (c) loading scripts in Python and JavaScript, (d) license (CC-BY-4.0 or equivalent open license).\n5. Datasets are validated: loading scripts run without error on fresh environment and produce expected record counts.\n6. Privacy: all datasets are reviewed for PII and sensitive information before publication; no real organization names, IP addresses, or credentials.\n7. Datasets are cited in project documentation and at least 1 external publication uses them.\n8. Evidence: dataset_publication_registry.json with per-dataset: name, version, DOI, record count, download URL, and citation count.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:39:36.870913606Z","created_by":"ubuntu","updated_at":"2026-02-20T15:28:08.209014118Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-16","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-2ad0","depends_on_id":"bd-f955","type":"blocks","created_at":"2026-02-20T07:43:26.592445237Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2ah","title":"[10.11] Adopt canonical obligation-tracked two-phase channel contracts (from `10.15`) for critical flows.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md — Section 10.11 (FrankenSQLite-Inspired Runtime Systems), cross-ref Section 9G.3, 9J.19\n\n## Why This Exists\n\nCritical product flows in franken_node — publish-then-promote, rollback-then-notify, migration-commit-then-cleanup — require atomicity guarantees that simple fire-and-forget messaging cannot provide. If the publish succeeds but the promote notification is lost, the system enters an inconsistent state where a trust artifact is stored but not visible. Enhancement Map 9G.3 mandates obligation-tracked two-phase workflows for critical publish/rollback paths, where each message in a workflow carries a tracked obligation that must be explicitly fulfilled or compensated, and the system can audit at any time which obligations are outstanding. 9J.19 extends this with cancellation-complete protocol discipline, requiring that even cancelled workflows produce obligation closure proofs demonstrating that no obligations were silently dropped.\n\nThis bead adopts the canonical obligation-tracked two-phase channel contracts from 10.15 (bd-1n5p obligation channels) into franken_node's product service layer, replacing ad hoc messaging in critical flows with channels where every sent message creates a tracked obligation, the receiver must acknowledge or reject, and the system maintains an obligation ledger for auditability.\n\n## What This Must Do\n\n1. Implement an `ObligationChannel<T>` abstraction that wraps inter-service communication for critical flows, where every `send()` creates a tracked obligation with a unique obligation ID, deadline, and originating trace context.\n2. The receiver must explicitly `fulfill(obligation_id)` or `reject(obligation_id, reason)` — if neither occurs before the deadline, the channel emits an `ObligationTimeout` event and triggers the configured timeout policy (retry, compensate, or escalate).\n3. Maintain an `ObligationLedger` that tracks all outstanding obligations, their creation time, deadline, and current status — queryable for operational dashboards and audit.\n4. On cancellation of a workflow that has outstanding obligations, produce an obligation closure proof: a signed record listing each obligation and its terminal state (fulfilled, rejected, compensated, or cancelled-with-reason).\n5. Integrate with the append-only decision stream (per 9G.9): all obligation state transitions (created, fulfilled, rejected, timed-out, cancelled) are recorded as immutable events.\n6. Provide a `TwoPhaseFlow` builder that composes obligation channels into multi-step workflows with explicit prepare and commit phases, where prepare can be rolled back atomically.\n\n## Context from Enhancement Maps\n\n- 9G.3: \"Obligation-tracked two-phase workflows for critical publish/rollback paths\"\n- 9J.19: \"Cancellation-complete protocol discipline with obligation closure proofs\"\n- Architecture invariant #4 (8.5): Two-phase effects — critical state changes must go through prepare/commit.\n- Architecture invariant #3 (8.5): Cancellation protocol semantics — cancelled workflows must produce closure proofs.\n- Architecture invariant #8 (8.5): Evidence-by-default — obligation state transitions are evidence and must be recorded.\n\n## Dependencies\n\n- Upstream: bd-1n5p (10.15 obligation-tracked two-phase channels), bd-7om (cancel-drain-finalize protocol for cancellation integration), bd-126h (10.14 append-only marker stream for decision stream recording)\n- Downstream: bd-390 (anti-entropy reconciliation applies records through obligation channels), bd-3hw (saga orchestrator composes with obligation channels), bd-93k (checkpoint placement integrates with obligation tracking), bd-1jpo (10.11 section-wide verification gate)\n\n## Acceptance Criteria\n\n1. Every critical flow (publish-promote, rollback-notify, migration-commit-cleanup) uses `ObligationChannel` instead of direct messaging — verified by code audit gate.\n2. An unfulfilled obligation triggers `ObligationTimeout` event within 1 second of the deadline, with the configured policy (retry/compensate/escalate) executing automatically.\n3. The `ObligationLedger` correctly reports all outstanding obligations with creation time, deadline, and status — verified by querying during a multi-step workflow.\n4. Cancellation of a workflow with 3 outstanding obligations produces a closure proof listing all 3 obligations and their terminal states.\n5. Obligation closure proofs are signed and verifiable; a tampered proof fails verification.\n6. All obligation state transitions appear in the append-only decision stream in causal order.\n7. `TwoPhaseFlow` builder: a prepare phase that succeeds followed by a commit phase that fails triggers automatic rollback of the prepare, verified end-to-end.\n8. Verification evidence JSON includes obligations_created, obligations_fulfilled, obligations_timed_out, obligations_cancelled, closure_proofs_generated, and ledger_query_latency_ms fields.\n\n## Testing & Logging Requirements\n\n- Unit tests: (a) ObligationChannel send creates obligation with correct ID and deadline; (b) fulfill() transitions obligation to fulfilled state; (c) reject() transitions to rejected state; (d) Timeout fires at deadline; (e) Closure proof includes all obligations from a cancelled workflow.\n- Integration tests: (a) Full publish-promote flow through obligation channels with success path; (b) Publish-promote with promote failure triggering compensation; (c) Multi-step TwoPhaseFlow with rollback on commit failure; (d) Obligation ledger query during active workflow returns correct state.\n- Adversarial tests: (a) Double-fulfill of the same obligation — verify idempotent acceptance or rejection; (b) Fulfill after deadline — verify late fulfillment is recorded but does not suppress the timeout event; (c) Cancellation during the prepare phase of a TwoPhaseFlow — verify rollback and closure proof; (d) Obligation ledger under high concurrency (1000 concurrent obligations) — verify no lost obligations.\n- Structured logs: Events use stable codes (FN-OB-001 through FN-OB-012), include `obligation_id`, `trace_id`, `deadline`, `status`, `workflow_id`. JSON-formatted.\n\n## Expected Artifacts\n\n- docs/specs/section_10_11/bd-2ah_contract.md\n- crates/franken-node/src/runtime/obligation_channel.rs (or equivalent module path)\n- crates/franken-node/src/runtime/obligation_ledger.rs\n- crates/franken-node/src/runtime/two_phase_flow.rs\n- scripts/check_obligation_channels.py (with --json flag and self_test())\n- tests/test_check_obligation_channels.py\n- artifacts/section_10_11/bd-2ah/verification_evidence.json\n- artifacts/section_10_11/bd-2ah/verification_summary.md","acceptance_criteria":"AC for bd-2ah:\n1. All critical flows adopt obligation-tracked two-phase channels from 10.15: Phase 1 (Prepare) reserves resources and returns an ObligationToken; Phase 2 (Commit/Rollback) consumes the token to finalize or release.\n2. An ObligationToken is a linear type (must be consumed exactly once); dropping an unconsumed token triggers a LEAKED_OBLIGATION panic in debug mode and a structured error log + forced rollback in release mode.\n3. The ObligationChannel<T> type enforces the two-phase contract: prepare(payload) -> Result<ObligationToken>, commit(token) -> Result<T>, rollback(token) -> Result<()>.\n4. Obligation tokens carry a monotonic sequence number and creation timestamp; tokens from a previous epoch (see bd-2gr) are automatically rejected with STALE_OBLIGATION_EPOCH.\n5. A configurable obligation timeout ensures that prepare-without-commit/rollback is detected: if a token is neither committed nor rolled back within the timeout, a background reaper logs OBLIGATION_TIMEOUT and forces rollback.\n6. The channel tracks outstanding obligation count as a gauge metric; the gauge must return to zero after all flows complete (verified in integration tests).\n7. Unit tests verify: (a) prepare -> commit round-trip succeeds, (b) prepare -> rollback releases resources, (c) dropped token triggers leak detection, (d) stale-epoch token is rejected, (e) timeout triggers forced rollback, (f) double-commit on same token returns OBLIGATION_ALREADY_CONSUMED.\n8. Structured log events: OBLIGATION_PREPARE / OBLIGATION_COMMIT / OBLIGATION_ROLLBACK / OBLIGATION_TIMEOUT / LEAKED_OBLIGATION with channel name, sequence number, and trace correlation ID.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:36:50.060962999Z","created_by":"ubuntu","updated_at":"2026-02-20T17:01:40.922902406Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-11","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-2ah","depends_on_id":"bd-1n5p","type":"blocks","created_at":"2026-02-20T15:00:17.732037151Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2aj","title":"[10.12] Implement ecosystem network-effect APIs (registry/reputation/compliance evidence).","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md — Section 10.12 (Frontier Programs), cross-ref Section 9H.5 (Ecosystem Network Effects Program)\n\n## Why This Exists\nfranken_node's category-defining thesis holds that trust-native operations are the differentiator and that adoption compounds through network effects. The Ecosystem Network Effects Program (9H.5) operationalizes this by building registry, reputation, and compliance evidence loops that lock in trust advantages and make switching costs asymmetric. This bead implements the API surface for those three pillars — extension registry queries, reputation scoring, and compliance evidence submission/retrieval — creating the programmatic foundation that all other frontier programs publish into and that external ecosystem participants consume. It directly advances the \">= 3 impossible-by-default capabilities broadly adopted\" floor target by making trust-verified ecosystem participation the default mode for extension publishers, operators, and auditors.\n\n## What This Must Do\n1. Implement a `RegistryAPI` module (`crates/franken-node/src/connector/ecosystem_registry.rs`) exposing: extension registration with signed metadata, version lineage queries, compatibility matrix lookups, and deprecation/revocation notifications. All mutations must produce immutable audit log entries.\n2. Implement a `ReputationAPI` module (`crates/franken-node/src/connector/ecosystem_reputation.rs`) that computes and serves reputation scores for extension publishers based on: compatibility pass rates (from 10.2), migration success rates (from 10.3/10.12 migration singularity), trust artifact validity (from 10.4/10.13 trust fabric), and verifier audit frequency (from 10.7/10.12 verifier economy). Scores must be deterministically reproducible from input evidence.\n3. Implement a `ComplianceEvidenceAPI` module (`crates/franken-node/src/connector/ecosystem_compliance.rs`) that accepts, stores, indexes, and serves compliance evidence artifacts (verification_evidence.json blobs, signed attestations, audit reports) with content-addressed storage and tamper-evident retrieval.\n4. Define a unified REST/gRPC API schema (`docs/specs/section_10_12/bd-2aj_api_schema.md`) with versioned endpoints, rate limiting, authentication (mTLS + API key), and pagination for all three pillars.\n5. Implement anti-gaming protections for the reputation system: Sybil resistance via publisher identity binding, rate-limited score updates, anomaly detection on sudden score changes, and a dispute/appeal mechanism.\n6. Provide a Python SDK wrapper (`scripts/ecosystem_network_sdk.py`) that other frontier demo gates and verification scripts can use to interact with all three APIs programmatically.\n7. Emit compliance evidence into the ecosystem APIs from at least two other frontier programs (migration singularity and trust fabric) as proof of integration.\n\n## Context from Enhancement Maps\n- 9H.5: \"Build registry + reputation + compliance evidence loops that compound adoption and lock in trust advantages.\" This bead is the direct implementation of that program's API surface.\n- Category-defining targets (Section 3.2): \">= 3 impossible-by-default capabilities broadly adopted\" — trust-verified registry participation, deterministic reputation scoring, and tamper-evident compliance evidence are three capabilities that are impossible-by-default in Node/Bun ecosystems. \">= 10x reduction in successful host compromise\" — the reputation and compliance evidence loops create early-warning signals for compromised or malicious extensions.\n\n## Dependencies\n- Upstream: bd-y0v (operator intelligence engine — produces recommendation evidence that feeds into compliance APIs), bd-3c2 (verifier-economy SDK — provides the independent validation workflows whose results populate reputation scores), bd-5si (trust fabric convergence — trust artifact validity is a reputation input), Section 10.2 (compatibility core — pass rates feed reputation), Section 10.3 (migration system — migration success rates feed reputation), Section 10.4/10.13 (trust/security — signed artifacts and revocation state feed registry and reputation).\n- Downstream: bd-n1w (frontier demo gates — demo gates publish results through these APIs), bd-1d6x (section-wide verification gate), bd-go4 (section 10.12 plan rollup).\n\n## Acceptance Criteria\n1. RegistryAPI supports extension registration, version lineage queries, compatibility matrix lookups, and deprecation notifications, with all mutations producing immutable audit log entries.\n2. ReputationAPI computes deterministic reputation scores from at least four input dimensions (compatibility pass rate, migration success rate, trust artifact validity, verifier audit frequency); given identical inputs, the score is byte-identical across runs.\n3. ComplianceEvidenceAPI stores and retrieves evidence artifacts using content-addressed storage (SHA-256 keyed); retrieval includes tamper-evidence verification (hash check on read).\n4. Anti-gaming protections are active: Sybil resistance rejects duplicate publisher identities; anomaly detection flags score changes exceeding 2 standard deviations from rolling mean.\n5. The Python SDK wrapper successfully exercises all three API surfaces in automated tests with >= 95% endpoint coverage.\n6. At least two other frontier programs (migration singularity, trust fabric) emit compliance evidence through the ComplianceEvidenceAPI, demonstrated in integration tests.\n7. API latency for read operations is < 50ms p99 for a corpus of 1,000 registered extensions (benchmark test).\n8. The verification evidence JSON records API endpoint coverage, reputation score determinism check, tamper-evidence retrieval check, and anti-gaming test results.\n\n## Testing & Logging Requirements\n- Unit tests: Test RegistryAPI CRUD operations with edge cases (duplicate registration, revoked extension queries, empty lineage); test ReputationAPI score computation determinism with fixed inputs; test ComplianceEvidenceAPI content-addressed storage and retrieval integrity; test anti-gaming anomaly detection thresholds.\n- Integration tests: End-to-end API contract tests for all three pillars (register extension -> compute reputation -> submit compliance evidence -> query all three); test mTLS authentication rejection for unauthorized clients; test rate limiting behavior under burst load.\n- E2E tests: Operator workflow — register a new extension, observe reputation score initialization, submit verification evidence from a migration run, query updated reputation, retrieve compliance evidence and verify tamper-evidence seal.\n- External reproducibility tests: An independent verifier can query the RegistryAPI and ComplianceEvidenceAPI to independently verify any reputation score by re-computing from the same evidence inputs.\n- Structured logs: Emit `REGISTRY_MUTATION`, `REGISTRY_QUERY`, `REPUTATION_COMPUTED`, `REPUTATION_ANOMALY`, `COMPLIANCE_EVIDENCE_STORED`, `COMPLIANCE_EVIDENCE_RETRIEVED`, `COMPLIANCE_TAMPER_CHECK_PASS`, `COMPLIANCE_TAMPER_CHECK_FAIL`, `API_AUTH_REJECT`, `API_RATE_LIMIT` events with trace correlation IDs, publisher identifiers, and operation durations.\n\n## Expected Artifacts\n- docs/specs/section_10_12/bd-2aj_contract.md\n- docs/specs/section_10_12/bd-2aj_api_schema.md\n- artifacts/section_10_12/bd-2aj/verification_evidence.json\n- artifacts/section_10_12/bd-2aj/verification_summary.md","acceptance_criteria":"1. Implement a ComponentRegistry API with: (a) register(component_id, metadata) -> RegistryEntry that registers a component (migration tool, verifier, plugin) with its name, version, capabilities, and public key, (b) lookup(component_id) -> Option<RegistryEntry>, (c) search(query) -> Vec<RegistryEntry> supporting filtering by capability, version range, and reputation score, (d) deregister(component_id, authority_key) -> Result.\n2. Implement a ReputationScoring system: each registered component accumulates a reputation score based on: (a) successful_verifications (count of times it produced correct verification results), (b) failed_verifications (incorrect results), (c) uptime_ratio (availability over rolling 30-day window), (d) compliance_attestations (count of signed compliance evidence submitted). Score = weighted combination with configurable weights, normalized to [0.0, 1.0].\n3. Implement ComplianceEvidence submission: (a) submit_evidence(component_id, evidence_type, evidence_hash, signer_key) -> EvidenceReceipt that records a compliance claim, (b) verify_evidence(receipt_id) -> VerificationResult that checks the evidence hash and signature. Evidence types include: AUDIT_PASS, PENTEST_CLEAR, LICENSE_COMPLIANT, INTEROP_CERTIFIED.\n4. Implement API access control: registry mutations (register, deregister, submit_evidence) require audience-bound tokens (from bd-1r2) with appropriate action scopes. Read operations (lookup, search) are public.\n5. Implement registry consistency: all mutations produce a monotonic sequence number and are append-only in the audit log. Provide a registry_audit_log(since_sequence) -> Vec<AuditEntry> API.\n6. Implement network-effect metrics: expose total_registered_components, average_reputation_score, total_compliance_attestations, and components_by_capability as structured metrics.\n7. Unit tests: (a) register/lookup/search lifecycle, (b) reputation score calculation, (c) compliance evidence submission and verification, (d) unauthorized mutation rejection, (e) deregister removes from search results, (f) audit log captures all mutations.\n8. Integration test: register 10 components, submit evidence for 5, compute reputation scores, verify search ranking matches reputation order.\n9. Verification: scripts/check_ecosystem_apis.py --json, artifacts at artifacts/section_10_12/bd-2aj/.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:36:51.234881133Z","created_by":"ubuntu","updated_at":"2026-02-20T17:01:50.172463672Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-12","self-contained","test-obligations"]}
{"id":"bd-2ao3","title":"[10.21] Implement temporal drift feature engine (velocity, acceleration, entropy, novelty, capability-creep gradient).","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.21 — Behavioral Phenotype Evolution Tracker Execution Track (9O)\n\nWhy This Exists:\nBPET longitudinal phenotype intelligence track for pre-compromise trajectory detection and integration with DGIS/ATC/migration/economic policy.\n\nTask Objective:\nImplement temporal drift feature engine (velocity, acceleration, entropy, novelty, capability-creep gradient).\n\nAcceptance Criteria:\n- Drift features are numerically stable and reproducible; feature-store interfaces support historical replay and windowed recomputation.\n\nExpected Artifacts:\n- `src/security/bpet/drift_features.rs`, `tests/security/bpet_drift_feature_stability.rs`, `artifacts/10.21/bpet_drift_feature_matrix.csv`.\n\n- Machine-readable verification artifact at `artifacts/section_10_21/bd-2ao3/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_21/bd-2ao3/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.21] Implement temporal drift feature engine (velocity, acceleration, entropy, novelty, capability-creep gradient).\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.21] Implement temporal drift feature engine (velocity, acceleration, entropy, novelty, capability-creep gradient).\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.21] Implement temporal drift feature engine (velocity, acceleration, entropy, novelty, capability-creep gradient).\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.21] Implement temporal drift feature engine (velocity, acceleration, entropy, novelty, capability-creep gradient).\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.21] Implement temporal drift feature engine (velocity, acceleration, entropy, novelty, capability-creep gradient).\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"- Drift features are numerically stable and reproducible; feature-store interfaces support historical replay and windowed recomputation.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:37:08.033916716Z","created_by":"ubuntu","updated_at":"2026-02-20T17:05:31.045247370Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-21","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-2ao3","depends_on_id":"bd-2xgs","type":"blocks","created_at":"2026-02-20T17:05:31.045200172Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2bj4","title":"[10.20] Implement deterministic graph ingestion pipeline from lockfiles, extension manifests, registry metadata, and local execution evidence.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.20 — Dependency Graph Immune System Execution Track (9N)\n\nWhy This Exists:\nDGIS topological immune system track for dependency contagion modeling, choke-point immunization, and graph-aware containment economics.\n\nTask Objective:\nImplement deterministic graph ingestion pipeline from lockfiles, extension manifests, registry metadata, and local execution evidence.\n\nAcceptance Criteria:\n- Ingestion reproducibly resolves graph state from the same source set; stale/missing provenance is explicitly surfaced as typed risk signals; ingestion supports replay.\n\nExpected Artifacts:\n- `src/security/dgis/graph_ingestion.rs`, `tests/conformance/dgis_graph_ingestion.rs`, `artifacts/10.20/dgis_ingestion_replay_report.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_20/bd-2bj4/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_20/bd-2bj4/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.20] Implement deterministic graph ingestion pipeline from lockfiles, extension manifests, registry metadata, and local execution evidence.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.20] Implement deterministic graph ingestion pipeline from lockfiles, extension manifests, registry metadata, and local execution evidence.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.20] Implement deterministic graph ingestion pipeline from lockfiles, extension manifests, registry metadata, and local execution evidence.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.20] Implement deterministic graph ingestion pipeline from lockfiles, extension manifests, registry metadata, and local execution evidence.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.20] Implement deterministic graph ingestion pipeline from lockfiles, extension manifests, registry metadata, and local execution evidence.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"- Ingestion reproducibly resolves graph state from the same source set; stale/missing provenance is explicitly surfaced as typed risk signals; ingestion supports replay.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:37:06.502245942Z","created_by":"ubuntu","updated_at":"2026-02-20T17:04:49.192584423Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-20","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-2bj4","depends_on_id":"bd-b541","type":"blocks","created_at":"2026-02-20T17:04:49.192534671Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2cfz","title":"Epic: Adversarial Trust Commons (ATC) [10.19]","description":"- type: epic","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-20T07:47:58.072163029Z","updated_at":"2026-02-20T07:49:21.311326149Z","closed_at":"2026-02-20T07:49:21.311305090Z","close_reason":"Superseded by canonical detailed master-plan graph (bd-33v) with full 10.N-10.21 and 11-16 coverage, explicit dependencies, and per-section verification gates.","source_repo":".","compaction_level":0,"original_size":0}
{"id":"bd-2d0","title":"Backfill beads graph from TODO_ULTRA_DETAILED","description":"Create initial actionable beads from docs/TODO_ULTRA_DETAILED.md and wire dependencies for parallel execution.\n\nTesting & Logging Requirements:\n- Comprehensive unit tests for all core logic, edge cases, and error paths.\n- Integration and end-to-end test scripts that exercise full user-facing workflows.\n- Detailed structured logging with stable error/status codes and trace correlation IDs.\n- Deterministic artifacts (reports/log bundles) sufficient to reproduce failures and verify fixes.\n","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-20T07:26:05.244464447Z","created_by":"ubuntu","updated_at":"2026-02-20T07:44:42.337185339Z","closed_at":"2026-02-20T07:44:42.337164901Z","close_reason":"Superseded by full 10.x + 11-16 master-plan graph in bd-33v; backlog seeding objective complete.","source_repo":".","compaction_level":0,"original_size":0}
{"id":"bd-2d17","title":"[10.20] Integrate DGIS health scoring into migration autopilot admission and progression gates.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.20 — Dependency Graph Immune System Execution Track (9N)\n\nWhy This Exists:\nDGIS topological immune system track for dependency contagion modeling, choke-point immunization, and graph-aware containment economics.\n\nTask Objective:\nIntegrate DGIS health scoring into migration autopilot admission and progression gates.\n\nAcceptance Criteria:\n- Migration plans include graph-health baselines and target thresholds; migrations that worsen cascade risk beyond policy budgets are blocked or auto-replanned.\n\nExpected Artifacts:\n- `src/migration/dgis_migration_gate.rs`, `tests/integration/dgis_migration_gate.rs`, `artifacts/10.20/dgis_migration_health_report.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_20/bd-2d17/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_20/bd-2d17/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.20] Integrate DGIS health scoring into migration autopilot admission and progression gates.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.20] Integrate DGIS health scoring into migration autopilot admission and progression gates.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.20] Integrate DGIS health scoring into migration autopilot admission and progression gates.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.20] Integrate DGIS health scoring into migration autopilot admission and progression gates.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.20] Integrate DGIS health scoring into migration autopilot admission and progression gates.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"- Migration plans include graph-health baselines and target thresholds; migrations that worsen cascade risk beyond policy budgets are blocked or auto-replanned.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:37:07.444273469Z","created_by":"ubuntu","updated_at":"2026-02-20T17:00:53.403057941Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-20","self-contained","test-obligations"]}
{"id":"bd-2de","title":"[10.0] Implement migration autopilot pipeline.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.0 — Top 10 Initiative Tracking (Initiative #2)\nCross-references: 9A.2, 9B.2, 9C.2, 9D.2\n\nWhy This Exists:\nMigration autopilot is the #2 strategic initiative. It provides one-command migration workflows that inventory APIs, detect risk hotspots, propose transformations, run compatibility validation, and emit rollout guidance with confidence grades. This is the primary mechanism for collapsing migration friction — the key barrier to franken_node adoption.\n\nTask Objective:\nBuild the full migration autopilot pipeline from audit through rollout: scan project -> score risks -> suggest rewrites -> validate with lockstep -> plan rollout stages -> generate confidence report -> export enterprise review bundle -> enable deterministic failure replay.\n\nDetailed Acceptance Criteria:\n1. Project scanner inventories all API/runtime/dependency usage with risk classification per band.\n2. Risk scoring model produces explainable feature-level scores with uncertainty bands.\n3. Rewrite suggestion engine generates transformation proposals with rollback plan artifacts and hypothesis-tested confidence intervals (9C.2).\n4. Validation runner executes lockstep checks against compatibility fixtures.\n5. Rollout planner creates staged deployment plan (shadow -> canary -> ramp -> default) per project.\n6. Migration confidence report includes uncertainty bands and per-API risk breakdowns.\n7. One-command enterprise review export produces self-contained migration assessment bundle.\n8. Deterministic failure replay tooling captures and replays any migration failure.\n9. Scan and transform throughput optimized with deterministic batching and cache reuse (9D.2).\n10. Incremental/self-adjusting computation so large project migrations re-run quickly and reproducibly (9B.2).\n\nKey Dependencies:\n- Depends on 10.2 (Compatibility Core) for fixture runners and band definitions.\n- Depends on 10.1 (Charter) for governance boundaries.\n- This is the foundation for 10.3 (Migration System) detailed implementation beads.\n- Consumed by 10.9 (Moonshot) for migration singularity demo pipeline.\n\nExpected Artifacts:\n- src/migration/ module with scanner, risk_scorer, rewrite_engine, validation_runner, rollout_planner, confidence_report, export, replay submodules.\n- docs/specs/section_10_0/bd-2de_contract.md\n- artifacts/section_10_0/bd-2de/verification_evidence.json\n- artifacts/section_10_0/bd-2de/verification_summary.md\n\nTesting and Logging Requirements:\n- Unit tests: scanner API detection, risk scoring accuracy, rewrite transformation correctness, rollout stage computation, confidence interval calibration.\n- Integration tests: full pipeline execution on sample Node.js projects with known API surfaces.\n- E2E tests: franken-node migrate audit/rewrite/validate CLI commands on real-world fixture projects.\n- Performance tests: scanner throughput on large monorepo fixtures (>10k files).\n- Structured logs: MIGRATION_SCAN_START, RISK_SCORED, REWRITE_PROPOSED, VALIDATION_PASSED/FAILED, ROLLOUT_PLANNED with trace IDs and per-API breakdown.","acceptance_criteria":"1. Single-command invocation (`franken-node migrate <project-dir>`) executes full pipeline: inventory, risk detection, transformation proposal, validation, rollout guidance.\n2. API inventory phase catalogs all Node.js/Bun API calls in target project with >= 99% recall (verified against manual audit of 5 reference projects).\n3. Risk hotspot detection identifies: deprecated APIs, polyfilled behaviors, platform-specific code paths, divergence-ledger matches; each hotspot assigned severity (critical/high/medium/low).\n4. Transformation proposals are generated as AST-safe patches (not regex); each proposal includes before/after diff, confidence grade (A/B/C/D), and rollback instruction.\n5. Migration velocity target: >= 3x faster than manual migration baseline (Section 3), measured on reference project corpus.\n6. Validation phase runs transformed code against compatibility envelope test suite; reports pass/fail per API category.\n7. Rollout guidance emits confidence grades per module: A (>= 95% pass), B (>= 85%), C (>= 70%), D (< 70% — manual review required).\n8. Pipeline produces machine-readable migration report (JSON) with: total APIs, migrated count, risk hotspots, confidence distribution, estimated remaining manual effort.\n9. Idempotent execution: running pipeline twice on same input produces identical output.\n10. Cross-references to enhancement maps 9A-9O verified; pipeline stages map to canonical owner tracks in 10.N.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:36:42.563832677Z","created_by":"ubuntu","updated_at":"2026-02-20T15:35:27.919235569Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-0","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-2de","depends_on_id":"bd-1qp","type":"blocks","created_at":"2026-02-20T07:43:10.137771649Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2e73","title":"[10.14] Implement bounded evidence ledger ring buffer plus lab spill-to-artifacts mode.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.14 — FrankenSQLite Deep-Mined Expansion Execution Track (9J)\n\nWhy This Exists:\nFrankenSQLite deep-mined control integrity track: evidence ledgers, proofs, epochs, remote effects, markers/MMR, and deterministic fault labs.\n\nTask Objective:\nImplement bounded evidence ledger ring buffer plus lab spill-to-artifacts mode.\n\nAcceptance Criteria:\n- Production ledger memory stays within configured bound; overflow policy is deterministic; lab mode writes full spill artifacts for failing scenarios.\n\nExpected Artifacts:\n- `src/observability/evidence_ledger.rs`, `tests/integration/evidence_ledger_bounds.rs`, `artifacts/10.14/evidence_spill_example.jsonl`.\n\n- Machine-readable verification artifact at `artifacts/section_10_14/bd-2e73/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_14/bd-2e73/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.14] Implement bounded evidence ledger ring buffer plus lab spill-to-artifacts mode.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.14] Implement bounded evidence ledger ring buffer plus lab spill-to-artifacts mode.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.14] Implement bounded evidence ledger ring buffer plus lab spill-to-artifacts mode.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.14] Implement bounded evidence ledger ring buffer plus lab spill-to-artifacts mode.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.14] Implement bounded evidence ledger ring buffer plus lab spill-to-artifacts mode.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"Production ledger memory stays within configured bound; overflow policy is deterministic; lab mode writes full spill artifacts for failing scenarios.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:36:55.222468326Z","created_by":"ubuntu","updated_at":"2026-02-20T15:44:12.537578423Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-14","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-2e73","depends_on_id":"bd-nupr","type":"blocks","created_at":"2026-02-20T07:43:14.244117446Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2eun","title":"[10.13] Implement quarantine-by-default store for unreferenced objects with quota + TTL enforcement.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.13 — FCP Deep-Mined Expansion Execution Track (9I)\n\nWhy This Exists:\nDeep connector/provider contract track: lifecycle FSMs, conformance harnesses, fencing, sandboxing, revocation freshness, and protocol hardening.\n\nTask Objective:\nImplement quarantine-by-default store for unreferenced objects with quota + TTL enforcement.\n\nAcceptance Criteria:\n- Unknown objects enter quarantine class by default; quota and TTL eviction enforce hard caps; quarantined objects are excluded from primary gossip state.\n\nExpected Artifacts:\n- `src/admission/quarantine_store.rs`, `tests/integration/quarantine_retention.rs`, `artifacts/10.13/quarantine_usage_metrics.csv`.\n\n- Machine-readable verification artifact at `artifacts/section_10_13/bd-2eun/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_13/bd-2eun/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.13] Implement quarantine-by-default store for unreferenced objects with quota + TTL enforcement.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.13] Implement quarantine-by-default store for unreferenced objects with quota + TTL enforcement.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.13] Implement quarantine-by-default store for unreferenced objects with quota + TTL enforcement.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.13] Implement quarantine-by-default store for unreferenced objects with quota + TTL enforcement.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.13] Implement quarantine-by-default store for unreferenced objects with quota + TTL enforcement.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","status":"closed","priority":1,"issue_type":"task","assignee":"CrimsonCrane","created_at":"2026-02-20T07:36:54.087635905Z","created_by":"ubuntu","updated_at":"2026-02-20T12:51:47.028565935Z","closed_at":"2026-02-20T12:51:47.028537071Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-13","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-2eun","depends_on_id":"bd-3b8m","type":"blocks","created_at":"2026-02-20T07:43:13.643348493Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2ew","title":"[10.3] Build automated rewrite suggestion engine with rollback plan artifacts.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.3 — Migration System\n\nWhy This Exists:\nMigration autopilot pipeline from audit to rollout, designed to collapse migration friction with deterministic confidence and replayability.\n\nTask Objective:\nBuild automated rewrite suggestion engine with rollback plan artifacts.\n\nAcceptance Criteria:\n- Implement with explicit success/failure invariants and deterministic behavior under normal, edge, and fault scenarios.\n- Ensure outcomes are verifiable via automated tests and reproducible artifact outputs.\n\nExpected Artifacts:\n- Section-owned design/spec artifact at `docs/specs/section_10_3/bd-2ew_contract.md`, capturing decision rationale, invariants, and interface boundaries.\n- Machine-readable verification artifact at `artifacts/section_10_3/bd-2ew/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_3/bd-2ew/verification_summary.md` linking implementation intent to measured outcomes.\n\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.3] Build automated rewrite suggestion engine with rollback plan artifacts.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.3] Build automated rewrite suggestion engine with rollback plan artifacts.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.3] Build automated rewrite suggestion engine with rollback plan artifacts.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.3] Build automated rewrite suggestion engine with rollback plan artifacts.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.3] Build automated rewrite suggestion engine with rollback plan artifacts.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","status":"closed","priority":1,"issue_type":"task","assignee":"CrimsonCrane","created_at":"2026-02-20T07:36:44.957177915Z","created_by":"ubuntu","updated_at":"2026-02-20T10:12:52.375674295Z","closed_at":"2026-02-20T10:12:52.375651132Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-3","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-2ew","depends_on_id":"bd-33x","type":"blocks","created_at":"2026-02-20T07:43:22.098871560Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2f43","title":"[13] Success criterion: low-risk migration pathways","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 13\n\nTask Objective:\nInstrument and verify that franken_node delivers practical low-risk migration pathways for Node/Bun cohorts.\n\nExecution Requirements:\n- Make this requirement machine-verifiable and release-gated where applicable.\n- Keep behavior self-documenting so future contributors do not need to re-open the master plan.\n\nTesting & Logging Requirements:\n- Unit tests for rule/contract logic and error conditions.\n- Integration/E2E tests for workflow-level enforcement.\n- Structured logs with stable codes and trace IDs.\n- Reproducible artifacts that prove contract satisfaction.\n\nAcceptance Criteria:\n- Success and failure invariants for [13] Success criterion: low-risk migration pathways are explicitly quantified and machine-verifiable.\n- Determinism requirements for [13] Success criterion: low-risk migration pathways are validated across repeated runs and adversarial perturbations.\n\n\nExpected Artifacts:\n- Machine-readable verification artifact with explicit canonical path under artifacts/section_13/bd-2f43/verification_evidence.json, suitable for CI/release gating.\n- Human-readable verification summary with explicit canonical path under artifacts/section_13/bd-2f43/verification_summary.md, linking implementation intent to measured validation outcomes.\n\nTask-Specific Clarification:\n- The implementation must deliver the exact capability named in the title, with no scope dilution and no silent feature omission.\n- Validation artifacts must include capability-specific thresholds, pass/fail criteria, and reproducible evidence bundles tied to this bead ID.\n- Program/section verification gates must be able to consume this bead's outputs without manual interpretation.\n\nWhy This Improves User Outcomes:\n- \"[13] Success criterion: low-risk migration pathways\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[13] Success criterion: low-risk migration pathways\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"1. Migration pathways are documented for the top 5 Node.js project archetypes: Express/Fastify web server, Next.js/Remix SSR app, CLI tool, npm library package, background worker/queue processor.\n2. Each pathway includes: pre-migration compatibility check, automated migration steps, manual intervention points, post-migration validation suite.\n3. Migration risk score (0-100) is computed automatically for each project before migration begins.\n4. Projects with risk score <= 30 ('low risk') complete migration with zero manual steps in >= 90% of cases.\n5. Bun migration pathway exists for at least 2 archetypes (web server, CLI tool) with documented compatibility delta.\n6. Rollback from franken_node to original runtime is supported and tested for all pathways.\n7. Evidence: migration_pathway_matrix.json mapping each archetype to pathway steps, risk score range, and success rate.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:39:34.303595863Z","created_by":"ubuntu","updated_at":"2026-02-20T16:08:27.977113562Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-13","self-contained","test-obligations"]}
{"id":"bd-2f5l","title":"[10.16] Build `fastapi_rust` service skeleton for required operator/verifier/fleet-control endpoints.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.16 — Adjacent Substrate Integration Execution Track (8.7)\n\nWhy This Exists:\nAdjacent substrate integration track to enforce deep composition with frankentui, frankensqlite, sqlmodel_rust, and fastapi_rust.\n\nTask Objective:\nBuild `fastapi_rust` service skeleton for required operator/verifier/fleet-control endpoints.\n\nAcceptance Criteria:\n- Skeleton exposes required endpoint groups with policy and trace correlation hooks; service conformance tests pass.\n\nExpected Artifacts:\n- `services/control_plane_fastapi_rust/*`, `tests/integration/fastapi_control_plane_endpoints.rs`, `artifacts/10.16/fastapi_endpoint_report.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_16/bd-2f5l/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_16/bd-2f5l/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.16] Build `fastapi_rust` service skeleton for required operator/verifier/fleet-control endpoints.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.16] Build `fastapi_rust` service skeleton for required operator/verifier/fleet-control endpoints.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.16] Build `fastapi_rust` service skeleton for required operator/verifier/fleet-control endpoints.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.16] Build `fastapi_rust` service skeleton for required operator/verifier/fleet-control endpoints.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.16] Build `fastapi_rust` service skeleton for required operator/verifier/fleet-control endpoints.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"- Skeleton exposes required endpoint groups with policy and trace correlation hooks; service conformance tests pass.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:37:02.431909915Z","created_by":"ubuntu","updated_at":"2026-02-20T17:05:24.092704527Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-16","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-2f5l","depends_on_id":"bd-3ndj","type":"blocks","created_at":"2026-02-20T17:05:24.092615261Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2fa","title":"[10.5] Implement counterfactual replay mode for policy simulation.","description":"# [10.5] Counterfactual Replay Mode for Policy Simulation\n\n## Why This Exists\n\nThis bead implements a core capability related to Impossible-by-Default capability #4 from Section 3.2: deterministic incident replay. While the incident replay bundle (bd-vll) provides the ability to replay historical incidents exactly as they occurred, counterfactual replay extends this by allowing operators to ask \"what if we had a different policy?\" and re-run the same incident with modified policy parameters. This is essential for policy improvement: without counterfactual simulation, operators must wait for the next real incident to learn whether a policy change would have helped.\n\nSection 10.5 (Security + Policy Product Surfaces) positions counterfactual replay as a policy simulation tool that feeds into the operator copilot (bd-2yc) and expected-loss scoring (bd-33b). It also supports the evidence ledger (Section 10.14) by producing comparison reports that can be attached to policy change proposals (bd-sh3) as impact evidence. The plan requires that counterfactual replays are fully deterministic: given the same incident bundle and the same modified policy, the replay must produce identical results every time.\n\n## What It Must Do\n\n1. **Incident Bundle Ingestion**: Accept a deterministic incident replay bundle (produced by bd-vll) as input. The bundle contains the full sequence of events, trust states, decisions, and outcomes from a historical incident. Validate bundle integrity (hash verification) before processing.\n\n2. **Policy Override Injection**: Accept a set of policy overrides that modify one or more policy parameters for the duration of the replay. Overrides are specified as a structured diff against the policy configuration that was active during the original incident. The system must validate that overrides are schema-compliant.\n\n3. **Deterministic Replay Engine**: Re-execute the incident timeline with the modified policy, producing a complete decision trace. The replay must be fully deterministic: no external I/O, no real-time clock dependencies, no randomness (or seeded randomness from the bundle). Every decision point must record which policy rule fired and why.\n\n4. **Comparison Report Generation**: Produce a structured comparison report showing, for each decision point: (a) the original decision, (b) the counterfactual decision, (c) whether they differ, (d) the expected-loss delta between the original and counterfactual outcomes, and (e) a cumulative outcome delta summarizing the overall impact of the policy change.\n\n5. **Multi-Scenario Support**: Support running multiple counterfactual scenarios in a single invocation (e.g., \"what if threshold was 0.8? 0.7? 0.6?\") and producing a comparison matrix across all scenarios.\n\n6. **Isolation Guarantees**: Counterfactual replays must have zero side effects on the live system. No audit events, state mutations, or external notifications are emitted during replay. The replay operates in a sandboxed execution context.\n\n7. **Evidence Package for Policy Changes**: The comparison report can be attached to a policy change proposal (bd-sh3) as evidence that the proposed change would have improved outcomes in historical incidents.\n\n## Acceptance Criteria\n\n1. The system accepts a valid incident replay bundle and a set of policy overrides, and produces a counterfactual decision trace that is fully deterministic (running the same inputs twice produces bitwise-identical output).\n2. Bundle integrity is verified (hash check) before replay begins; a corrupted bundle is rejected with a structured error identifying the corrupted segment.\n3. Policy overrides are validated against the policy schema; invalid overrides are rejected with structured validation errors before replay begins.\n4. The comparison report contains, for every decision point: original decision, counterfactual decision, match/diff flag, expected-loss delta, and a cumulative outcome summary.\n5. Multi-scenario mode accepts up to 20 policy override sets and produces a comparison matrix; the total execution time scales linearly (not quadratically) with the number of scenarios.\n6. Counterfactual replay produces zero side effects: no audit trail entries, no state mutations, no external API calls. This is verified by an isolation test that monitors all output channels during replay.\n7. The comparison report is machine-readable JSON and includes a metadata block with: bundle hash, policy override diffs, replay timestamp, and engine version.\n8. Replay of a 1000-event incident bundle completes within 5 seconds on the target hardware (benchmark test required).\n9. The comparison report can be referenced by ID in a policy change proposal (bd-sh3 integration point).\n10. All counterfactual replay invocations are logged (at the invocation level, not per-event) with stable event codes (COUNTERFACTUAL_REPLAY_STARTED, COUNTERFACTUAL_REPLAY_COMPLETED, COUNTERFACTUAL_BUNDLE_INVALID, COUNTERFACTUAL_OVERRIDE_INVALID) and trace correlation IDs.\n11. The replay engine correctly handles incident bundles that include degraded-mode transitions (bd-3nr), replaying the degraded-mode logic with potentially different degraded-mode policy parameters.\n\n## Key Dependencies\n\n- **Depends on bd-vll** (incident replay bundle generation): counterfactual replay consumes bundles produced by bd-vll.\n- **Depends on the policy engine**: the replay engine must be able to evaluate policy rules with overridden parameters.\n- **Depends on expected-loss scoring (bd-33b)**: the comparison report includes expected-loss deltas.\n- **Depended on by bd-2yc** (operator copilot): the copilot can invoke counterfactual replay to support \"what-if\" queries.\n- **Depended on by bd-sh3** (policy change workflows): comparison reports serve as evidence for policy change proposals.\n- **Depended on by bd-1koz** (section-wide verification gate).\n- **Depended on by bd-20a** (section rollup).\n\n## Testing and Logging Requirements\n\n- **Unit tests**: Bundle ingestion and hash verification (valid and corrupted bundles). Policy override validation (valid, invalid, partial). Determinism test (replay same bundle+overrides 100 times, assert identical output). Comparison report field completeness. Multi-scenario linear scaling verification.\n- **Integration tests**: Full counterfactual replay pipeline: ingest bundle from bd-vll, apply overrides, replay, generate comparison report, verify report structure and content. Test with bundles containing degraded-mode transitions.\n- **E2E tests**: Simulate a historical incident, generate a bundle with bd-vll, run counterfactual replay with a policy change that would have prevented the incident, verify the comparison report shows the expected decision differences and improved outcome.\n- **Isolation tests**: Run a counterfactual replay while monitoring all output channels (audit trail, state store, external APIs) and assert zero writes/calls.\n- **Benchmark tests**: Replay a 1000-event bundle and assert completion within 5 seconds. Track regression across builds.\n- **Adversarial tests**: Supply a bundle with events out of chronological order. Supply overrides that create contradictory policy rules. Supply a bundle from an incompatible engine version. Verify graceful handling in all cases.\n- **Logging**: Replay invocations at INFO level. Bundle validation at DEBUG level. Override validation at DEBUG level. Replay completion (with timing) at INFO level. Errors at ERROR level.\n\n## Expected Artifacts\n\n- `docs/specs/section_10_5/bd-2fa_contract.md` — Design spec with replay engine architecture, bundle schema reference, override diff format, comparison report schema, and isolation guarantees.\n- `artifacts/section_10_5/bd-2fa/verification_evidence.json` — Machine-readable pass/fail evidence.\n- `artifacts/section_10_5/bd-2fa/verification_summary.md` — Human-readable summary.\n- Rust module(s) in `crates/franken-node/src/` implementing the counterfactual replay engine and comparison report generator.\n- Python verification script `scripts/check_counterfactual_replay.py` with `--json` flag and `self_test()`.\n- Unit tests in `tests/test_check_counterfactual_replay.py`.\n- Benchmark fixture for replay performance testing.\n- Sample incident bundle fixture in `fixtures/` for determinism and integration tests.","acceptance_criteria":"1. Implement a CounterfactualReplayEngine that accepts a ReplayBundle (from bd-vll) and an alternate PolicyConfig, then re-executes the timeline under the alternate policy to produce a CounterfactualResult.\n2. CounterfactualResult contains: original_outcomes (Vec<DecisionPoint>), counterfactual_outcomes (Vec<DecisionPoint>), divergence_points (Vec<DivergenceRecord>), and summary_statistics (struct with fields: total_decisions, changed_decisions, severity_delta).\n3. Each DivergenceRecord includes: sequence_number, original_decision, counterfactual_decision, original_rationale, counterfactual_rationale, and impact_estimate (enum: None | Low | Medium | High | Critical).\n4. Replay must be fully deterministic and side-effect-free: no I/O, no network calls, no mutable global state; enforce this via a SandboxedExecutor trait that only permits pure computation.\n5. Support at least two simulation modes: SinglePolicySwap (replace one policy entirely) and ParameterSweep (vary a numeric parameter across N values, returning N CounterfactualResults).\n6. Execution must be bounded: enforce a max_replay_steps (default 100,000) and max_wall_clock (default 30s) timeout; exceeding either returns Err with partial results.\n7. Verification: scripts/check_counterfactual.py --json runs a counterfactual replay on a known fixture bundle with a policy that flips at least one decision, asserts divergence_points is non-empty; unit tests in tests/test_check_counterfactual.py cover both modes and the timeout guard; evidence in artifacts/section_10_5/bd-2fa/.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:36:46.300977215Z","created_by":"ubuntu","updated_at":"2026-02-20T16:59:01.600513391Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-5","self-contained","test-obligations"]}
{"id":"bd-2fid","title":"[10.20] Implement critical-node immunization planner and choke-point barrier synthesis engine.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.20 — Dependency Graph Immune System Execution Track (9N)\n\nWhy This Exists:\nDGIS topological immune system track for dependency contagion modeling, choke-point immunization, and graph-aware containment economics.\n\nTask Objective:\nImplement critical-node immunization planner and choke-point barrier synthesis engine.\n\nAcceptance Criteria:\n- Planner proposes minimum-cost barrier sets that reduce expected cascade loss under policy/performance constraints; recommendation rationale is machine-readable and replayable.\n\nExpected Artifacts:\n- `docs/specs/dgis_immunization_planner.md`, `src/security/dgis/immunization_planner.rs`, `artifacts/10.20/dgis_barrier_plan_catalog.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_20/bd-2fid/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_20/bd-2fid/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.20] Implement critical-node immunization planner and choke-point barrier synthesis engine.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.20] Implement critical-node immunization planner and choke-point barrier synthesis engine.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.20] Implement critical-node immunization planner and choke-point barrier synthesis engine.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.20] Implement critical-node immunization planner and choke-point barrier synthesis engine.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.20] Implement critical-node immunization planner and choke-point barrier synthesis engine.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"- Planner proposes minimum-cost barrier sets that reduce expected cascade loss under policy/performance constraints; recommendation rationale is machine-readable and replayable.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:37:06.831761194Z","created_by":"ubuntu","updated_at":"2026-02-20T17:05:01.506816409Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-20","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-2fid","depends_on_id":"bd-t89w","type":"blocks","created_at":"2026-02-20T17:05:01.506766966Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2fkq","title":"[14] Metric family: migration speed and failure-rate improvements","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 14\n\nTask Objective:\nInstrument migration speed/failure-rate improvement metric family.\n\nExecution Requirements:\n- Make this requirement machine-verifiable and release-gated where applicable.\n- Keep behavior self-documenting so future contributors do not need to re-open the master plan.\n\nTesting & Logging Requirements:\n- Unit tests for rule/contract logic and error conditions.\n- Integration/E2E tests for workflow-level enforcement.\n- Structured logs with stable codes and trace IDs.\n- Reproducible artifacts that prove contract satisfaction.\n\nAcceptance Criteria:\n- Success and failure invariants for [14] Metric family: migration speed and failure-rate improvements are explicitly quantified and machine-verifiable.\n- Determinism requirements for [14] Metric family: migration speed and failure-rate improvements are validated across repeated runs and adversarial perturbations.\n\n\nExpected Artifacts:\n- Machine-readable verification artifact with explicit canonical path under artifacts/section_14/bd-2fkq/verification_evidence.json, suitable for CI/release gating.\n- Human-readable verification summary with explicit canonical path under artifacts/section_14/bd-2fkq/verification_summary.md, linking implementation intent to measured validation outcomes.\n\nTask-Specific Clarification:\n- The implementation must deliver the exact capability named in the title, with no scope dilution and no silent feature omission.\n- Validation artifacts must include capability-specific thresholds, pass/fail criteria, and reproducible evidence bundles tied to this bead ID.\n- Program/section verification gates must be able to consume this bead's outputs without manual interpretation.\n\nWhy This Improves User Outcomes:\n- \"[14] Metric family: migration speed and failure-rate improvements\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[14] Metric family: migration speed and failure-rate improvements\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"METRIC FAMILY: Migration speed and failure-rate improvements.\n1. Metrics measured: (a) migration wall-clock time per project archetype, (b) manual intervention count per migration, (c) migration failure rate (% of projects that fail to complete migration), (d) post-migration defect rate (bugs found within 7 days), (e) velocity improvement ratio (tooled vs manual).\n2. Measured across project archetypes: Express/Fastify, Next.js/Remix, CLI tool, library, worker service, monorepo.\n3. Velocity target: >= 3x improvement (tooled migration takes <= 1/3 of manual time).\n4. Failure rate target: <= 5% of migration attempts fail (defined as: migrated project does not pass its original test suite).\n5. Post-migration defect rate target: <= 2 defects per migration within 7-day window.\n6. Metrics tracked over time: each release measures migration metrics on the standard cohort; regressions > 10% trigger investigation.\n7. Publication: migration metrics in benchmark report with per-archetype breakdown and trend charts.\n8. Evidence: migration_speed_metrics.json with per-archetype timings, failure rates, defect rates, and velocity ratios.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:39:36.071625027Z","created_by":"ubuntu","updated_at":"2026-02-20T15:25:34.264601049Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-14","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-2fkq","depends_on_id":"bd-2ps7","type":"blocks","created_at":"2026-02-20T07:43:26.157242133Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2fpj","title":"[11] Contract field: expected-loss model","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 11\n\nTask Objective:\nRequire explicit expected-loss model inputs, assumptions, and output for decision-bearing changes.\n\nExecution Requirements:\n- Make this requirement machine-verifiable and release-gated where applicable.\n- Keep behavior self-documenting so future contributors do not need to re-open the master plan.\n\nTesting & Logging Requirements:\n- Unit tests for rule/contract logic and error conditions.\n- Integration/E2E tests for workflow-level enforcement.\n- Structured logs with stable codes and trace IDs.\n- Reproducible artifacts that prove contract satisfaction.\n\nAcceptance Criteria:\n- Success and failure invariants for [11] Contract field: expected-loss model are explicitly quantified and machine-verifiable.\n- Determinism requirements for [11] Contract field: expected-loss model are validated across repeated runs and adversarial perturbations.\n\n\nExpected Artifacts:\n- Machine-readable verification artifact with explicit canonical path under artifacts/section_11/bd-2fpj/verification_evidence.json, suitable for CI/release gating.\n- Human-readable verification summary with explicit canonical path under artifacts/section_11/bd-2fpj/verification_summary.md, linking implementation intent to measured validation outcomes.\n\nTask-Specific Clarification:\n- The implementation must deliver the exact capability named in the title, with no scope dilution and no silent feature omission.\n- Validation artifacts must include capability-specific thresholds, pass/fail criteria, and reproducible evidence bundles tied to this bead ID.\n- Program/section verification gates must be able to consume this bead's outputs without manual interpretation.\n\nWhy This Improves User Outcomes:\n- \"[11] Contract field: expected-loss model\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[11] Contract field: expected-loss model\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"1. Every evidence contract includes an expected-loss model section with: (a) probability of adverse outcome (numeric, 0-1), (b) estimated cost/impact in concrete units (latency ms, error rate %, incidents/month), (c) expected loss = probability * impact.\n2. The model must specify at least two scenarios: best-case and worst-case, with distinct probability/impact pairs.\n3. Loss estimates must cite data sources: historical metrics, benchmark results, or threat model assumptions.\n4. CI rejects contracts where expected-loss section is missing or contains non-numeric probability/impact values.\n5. Unit test: contract with valid two-scenario model passes; contract with single scenario or non-numeric values fails.\n6. Expected loss must be cross-referenced with the EV score — if expected loss exceeds EV benefit, the contract must include explicit justification.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:39:32.734458311Z","created_by":"ubuntu","updated_at":"2026-02-20T15:16:16.675389872Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-11","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-2fpj","depends_on_id":"bd-1jmq","type":"blocks","created_at":"2026-02-20T07:43:24.419804592Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2g0","title":"[10.0] Implement economic trust layer.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.0 — Top 10 Initiative Tracking (Initiative #9)\nCross-references: 9A.9, 9B.9, 9C.9, 9D.9\n\nWhy This Exists:\nEconomic trust layer is the #9 strategic initiative. It quantifies attack-cost amplification and privilege-risk pricing so trust policy tuning is economically grounded instead of threshold folklore. This makes security policy decisions rigorous and defensible.\n\nTask Objective:\nBuild the economic trust layer that models attacker economics (cost of attack, cost of defense, expected damage) and uses these to inform policy thresholds, trust card risk tiers, and copilot recommendations with quantified rationale.\n\nDetailed Acceptance Criteria:\n1. Attack-cost amplification model: given extension/publisher profile, estimate cost to compromise.\n2. Privilege-risk pricing: quantify the risk exposure from granting specific capabilities to extensions.\n3. Policy threshold derivation: automatically derive recommended thresholds from economic model outputs.\n4. Decision-theoretic expected-loss and robust posterior updates for pricing and policy recommendations (9B.9).\n5. Posterior attacker ROI models maintained and calibration diagnostics published (9C.9).\n6. Model update and scoring hot paths optimized under heavy event streams (9D.9).\n7. Integration with trust cards for risk tier assignment and operator copilot for recommendation scoring.\n8. Dashboard surface showing attacker-ROI deltas and category-shift reporting (10.9).\n\nKey Dependencies:\n- Depends on trust cards (10.0.3) for extension risk data.\n- Depends on 10.5 (Security) for expected-loss action scoring framework.\n- Consumed by operator copilot (10.0.8) for action recommendation scoring.\n- Consumed by 10.21 (BPET) for evolution-risk scoring integration.\n\nExpected Artifacts:\n- src/security/economic_trust.rs — attack cost model, privilege pricing, threshold derivation.\n- src/security/attacker_roi.rs — posterior ROI models with calibration.\n- docs/specs/section_10_0/bd-2g0_contract.md\n- artifacts/section_10_0/bd-2g0/verification_evidence.json\n\nTesting and Logging Requirements:\n- Unit tests: cost model computation, risk pricing correctness, threshold derivation, posterior update determinism.\n- Integration tests: economic model integration with trust card data and copilot scoring.\n- E2E tests: end-to-end flow from extension risk data through economic model to policy recommendation.\n- Calibration tests: model predictions vs historical incident data (when available).\n- Structured logs: ATTACK_COST_COMPUTED, RISK_PRICED, THRESHOLD_DERIVED, POSTERIOR_UPDATED, CALIBRATION_DIAGNOSTIC with trace IDs.","acceptance_criteria":"1. Attack-cost amplification: system quantifies minimum attack cost for each privilege level; target >= 10x cost amplification vs. baseline (unauthenticated attacker baseline).\n2. Privilege-risk pricing: each privilege grant has an associated risk price (computed from: blast radius, historical exploit frequency, asset value); price visible in policy decisions.\n3. Trust policy tuning: economic model provides recommended policy threshold adjustments based on cost-benefit analysis; tuning suggestions include expected-loss delta and confidence interval.\n4. Risk pricing model is deterministic: same input state produces identical risk prices across runs; model parameters versioned and auditable.\n5. Economic signals consumed by operator safety copilot (bd-1nf): risk prices feed into expected-loss calculations for recommended actions.\n6. Integration with trust cards (bd-y4g): extension trust score incorporates economic risk price; low-trust extensions face higher privilege-risk prices (economic deterrent).\n7. Compromise reduction contribution: economic trust layer contributes to >= 10x compromise reduction target (Section 3) by making attacks economically irrational at policy-enforced thresholds.\n8. CLI queryable: `franken-node trust economics --extension <name> --format json` outputs risk price decomposition, attack-cost estimate, and policy threshold status.\n9. Model transparency: all pricing parameters, formulas, and calibration data documented and reproducible; no opaque ML models without explainability layer.\n10. Verification evidence includes: attack-cost amplification measurement for 3+ attack scenarios, privilege-risk price calculation test, policy tuning recommendation validation, economic model determinism test.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:36:43.124138307Z","created_by":"ubuntu","updated_at":"2026-02-20T15:36:33.858594641Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-0","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-2g0","depends_on_id":"bd-1nf","type":"blocks","created_at":"2026-02-20T07:43:10.436537951Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2g6r","title":"[10.15] Enforce Cx-first signature policy for control-plane async entrypoints.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.15 — Asupersync-First Integration Execution Track (8.4-8.6)\n\nWhy This Exists:\nAsupersync-first control-plane migration track enforcing Cx-first, region ownership, cancellation correctness, and protocol-grade verification gates.\n\nTask Objective:\nEnforce Cx-first signature policy for control-plane async entrypoints.\n\nAcceptance Criteria:\n- Lint/gate rejects new high-impact async APIs missing `&Cx`; existing exceptions are enumerated and time-bounded.\n\nExpected Artifacts:\n- `tools/lints/cx_first_policy.rs`, `tests/conformance/cx_first_api_gate.rs`, `artifacts/10.15/cx_first_compliance.csv`.\n\n- Machine-readable verification artifact at `artifacts/section_10_15/bd-2g6r/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_15/bd-2g6r/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.15] Enforce Cx-first signature policy for control-plane async entrypoints.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.15] Enforce Cx-first signature policy for control-plane async entrypoints.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.15] Enforce Cx-first signature policy for control-plane async entrypoints.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.15] Enforce Cx-first signature policy for control-plane async entrypoints.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.15] Enforce Cx-first signature policy for control-plane async entrypoints.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"- Lint/gate rejects new high-impact async APIs missing `&Cx`; existing exceptions are enumerated and time-bounded.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:36:59.645330297Z","created_by":"ubuntu","updated_at":"2026-02-20T17:00:43.271435995Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-15","self-contained","test-obligations"]}
{"id":"bd-2g8","title":"[PLAN 12] Risk Register Countermeasure Program","description":"Section 12 risk-control epic. Convert each enumerated risk (compat illusion, scope explosion, trust complexity, migration friction, perf regressions, federation privacy/poisoning, topology blind spots, false positives, temporal drift, trajectory privacy, camouflage attacks) into explicit validation and control gates.\n\n## Success Criteria\n- All child beads for this section are completed with acceptance artifacts attached and no unresolved blockers.\n- Section-level verification gate for comprehensive unit tests, integration/e2e workflows, and detailed structured logging evidence is green.\n- Scope-to-plan traceability is explicit, with no feature loss versus the canonical plan section.\n\n## Optimization Notes\n- User-Outcome Lens: \"[PLAN 12] Risk Register Countermeasure Program\" must improve real operator and end-user reliability, explainability, and time-to-safe-outcome under both normal and degraded conditions.\n- Cross-Section Coordination: this epic's child beads must explicitly encode integration expectations with neighboring sections to prevent local optimizations that break global behavior.\n- Verification Non-Negotiable: completion requires deterministic unit coverage, integration/E2E replay evidence, and structured logs sufficient for independent third-party reproduction and triage.\n- Scope Protection: any proposed simplification must prove feature parity against plan intent and document tradeoffs explicitly; silent scope reduction is forbidden.","status":"open","priority":1,"issue_type":"epic","created_at":"2026-02-20T07:36:42.092936537Z","created_by":"ubuntu","updated_at":"2026-02-20T08:12:47.650465713Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-12"],"dependencies":[{"issue_id":"bd-2g8","depends_on_id":"bd-13yn","type":"blocks","created_at":"2026-02-20T07:39:33.807060717Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2g8","depends_on_id":"bd-1n1t","type":"blocks","created_at":"2026-02-20T07:39:33.893841732Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2g8","depends_on_id":"bd-1nab","type":"blocks","created_at":"2026-02-20T07:39:33.716859451Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2g8","depends_on_id":"bd-1rff","type":"blocks","created_at":"2026-02-20T07:39:34.170254321Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2g8","depends_on_id":"bd-1ta","type":"blocks","created_at":"2026-02-20T07:38:35.059186428Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2g8","depends_on_id":"bd-1wz","type":"blocks","created_at":"2026-02-20T07:38:35.230482467Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2g8","depends_on_id":"bd-1xg","type":"blocks","created_at":"2026-02-20T07:38:34.858388615Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2g8","depends_on_id":"bd-20a","type":"blocks","created_at":"2026-02-20T07:38:34.902411025Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2g8","depends_on_id":"bd-229","type":"blocks","created_at":"2026-02-20T07:38:34.812920813Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2g8","depends_on_id":"bd-2w4u","type":"blocks","created_at":"2026-02-20T07:39:33.625969943Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2g8","depends_on_id":"bd-2x1e","type":"blocks","created_at":"2026-02-20T07:48:28.831969458Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2g8","depends_on_id":"bd-32p","type":"blocks","created_at":"2026-02-20T07:38:35.271740839Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2g8","depends_on_id":"bd-35m7","type":"blocks","created_at":"2026-02-20T07:39:34.259054778Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2g8","depends_on_id":"bd-37i","type":"blocks","created_at":"2026-02-20T07:38:35.404410229Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2g8","depends_on_id":"bd-38ri","type":"blocks","created_at":"2026-02-20T07:39:33.370620378Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2g8","depends_on_id":"bd-39a","type":"blocks","created_at":"2026-02-20T07:38:35.313287627Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2g8","depends_on_id":"bd-3il","type":"blocks","created_at":"2026-02-20T07:38:34.768066494Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2g8","depends_on_id":"bd-3jc1","type":"blocks","created_at":"2026-02-20T07:39:33.541241801Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2g8","depends_on_id":"bd-3qo","type":"blocks","created_at":"2026-02-20T07:38:35.146848574Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2g8","depends_on_id":"bd-3rc","type":"blocks","created_at":"2026-02-20T07:38:34.944491768Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2g8","depends_on_id":"bd-5rh","type":"blocks","created_at":"2026-02-20T07:38:35.102416181Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2g8","depends_on_id":"bd-c4f","type":"blocks","created_at":"2026-02-20T07:38:34.986878411Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2g8","depends_on_id":"bd-kiqr","type":"blocks","created_at":"2026-02-20T07:39:33.456215505Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2g8","depends_on_id":"bd-n71","type":"blocks","created_at":"2026-02-20T07:38:35.189081501Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2g8","depends_on_id":"bd-paui","type":"blocks","created_at":"2026-02-20T07:39:33.980269069Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2g8","depends_on_id":"bd-s4cu","type":"blocks","created_at":"2026-02-20T07:39:33.283669537Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2g8","depends_on_id":"bd-v4ps","type":"blocks","created_at":"2026-02-20T07:39:34.076647050Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2g8","depends_on_id":"bd-ybe","type":"blocks","created_at":"2026-02-20T07:38:35.358534437Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2gh","title":"[10.13] Implement connector lifecycle enum, transition table, and illegal-transition rejection tests.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.13 — FCP Deep-Mined Expansion Execution Track (9I)\n\nWhy This Exists:\nDeep connector/provider contract track: lifecycle FSMs, conformance harnesses, fencing, sandboxing, revocation freshness, and protocol hardening.\n\nTask Objective:\nImplement connector lifecycle enum, transition table, and illegal-transition rejection tests.\n\nAcceptance Criteria:\n- FSM is complete and deterministic for all states; illegal transitions return stable codes; full transition matrix tests pass.\n\nExpected Artifacts:\n- `docs/specs/connector_lifecycle.md`, `tests/conformance/connector_lifecycle_transitions.rs`, `artifacts/10.13/lifecycle_transition_matrix.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_13/bd-2gh/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_13/bd-2gh/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.13] Implement connector lifecycle enum, transition table, and illegal-transition rejection tests.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.13] Implement connector lifecycle enum, transition table, and illegal-transition rejection tests.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.13] Implement connector lifecycle enum, transition table, and illegal-transition rejection tests.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.13] Implement connector lifecycle enum, transition table, and illegal-transition rejection tests.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.13] Implement connector lifecycle enum, transition table, and illegal-transition rejection tests.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","status":"closed","priority":1,"issue_type":"task","assignee":"CrimsonCrane","created_at":"2026-02-20T07:36:51.393382090Z","created_by":"ubuntu","updated_at":"2026-02-20T10:32:12.375932254Z","closed_at":"2026-02-20T10:32:12.375907738Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-13","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-2gh","depends_on_id":"bd-1ow","type":"blocks","created_at":"2026-02-20T07:46:32.585807024Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2gh","depends_on_id":"bd-cda","type":"blocks","created_at":"2026-02-20T07:46:32.648320507Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2gr","title":"[10.11] Integrate canonical monotonic security epochs and transition barriers (from `10.14`) across product services.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md — Section 10.11 (FrankenSQLite-Inspired Runtime Systems), cross-ref Section 9G.6, 9J.12, 9J.13\n\n## Why This Exists\n\nTrust artifacts in franken_node (signed manifests, capability grants, quarantine verdicts, migration tickets) are only valid within the security epoch in which they were created. When the system transitions to a new epoch — triggered by key rotation, trust-root update, or administrative policy change — all in-flight artifacts from the prior epoch must be re-evaluated or rejected, and all participating services must reach quiescence before the new epoch's key material becomes active. Enhancement Map 9G.6 mandates epoch-scoped validity windows and key derivation for trust transitions. 9J.12 requires that validity windows be strictly epoch-scoped, and 9J.13 requires transition barriers with participant quiescence to prevent split-brain scenarios where some services operate under the old epoch while others have moved to the new one.\n\nThis bead integrates the canonical epoch primitives from 10.14 (bd-3hdv monotonic epoch definition, bd-3cs3 epoch-scoped key derivation, bd-2xv8 fail-closed validity check, bd-2wsm transition barrier protocol, bd-1vsr abort-on-timeout) and the epoch integration from 10.15 (bd-181w validity windows, bd-1hbw transition barriers) into franken_node's product service layer, ensuring that every trust-sensitive operation checks the current epoch, every trust artifact is tagged with its creation epoch, and epoch transitions are coordinated across all product services.\n\n## What This Must Do\n\n1. Implement an `EpochGuard` middleware that wraps every trust-sensitive product API, rejecting requests that present artifacts from a non-current epoch with a structured `EpochMismatch` error.\n2. Integrate epoch-scoped key derivation (from bd-3cs3): all trust artifact signing and verification uses keys derived from the current epoch, ensuring artifacts cannot be forged across epoch boundaries.\n3. Implement the epoch transition barrier at the product layer: when a transition is initiated, all product services enter a drain phase (no new trust operations accepted), complete in-flight operations, and signal quiescence before the new epoch activates.\n4. Enforce fail-closed semantics (from bd-2xv8): if epoch validation is indeterminate (e.g., epoch service unreachable), the product service rejects the operation rather than proceeding optimistically.\n5. Implement transition abort on timeout (from bd-1vsr): if quiescence is not achieved within a configurable deadline, the transition is aborted and the system remains on the current epoch, with a structured alert emitted.\n6. Tag every trust artifact created by product services with its creation epoch, stored as an immutable field in the artifact metadata.\n\n## Context from Enhancement Maps\n\n- 9G.6: \"Epoch-scoped validity windows and key derivation for trust transitions\"\n- 9J.12: \"Epoch-scoped validity windows for trust artifacts\"\n- 9J.13: \"Epoch transition barriers with participant quiescence\"\n- Architecture invariant #7 (8.5): Epoch barriers — epoch transitions must be coordinated across all services.\n- Architecture invariant #8 (8.5): Evidence-by-default — every epoch transition must produce auditable evidence.\n- Architecture invariant #3 (8.5): Cancellation protocol semantics — epoch drain phase must honor cancel signals for non-critical operations.\n\n## Dependencies\n\n- Upstream: bd-3hdv (10.14 monotonic epoch definition), bd-3cs3 (10.14 epoch-scoped key derivation), bd-2xv8 (10.14 fail-closed validity check), bd-2wsm (10.14 transition barrier protocol), bd-1vsr (10.14 transition abort semantics), bd-181w (10.15 epoch validity windows), bd-1hbw (10.15 epoch transition barriers)\n- Downstream: bd-390 (anti-entropy reconciliation checks epoch validity), bd-3hw (saga aborts on epoch change), bd-3vm (ambient-authority audit includes epoch context), bd-1jpo (10.11 section-wide verification gate)\n\n## Acceptance Criteria\n\n1. An artifact presented with epoch E when the current epoch is E+1 is rejected with `EpochMismatch` error containing both the artifact epoch and the current epoch.\n2. An artifact presented with a future epoch (E+2 when current is E) is rejected with `FutureEpochRejected` error (fail-closed).\n3. Key derivation produces distinct keys for distinct epochs; a signature created with epoch E's key fails verification under epoch E+1's key.\n4. Epoch transition barrier achieves quiescence across all registered product services within the configured timeout, verified by integration test with 5 simulated services.\n5. Transition abort fires if any service fails to reach quiescence within the deadline; the system remains on the prior epoch and emits a structured `EpochTransitionAborted` alert.\n6. Every trust artifact created by product services includes an immutable `creation_epoch` field that cannot be modified after creation.\n7. Fail-closed: when the epoch service is unreachable, all trust-sensitive operations return `EpochUnavailable` error within 100ms (no hanging).\n8. Verification evidence JSON includes epoch_transitions_attempted, epoch_transitions_completed, epoch_transitions_aborted, artifacts_rejected_stale_epoch, and quiescence_latency_ms fields.\n\n## Testing & Logging Requirements\n\n- Unit tests: (a) EpochGuard rejects stale-epoch artifacts; (b) EpochGuard rejects future-epoch artifacts; (c) Key derivation determinism within same epoch; (d) Key derivation distinctness across epochs; (e) Artifact creation epoch immutability.\n- Integration tests: (a) Full epoch transition lifecycle across 3 simulated product services; (b) Transition abort on timeout with one slow service; (c) Concurrent trust operations during epoch drain phase are properly queued/rejected; (d) Anti-entropy reconciliation (bd-390) correctly rejects cross-epoch records.\n- Adversarial tests: (a) Rapid epoch transitions (3 transitions in 1 second) — verify no split-brain; (b) Epoch service crash during transition — verify abort and rollback; (c) Forged artifact with manipulated epoch tag — verify signature validation catches it; (d) Service that never signals quiescence — verify timeout and abort.\n- Structured logs: Events use stable codes (FN-EP-001 through FN-EP-012), include `epoch_current`, `epoch_artifact`, `transition_id`, `trace_id`, `quiescence_status`. JSON-formatted.\n\n## Expected Artifacts\n\n- docs/specs/section_10_11/bd-2gr_contract.md\n- crates/franken-node/src/runtime/epoch_guard.rs (or equivalent module path)\n- crates/franken-node/src/runtime/epoch_transition.rs (barrier + quiescence coordination)\n- scripts/check_epoch_integration.py (with --json flag and self_test())\n- tests/test_check_epoch_integration.py\n- artifacts/section_10_11/bd-2gr/verification_evidence.json\n- artifacts/section_10_11/bd-2gr/verification_summary.md","acceptance_criteria":"AC for bd-2gr:\n1. Integrate the canonical monotonic security epoch model from 10.14: every product service reads the current epoch_id from the shared manifest state and binds all operations to that epoch; epoch_id is a monotonically increasing u64 that never decreases.\n2. An EpochBarrier mechanism gates epoch transitions: before epoch N+1 activates, all in-flight operations bound to epoch N must complete or be cancelled via the drain protocol (bd-7om); the barrier blocks new-epoch operations until drain is confirmed.\n3. Operations that arrive with a stale epoch_id (< current) are rejected with STALE_EPOCH_REJECTED error code and a structured log event; operations must not silently proceed under an old epoch.\n4. The epoch transition sequence is: (a) propose new epoch, (b) broadcast drain signal for current epoch, (c) await drain confirmation from all services, (d) atomically advance epoch_id, (e) unblock new-epoch operations.\n5. A split-brain guard prevents two services from simultaneously believing they are in different epochs: the epoch_id is sourced from a single authoritative store, and services poll/subscribe with bounded staleness (configurable max_epoch_lag, default: 1).\n6. Epoch metadata (transition timestamp, reason, initiator) is recorded in an append-only epoch history log for audit purposes.\n7. Unit tests verify: (a) monotonicity invariant (epoch never decreases), (b) stale-epoch operations are rejected, (c) barrier blocks new-epoch work until drain completes, (d) concurrent epoch advance attempts are serialized (only one wins), (e) epoch history log records all transitions.\n8. Integration test: three simulated services coordinate an epoch transition with one slow-draining service, verifying barrier wait and eventual successful transition.\n9. Structured log events: EPOCH_PROPOSED / EPOCH_DRAIN_REQUESTED / EPOCH_DRAIN_CONFIRMED / EPOCH_ADVANCED / STALE_EPOCH_REJECTED with epoch_id, service_id, and transition_reason.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:36:50.469186589Z","created_by":"ubuntu","updated_at":"2026-02-20T17:01:40.227407504Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-11","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-2gr","depends_on_id":"bd-2wsm","type":"blocks","created_at":"2026-02-20T15:00:18.464825451Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2gr","depends_on_id":"bd-3hdv","type":"blocks","created_at":"2026-02-20T15:00:18.283439361Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2h2s","title":"[10.15] Add migration plan for existing non-asupersync control surfaces with scope burn-down tracking.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.15 — Asupersync-First Integration Execution Track (8.4-8.6)\n\nWhy This Exists:\nAsupersync-first control-plane migration track enforcing Cx-first, region ownership, cancellation correctness, and protocol-grade verification gates.\n\nTask Objective:\nAdd migration plan for existing non-asupersync control surfaces with scope burn-down tracking.\n\nAcceptance Criteria:\n- Legacy control paths are inventoried with migration status and closure criteria; remaining exceptions are explicitly justified.\n\nExpected Artifacts:\n- `docs/migration/asupersync_control_surface_migration.md`, `artifacts/10.15/control_surface_burndown.csv`.\n\n- Machine-readable verification artifact at `artifacts/section_10_15/bd-2h2s/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_15/bd-2h2s/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.15] Add migration plan for existing non-asupersync control surfaces with scope burn-down tracking.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.15] Add migration plan for existing non-asupersync control surfaces with scope burn-down tracking.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.15] Add migration plan for existing non-asupersync control surfaces with scope burn-down tracking.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.15] Add migration plan for existing non-asupersync control surfaces with scope burn-down tracking.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.15] Add migration plan for existing non-asupersync control surfaces with scope burn-down tracking.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"- Legacy control paths are inventoried with migration status and closure criteria; remaining exceptions are explicitly justified.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:37:01.282248904Z","created_by":"ubuntu","updated_at":"2026-02-20T17:04:44.244893562Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-15","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-2h2s","depends_on_id":"bd-2177","type":"blocks","created_at":"2026-02-20T17:04:44.244841625Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2hjg","title":"[10.18] Section-wide verification gate: comprehensive unit+e2e+logging","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.18\n\nTask Objective:\nCreate a hard section completion gate that verifies all implemented behavior in this section with comprehensive unit tests, integration/e2e scripts, and detailed structured logging evidence.\n\nAcceptance Criteria:\n- Section test matrix covers happy-path, edge-case, and adversarial/error-path scenarios for all section deliverables.\n- E2E scripts execute representative end-user workflows and policy/control-plane transitions for this section.\n- Structured logs include stable event/error codes, trace correlation IDs, and enough context for deterministic replay triage.\n- Verification report is deterministic and machine-readable for CI/release gating.\n\nExpected Artifacts:\n- Section-level test matrix and coverage summary artifact.\n- E2E script suite with pass/fail outputs and reproducible fixtures/seeds.\n- Structured log validation report and traceability bundle.\n- Gate verdict artifact consumable by release automation.\n\n- Machine-readable verification artifact at `artifacts/section_10_18/bd-2hjg/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_18/bd-2hjg/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests must validate contracts, invariants, and failure semantics.\n- E2E tests must validate cross-component behavior from user/operator entrypoints to persisted effects.\n- Logging must support root-cause analysis without hidden context requirements.\n\nTask-Specific Clarification:\n- For \"[10.18] Section-wide verification gate: comprehensive unit+e2e+logging\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.18] Section-wide verification gate: comprehensive unit+e2e+logging\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.18] Section-wide verification gate: comprehensive unit+e2e+logging\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.18] Section-wide verification gate: comprehensive unit+e2e+logging\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.18] Section-wide verification gate: comprehensive unit+e2e+logging\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:48:17.956638503Z","created_by":"ubuntu","updated_at":"2026-02-20T08:43:51.194721206Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-18","test-obligations","verification-gate"],"dependencies":[{"issue_id":"bd-2hjg","depends_on_id":"bd-16fq","type":"blocks","created_at":"2026-02-20T07:48:18.683822682Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2hjg","depends_on_id":"bd-1dpd","type":"blocks","created_at":"2026-02-20T08:24:26.013679638Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2hjg","depends_on_id":"bd-1o4v","type":"blocks","created_at":"2026-02-20T07:48:18.441228177Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2hjg","depends_on_id":"bd-1u8m","type":"blocks","created_at":"2026-02-20T07:48:18.489261712Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2hjg","depends_on_id":"bd-28u0","type":"blocks","created_at":"2026-02-20T07:48:18.537473529Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2hjg","depends_on_id":"bd-2twu","type":"blocks","created_at":"2026-02-20T08:20:51.790676075Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2hjg","depends_on_id":"bd-3g4k","type":"blocks","created_at":"2026-02-20T07:48:18.585332549Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2hjg","depends_on_id":"bd-3go4","type":"blocks","created_at":"2026-02-20T07:48:18.205857867Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2hjg","depends_on_id":"bd-3lzk","type":"blocks","created_at":"2026-02-20T07:48:18.060944819Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2hjg","depends_on_id":"bd-3pds","type":"blocks","created_at":"2026-02-20T07:48:18.253250879Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2hjg","depends_on_id":"bd-3ptu","type":"blocks","created_at":"2026-02-20T07:48:18.158148506Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2hjg","depends_on_id":"bd-4jh9","type":"blocks","created_at":"2026-02-20T07:48:18.304202149Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2hjg","depends_on_id":"bd-8qlj","type":"blocks","created_at":"2026-02-20T07:48:18.377396307Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2hjg","depends_on_id":"bd-p73r","type":"blocks","created_at":"2026-02-20T07:48:18.634851360Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2hjg","depends_on_id":"bd-ufk5","type":"blocks","created_at":"2026-02-20T07:48:18.109688547Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2hrg","title":"[3.2] Impossible-by-Default Capability Index — 10 mandatory category differentiators","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md — Section 3.2\n\n## Why This Exists\nThis is the CORE differentiator list. These 10 capabilities define what franken_node can do that incumbents (Node.js, Bun) CANNOT do by default. Every one of these must be productionized. If users can get the same outcomes with a thin wrapper around Node/Bun defaults, the feature is insufficient. If claims cannot be independently verified, the feature is insufficient.\n\n## The 10 Impossible-by-Default Capabilities (Non-Negotiable)\n\n1. **Policy-visible compatibility behavior with explicit divergence receipts.** Owner: 10.2 + 10.5. Every compatibility shim is typed, auditable, and policy-gated.\n2. **One-command migration audit and risk map for Node/Bun projects.** Owner: 10.3 + 10.12. Zero-to-first-safe-run pipeline with rollout guidance.\n3. **Signed policy checkpoints and revocation-aware execution gates.** Owner: 10.13 + 10.10. Revocation freshness semantics per safety tier with degraded-mode policy.\n4. **Deterministic incident replay with counterfactual policy simulation.** Owner: 10.5 + 10.17. Full-fidelity deterministic capture/replay for extension-host execution.\n5. **Fleet quarantine propagation with bounded convergence guarantees.** Owner: 10.8 + 10.20. Global scope, blast-radius views, convergence indicators, rollback controls.\n6. **Extension trust cards combining provenance, behavior, and revocation state.** Owner: 10.4 + 10.21. Single explainable trust model for humans and automation.\n7. **Compatibility lockstep oracle across Node/Bun/franken_node.** Owner: 10.2 (L1) + 10.17 (L2). Dual-layer oracle for external behavior and runtime integrity.\n8. **Control-plane recommended actions with expected-loss rationale.** Owner: 10.5 + 10.17. VOI-based ranking, confidence context, deterministic rollback commands.\n9. **Ecosystem reputation graph with explainable trust transitions.** Owner: 10.4 + 10.19 + 10.21. Bayesian adversary graph, publisher reputation, federated intelligence.\n10. **Public verifier toolkit for benchmark and security claims.** Owner: 10.17 + 10.14. Universal verifier SDK, replay capsules, claim compiler, public trust scoreboard.\n\n## Category-Creation Test\n- If users can get the same outcomes with a thin wrapper around Node/Bun defaults, the feature is insufficient.\n- If claims cannot be independently verified, the feature is insufficient.\n- If migration cost remains high for real teams, the feature is insufficient.\n\n## Quantitative Targets (from Section 3)\n- >= 95% pass on targeted compatibility corpus for high-value Node/Bun usage bands\n- >= 3x migration throughput and confidence quality versus baseline\n- >= 10x reduction in successful host compromise under adversarial extension campaigns\n- 100% deterministic replay artifact availability for high-severity incidents\n- >= 3 impossible-by-default capabilities broadly adopted by production users\n\n## Acceptance Criteria\n- All 10 capabilities are productionized with verifiable evidence artifacts\n- Each capability maps to at least one \"impossible without franken_node\" demo scenario\n- Independent external verification of at least 5/10 capabilities\n- Public documentation showing how each capability surpasses incumbent defaults\n\n\n## Success Criteria\n- Each of the 10 impossible-by-default capabilities remains mapped to active implementation and verification beads with no unmapped capability.\n- Capability-level evidence expectations are explicit, reproducible, and externally verifiable for independent evaluators.\n- Downstream planning decisions preserve all capability semantics without feature compression or silent scope erosion.\n\n## Testing & Logging Requirements\n- Unit tests for capability-mapping validators and completeness checks across the 10-item index.\n- E2E capability-audit scripts that walk implementation beads, verify dependency coverage, and emit deterministic pass/fail evidence.\n- Structured logs for each capability-audit run, including missing mappings, stale references, and remediation actions.","status":"open","priority":0,"issue_type":"epic","created_at":"2026-02-20T16:13:40.354320581Z","created_by":"ubuntu","updated_at":"2026-02-20T17:08:27.608702692Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["capability-index","category-defining","plan","section-3"],"dependencies":[{"issue_id":"bd-2hrg","depends_on_id":"bd-3hyk","type":"blocks","created_at":"2026-02-20T16:17:12.686609541Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2hs","title":"[10.2] Create the four-doc spec pack for compatibility extraction (`PLAN_TO_PORT_NODE_BUN_SURFACES_TO_RUST.md`, `EXISTING_NODE_BUN_STRUCTURE.md`, `PROPOSED_ARCHITECTURE.md`, `FEATURE_PARITY.md`) and keep it release-gated.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.2 — Compatibility Core\n\nWhy This Exists:\nCompatibility core as strategic wedge: policy-visible behavior, divergence receipts, L1/L2 lockstep, and spec-first fixture governance.\n\nTask Objective:\nCreate the four-doc spec pack for compatibility extraction (`PLAN_TO_PORT_NODE_BUN_SURFACES_TO_RUST.md`, `EXISTING_NODE_BUN_STRUCTURE.md`, `PROPOSED_ARCHITECTURE.md`, `FEATURE_PARITY.md`) and keep it release-gated.\n\nAcceptance Criteria:\n- Implement with explicit success/failure invariants and deterministic behavior under normal, edge, and fault scenarios.\n- Ensure outcomes are verifiable via automated tests and reproducible artifact outputs.\n\nExpected Artifacts:\n- Section-owned design/spec artifact at `docs/specs/section_10_2/bd-2hs_contract.md`, capturing decision rationale, invariants, and interface boundaries.\n- Machine-readable verification artifact at `artifacts/section_10_2/bd-2hs/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_2/bd-2hs/verification_summary.md` linking implementation intent to measured outcomes.\n\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.2] Create the four-doc spec pack for compatibility extraction (`PLAN_TO_PORT_NODE_BUN_SURFACES_TO_RUST.md`, `EXISTING_NODE_BUN_STRUCTURE.md`, `PROPOSED_ARCHITECTURE.md`, `FEATURE_PARITY.md`) and keep it release-gated.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.2] Create the four-doc spec pack for compatibility extraction (`PLAN_TO_PORT_NODE_BUN_SURFACES_TO_RUST.md`, `EXISTING_NODE_BUN_STRUCTURE.md`, `PROPOSED_ARCHITECTURE.md`, `FEATURE_PARITY.md`) and keep it release-gated.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.2] Create the four-doc spec pack for compatibility extraction (`PLAN_TO_PORT_NODE_BUN_SURFACES_TO_RUST.md`, `EXISTING_NODE_BUN_STRUCTURE.md`, `PROPOSED_ARCHITECTURE.md`, `FEATURE_PARITY.md`) and keep it release-gated.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.2] Create the four-doc spec pack for compatibility extraction (`PLAN_TO_PORT_NODE_BUN_SURFACES_TO_RUST.md`, `EXISTING_NODE_BUN_STRUCTURE.md`, `PROPOSED_ARCHITECTURE.md`, `FEATURE_PARITY.md`) and keep it release-gated.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.2] Create the four-doc spec pack for compatibility extraction (`PLAN_TO_PORT_NODE_BUN_SURFACES_TO_RUST.md`, `EXISTING_NODE_BUN_STRUCTURE.md`, `PROPOSED_ARCHITECTURE.md`, `FEATURE_PARITY.md`) and keep it release-gated.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","status":"closed","priority":1,"issue_type":"task","assignee":"CrimsonCrane","created_at":"2026-02-20T07:36:44.562237978Z","created_by":"ubuntu","updated_at":"2026-02-20T09:52:58.480235289Z","closed_at":"2026-02-20T09:52:58.480209722Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-2","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-2hs","depends_on_id":"bd-240","type":"blocks","created_at":"2026-02-20T07:43:20.432482358Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2igi","title":"[10.14] Implement Bayesian posterior diagnostics for explainable policy ranking.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.14 — FrankenSQLite Deep-Mined Expansion Execution Track (9J)\n\nWhy This Exists:\nFrankenSQLite deep-mined control integrity track: evidence ledgers, proofs, epochs, remote effects, markers/MMR, and deterministic fault labs.\n\nTask Objective:\nImplement Bayesian posterior diagnostics for explainable policy ranking.\n\nAcceptance Criteria:\n- Posterior metrics are surfaced for ranking diagnostics; diagnostics do not bypass hard guardrails; posterior updates are reproducible from stored observations.\n\nExpected Artifacts:\n- `src/policy/bayesian_diagnostics.rs`, `tests/integration/bayesian_policy_ranking.rs`, `artifacts/10.14/posterior_diagnostics_report.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_14/bd-2igi/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_14/bd-2igi/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.14] Implement Bayesian posterior diagnostics for explainable policy ranking.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.14] Implement Bayesian posterior diagnostics for explainable policy ranking.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.14] Implement Bayesian posterior diagnostics for explainable policy ranking.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.14] Implement Bayesian posterior diagnostics for explainable policy ranking.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.14] Implement Bayesian posterior diagnostics for explainable policy ranking.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"Posterior metrics are surfaced for ranking diagnostics; diagnostics do not bypass hard guardrails; posterior updates are reproducible from stored observations.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:36:55.792973375Z","created_by":"ubuntu","updated_at":"2026-02-20T16:23:28.311437152Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-14","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-2igi","depends_on_id":"bd-nupr","type":"blocks","created_at":"2026-02-20T16:23:28.311381107Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2iyk","title":"[10.17] Implement information-flow lineage and exfiltration sentinel.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.17 — Radical Expansion Execution Track (9K)\n\nWhy This Exists:\nRadical expansion track for proof-carrying speculation, Bayesian adversary control, time-travel replay, ZK attestations, and claim compiler systems.\n\nTask Objective:\nImplement information-flow lineage and exfiltration sentinel.\n\nAcceptance Criteria:\n- Sensitive lineage tags persist across supported execution flows; simulated covert exfiltration scenarios are detected and auto-contained above defined recall/precision thresholds.\n\nExpected Artifacts:\n- `docs/specs/information_flow_sentinel.md`, `src/security/lineage_tracker.rs`, `tests/security/exfiltration_sentinel_scenarios.rs`, `artifacts/10.17/exfiltration_detector_metrics.csv`.\n\n- Machine-readable verification artifact at `artifacts/section_10_17/bd-2iyk/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_17/bd-2iyk/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.17] Implement information-flow lineage and exfiltration sentinel.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.17] Implement information-flow lineage and exfiltration sentinel.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.17] Implement information-flow lineage and exfiltration sentinel.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.17] Implement information-flow lineage and exfiltration sentinel.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.17] Implement information-flow lineage and exfiltration sentinel.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"- Sensitive lineage tags persist across supported execution flows; simulated covert exfiltration scenarios are detected and auto-contained above defined recall/precision thresholds.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:37:03.761723914Z","created_by":"ubuntu","updated_at":"2026-02-20T15:45:58.844019725Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-17","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-2iyk","depends_on_id":"bd-3l2p","type":"blocks","created_at":"2026-02-20T07:43:18.686029611Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2j9w","title":"[PROGRAM] Program-wide verification gate: comprehensive unit+e2e+logging","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md (Cross-cutting final verification layer)\nSection: PROGRAM (Cross-cutting final verification)\n\nTask Objective:\nCreate a hard program-wide verification gate that requires section-level gates, bootstrap gate, and program-level integration evidence to all be green before master-plan closure.\n\nWhy This Improves User Outcomes:\nThis enforces that local correctness and global integration both hold, preventing release of systems that are individually valid but compositionally unsafe.\n\nAcceptance Criteria:\n- Gate consumes all section verification gate artifacts, bootstrap verification gate output, and program-level orchestration evidence.\n- Gate fails closed on missing, stale, nondeterministic, or contradictory evidence.\n- Verdict is deterministic and machine-readable for CI/release automation.\n- Failure output pinpoints failing section/integration dimension and required remediation bead families.\n\nExpected Artifacts:\n- Program-wide gate policy/spec with evidence contract.\n- Deterministic gate verdict schema and sample pass/fail bundles.\n- Traceability report linking every section/program verification obligation to consumed evidence.\n\n- Machine-readable verification artifact at `artifacts/section_program/bd-2j9w/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_program/bd-2j9w/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for gate aggregation logic, conflict resolution, and fail-closed semantics.\n- E2E tests for green, partial, contradictory, and failing evidence scenarios.\n- Detailed structured gate logs with explicit failing-dimension tags, evidence IDs, and trace-correlation IDs.\n\nTask-Specific Clarification:\n- This gate is additive and must not weaken or bypass any existing section/bootstrap verification gate.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T08:08:05.794261594Z","created_by":"ubuntu","updated_at":"2026-02-20T08:46:07.992035368Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","program-integration","test-obligations","verification-gate"],"dependencies":[{"issue_id":"bd-2j9w","depends_on_id":"bd-10g0","type":"blocks","created_at":"2026-02-20T08:08:09.391224571Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2j9w","depends_on_id":"bd-16sk","type":"blocks","created_at":"2026-02-20T08:08:10.467877163Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2j9w","depends_on_id":"bd-1d6x","type":"blocks","created_at":"2026-02-20T08:08:10.014682301Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2j9w","depends_on_id":"bd-1dpd","type":"blocks","created_at":"2026-02-20T08:24:23.341844703Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2j9w","depends_on_id":"bd-1fi2","type":"blocks","created_at":"2026-02-20T08:08:07.518405352Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2j9w","depends_on_id":"bd-1jjq","type":"blocks","created_at":"2026-02-20T08:08:10.319492344Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2j9w","depends_on_id":"bd-1jpo","type":"blocks","created_at":"2026-02-20T08:08:10.164900284Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2j9w","depends_on_id":"bd-1kfq","type":"blocks","created_at":"2026-02-20T08:08:07.369263845Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2j9w","depends_on_id":"bd-1koz","type":"blocks","created_at":"2026-02-20T08:08:07.965907530Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2j9w","depends_on_id":"bd-1neb","type":"blocks","created_at":"2026-02-20T08:08:07.220572606Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2j9w","depends_on_id":"bd-1rwq","type":"blocks","created_at":"2026-02-20T08:08:07.666540506Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2j9w","depends_on_id":"bd-20eg","type":"blocks","created_at":"2026-02-20T08:08:09.542574211Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2j9w","depends_on_id":"bd-23ys","type":"blocks","created_at":"2026-02-20T08:08:08.768121611Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2j9w","depends_on_id":"bd-261k","type":"blocks","created_at":"2026-02-20T08:08:08.167768015Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2j9w","depends_on_id":"bd-2hjg","type":"blocks","created_at":"2026-02-20T08:08:09.072856965Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2j9w","depends_on_id":"bd-2l4i","type":"blocks","created_at":"2026-02-20T08:08:06.619991807Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2j9w","depends_on_id":"bd-2nlu","type":"blocks","created_at":"2026-02-20T08:08:06.091914616Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2j9w","depends_on_id":"bd-2nre","type":"blocks","created_at":"2026-02-20T08:08:06.454722208Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2j9w","depends_on_id":"bd-2twu","type":"blocks","created_at":"2026-02-20T08:20:48.639817526Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2j9w","depends_on_id":"bd-2x1e","type":"blocks","created_at":"2026-02-20T08:08:06.923504764Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2j9w","depends_on_id":"bd-3enl","type":"blocks","created_at":"2026-02-20T08:08:08.317360262Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2j9w","depends_on_id":"bd-3epz","type":"blocks","created_at":"2026-02-20T08:08:09.697439991Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2j9w","depends_on_id":"bd-3hr2","type":"blocks","created_at":"2026-02-20T08:08:08.921923609Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2j9w","depends_on_id":"bd-3ohj","type":"blocks","created_at":"2026-02-20T08:08:10.780524784Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2j9w","depends_on_id":"bd-3p9n","type":"blocks","created_at":"2026-02-20T08:08:07.816820113Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2j9w","depends_on_id":"bd-3po7","type":"blocks","created_at":"2026-02-20T08:08:08.619979364Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2j9w","depends_on_id":"bd-3qsp","type":"blocks","created_at":"2026-02-20T08:08:10.630467972Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2j9w","depends_on_id":"bd-3t08","type":"blocks","created_at":"2026-02-20T08:08:09.231159707Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2j9w","depends_on_id":"bd-3uoo","type":"blocks","created_at":"2026-02-20T08:08:09.861617386Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2j9w","depends_on_id":"bd-c781","type":"blocks","created_at":"2026-02-20T08:08:07.071651649Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2j9w","depends_on_id":"bd-unkm","type":"blocks","created_at":"2026-02-20T08:08:06.306728528Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2j9w","depends_on_id":"bd-z7bt","type":"blocks","created_at":"2026-02-20T08:08:06.773912857Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2j9w","depends_on_id":"bd-zm5b","type":"blocks","created_at":"2026-02-20T08:08:08.466281760Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2ja","title":"[10.7] Build compatibility golden corpus and fixture metadata schema.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.7 — Conformance + Verification (Item 1 of 6)\n\nWhy This Exists:\nThe compatibility golden corpus is the authoritative reference set of test fixtures that define expected behavior for Node/Bun API compatibility. It serves as the ground truth for the lockstep oracle (10.0 Initiative #4), the migration autopilot (10.0 Initiative #2), and all conformance claims.\n\nTask Objective:\nBuild and maintain the compatibility golden corpus: a comprehensive, version-controlled collection of test fixtures covering all API bands, with structured metadata enabling automated comparison across runtimes.\n\nDetailed Acceptance Criteria:\n1. Corpus covers all four compatibility bands (core/high-value/edge/unsafe) with representative fixtures per API family.\n2. Fixture metadata schema includes: API surface, band classification, expected behavior description, Node.js reference version, Bun reference version, fixture inputs, expected outputs, edge cases, known divergences.\n3. Fixtures are deterministic and reproducible across environments (pinned dependencies, controlled randomness).\n4. Corpus version-controlled and release-gated — new fixtures require review and linkage to spec sections.\n5. Integration with lockstep runner (10.2) for automated three-runtime comparison.\n6. Spec-first governance: fixtures cite spec sections and fixture IDs per 10.1 governance policy.\n7. Prioritized by API family importance: CLI/process/fs/network/module/tooling bands per 10.2.\n\nExpected Artifacts:\n- fixtures/ directory with organized corpus per API band.\n- Fixture metadata schema (JSON) and validation tooling.\n- docs/specs/section_10_7/bd-2ja_contract.md\n- artifacts/section_10_7/bd-2ja/verification_evidence.json\n\nTesting and Logging Requirements:\n- Unit tests: fixture metadata validation, band classification correctness.\n- Integration tests: full corpus execution against each runtime producing comparison reports.\n- E2E tests: franken-node verify lockstep consuming corpus and producing divergence reports.\n- Structured logs: CORPUS_LOADED, FIXTURE_EXECUTED, COMPARISON_COMPLETED, DIVERGENCE_FOUND with fixture IDs and band metadata.","acceptance_criteria":"1. Golden corpus contains at least 50 representative compatibility test fixtures covering: module formats (CJS, ESM, dual), API shims, extension hooks, and edge cases.\n2. Fixture metadata schema is defined in a JSON Schema file under spec/ and every fixture includes a conforming metadata sidecar.\n3. Metadata fields include: fixture ID, category, expected-output hash, compatibility dimensions tested, and provenance (which spec section it validates).\n4. Corpus is versioned: schema changes bump a semver version and old fixtures are migrated or explicitly deprecated.\n5. A validation script (scripts/validate_corpus.py) checks all fixtures against the schema and reports any violations as structured JSON.\n6. Per Section 5.4 porting discipline: each fixture's provenance traces back to a specific extraction-and-proof chain.\n7. Corpus is usable by both the lockstep oracle (Rust) and the external verifier toolkit (Section 3.2 capability #10) without format conversion.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:36:47.265087454Z","created_by":"ubuntu","updated_at":"2026-02-20T15:16:36.389538959Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-7","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-2ja","depends_on_id":"bd-1ta","type":"blocks","created_at":"2026-02-20T07:46:37.081710626Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2ja","depends_on_id":"bd-229","type":"blocks","created_at":"2026-02-20T07:46:37.126480Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2ja","depends_on_id":"bd-3il","type":"blocks","created_at":"2026-02-20T07:46:37.171136664Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2ja","depends_on_id":"bd-5rh","type":"blocks","created_at":"2026-02-20T07:46:37.215490705Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2ja","depends_on_id":"bd-cda","type":"blocks","created_at":"2026-02-20T07:46:37.260186052Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2ji2","title":"[10.16] Add claim-language gate tying UI/service/storage claims to substrate-backed evidence.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.16 — Adjacent Substrate Integration Execution Track (8.7)\n\nWhy This Exists:\nAdjacent substrate integration track to enforce deep composition with frankentui, frankensqlite, sqlmodel_rust, and fastapi_rust.\n\nTask Objective:\nAdd claim-language gate tying UI/service/storage claims to substrate-backed evidence.\n\nAcceptance Criteria:\n- Documentation and release claims about TUI/API/storage behavior require linked substrate conformance artifacts; unlinked claims are blocked.\n\nExpected Artifacts:\n- `docs/policy/adjacent_substrate_claim_language.md`, `tests/conformance/adjacent_claim_language_gate.rs`, `artifacts/10.16/adjacent_claim_language_gate_report.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_16/bd-2ji2/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_16/bd-2ji2/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.16] Add claim-language gate tying UI/service/storage claims to substrate-backed evidence.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.16] Add claim-language gate tying UI/service/storage claims to substrate-backed evidence.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.16] Add claim-language gate tying UI/service/storage claims to substrate-backed evidence.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.16] Add claim-language gate tying UI/service/storage claims to substrate-backed evidence.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.16] Add claim-language gate tying UI/service/storage claims to substrate-backed evidence.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"- Documentation and release claims about TUI/API/storage behavior require linked substrate conformance artifacts; unlinked claims are blocked.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:37:02.845922819Z","created_by":"ubuntu","updated_at":"2026-02-20T17:00:55.717458936Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-16","self-contained","test-obligations"]}
{"id":"bd-2jns","title":"[10.20] Implement maintainer/publisher fragility model and single-point-of-failure detector.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.20 — Dependency Graph Immune System Execution Track (9N)\n\nWhy This Exists:\nDGIS topological immune system track for dependency contagion modeling, choke-point immunization, and graph-aware containment economics.\n\nTask Objective:\nImplement maintainer/publisher fragility model and single-point-of-failure detector.\n\nAcceptance Criteria:\n- Graph nodes with concentrated maintainer or provenance risk are flagged with stable severity classes; false-negatives against seeded risk fixtures remain below defined threshold.\n\nExpected Artifacts:\n- `docs/specs/dgis_maintainer_fragility.md`, `src/security/dgis/fragility_model.rs`, `artifacts/10.20/dgis_fragility_findings.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_20/bd-2jns/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_20/bd-2jns/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.20] Implement maintainer/publisher fragility model and single-point-of-failure detector.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.20] Implement maintainer/publisher fragility model and single-point-of-failure detector.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.20] Implement maintainer/publisher fragility model and single-point-of-failure detector.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.20] Implement maintainer/publisher fragility model and single-point-of-failure detector.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.20] Implement maintainer/publisher fragility model and single-point-of-failure detector.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"- Graph nodes with concentrated maintainer or provenance risk are flagged with stable severity classes; false-negatives against seeded risk fixtures remain below defined threshold.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:37:06.667070488Z","created_by":"ubuntu","updated_at":"2026-02-20T17:04:55.419950381Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-20","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-2jns","depends_on_id":"bd-2bj4","type":"blocks","created_at":"2026-02-20T17:04:55.419900508Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2k74","title":"[10.13] Implement per-peer admission budgets (bytes/symbols/failed-auth/inflight-decode/decode-cpu).","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.13 — FCP Deep-Mined Expansion Execution Track (9I)\n\nWhy This Exists:\nDeep connector/provider contract track: lifecycle FSMs, conformance harnesses, fencing, sandboxing, revocation freshness, and protocol hardening.\n\nTask Objective:\nImplement per-peer admission budgets (bytes/symbols/failed-auth/inflight-decode/decode-cpu).\n\nAcceptance Criteria:\n- Admission checks enforce all budget dimensions; limit breaches are rate-limited and logged; budgets can be tuned without code changes.\n\nExpected Artifacts:\n- `docs/specs/admission_budget_model.md`, `tests/security/per_peer_budget_enforcement.rs`, `artifacts/10.13/admission_budget_violation_report.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_13/bd-2k74/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_13/bd-2k74/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.13] Implement per-peer admission budgets (bytes/symbols/failed-auth/inflight-decode/decode-cpu).\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.13] Implement per-peer admission budgets (bytes/symbols/failed-auth/inflight-decode/decode-cpu).\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.13] Implement per-peer admission budgets (bytes/symbols/failed-auth/inflight-decode/decode-cpu).\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.13] Implement per-peer admission budgets (bytes/symbols/failed-auth/inflight-decode/decode-cpu).\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.13] Implement per-peer admission budgets (bytes/symbols/failed-auth/inflight-decode/decode-cpu).\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","status":"closed","priority":1,"issue_type":"task","assignee":"CrimsonCrane","created_at":"2026-02-20T07:36:53.923542532Z","created_by":"ubuntu","updated_at":"2026-02-20T12:45:43.136833278Z","closed_at":"2026-02-20T12:45:43.136806638Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-13","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-2k74","depends_on_id":"bd-91gg","type":"blocks","created_at":"2026-02-20T07:43:13.560138166Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2kd9","title":"[10.17] Implement claim compiler and public trust scoreboard pipeline.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.17 — Radical Expansion Execution Track (9K)\n\nWhy This Exists:\nRadical expansion track for proof-carrying speculation, Bayesian adversary control, time-travel replay, ZK attestations, and claim compiler systems.\n\nTask Objective:\nImplement claim compiler and public trust scoreboard pipeline.\n\nAcceptance Criteria:\n- External claims must compile to executable evidence contracts; unverifiable claim text is blocked and scoreboard updates publish signed evidence links.\n\nExpected Artifacts:\n- `docs/specs/claim_compiler.md`, `src/claims/claim_compiler.rs`, `tests/conformance/claim_compiler_gate.rs`, `artifacts/10.17/public_trust_scoreboard_snapshot.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_17/bd-2kd9/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_17/bd-2kd9/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.17] Implement claim compiler and public trust scoreboard pipeline.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.17] Implement claim compiler and public trust scoreboard pipeline.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.17] Implement claim compiler and public trust scoreboard pipeline.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.17] Implement claim compiler and public trust scoreboard pipeline.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.17] Implement claim compiler and public trust scoreboard pipeline.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"- External claims must compile to executable evidence contracts; unverifiable claim text is blocked and scoreboard updates publish signed evidence links.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:37:04.089509755Z","created_by":"ubuntu","updated_at":"2026-02-20T15:45:57.814970944Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-17","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-2kd9","depends_on_id":"bd-383z","type":"blocks","created_at":"2026-02-20T07:43:18.867528418Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2ke","title":"[PLAN 14] Benchmark + Standardization Ownership","description":"Section 14 standards epic. Publish benchmark specs/harnesses/datasets/scoring, include security+trust co-metrics, ship verifier toolkit, and version standards with migration guidance.\n\n## Success Criteria\n- All child beads for this section are completed with acceptance artifacts attached and no unresolved blockers.\n- Section-level verification gate for comprehensive unit tests, integration/e2e workflows, and detailed structured logging evidence is green.\n- Scope-to-plan traceability is explicit, with no feature loss versus the canonical plan section.\n\n## Optimization Notes\n- User-Outcome Lens: \"[PLAN 14] Benchmark + Standardization Ownership\" must improve real operator and end-user reliability, explainability, and time-to-safe-outcome under both normal and degraded conditions.\n- Cross-Section Coordination: this epic's child beads must explicitly encode integration expectations with neighboring sections to prevent local optimizations that break global behavior.\n- Verification Non-Negotiable: completion requires deterministic unit coverage, integration/E2E replay evidence, and structured logs sufficient for independent third-party reproduction and triage.\n- Scope Protection: any proposed simplification must prove feature parity against plan intent and document tradeoffs explicitly; silent scope reduction is forbidden.","status":"open","priority":2,"issue_type":"epic","created_at":"2026-02-20T07:36:42.247104303Z","created_by":"ubuntu","updated_at":"2026-02-20T08:12:49.352145606Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-14"],"dependencies":[{"issue_id":"bd-2ke","depends_on_id":"bd-18ie","type":"blocks","created_at":"2026-02-20T07:39:35.682867365Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2ke","depends_on_id":"bd-1wz","type":"blocks","created_at":"2026-02-20T07:38:36.031629879Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2ke","depends_on_id":"bd-26k","type":"blocks","created_at":"2026-02-20T07:38:35.986646149Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2ke","depends_on_id":"bd-2a6g","type":"blocks","created_at":"2026-02-20T07:39:35.856096616Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2ke","depends_on_id":"bd-2fkq","type":"blocks","created_at":"2026-02-20T07:39:36.115155149Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2ke","depends_on_id":"bd-2l4i","type":"blocks","created_at":"2026-02-20T07:48:30.433694147Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2ke","depends_on_id":"bd-2ps7","type":"blocks","created_at":"2026-02-20T07:39:36.025058889Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2ke","depends_on_id":"bd-32p","type":"blocks","created_at":"2026-02-20T07:38:36.073942985Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2ke","depends_on_id":"bd-37i","type":"blocks","created_at":"2026-02-20T07:38:36.204175767Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2ke","depends_on_id":"bd-39a","type":"blocks","created_at":"2026-02-20T07:38:36.116020652Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2ke","depends_on_id":"bd-3h1g","type":"blocks","created_at":"2026-02-20T07:39:35.340860336Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2ke","depends_on_id":"bd-3rc","type":"blocks","created_at":"2026-02-20T07:38:35.944204053Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2ke","depends_on_id":"bd-3v8g","type":"blocks","created_at":"2026-02-20T07:39:35.597294651Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2ke","depends_on_id":"bd-jbp1","type":"blocks","created_at":"2026-02-20T07:39:35.940582236Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2ke","depends_on_id":"bd-ka0n","type":"blocks","created_at":"2026-02-20T07:39:35.770767445Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2ke","depends_on_id":"bd-wzjl","type":"blocks","created_at":"2026-02-20T07:39:35.425761880Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2ke","depends_on_id":"bd-ybe","type":"blocks","created_at":"2026-02-20T07:38:36.158563887Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2ke","depends_on_id":"bd-yz3t","type":"blocks","created_at":"2026-02-20T07:39:35.511596563Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2kf","title":"[10.2] Implement compatibility mode selection policy (`strict`, `balanced`, `legacy-risky`).","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.2 — Compatibility Core\n\nWhy This Exists:\nCompatibility core as strategic wedge: policy-visible behavior, divergence receipts, L1/L2 lockstep, and spec-first fixture governance.\n\nTask Objective:\nImplement compatibility mode selection policy (`strict`, `balanced`, `legacy-risky`).\n\nAcceptance Criteria:\n- Implement with explicit success/failure invariants and deterministic behavior under normal, edge, and fault scenarios.\n- Ensure outcomes are verifiable via automated tests and reproducible artifact outputs.\n\nExpected Artifacts:\n- Section-owned design/spec artifact at `docs/specs/section_10_2/bd-2kf_contract.md`, capturing decision rationale, invariants, and interface boundaries.\n- Machine-readable verification artifact at `artifacts/section_10_2/bd-2kf/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_2/bd-2kf/verification_summary.md` linking implementation intent to measured outcomes.\n\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.2] Implement compatibility mode selection policy (`strict`, `balanced`, `legacy-risky`).\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.2] Implement compatibility mode selection policy (`strict`, `balanced`, `legacy-risky`).\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.2] Implement compatibility mode selection policy (`strict`, `balanced`, `legacy-risky`).\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.2] Implement compatibility mode selection policy (`strict`, `balanced`, `legacy-risky`).\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.2] Implement compatibility mode selection policy (`strict`, `balanced`, `legacy-risky`).\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","status":"closed","priority":1,"issue_type":"task","assignee":"CrimsonCrane","created_at":"2026-02-20T07:36:44.077237537Z","created_by":"ubuntu","updated_at":"2026-02-20T09:38:09.529574909Z","closed_at":"2026-02-20T09:38:09.529546275Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-2","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-2kf","depends_on_id":"bd-38l","type":"blocks","created_at":"2026-02-20T07:43:20.178807085Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2ko","title":"[10.11] Adopt canonical deterministic lab runtime and protocol scenario suites (from `10.14` + `10.15`) for product control-plane logic.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md — Section 10.11 (FrankenSQLite-Inspired Runtime Systems), cross-ref Section 9G.4, 9J.18\n\n## Why This Exists\n\nControl-plane logic in franken_node — migration orchestration, epoch transitions, rollout state machines, trust-rotation coordination — contains subtle race conditions and ordering dependencies that are nearly impossible to expose through conventional integration tests against real network stacks. Enhancement Map 9G.4 mandates a deterministic lab runtime for migration/control-plane race exploration, enabling developers to write scenario suites that exercise specific interleaving schedules, network fault patterns, and timing anomalies in a fully reproducible manner. 9J.18 extends this with virtual-network fault labs that can inject drop, reorder, corrupt, and delay at the transport layer, enabling protocol hardening without flaky CI.\n\nThis bead adopts the canonical deterministic lab primitives from 10.14 (bd-2qqu virtual transport fault harness, bd-2808 deterministic repro bundle export, bd-22yy DPOR-style schedule exploration) and the lab integration from 10.15 (bd-145n lab runtime scenarios, bd-3u6o virtual transport enforcement, bd-25oa DPOR enforcement) into franken_node's product-layer test infrastructure, providing a `LabRuntime` harness that product engineers use to write deterministic scenario tests for all high-impact control-plane workflows.\n\n## What This Must Do\n\n1. Implement a `LabRuntime` test harness that replaces real async I/O, timers, and network with deterministic virtual implementations, allowing tests to control the exact order of event delivery and timer expiration.\n2. Integrate the virtual transport fault harness (from bd-2qqu): support programmatic injection of packet drop, reorder, corruption, and delay on any virtual network link within a test scenario.\n3. Integrate DPOR-style (Dynamic Partial Order Reduction) schedule exploration (from bd-22yy): given a scenario, systematically explore all relevant task interleavings to find bugs that only manifest under specific schedules.\n4. Implement deterministic repro bundle export (from bd-2808): when a scenario fails, automatically capture the exact schedule, fault injections, and event sequence as a self-contained bundle that reproduces the failure deterministically.\n5. Provide a scenario DSL or builder API that lets product engineers define multi-node topologies, fault patterns, and assertions in a readable format.\n6. Require that all high-impact control-plane workflows (migration, epoch transition, rollout state changes, trust rotation) have at least one LabRuntime scenario test in the CI pipeline.\n\n## Context from Enhancement Maps\n\n- 9G.4: \"Deterministic lab runtime for migration/control-plane race exploration\"\n- 9J.18: \"Deterministic virtual-network fault labs for protocol hardening\"\n- Architecture invariant #9 (8.5): Deterministic verification gates — all critical protocol logic must be verifiable under deterministic replay.\n- Architecture invariant #3 (8.5): Cancellation protocol semantics — lab scenarios must be able to inject cancellation at arbitrary points.\n- Architecture invariant #7 (8.5): Epoch barriers — lab scenarios must be able to simulate epoch transitions under adversarial timing.\n\n## Dependencies\n\n- Upstream: bd-2qqu (10.14 virtual transport fault harness), bd-2808 (10.14 deterministic repro bundle export), bd-22yy (10.14 DPOR-style schedule exploration), bd-145n (10.15 lab runtime scenarios), bd-3u6o (10.15 virtual transport enforcement), bd-25oa (10.15 DPOR enforcement)\n- Downstream: All 10.11 beads benefit from LabRuntime for their own testing; bd-1jpo (10.11 section-wide verification gate)\n\n## Acceptance Criteria\n\n1. `LabRuntime` replaces all async I/O with deterministic virtual implementations: tests using LabRuntime produce identical results across runs given the same seed.\n2. Virtual transport fault injection is configurable per-link: a test can specify \"drop 30% on link A->B, delay 50ms on link B->C\" and these faults are applied deterministically.\n3. DPOR exploration discovers a known race condition in a reference scenario (provided as a golden test) that random testing misses in 1000 iterations.\n4. Repro bundle export produces a self-contained file that, when replayed, reproduces the exact failure sequence without any external dependencies.\n5. Scenario builder API supports defining topologies of 2-10 virtual nodes with named links and per-link fault profiles.\n6. CI pipeline includes LabRuntime scenarios for: (a) migration orchestration, (b) epoch transition, (c) rollout state machine, (d) trust rotation — minimum one scenario each.\n7. All LabRuntime scenario failures produce structured test output including the schedule trace, fault injection log, and assertion failure location.\n8. Verification evidence JSON includes scenarios_executed, interleavings_explored, bugs_found, repro_bundles_generated, and determinism_verified (boolean) fields.\n\n## Testing & Logging Requirements\n\n- Unit tests: (a) LabRuntime timer determinism — same seed produces same timer ordering; (b) Virtual transport drop/reorder/corrupt injection works as configured; (c) DPOR explores at least N distinct interleavings for a 3-task scenario; (d) Repro bundle round-trip: export then replay produces same outcome.\n- Integration tests: (a) Full migration orchestration scenario with 3 virtual nodes and injected partition; (b) Epoch transition scenario with one node failing to reach quiescence; (c) Rollout state machine scenario with concurrent cancel and promote signals; (d) Trust rotation scenario with key delivery delay.\n- Adversarial tests: (a) Scenario with 100% packet loss on one link — verify protocol detects and handles; (b) Scenario with message corruption — verify integrity checks catch it; (c) Scenario with extreme clock skew between virtual nodes; (d) Scenario with all faults simultaneously — verify no panic or undefined behavior.\n- Structured logs: Lab scenario events use stable codes (FN-LB-001 through FN-LB-010), include `scenario_name`, `seed`, `interleaving_id`, `virtual_time`, `fault_type`. JSON-formatted.\n\n## Expected Artifacts\n\n- docs/specs/section_10_11/bd-2ko_contract.md\n- crates/franken-node/src/testing/lab_runtime.rs (or equivalent module path)\n- crates/franken-node/src/testing/virtual_transport.rs\n- crates/franken-node/src/testing/scenario_builder.rs\n- scripts/check_deterministic_lab.py (with --json flag and self_test())\n- tests/test_check_deterministic_lab.py\n- artifacts/section_10_11/bd-2ko/verification_evidence.json\n- artifacts/section_10_11/bd-2ko/verification_summary.md","acceptance_criteria":"AC for bd-2ko:\n1. Product control-plane logic runs under a deterministic lab runtime (DeterministicRuntime) that replaces all sources of non-determinism: clocks return injectable timestamps, random generators use seeded PRNGs, I/O is replaced by in-memory fakes with scripted responses.\n2. The DeterministicRuntime implements the same trait interfaces as the production runtime, allowing control-plane code to be runtime-agnostic via generic bounds (Runtime: Clock + Rng + Transport).\n3. Protocol scenario suites are defined as declarative YAML/JSON fixtures specifying: initial state, sequence of events (with injected faults), and expected final state + emitted side-effects.\n4. A scenario runner loads fixtures, replays them through the DeterministicRuntime, and produces a pass/fail verdict with a diff if the actual outcome diverges from expected.\n5. Every protocol in the 10.11 scope (cancellation, two-phase obligations, supervision, capability narrowing) has at least 3 scenario fixtures: happy path, single fault, and cascading fault.\n6. Scenario execution is fully reproducible: given the same seed and fixture, the output is bit-for-bit identical across runs; a CI check verifies this by running each scenario twice and comparing outputs.\n7. Fault injection covers: message delay/drop/reorder, clock skew, OOM simulation, and cancellation-during-prepare.\n8. Unit tests verify: (a) seeded runtime produces identical outputs across runs, (b) clock injection correctly advances time, (c) fault-injected scenario detects expected failure mode, (d) scenario runner correctly diffs expected vs actual.\n9. Structured log events: SCENARIO_START / SCENARIO_STEP / SCENARIO_PASS / SCENARIO_FAIL / FAULT_INJECTED with scenario name, step index, and seed.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:36:50.224385543Z","created_by":"ubuntu","updated_at":"2026-02-20T17:01:40.645718841Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-11","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-2ko","depends_on_id":"bd-145n","type":"blocks","created_at":"2026-02-20T15:00:18.104446139Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2ko","depends_on_id":"bd-2qqu","type":"blocks","created_at":"2026-02-20T15:00:17.912572305Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2l1k","title":"[13] Concrete target gate: 100% replay artifact coverage","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 13\n\nTask Objective:\nBuild KPI gate for complete high-severity replay artifact coverage.\n\nExecution Requirements:\n- Make this requirement machine-verifiable and release-gated where applicable.\n- Keep behavior self-documenting so future contributors do not need to re-open the master plan.\n\nTesting & Logging Requirements:\n- Unit tests for rule/contract logic and error conditions.\n- Integration/E2E tests for workflow-level enforcement.\n- Structured logs with stable codes and trace IDs.\n- Reproducible artifacts that prove contract satisfaction.\n\nAcceptance Criteria:\n- Success and failure invariants for [13] Concrete target gate: 100% replay artifact coverage are explicitly quantified and machine-verifiable.\n- Determinism requirements for [13] Concrete target gate: 100% replay artifact coverage are validated across repeated runs and adversarial perturbations.\n\n\nExpected Artifacts:\n- Machine-readable verification artifact with explicit canonical path under artifacts/section_13/bd-2l1k/verification_evidence.json, suitable for CI/release gating.\n- Human-readable verification summary with explicit canonical path under artifacts/section_13/bd-2l1k/verification_summary.md, linking implementation intent to measured validation outcomes.\n\nTask-Specific Clarification:\n- The implementation must deliver the exact capability named in the title, with no scope dilution and no silent feature omission.\n- Validation artifacts must include capability-specific thresholds, pass/fail criteria, and reproducible evidence bundles tied to this bead ID.\n- Program/section verification gates must be able to consume this bead's outputs without manual interpretation.\n\nWhy This Improves User Outcomes:\n- \"[13] Concrete target gate: 100% replay artifact coverage\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[13] Concrete target gate: 100% replay artifact coverage\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"1. Every high-severity incident type has a corresponding replay artifact that captures sufficient state to reproduce the incident deterministically.\n2. High-severity incident types are enumerated: RCE, privilege escalation, data exfiltration, sandbox escape, trust system bypass, supply-chain compromise, denial of service, memory corruption.\n3. 100% coverage means: for every enumerated incident type, at least one replay artifact exists and successfully reproduces the incident in a test environment.\n4. Replay artifacts include: initial state snapshot, input sequence, expected behavior trace, actual behavior trace, and divergence point.\n5. Replay is deterministic: running the replay artifact 10 times produces identical behavior traces each time.\n6. New high-severity incident types added to the enumeration must have replay artifacts within 1 sprint of discovery.\n7. Evidence artifact: replay_coverage_matrix.json mapping each incident type to its replay artifact path and last-verified date.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:39:35.128318458Z","created_by":"ubuntu","updated_at":"2026-02-20T15:21:39.230413969Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-13","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-2l1k","depends_on_id":"bd-3cpa","type":"blocks","created_at":"2026-02-20T07:43:25.663870672Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2l4i","title":"[14] Section-wide verification gate: comprehensive unit+e2e+logging","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 14\n\nTask Objective:\nCreate a hard section completion gate that verifies all implemented behavior in this section with comprehensive unit tests, integration/e2e scripts, and detailed structured logging evidence.\n\nAcceptance Criteria:\n- Section test matrix covers happy-path, edge-case, and adversarial/error-path scenarios for all section deliverables.\n- E2E scripts execute representative end-user workflows and policy/control-plane transitions for this section.\n- Structured logs include stable event/error codes, trace correlation IDs, and enough context for deterministic replay triage.\n- Verification report is deterministic and machine-readable for CI/release gating.\n\nExpected Artifacts:\n- Section-level test matrix and coverage summary artifact.\n- E2E script suite with pass/fail outputs and reproducible fixtures/seeds.\n- Structured log validation report and traceability bundle.\n- Gate verdict artifact consumable by release automation.\n\n- Machine-readable verification artifact at `artifacts/section_14/bd-2l4i/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_14/bd-2l4i/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests must validate contracts, invariants, and failure semantics.\n- E2E tests must validate cross-component behavior from user/operator entrypoints to persisted effects.\n- Logging must support root-cause analysis without hidden context requirements.\n\nTask-Specific Clarification:\n- For \"[14] Section-wide verification gate: comprehensive unit+e2e+logging\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[14] Section-wide verification gate: comprehensive unit+e2e+logging\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[14] Section-wide verification gate: comprehensive unit+e2e+logging\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[14] Section-wide verification gate: comprehensive unit+e2e+logging\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[14] Section-wide verification gate: comprehensive unit+e2e+logging\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"1. Section 14 verification gate runs all benchmark and standardization check scripts and confirms 100% pass rate.\n2. Gate validates: (a) benchmark harness runs end-to-end on CI and produces valid output, (b) all 6 metric families produce results, (c) verifier toolkit installs and runs successfully, (d) version standards are documented.\n3. All metric families have defined thresholds and currently meet them.\n4. Gate produces section_14_verification_summary.md with per-metric-family status and publication readiness checklist.\n5. Publication artifacts are present: benchmark spec, harness, datasets, verifier toolkit, version migration guide.\n6. The gate itself has a unit test verifying correct metric aggregation and threshold evaluation.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:48:29.845475628Z","created_by":"ubuntu","updated_at":"2026-02-20T15:25:45.559253447Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-14","test-obligations","verification-gate"],"dependencies":[{"issue_id":"bd-2l4i","depends_on_id":"bd-18ie","type":"blocks","created_at":"2026-02-20T07:48:30.188691617Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2l4i","depends_on_id":"bd-1dpd","type":"blocks","created_at":"2026-02-20T08:24:23.742176064Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2l4i","depends_on_id":"bd-2a6g","type":"blocks","created_at":"2026-02-20T07:48:30.092623735Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2l4i","depends_on_id":"bd-2fkq","type":"blocks","created_at":"2026-02-20T07:48:29.945150438Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2l4i","depends_on_id":"bd-2ps7","type":"blocks","created_at":"2026-02-20T07:48:29.994460220Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2l4i","depends_on_id":"bd-2twu","type":"blocks","created_at":"2026-02-20T08:20:49.135077279Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2l4i","depends_on_id":"bd-3h1g","type":"blocks","created_at":"2026-02-20T07:48:30.383099582Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2l4i","depends_on_id":"bd-3v8g","type":"blocks","created_at":"2026-02-20T07:48:30.235872834Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2l4i","depends_on_id":"bd-jbp1","type":"blocks","created_at":"2026-02-20T07:48:30.044008466Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2l4i","depends_on_id":"bd-ka0n","type":"blocks","created_at":"2026-02-20T07:48:30.141209208Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2l4i","depends_on_id":"bd-wzjl","type":"blocks","created_at":"2026-02-20T07:48:30.334669959Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2l4i","depends_on_id":"bd-yz3t","type":"blocks","created_at":"2026-02-20T07:48:30.286007353Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2lb","title":"Bootstrap clap CLI surface for franken-node","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md (Bootstrap bridge for Sections 10.0, 10.2, 10.4)\nSection: BOOTSTRAP (Initial CLI command surface)\n\nBootstrap Context:\nThis bead establishes the minimal but production-worthy CLI baseline so implementation work can proceed without command-surface ambiguity.\n\nTask Objective:\nImplement the foundational Clap CLI in `crates/franken-node` with two real command paths (`run`, `doctor`) and deterministic dispatch/exit behavior.\n\nIn Scope:\n- Top-level command parser, stable help output, and deterministic exit code mapping.\n- `run` command path that routes JavaScript eval input through HybridRouter.\n- `doctor` command path that reports extension-host snapshot path/state availability.\n\nOut of Scope:\n- Full command-family scaffolding (`init`, `migrate`, `verify`, `trust`, `incident`) which is handled in `bd-3vk`.\n\nAcceptance Criteria:\n- CLI parsing is deterministic and rejects malformed flags/args with stable diagnostics.\n- `run` path executes through the intended router path and emits deterministic success/failure envelopes.\n- `doctor` path returns actionable diagnostics for missing/corrupt snapshot metadata.\n- Surface is ready for extension by `bd-3vk` without refactoring command-core internals.\n\nExpected Artifacts:\n- CLI command/argument contract note with examples.\n- Golden output fixtures for `--help`, `run`, and `doctor` command paths.\n- Machine-readable run summaries usable by CI.\n\n- Machine-readable verification artifact at `artifacts/section_bootstrap/bd-2lb/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_bootstrap/bd-2lb/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for parsing, dispatch, and deterministic exit-code/error mapping.\n- Integration tests for `run` and `doctor` command behavior across normal + failure cases.\n- E2E smoke script invoking representative command sequences.\n- Structured logs with stable event/error codes and trace correlation IDs for each command invocation.\n\nTask-Specific Clarification:\n- For \"Bootstrap clap CLI surface for franken-node\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"Bootstrap clap CLI surface for franken-node\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"Bootstrap clap CLI surface for franken-node\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"Bootstrap clap CLI surface for franken-node\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"Bootstrap clap CLI surface for franken-node\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"1) CLI subcommands parse and dispatch cleanly. 2) `franken-node run` accepts script/eval input and executes through HybridRouter. 3) `franken-node doctor` reports extension-host snapshot path/state. 4) Build and lint pass.","status":"closed","priority":1,"issue_type":"task","assignee":"JadeHollow","created_at":"2026-02-20T07:25:48.975612375Z","created_by":"ubuntu","updated_at":"2026-02-20T13:08:49.066726161Z","closed_at":"2026-02-20T13:08:49.066687870Z","close_reason":"done","closed_by_session":"CoralReef","source_repo":".","compaction_level":0,"original_size":0,"labels":["bootstrap","cli"]}
{"id":"bd-2lll","title":"[10.21] Implement changepoint and regime-shift detection layer (Bayesian changepoint + HMM state transitions).","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.21 — Behavioral Phenotype Evolution Tracker Execution Track (9O)\n\nWhy This Exists:\nBPET longitudinal phenotype intelligence track for pre-compromise trajectory detection and integration with DGIS/ATC/migration/economic policy.\n\nTask Objective:\nImplement changepoint and regime-shift detection layer (Bayesian changepoint + HMM state transitions).\n\nAcceptance Criteria:\n- Regime shifts are detected with calibrated false-positive/false-negative bounds on historical and synthetic trajectories; shift explanations include dominant contributing dimensions.\n\nExpected Artifacts:\n- `src/security/bpet/regime_shift_detector.rs`, `tests/security/bpet_regime_shift_suite.rs`, `artifacts/10.21/bpet_regime_shift_eval.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_21/bd-2lll/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_21/bd-2lll/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.21] Implement changepoint and regime-shift detection layer (Bayesian changepoint + HMM state transitions).\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.21] Implement changepoint and regime-shift detection layer (Bayesian changepoint + HMM state transitions).\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.21] Implement changepoint and regime-shift detection layer (Bayesian changepoint + HMM state transitions).\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.21] Implement changepoint and regime-shift detection layer (Bayesian changepoint + HMM state transitions).\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.21] Implement changepoint and regime-shift detection layer (Bayesian changepoint + HMM state transitions).\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"- Regime shifts are detected with calibrated false-positive/false-negative bounds on historical and synthetic trajectories; shift explanations include dominant contributing dimensions.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:37:08.119916096Z","created_by":"ubuntu","updated_at":"2026-02-20T17:05:34.421133647Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-21","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-2lll","depends_on_id":"bd-2ao3","type":"blocks","created_at":"2026-02-20T17:05:34.421086038Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2m2b","title":"[10.13] Implement Network Guard egress layer with HTTP+TCP policy enforcement and audit emission.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.13 — FCP Deep-Mined Expansion Execution Track (9I)\n\nWhy This Exists:\nDeep connector/provider contract track: lifecycle FSMs, conformance harnesses, fencing, sandboxing, revocation freshness, and protocol hardening.\n\nTask Objective:\nImplement Network Guard egress layer with HTTP+TCP policy enforcement and audit emission.\n\nAcceptance Criteria:\n- All connector egress traverses guard path; allow/deny enforcement matches policy semantics; every decision emits structured audit event.\n\nExpected Artifacts:\n- `src/security/network_guard.rs`, `tests/conformance/network_guard_policy.rs`, `artifacts/10.13/network_guard_audit_samples.jsonl`.\n\n- Machine-readable verification artifact at `artifacts/section_10_13/bd-2m2b/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_13/bd-2m2b/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.13] Implement Network Guard egress layer with HTTP+TCP policy enforcement and audit emission.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.13] Implement Network Guard egress layer with HTTP+TCP policy enforcement and audit emission.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.13] Implement Network Guard egress layer with HTTP+TCP policy enforcement and audit emission.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.13] Implement Network Guard egress layer with HTTP+TCP policy enforcement and audit emission.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.13] Implement Network Guard egress layer with HTTP+TCP policy enforcement and audit emission.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","status":"closed","priority":1,"issue_type":"task","assignee":"CrimsonCrane","created_at":"2026-02-20T07:36:52.286074341Z","created_by":"ubuntu","updated_at":"2026-02-20T11:20:08.653028207Z","closed_at":"2026-02-20T11:20:08.652999594Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-13","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-2m2b","depends_on_id":"bd-1vvs","type":"blocks","created_at":"2026-02-20T07:43:12.700474500Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2ms","title":"[10.10] Implement rollback/fork detection in control-plane state propagation using canonical divergence and marker proofs (from `10.14`).","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md — Section 10.10 (FCP-Inspired Hardening), cross-ref Section 9E.3\n\n## Why This Exists\n\nEnhancement Map 9E.3 requires rollback resistance for control-plane state propagation. While bd-174 provides the policy checkpoint chain, this bead addresses the complementary problem: detecting when control-plane state has diverged or been rolled back across distributed nodes. By integrating canonical divergence detection and marker proofs from Section 10.14's append-only marker stream, this bead ensures that if any node in the three-kernel architecture (franken_engine + asupersync + franken_node) observes a different state history, the divergence is detected immediately and the system halts unsafe operations. Without this, a compromised or partitioned node could silently operate under a stale or forked policy, violating the no-ambient-authority invariant (8.5).\n\n## What This Must Do\n\n1. Implement a `DivergenceDetector` that compares marker-ID prefixes from 10.14's append-only marker stream (bd-126h) to detect fork points using binary search over marker sequences.\n2. Integrate with 10.14's fork/divergence detection API (bd-xwk5) to receive divergence alerts and translate them into product-level control-plane actions (halt propagation, quarantine divergent state, alert operators).\n3. Implement `MarkerProofVerifier` that validates inclusion and prefix proofs from 10.14's optional MMR checkpoint API (bd-1dar) against the local policy checkpoint chain from bd-174.\n4. Define product-level responses to divergence: HALT (stop all control-plane mutations), QUARANTINE (isolate divergent state partition), ALERT (structured notification to operator with divergence evidence), and RECOVER (re-sync from authoritative checkpoint after operator approval).\n5. Ensure that all control-plane state propagation operations (policy updates, token issuance, zone boundary changes) check for divergence before executing — no propagation without freshness proof.\n6. Emit structured divergence events with sufficient context for automated remediation tooling.\n\n## Context from Enhancement Maps\n\n- 9E.3: \"Checkpointed policy frontier for release channels and rollback resistance\"\n- 9E.7 (cross-ref): Revocation freshness semantics (bd-2sx) also depend on divergence-free state to avoid accepting revoked credentials under a forked view.\n- 9D.2 (Interop): Divergence detection must work across kernel boundaries, requiring canonical marker format compatibility.\n- 9A.4 (Observability): Divergence events are high-severity alerts requiring structured context for incident response.\n\n## Dependencies\n\n- Upstream: bd-1dar ([10.14] Implement optional MMR checkpoints and inclusion/prefix proof APIs for external verifiers) — provides the proof APIs this bead consumes.\n- Upstream: bd-xwk5 ([10.14] Implement fork/divergence detection via marker-id prefix comparison and binary search) — provides the core divergence detection algorithm.\n- Upstream: bd-126h ([10.14] Implement append-only marker stream for high-impact control events with dense sequence invariant checks) — provides the marker stream that divergence detection operates over.\n- Upstream: bd-174 ([10.10] Implement policy checkpoint chain for product release channels) — provides the local policy chain that marker proofs are verified against.\n- Downstream: bd-1r2 ([10.10] Implement audience-bound token chains for control actions) — token issuance depends on divergence-free state.\n- Downstream: bd-1jjq ([10.10] Section-wide verification gate) — aggregates verification evidence.\n\n## Acceptance Criteria\n\n1. `DivergenceDetector` correctly identifies fork points in marker streams with O(log n) binary search, verified against synthetic fork scenarios with streams of 10,000+ markers.\n2. All four response modes (HALT, QUARANTINE, ALERT, RECOVER) are implemented with documented state machine transitions and tested individually.\n3. No control-plane mutation (policy update, token issuance, zone change) can proceed when a divergence is detected — verified by attempting mutations during active divergence and confirming rejection with `DIVERGENCE_BLOCK` error code.\n4. `MarkerProofVerifier` validates inclusion proofs and prefix proofs against MMR checkpoints with zero false positives on valid proofs and 100% detection of tampered proofs.\n5. Recovery (re-sync) requires explicit operator approval via a signed authorization — no automatic recovery from divergence.\n6. Divergence detection latency is under 100ms for marker streams up to 100,000 entries (benchmark test required).\n7. Integration with bd-174's policy frontier is verified: a rollback of the policy chain triggers divergence detection within one propagation cycle.\n8. Verification evidence JSON includes divergence scenarios tested, detection latencies, and proof validation counts.\n\n## Testing & Logging Requirements\n\n- Unit tests: Construct synthetic marker streams with known fork points and verify detection. Test all four response modes independently. Test binary search correctness at stream boundaries (fork at first marker, fork at last marker, no fork). Test with empty streams and single-entry streams.\n- Integration tests: Run a two-node simulation where one node's marker stream is artificially forked, and verify that divergence is detected and HALT is triggered before any mutation proceeds. Test crash recovery: kill during divergence response and verify correct state on restart. Verify that marker proofs from bd-1dar are correctly validated against local checkpoints.\n- Adversarial tests: Attempt to suppress divergence alerts by replaying old markers. Attempt to forge a marker proof that passes validation (should fail). Test with marker streams that have been truncated (prefix attack). Simulate a slow-roll fork where divergence grows by one marker per cycle.\n- Structured logs: `DIVERGENCE_DETECTED` (fork_point_sequence, local_head, remote_head, detection_latency_ms). `DIVERGENCE_RESPONSE` (response_mode, affected_partitions, operator_notified). `DIVERGENCE_RECOVERED` (authorizing_operator, resync_checkpoint, markers_replayed). `MARKER_PROOF_VERIFIED` / `MARKER_PROOF_REJECTED` (proof_type, marker_sequence, reason). All events include `trace_id`, `epoch_id`, and `node_id`.\n\n## Expected Artifacts\n\n- docs/specs/section_10_10/bd-2ms_contract.md\n- crates/franken-node/src/connector/divergence_detector.rs (or similar module path)\n- scripts/check_divergence_detection.py with --json flag and self_test()\n- tests/test_check_divergence_detection.py\n- artifacts/section_10_10/bd-2ms/verification_evidence.json\n- artifacts/section_10_10/bd-2ms/verification_summary.md","acceptance_criteria":"1. Define a StateVector struct containing: (a) epoch (u64), (b) marker_id (TrustObjectId with MARKER domain from bd-1l5), (c) state_hash (SHA-256 of canonical-serialized state at this epoch), (d) parent_state_hash (SHA-256 of previous epoch state), (e) timestamp.\n2. Implement a DivergenceDetector that compares two StateVectors from different replicas: (a) if epochs match and state_hashes match => CONVERGED, (b) if epochs match and state_hashes differ => FORKED, (c) if epochs differ by >1 => GAP_DETECTED, (d) if parent_state_hash of the newer does not match state_hash of the older => ROLLBACK_DETECTED.\n3. Implement marker proof verification: given a marker_id chain (from 10.14 bd-126h), verify that the state vector's marker_id appears in the append-only marker stream at the claimed epoch. Return MarkerNotFound or MarkerEpochMismatch on failure.\n4. Implement a rollback proof struct: RollbackProof containing (a) the two divergent StateVectors, (b) the expected parent hash, (c) the actual parent hash, (d) detection timestamp. This struct MUST be serializable for audit logging.\n5. On fork or rollback detection, emit a structured log event with severity=CRITICAL containing the RollbackProof fields and trace correlation ID.\n6. Implement a reconciliation suggestion: for GAP_DETECTED, return the range of missing epochs; for FORKED, return both state hashes for operator review.\n7. Unit tests: (a) CONVERGED case, (b) FORKED case, (c) GAP_DETECTED case, (d) ROLLBACK_DETECTED case, (e) marker proof valid/invalid, (f) RollbackProof serialization round-trip.\n8. Integration test: simulate a 100-epoch sequence, inject a fork at epoch 50, verify detection occurs at epoch 51.\n9. Verification: scripts/check_rollback_detection.py --json, artifacts at artifacts/section_10_10/bd-2ms/.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:36:49.002546997Z","created_by":"ubuntu","updated_at":"2026-02-20T17:01:30.822928663Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-10","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-2ms","depends_on_id":"bd-126h","type":"blocks","created_at":"2026-02-20T14:59:54.053342610Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2ms","depends_on_id":"bd-1dar","type":"blocks","created_at":"2026-02-20T14:59:54.489020699Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2ms","depends_on_id":"bd-xwk5","type":"blocks","created_at":"2026-02-20T14:59:54.277489864Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2nd","title":"Add explicit franken_node product charter document","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.1 — Charter + Split Governance\n\nTask Objective:\nProduce a canonical franken_node product charter document that codifies scope boundaries, split-governance constraints with franken_engine, non-negotiables, and decision rules for future roadmap changes.\n\nAcceptance Criteria:\n- Charter clearly defines product purpose, in-scope/out-of-scope boundaries, and ownership demarcation against franken_engine.\n- Governance section documents decision authority, escalation paths, and change-control criteria.\n- Document is cross-linked from README and key roadmap/spec docs so future agents/operators can find it deterministically.\n\nExpected Artifacts:\n- docs/PRODUCT_CHARTER.md with stable section structure.\n- Cross-link updates in docs/ROADMAP.md and docs/ENGINE_SPLIT_CONTRACT.md (or explicit notes if already linked).\n- Review note capturing rationale for any deliberate boundary decisions.\n\n- Machine-readable verification artifact at `artifacts/section_10_1/bd-2nd/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_1/bd-2nd/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit-style doc lint checks (format/heading/link validity) pass for the charter and updated references.\n- E2E documentation validation script confirms a newcomer can navigate from README to charter to split contract without dead links.\n- Detailed command/output logs from doc validation are attached for reproducible review.\n\nTask-Specific Clarification:\n- For \"Add explicit franken_node product charter document\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"Add explicit franken_node product charter document\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"Add explicit franken_node product charter document\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"Add explicit franken_node product charter document\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"Add explicit franken_node product charter document\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","status":"closed","priority":1,"issue_type":"docs","assignee":"CrimsonCrane","created_at":"2026-02-20T07:26:48.287664174Z","created_by":"ubuntu","updated_at":"2026-02-20T09:02:07.409952851Z","closed_at":"2026-02-20T09:02:07.409915422Z","close_reason":"Product charter created with 6/6 verification checks passing. Charter covers: product purpose, scope boundary, target users, non-negotiables, success criteria, impossible-by-default capabilities, governance model, execution tracks, off-charter behaviors, and cross-references. README and ROADMAP cross-linked.","source_repo":".","compaction_level":0,"original_size":0,"labels":["charter","governance"]}
{"id":"bd-2nlu","title":"[PROGRAM] Implement full-program e2e/chaos orchestration with trace-stable evidence bundles","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md (Cross-cutting across Sections 10.0–10.21 and 11–16)\nSection: PROGRAM (Cross-cutting E2E and chaos orchestration)\n\nTask Objective:\nImplement a program-level E2E + chaos orchestration suite that executes cross-section journeys from the integration matrix and emits deterministic evidence bundles for pass/fail and replay analysis.\n\nWhy This Improves User Outcomes:\nUsers are harmed by integration regressions that pass local tests. This suite detects emergent multi-system failures, safety regressions, and degraded-mode blind spots before release.\n\nAcceptance Criteria:\n- Orchestration executes all matrix-defined critical journeys and records deterministic verdicts.\n- Chaos/failure injections cover representative policy, dependency, and trust failure classes.\n- Outputs include machine-readable pass/fail evidence with stable failure categories and remediation pointers.\n- Runs are reproducible across equivalent inputs/environments with bounded nondeterminism policy.\n\nExpected Artifacts:\n- Program-level E2E orchestration scripts/harness configuration.\n- Chaos scenario catalog with expected invariant checks.\n- Evidence bundle schema + sample bundles for green and failing runs.\n\n- Machine-readable verification artifact at `artifacts/section_program_cross_cutting_e2e_and_chaos_orchestration/bd-2nlu/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_program_cross_cutting_e2e_and_chaos_orchestration/bd-2nlu/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for orchestration helpers, scenario loaders, and verdict aggregators.\n- E2E tests for full journey execution under normal and induced-failure conditions.\n- Detailed structured logs with journey IDs, scenario IDs, invariant IDs, durations, and trace-correlation IDs.\n\nTask-Specific Clarification:\n- This is not a duplicate of section E2E tests; it validates cross-section composition behavior and seam integrity.","status":"closed","priority":1,"issue_type":"task","assignee":"ubuntu","created_at":"2026-02-20T08:08:05.640353368Z","created_by":"ubuntu","updated_at":"2026-02-20T08:45:45.637728561Z","closed_at":"2026-02-20T08:45:45.637637120Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["e2e","logging","plan","program-integration","verification"],"dependencies":[{"issue_id":"bd-2nlu","depends_on_id":"bd-1dpd","type":"blocks","created_at":"2026-02-20T08:24:23.210733876Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2nlu","depends_on_id":"bd-295v","type":"blocks","created_at":"2026-02-20T08:08:05.941053906Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2nlu","depends_on_id":"bd-2twu","type":"blocks","created_at":"2026-02-20T08:20:48.473552031Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2nre","title":"[15] Section-wide verification gate: comprehensive unit+e2e+logging","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 15\n\nTask Objective:\nCreate a hard section completion gate that verifies all implemented behavior in this section with comprehensive unit tests, integration/e2e scripts, and detailed structured logging evidence.\n\nAcceptance Criteria:\n- Section test matrix covers happy-path, edge-case, and adversarial/error-path scenarios for all section deliverables.\n- E2E scripts execute representative end-user workflows and policy/control-plane transitions for this section.\n- Structured logs include stable event/error codes, trace correlation IDs, and enough context for deterministic replay triage.\n- Verification report is deterministic and machine-readable for CI/release gating.\n\nExpected Artifacts:\n- Section-level test matrix and coverage summary artifact.\n- E2E script suite with pass/fail outputs and reproducible fixtures/seeds.\n- Structured log validation report and traceability bundle.\n- Gate verdict artifact consumable by release automation.\n\n- Machine-readable verification artifact at `artifacts/section_15/bd-2nre/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_15/bd-2nre/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests must validate contracts, invariants, and failure semantics.\n- E2E tests must validate cross-component behavior from user/operator entrypoints to persisted effects.\n- Logging must support root-cause analysis without hidden context requirements.\n\nTask-Specific Clarification:\n- For \"[15] Section-wide verification gate: comprehensive unit+e2e+logging\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[15] Section-wide verification gate: comprehensive unit+e2e+logging\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[15] Section-wide verification gate: comprehensive unit+e2e+logging\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[15] Section-wide verification gate: comprehensive unit+e2e+logging\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[15] Section-wide verification gate: comprehensive unit+e2e+logging\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"1. Section 15 verification gate runs all ecosystem-capture check scripts and confirms 100% pass rate.\n2. Gate validates: (a) extension registry operational with signing enforcement, (b) migration kits exist for >= 5 archetypes, (c) enterprise integrations tested, (d) reputation API spec published, (e) partner program active, (f) onboarding pathway tested end-to-end.\n3. Adoption metrics summarized: extension count, migration kit usage, partner count, case study count.\n4. Gate produces section_15_verification_summary.md with per-pillar status and adoption metrics.\n5. Any pillar below minimum viability threshold is flagged with gap analysis.\n6. The gate itself has a unit test verifying correct aggregation of sub-check results.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:48:30.630221561Z","created_by":"ubuntu","updated_at":"2026-02-20T15:27:41.359675750Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-15","test-obligations","verification-gate"],"dependencies":[{"issue_id":"bd-2nre","depends_on_id":"bd-1961","type":"blocks","created_at":"2026-02-20T07:48:30.901040145Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2nre","depends_on_id":"bd-1dpd","type":"blocks","created_at":"2026-02-20T08:24:23.605937283Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2nre","depends_on_id":"bd-209w","type":"blocks","created_at":"2026-02-20T07:48:31.046335606Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2nre","depends_on_id":"bd-2twu","type":"blocks","created_at":"2026-02-20T08:20:48.969307218Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2nre","depends_on_id":"bd-31tg","type":"blocks","created_at":"2026-02-20T07:48:30.852218883Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2nre","depends_on_id":"bd-3mj9","type":"blocks","created_at":"2026-02-20T07:48:30.949711478Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2nre","depends_on_id":"bd-cv49","type":"blocks","created_at":"2026-02-20T07:48:30.730117172Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2nre","depends_on_id":"bd-elog","type":"blocks","created_at":"2026-02-20T07:48:30.804530571Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2nre","depends_on_id":"bd-sxt5","type":"blocks","created_at":"2026-02-20T08:02:26.222071241Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2nre","depends_on_id":"bd-wpck","type":"blocks","created_at":"2026-02-20T07:48:30.998305196Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2nt","title":"[10.11] Implement VOI-budgeted monitor scheduling for expensive diagnostics.","description":"[10.11] Implement VOI-budgeted monitor scheduling for expensive diagnostics.\n\n## Why This Exists\n\nDiagnostic operations in franken_node range from cheap (health pings, counter reads) to extremely expensive (full state replay validation, cryptographic proof generation, deep trust-chain traversal). Running all diagnostics at maximum frequency wastes compute and can cause diagnostic storms that degrade production traffic. Value of Information (VOI) budgeting provides a principled scheduling framework: each diagnostic is scored by its expected information gain per unit of compute cost, and a global budget constrains total diagnostic spending. This ensures the system always runs the most valuable diagnostics first within its resource envelope.\n\n## What It Must Do\n\n1. **Diagnostic registry.** Define a registry of all diagnostic operations with metadata: name, estimated compute cost (in abstract cost units), estimated wall-clock time, information domains (what questions it answers), staleness tolerance (how long results remain valid), and priority class (critical/standard/background).\n\n2. **VOI scoring function.** Implement a scoring function that computes expected information value for each diagnostic based on:\n   - **Staleness**: diagnostics whose last result is older than their staleness tolerance get higher scores.\n   - **Uncertainty reduction**: diagnostics that resolve higher uncertainty (e.g., after a regime shift from bd-3u4) get higher scores.\n   - **Downstream impact**: diagnostics that gate downstream decisions (release gates, trust decisions) get higher scores.\n   - **Historical informativeness**: diagnostics that historically produced actionable findings get higher scores (exponentially weighted moving average).\n\n3. **Budget allocation.** A global diagnostic budget (configurable in cost units per time window, default: 1000 units per 60 seconds) constrains total diagnostic spending. The scheduler greedily selects diagnostics in descending VOI/cost order until the budget is exhausted.\n\n4. **Preemption and priority.** Critical diagnostics (e.g., security-triggered trust validation) can preempt background diagnostics. Preempted diagnostics are re-queued for the next scheduling cycle. Priority classes define preemption rules.\n\n5. **Budget storm protection.** If diagnostic demand exceeds 3x the budget for more than 2 consecutive windows, the scheduler emits a `diagnostic_storm` alert and enters a conservative mode that only runs critical-class diagnostics until demand subsides.\n\n6. **Scheduling telemetry.** Every scheduling decision is logged: which diagnostics were selected, their VOI scores, budget consumed, and which were deferred. This telemetry feeds into the operational dashboard and regime detector (bd-3u4).\n\n7. **Dynamic budget adjustment.** After a regime shift (from bd-3u4), the diagnostic budget can be temporarily increased (configurable multiplier, default: 2x for 5 minutes) to allow faster information gathering during transitions.\n\n## Acceptance Criteria\n\n1. Diagnostic registry with at least 10 registered diagnostics in `crates/franken-node/src/connector/diagnostic_registry.rs`.\n2. VOI scoring function implemented with staleness, uncertainty, downstream impact, and historical informativeness components.\n3. Greedy budget-constrained scheduler selects optimal diagnostic set per cycle.\n4. Preemption correctly interrupts background diagnostics for critical ones.\n5. Storm protection activates at 3x budget sustained over 2 windows.\n6. Scheduling telemetry emitted for every cycle with full decision trace.\n7. Dynamic budget adjustment activates on regime shift signal from bd-3u4.\n8. Verification script `scripts/check_voi_scheduler.py` with `--json` flag confirms all criteria.\n9. Evidence artifacts written to `artifacts/section_10_11/bd-2nt/`.\n\n## Key Dependencies\n\n- bd-3u4 (BOCPD regime detector) — triggers dynamic budget adjustment.\n- 10.13 telemetry namespace — scheduling events conform to telemetry schema.\n- 10.13 stable error namespace — scheduler errors use registered codes.\n- Health gate system (health_gate.rs) — diagnostic results feed health status.\n\n## Testing & Logging Requirements\n\n- Unit tests covering VOI scoring with mocked staleness, uncertainty, and history inputs.\n- Integration test with 15 mock diagnostics and a budget that forces selection of only the top 5.\n- Simulation test of a diagnostic storm scenario confirming conservative mode activation.\n- Property-based test confirming budget is never exceeded in any scheduling cycle.\n- Self-test mode with synthetic diagnostic workload and known optimal selection.\n- Structured logging: `voi.schedule_cycle`, `voi.diagnostic_selected`, `voi.diagnostic_deferred`, `voi.preemption`, `voi.storm_detected`, `voi.budget_adjusted` events.\n\n## Expected Artifacts\n\n- `docs/specs/section_10_11/bd-2nt_contract.md` — specification document.\n- `crates/franken-node/src/connector/diagnostic_registry.rs` — registry and scheduler.\n- `scripts/check_voi_scheduler.py` — verification script.\n- `tests/test_check_voi_scheduler.py` — unit tests.\n- `artifacts/section_10_11/bd-2nt/verification_evidence.json` — evidence.\n- `artifacts/section_10_11/bd-2nt/verification_summary.md` — summary.","acceptance_criteria":"AC for bd-2nt:\n1. Implement a VOI-budgeted (Value of Information) monitor scheduler that prioritizes expensive diagnostic probes based on their expected information gain relative to their cost, subject to a per-epoch diagnostic budget.\n2. Each diagnostic probe is registered with: cost (in abstract budget units), a VOI estimator function that returns the expected reduction in decision uncertainty if the probe is executed, and a staleness threshold after which the probe's cached result expires.\n3. The scheduler solves a knapsack-style selection each scheduling epoch: maximize total VOI subject to total cost <= budget. The selection algorithm runs in O(n log n) time using a greedy VOI/cost ratio ranking (with optional exact solver for small n).\n4. The diagnostic budget is configurable per epoch (default: 100 units) and may be dynamically adjusted by the BOCPD regime detector (bd-3u4): regime-change events temporarily increase the budget by a configurable multiplier (default: 3x) for K epochs to accelerate diagnosis.\n5. Probes that have not been executed within their staleness threshold are promoted to mandatory priority (always scheduled regardless of VOI, consuming budget).\n6. The scheduler emits a per-epoch ScheduleReport JSON containing: probes selected, probes skipped, total VOI captured, total cost consumed, budget remaining, and any mandatory promotions.\n7. Unit tests verify: (a) highest-VOI/cost probes are selected first, (b) budget constraint is respected, (c) stale probes are force-promoted, (d) regime-change budget boost increases the number of probes scheduled, (e) zero-budget epoch schedules only mandatory stale probes.\n8. Integration test: a simulated monitoring scenario with 20 probes of varying cost/VOI demonstrates that the scheduler captures at least 80% of the theoretical maximum VOI given the budget.\n9. Structured log events: VOI_SCHEDULE_EPOCH / VOI_PROBE_SELECTED / VOI_PROBE_SKIPPED / VOI_BUDGET_BOOST / VOI_STALE_PROMOTION with probe_id, voi_score, cost, and epoch_id.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:36:50.387047590Z","created_by":"ubuntu","updated_at":"2026-02-20T17:01:40.367296949Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-11","self-contained","test-obligations"]}
{"id":"bd-2o8b","title":"[10.17] Implement heterogeneous hardware planner with policy-evidenced placements.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.17 — Radical Expansion Execution Track (9K)\n\nWhy This Exists:\nRadical expansion track for proof-carrying speculation, Bayesian adversary control, time-travel replay, ZK attestations, and claim compiler systems.\n\nTask Objective:\nImplement heterogeneous hardware planner with policy-evidenced placements.\n\nAcceptance Criteria:\n- Placement decisions satisfy capability/risk constraints and remain reproducible from identical inputs; planner reports policy reasoning and fallback path on resource contention; dispatch executes through approved runtime/engine interfaces.\n\nExpected Artifacts:\n- `docs/architecture/hardware_execution_planner.md`, `src/runtime/hardware_planner.rs`, `tests/perf/hardware_planner_policy_conformance.rs`, `artifacts/10.17/hardware_placement_trace.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_17/bd-2o8b/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_17/bd-2o8b/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.17] Implement heterogeneous hardware planner with policy-evidenced placements.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.17] Implement heterogeneous hardware planner with policy-evidenced placements.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.17] Implement heterogeneous hardware planner with policy-evidenced placements.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.17] Implement heterogeneous hardware planner with policy-evidenced placements.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.17] Implement heterogeneous hardware planner with policy-evidenced placements.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"- Placement decisions satisfy capability/risk constraints and remain reproducible from identical inputs; planner reports policy reasoning and fallback path on resource contention; dispatch executes through approved runtime/engine interfaces.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:37:03.922820576Z","created_by":"ubuntu","updated_at":"2026-02-20T15:45:58.383935289Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-17","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-2o8b","depends_on_id":"bd-nbwo","type":"blocks","created_at":"2026-02-20T07:43:18.784240166Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2ona","title":"[10.14] Add evidence-ledger replay validator that reproduces chosen action from captured inputs.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.14 — FrankenSQLite Deep-Mined Expansion Execution Track (9J)\n\nWhy This Exists:\nFrankenSQLite deep-mined control integrity track: evidence ledgers, proofs, epochs, remote effects, markers/MMR, and deterministic fault labs.\n\nTask Objective:\nAdd evidence-ledger replay validator that reproduces chosen action from captured inputs.\n\nAcceptance Criteria:\n- Validator deterministically replays recorded decision contexts; mismatches are reported with minimal diff; replay passes on canonical fixtures.\n\nExpected Artifacts:\n- `src/tools/evidence_replay_validator.rs`, `tests/conformance/evidence_replay_validator.rs`, `artifacts/10.14/evidence_replay_results.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_14/bd-2ona/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_14/bd-2ona/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.14] Add evidence-ledger replay validator that reproduces chosen action from captured inputs.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.14] Add evidence-ledger replay validator that reproduces chosen action from captured inputs.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.14] Add evidence-ledger replay validator that reproduces chosen action from captured inputs.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.14] Add evidence-ledger replay validator that reproduces chosen action from captured inputs.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.14] Add evidence-ledger replay validator that reproduces chosen action from captured inputs.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"Validator deterministically replays recorded decision contexts; mismatches are reported with minimal diff; replay passes on canonical fixtures.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:36:55.468653651Z","created_by":"ubuntu","updated_at":"2026-02-20T16:23:41.306458723Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-14","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-2ona","depends_on_id":"bd-2e73","type":"blocks","created_at":"2026-02-20T16:23:40.924877322Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2ona","depends_on_id":"bd-oolt","type":"blocks","created_at":"2026-02-20T16:23:41.306367202Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2ona","depends_on_id":"bd-sddz","type":"blocks","created_at":"2026-02-20T16:23:41.119480426Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2owx","title":"[10.16] Publish substrate policy contract for `frankentui`, `frankensqlite`, `sqlmodel_rust`, and `fastapi_rust`.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.16 — Adjacent Substrate Integration Execution Track (8.7)\n\nWhy This Exists:\nAdjacent substrate integration track to enforce deep composition with frankentui, frankensqlite, sqlmodel_rust, and fastapi_rust.\n\nTask Objective:\nPublish substrate policy contract for `frankentui`, `frankensqlite`, `sqlmodel_rust`, and `fastapi_rust`.\n\nAcceptance Criteria:\n- Policy contract defines mandatory/should-use scopes, exceptions, and waiver process; CI can parse contract metadata.\n\nExpected Artifacts:\n- `docs/architecture/adjacent_substrate_policy.md`, `artifacts/10.16/adjacent_substrate_policy_manifest.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_16/bd-2owx/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_16/bd-2owx/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.16] Publish substrate policy contract for `frankentui`, `frankensqlite`, `sqlmodel_rust`, and `fastapi_rust`.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.16] Publish substrate policy contract for `frankentui`, `frankensqlite`, `sqlmodel_rust`, and `fastapi_rust`.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.16] Publish substrate policy contract for `frankentui`, `frankensqlite`, `sqlmodel_rust`, and `fastapi_rust`.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.16] Publish substrate policy contract for `frankentui`, `frankensqlite`, `sqlmodel_rust`, and `fastapi_rust`.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.16] Publish substrate policy contract for `frankentui`, `frankensqlite`, `sqlmodel_rust`, and `fastapi_rust`.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"- Policy contract defines mandatory/should-use scopes, exceptions, and waiver process; CI can parse contract metadata.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:37:01.526840450Z","created_by":"ubuntu","updated_at":"2026-02-20T15:45:48.543572439Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-16","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-2owx","depends_on_id":"bd-1ow","type":"blocks","created_at":"2026-02-20T07:46:33.512084532Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2owx","depends_on_id":"bd-3qo","type":"blocks","created_at":"2026-02-20T07:46:33.557665779Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2owx","depends_on_id":"bd-cda","type":"blocks","created_at":"2026-02-20T07:46:33.602458817Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2ozr","title":"[10.19] Implement poisoning-resilient aggregation and outlier-robust global prior updates.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.19 — Adversarial Trust Commons Execution Track (9M)\n\nWhy This Exists:\nATC federated intelligence track for privacy-preserving cross-deployment threat learning, robust aggregation, and verifier-backed trust metrics.\n\nTask Objective:\nImplement poisoning-resilient aggregation and outlier-robust global prior updates.\n\nAcceptance Criteria:\n- Aggregation resists bounded adversarial submissions per policy assumptions; poisoning test suites show bounded degradation and fail-closed behavior on threshold breach.\n\nExpected Artifacts:\n- `docs/security/atc_poisoning_resilience.md`, `tests/security/atc_poisoning_attack_suite.rs`, `artifacts/10.19/atc_poisoning_resilience_results.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_19/bd-2ozr/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_19/bd-2ozr/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.19] Implement poisoning-resilient aggregation and outlier-robust global prior updates.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.19] Implement poisoning-resilient aggregation and outlier-robust global prior updates.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.19] Implement poisoning-resilient aggregation and outlier-robust global prior updates.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.19] Implement poisoning-resilient aggregation and outlier-robust global prior updates.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.19] Implement poisoning-resilient aggregation and outlier-robust global prior updates.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"- Aggregation resists bounded adversarial submissions per policy assumptions; poisoning test suites show bounded degradation and fail-closed behavior on threshold breach.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:37:05.755245088Z","created_by":"ubuntu","updated_at":"2026-02-20T17:00:54.090011447Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-19","self-contained","test-obligations"]}
{"id":"bd-2ps7","title":"[14] Metric family: adversarial resilience","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 14\n\nTask Objective:\nInstrument adversarial resilience metric family across evolving campaign corpora.\n\nExecution Requirements:\n- Make this requirement machine-verifiable and release-gated where applicable.\n- Keep behavior self-documenting so future contributors do not need to re-open the master plan.\n\nTesting & Logging Requirements:\n- Unit tests for rule/contract logic and error conditions.\n- Integration/E2E tests for workflow-level enforcement.\n- Structured logs with stable codes and trace IDs.\n- Reproducible artifacts that prove contract satisfaction.\n\nAcceptance Criteria:\n- Success and failure invariants for [14] Metric family: adversarial resilience are explicitly quantified and machine-verifiable.\n- Determinism requirements for [14] Metric family: adversarial resilience are validated across repeated runs and adversarial perturbations.\n\n\nExpected Artifacts:\n- Machine-readable verification artifact with explicit canonical path under artifacts/section_14/bd-2ps7/verification_evidence.json, suitable for CI/release gating.\n- Human-readable verification summary with explicit canonical path under artifacts/section_14/bd-2ps7/verification_summary.md, linking implementation intent to measured validation outcomes.\n\nTask-Specific Clarification:\n- The implementation must deliver the exact capability named in the title, with no scope dilution and no silent feature omission.\n- Validation artifacts must include capability-specific thresholds, pass/fail criteria, and reproducible evidence bundles tied to this bead ID.\n- Program/section verification gates must be able to consume this bead's outputs without manual interpretation.\n\nWhy This Improves User Outcomes:\n- \"[14] Metric family: adversarial resilience\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[14] Metric family: adversarial resilience\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"METRIC FAMILY: Adversarial resilience under evolving campaigns.\n1. Metrics measured: (a) detection rate of known attack patterns (%), (b) detection rate of novel/mutated attacks (%), (c) false positive rate under adversarial noise (%), (d) time-to-adapt (how quickly defenses update after new attack pattern identified), (e) resilience decay (detection rate over successive attack rounds).\n2. Attack campaign types: signal poisoning, Sybil attacks, mimicry/camouflage, supply-chain injection, evasion mutation.\n3. Detection gates: known patterns >= 95% detection, novel patterns >= 70% detection, false positive rate <= 2%.\n4. Time-to-adapt: defenses incorporate new attack patterns within 24 hours of identification (measured in CI simulation).\n5. Resilience decay: detection rate does not drop > 10% over 10 successive adversarial rounds.\n6. Measured under multi-round adaptive adversary simulation (adversary evolves strategy each round).\n7. Publication: adversarial resilience metrics in benchmark report with per-campaign-type breakdown.\n8. Evidence: adversarial_resilience_metrics.json with per-attack-type detection rates, FP rates, and round-by-round decay.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:39:35.985600891Z","created_by":"ubuntu","updated_at":"2026-02-20T15:25:19.038325820Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-14","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-2ps7","depends_on_id":"bd-jbp1","type":"blocks","created_at":"2026-02-20T07:43:26.113555718Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2pu","title":"[10.7] Add external-reproduction playbook and automation scripts.","description":"## [10.7] External-Reproduction Playbook and Automation Scripts\n\n### Why This Exists\n\nSection 13 defines a key success criterion: \">= 2 independent external reproductions\" of franken_node's headline claims (compatibility with Node.js, security properties, performance benchmarks). External parties — academic researchers, independent auditors, competing runtime teams — must be able to reproduce results without insider knowledge or access to internal tooling. This bead creates a self-contained reproduction playbook and automation scripts that make independent verification a one-command operation, removing barriers to external validation and building trust in franken_node's claims.\n\n### What It Must Do\n\n**Reproduction playbook**: A comprehensive written guide (`docs/reproduction_playbook.md`) covering: (1) environment setup (OS requirements, toolchain installation, dependency list with pinned versions), (2) fixture download (where to get test fixtures, expected checksums), (3) benchmark execution (exact commands, expected duration, resource requirements), (4) result comparison (how to interpret outputs, what constitutes a pass/fail, acceptable variance ranges for performance numbers), and (5) troubleshooting (common issues and solutions).\n\n**Automation scripts**: A single entry point (`scripts/reproduce.sh` or `scripts/reproduce.py`) that automates the entire playbook: installs dependencies (with user confirmation), downloads fixtures, runs all verification suites, collects results, and produces a structured reproduction report. The script is idempotent and can be re-run safely. It supports `--skip-install` for environments where dependencies are pre-installed.\n\n**Headline claims registry**: A machine-readable file (`docs/headline_claims.toml`) listing each headline claim with: claim text, verification method, acceptance threshold, and reference to the specific test/benchmark that validates it. The reproduction script uses this registry to determine what to run and how to evaluate results.\n\n**Reproduction report**: The automation script produces a structured report (`reproduction_report.json`) containing: environment fingerprint (OS, CPU, memory, toolchain versions), claim-by-claim results (claim text, measured value, threshold, pass/fail), overall verdict, timestamp, and duration. This report is designed to be shared with the franken_node team as evidence of independent reproduction.\n\n**Minimal external dependencies**: The reproduction flow must work with only standard toolchain installations (Rust, Node.js, Python). It must not require access to internal CI systems, private registries, or proprietary tools. All fixtures are either generated on the fly or downloadable from public URLs.\n\n### Acceptance Criteria\n\n1. `docs/reproduction_playbook.md` provides complete step-by-step instructions for independent reproduction, assuming no insider knowledge.\n2. `scripts/reproduce.sh` (or `.py`) automates the full reproduction flow as a single command with structured JSON output.\n3. `docs/headline_claims.toml` lists every headline claim with verification method, acceptance threshold, and test reference.\n4. Reproduction report (`reproduction_report.json`) includes environment fingerprint, per-claim results, overall verdict, and timestamp.\n5. The reproduction flow works with only publicly available tools and fixtures — no internal CI access required.\n6. Script is idempotent and supports `--skip-install` for pre-configured environments.\n7. Verification script `scripts/check_reproduction_playbook.py` with `--json` flag validates playbook completeness, claim registry coverage, and script functionality.\n8. Unit tests in `tests/test_check_reproduction_playbook.py` cover claim registry parsing, report generation, environment fingerprinting, and threshold evaluation.\n\n### Key Dependencies\n\n- All verification gates from 10.2-10.6 (these produce the results being reproduced).\n- Performance benchmarks from bd-3lh (latency gates).\n- Compatibility test suite from 10.2.\n- Public fixture hosting (or fixture generation scripts).\n\n### Testing & Logging Requirements\n\n- Dry-run test: run `reproduce.sh --dry-run` to verify it lists all steps without executing.\n- Claim coverage test: verify every headline claim in the registry has a corresponding executable test.\n- Report schema test: verify reproduction report conforms to its JSON schema.\n- Environment fingerprint test: verify fingerprint captures OS, CPU, memory, and toolchain versions.\n- Structured JSON logs for each reproduction step: step name, duration, result, any warnings.\n\n### Expected Artifacts\n\n- `docs/reproduction_playbook.md` — written guide.\n- `docs/headline_claims.toml` — claim registry.\n- `scripts/reproduce.sh` (or `.py`) — automation script.\n- `scripts/check_reproduction_playbook.py` — verification script.\n- `tests/test_check_reproduction_playbook.py` — unit tests.\n- `artifacts/section_10_7/bd-2pu/verification_evidence.json` — gate results.\n- `artifacts/section_10_7/bd-2pu/verification_summary.md` — human-readable summary.\n\n### Additional E2E Requirement\n\n- E2E test scripts must execute full end-to-end workflows from clean fixtures, emit deterministic machine-readable pass/fail evidence, and capture stepwise structured logs for root-cause triage.","acceptance_criteria":"1. External-reproduction playbook is a standalone Markdown document under docs/ that enables a third party to reproduce all verification results from scratch.\n2. Playbook requires only: git clone, a supported Rust toolchain, and standard POSIX tools — no internal infrastructure dependencies.\n3. Automation scripts (scripts/reproduce_all.sh or equivalent) execute the full reproduction pipeline with a single command.\n4. Reproduction output is diff-comparable against published verification evidence: script exits 0 if results match, non-zero with a diff on mismatch.\n5. Playbook includes troubleshooting section for common environment differences (OS, toolchain version, locale).\n6. Scripts support a --subset flag to reproduce a specific section or bead's evidence without running the entire suite.\n7. At least one CI job runs the external-reproduction pipeline on a clean container image to validate the playbook stays current.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:36:47.670221550Z","created_by":"ubuntu","updated_at":"2026-02-20T17:08:34.791392853Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-7","self-contained","test-obligations"]}
{"id":"bd-2pw","title":"[10.6] Add artifact signing and checksum verification for releases.","description":"## [10.6] Artifact Signing and Checksum Verification for Releases\n\n### Why This Exists\n\nSupply-chain integrity is a first-class requirement for franken_node, per Section 9I.9. Every release artifact — binary, configuration bundle, compliance evidence archive — must be cryptographically signed so that consumers can verify authenticity and detect tampering before installation. Without verifiable signatures and checksums, downstream users cannot distinguish legitimate releases from compromised ones. This bead establishes the signing infrastructure, verification CLI, and CI gates that enforce integrity for all release artifacts.\n\n### What It Must Do\n\n**Signing**: Every release artifact is signed using Ed25519 (or equivalent modern scheme). The signing key is managed outside the repository (CI secret or HSM reference). Where threshold signatures are configured via 10.13's fencing protocol, multi-party signing is used — no single key holder can produce a valid release signature. The signing step produces a detached signature file (`.sig`) and a checksums manifest (`SHA256SUMS`) for each artifact.\n\n**Checksum manifest**: A `SHA256SUMS` file lists every artifact with its SHA-256 hash. The manifest itself is signed. This provides two layers of verification: individual file checksums and manifest-level signature.\n\n**Verification CLI**: `franken-node verify-release <path>` validates: (1) the manifest signature against the known public key, (2) each file's SHA-256 hash matches the manifest entry, (3) individual `.sig` files are valid for their corresponding artifacts. The command exits 0 on success, non-zero on any failure, with structured JSON output describing each check's result.\n\n**CI gate**: The release pipeline refuses to publish artifacts that lack valid signatures or checksums. The gate runs the verification CLI against the staged release before upload.\n\n**Key rotation**: Public keys are versioned. The verification CLI accepts a key directory containing current and previous public keys, identified by key ID embedded in the signature metadata. Key rotation requires a signed transition record (old key signs endorsement of new key).\n\n### Acceptance Criteria\n\n1. Every release artifact has a corresponding `.sig` (detached Ed25519 signature) and is listed in a signed `SHA256SUMS` manifest.\n2. `franken-node verify-release <path>` validates manifest signature, individual checksums, and individual signatures, exiting non-zero on any failure.\n3. Verification output is structured JSON with per-artifact pass/fail status, key ID used, and failure reason if applicable.\n4. CI release gate blocks publication of unsigned or checksum-mismatched artifacts.\n5. Threshold signing is supported when configured (requires M-of-N partial signatures to produce a valid release signature).\n6. Key rotation is supported via signed transition records; old keys remain valid for artifacts signed before rotation.\n7. Verification script `scripts/check_artifact_signing.py` with `--json` flag validates the signing infrastructure.\n8. Unit tests in `tests/test_check_artifact_signing.py` cover signature generation, verification, checksum computation, manifest parsing, key rotation, and threshold signing logic.\n\n### Key Dependencies\n\n- Threshold signature infrastructure from 10.13 fencing protocol (optional, for multi-party signing).\n- Release pipeline (CI/CD) for gate integration.\n- Ed25519 signing library (e.g., `ed25519-dalek` in Rust).\n\n### Testing & Logging Requirements\n\n- Round-trip test: sign an artifact, verify it, assert success.\n- Tamper test: sign an artifact, modify one byte, verify, assert failure with clear error.\n- Manifest consistency test: add an artifact without updating the manifest, verify, assert failure.\n- Key rotation test: rotate keys, verify old artifact with old key and new artifact with new key.\n- Structured JSON logs for every signing and verification operation: artifact name, key ID, operation, result.\n\n### Expected Artifacts\n\n- Signing module in `crates/franken-node/src/` or `scripts/`.\n- Verification CLI subcommand (`verify-release`).\n- `scripts/check_artifact_signing.py` — verification script.\n- `tests/test_check_artifact_signing.py` — unit tests.\n- `artifacts/section_10_6/bd-2pw/verification_evidence.json` — gate results.\n- `artifacts/section_10_6/bd-2pw/verification_summary.md` — human-readable summary.","acceptance_criteria":"1. All release artifacts (binaries, tarballs, packages) are signed with a project-controlled key using a documented signing scheme (e.g., minisign, cosign, or GPG).\n2. SHA-256 checksums are generated for every release artifact and published alongside the artifact in a CHECKSUMS.txt file.\n3. A verification command (e.g., scripts/verify_release.sh) validates both signature and checksum for any downloaded artifact.\n4. CI pipeline automatically signs and checksums artifacts during the release build step — no manual signing allowed.\n5. Signature key rotation procedure is documented, including how to verify artifacts signed with prior keys.\n6. Verification script returns a structured JSON result with pass/fail status and details per artifact.\n7. Tampered artifacts (bit-flip test) are correctly rejected by the verification command.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:36:47.101599959Z","created_by":"ubuntu","updated_at":"2026-02-20T16:59:02.299133805Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-6","self-contained","test-obligations"]}
{"id":"bd-2q5","title":"[10.6] Optimize migration scanner throughput for large monorepos.","description":"## [10.6] Optimize Migration Scanner Throughput for Large Monorepos\n\n### Why This Exists\n\nSection 9D.2 mandates \"optimize scan and transform throughput with deterministic batching and cache reuse.\" The migration scanner (10.3) must handle monorepos with 10,000+ source files without unacceptable wall-clock time. Current scanning is sequential and re-scans unchanged files on every run. For enterprise adoption, migration operations must complete within minutes, not hours, even on the largest codebases. This bead makes the scanner production-ready for monorepo-scale workloads.\n\n### What It Must Do\n\nImplement three optimization strategies for the migration scanner:\n\n**Incremental scanning**: Maintain a scan cache that records file path, content hash, and last scan result. On subsequent runs, only re-scan files whose content hash has changed. Cache invalidation must be deterministic — given the same file set and content, the cache always produces the same result regardless of run order or timing. The cache format is versioned so upgrades don't silently use stale data.\n\n**Parallel file processing**: Partition the file set into batches and process batches in parallel using a configurable worker pool. Batch assignment must be deterministic (sorted by path, then chunked) so results are reproducible. The worker count defaults to available CPU cores but is overridable via `--workers N`.\n\n**Cache reuse across runs**: The scan cache persists to disk between runs (default location: `.franken_node/scan_cache.json`). Cache entries expire after a configurable TTL (default: 7 days) to prevent unbounded growth. A `--clear-cache` flag forces a full rescan.\n\nBefore/after benchmarks must be collected on representative monorepo fixtures (synthetic fixture with 10k+ files of varying sizes and complexity). Benchmarks measure wall-clock time, files-per-second throughput, cache hit ratio, and peak memory.\n\n### Acceptance Criteria\n\n1. Incremental scanning skips files whose content hash matches the cached hash; a 10k-file re-scan with 1% changes completes in under 10% of the full-scan time.\n2. Parallel processing with N workers achieves near-linear speedup up to 4 workers on the reference fixture.\n3. Deterministic batching: two runs with identical input produce byte-identical scan results regardless of worker count.\n4. Scan cache persists to `.franken_node/scan_cache.json` with a versioned format; format version mismatch triggers full rescan with a warning.\n5. Cache entries expire after configurable TTL; `--clear-cache` forces full rescan.\n6. Before/after benchmarks recorded in `artifacts/section_10_6/bd-2q5/benchmark_comparison.json`.\n7. Verification script `scripts/check_scanner_throughput.py` with `--json` flag validates throughput targets.\n8. Unit tests in `tests/test_check_scanner_throughput.py` cover cache logic, batch partitioning, hash computation, and TTL expiration.\n\n### Key Dependencies\n\n- Migration scanner from 10.3.\n- Synthetic monorepo fixture generator (or checked-in fixture).\n- Content hashing library (SHA-256 or BLAKE3).\n\n### Testing & Logging Requirements\n\n- Determinism test: run twice with same input and different worker counts, assert identical results.\n- Cache hit test: run, modify one file, re-run, assert only modified file is re-scanned.\n- TTL test: create cache entry with expired TTL, run, assert re-scan occurs.\n- Structured JSON logs for each run: total files, cache hits, cache misses, workers used, wall-clock time, files/second.\n\n### Expected Artifacts\n\n- Optimized scanner code in `crates/franken-node/src/` or `scripts/`.\n- `scripts/check_scanner_throughput.py` — verification script.\n- `tests/test_check_scanner_throughput.py` — unit tests.\n- `artifacts/section_10_6/bd-2q5/benchmark_comparison.json` — before/after benchmarks.\n- `artifacts/section_10_6/bd-2q5/verification_evidence.json` — gate results.\n- `artifacts/section_10_6/bd-2q5/verification_summary.md` — human-readable summary.\n\n### Additional E2E Requirement\n\n- E2E test scripts must execute full end-to-end workflows from clean fixtures, emit deterministic machine-readable pass/fail evidence, and capture stepwise structured logs for root-cause triage.","acceptance_criteria":"1. Migration scanner throughput on a 10k-file monorepo benchmark completes within a defined time ceiling (documented in spec).\n2. Scanner supports incremental mode: re-scanning after a single-file change processes only the delta, not the full tree.\n3. Before/after throughput comparison table is produced per Section 7 performance doctrine.\n4. Memory usage stays bounded (no O(n^2) growth) — verified with a memory profile artifact for the large-monorepo benchmark.\n5. Parallelism: scanner utilizes available CPU cores (configurable concurrency level) for file traversal and analysis.\n6. Correctness proof: scanner output on the golden corpus is identical before and after optimization.\n7. Benchmark is reproducible from a clean checkout with a single command and produces structured JSON results.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:36:46.936795081Z","created_by":"ubuntu","updated_at":"2026-02-20T17:08:36.274883671Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-6","self-contained","test-obligations"]}
{"id":"bd-2qf","title":"[10.2] Implement compatibility behavior registry with typed shim metadata.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.2 — Compatibility Core\n\nWhy This Exists:\nCompatibility core as strategic wedge: policy-visible behavior, divergence receipts, L1/L2 lockstep, and spec-first fixture governance.\n\nTask Objective:\nImplement compatibility behavior registry with typed shim metadata.\n\nAcceptance Criteria:\n- Implement with explicit success/failure invariants and deterministic behavior under normal, edge, and fault scenarios.\n- Ensure outcomes are verifiable via automated tests and reproducible artifact outputs.\n\nExpected Artifacts:\n- Section-owned design/spec artifact at `docs/specs/section_10_2/bd-2qf_contract.md`, capturing decision rationale, invariants, and interface boundaries.\n- Machine-readable verification artifact at `artifacts/section_10_2/bd-2qf/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_2/bd-2qf/verification_summary.md` linking implementation intent to measured outcomes.\n\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.2] Implement compatibility behavior registry with typed shim metadata.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.2] Implement compatibility behavior registry with typed shim metadata.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.2] Implement compatibility behavior registry with typed shim metadata.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.2] Implement compatibility behavior registry with typed shim metadata.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.2] Implement compatibility behavior registry with typed shim metadata.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","status":"closed","priority":1,"issue_type":"task","assignee":"CrimsonCrane","created_at":"2026-02-20T07:36:43.914447842Z","created_by":"ubuntu","updated_at":"2026-02-20T09:34:03.425939378Z","closed_at":"2026-02-20T09:34:03.425914892Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-2","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-2qf","depends_on_id":"bd-2wz","type":"blocks","created_at":"2026-02-20T07:43:20.095228292Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2qmf","title":"Epic: Asupersync Lab + Release Gates [10.15d]","description":"- type: epic","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-20T07:47:58.072163029Z","updated_at":"2026-02-20T07:49:21.272669298Z","closed_at":"2026-02-20T07:49:21.272649331Z","close_reason":"Superseded by canonical detailed master-plan graph (bd-33v) with full 10.N-10.21 and 11-16 coverage, explicit dependencies, and per-section verification gates.","source_repo":".","compaction_level":0,"original_size":0}
{"id":"bd-2qqu","title":"[10.14] Implement virtual transport fault harness (drop/reorder/corrupt) for remote-control protocol testing.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.14 — FrankenSQLite Deep-Mined Expansion Execution Track (9J)\n\nWhy This Exists:\nFrankenSQLite deep-mined control integrity track: evidence ledgers, proofs, epochs, remote effects, markers/MMR, and deterministic fault labs.\n\nTask Objective:\nImplement virtual transport fault harness (drop/reorder/corrupt) for remote-control protocol testing.\n\nAcceptance Criteria:\n- Harness supports deterministic fault schedules from seed; scenarios cover drop/reorder/corrupt classes; reproductions include exact fault sequence.\n\nExpected Artifacts:\n- `tests/harness/virtual_transport_faults.rs`, `docs/testing/virtual_transport_harness.md`, `artifacts/10.14/virtual_fault_campaign_results.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_14/bd-2qqu/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_14/bd-2qqu/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.14] Implement virtual transport fault harness (drop/reorder/corrupt) for remote-control protocol testing.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.14] Implement virtual transport fault harness (drop/reorder/corrupt) for remote-control protocol testing.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.14] Implement virtual transport fault harness (drop/reorder/corrupt) for remote-control protocol testing.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.14] Implement virtual transport fault harness (drop/reorder/corrupt) for remote-control protocol testing.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.14] Implement virtual transport fault harness (drop/reorder/corrupt) for remote-control protocol testing.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"Harness supports deterministic fault schedules from seed; scenarios cover drop/reorder/corrupt classes; reproductions include exact fault sequence.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:36:59.145618233Z","created_by":"ubuntu","updated_at":"2026-02-20T16:24:29.742665427Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-14","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-2qqu","depends_on_id":"bd-1nfu","type":"blocks","created_at":"2026-02-20T16:24:29.742582332Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2rg1","title":"Epic: Epoch Management + Marker Streams [10.14g]","description":"- type: epic","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-20T07:47:58.072163029Z","updated_at":"2026-02-20T07:49:21.244772489Z","closed_at":"2026-02-20T07:49:21.244751930Z","close_reason":"Superseded by canonical detailed master-plan graph (bd-33v) with full 10.N-10.21 and 11-16 coverage, explicit dependencies, and per-section verification gates.","source_repo":".","compaction_level":0,"original_size":0}
{"id":"bd-2rwm","title":"Epic: Verifiable Execution Fabric (VEF) [10.18]","description":"- type: epic","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-20T07:47:58.072163029Z","updated_at":"2026-02-20T07:49:21.305829281Z","closed_at":"2026-02-20T07:49:21.305808733Z","close_reason":"Superseded by canonical detailed master-plan graph (bd-33v) with full 10.N-10.21 and 11-16 coverage, explicit dependencies, and per-section verification gates.","source_repo":".","compaction_level":0,"original_size":0}
{"id":"bd-2sbj","title":"Epic: Radical Expansion - Advanced Security [10.17c]","description":"- type: epic","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-20T07:47:58.072163029Z","updated_at":"2026-02-20T07:49:21.294762809Z","closed_at":"2026-02-20T07:49:21.294743764Z","close_reason":"Superseded by canonical detailed master-plan graph (bd-33v) with full 10.N-10.21 and 11-16 coverage, explicit dependencies, and per-section verification gates.","source_repo":".","compaction_level":0,"original_size":0}
{"id":"bd-2st","title":"[10.3] Build migration validation runner with lockstep checks.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.3 — Migration System\n\nWhy This Exists:\nMigration autopilot pipeline from audit to rollout, designed to collapse migration friction with deterministic confidence and replayability.\n\nTask Objective:\nBuild migration validation runner with lockstep checks.\n\nAcceptance Criteria:\n- Implement with explicit success/failure invariants and deterministic behavior under normal, edge, and fault scenarios.\n- Ensure outcomes are verifiable via automated tests and reproducible artifact outputs.\n\nExpected Artifacts:\n- Section-owned design/spec artifact at `docs/specs/section_10_3/bd-2st_contract.md`, capturing decision rationale, invariants, and interface boundaries.\n- Machine-readable verification artifact at `artifacts/section_10_3/bd-2st/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_3/bd-2st/verification_summary.md` linking implementation intent to measured outcomes.\n\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.3] Build migration validation runner with lockstep checks.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.3] Build migration validation runner with lockstep checks.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.3] Build migration validation runner with lockstep checks.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.3] Build migration validation runner with lockstep checks.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.3] Build migration validation runner with lockstep checks.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","status":"closed","priority":1,"issue_type":"task","assignee":"CrimsonCrane","created_at":"2026-02-20T07:36:45.035952247Z","created_by":"ubuntu","updated_at":"2026-02-20T10:14:47.613427595Z","closed_at":"2026-02-20T10:14:47.613402448Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-3","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-2st","depends_on_id":"bd-2ew","type":"blocks","created_at":"2026-02-20T07:43:22.142282181Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2sx","title":"[10.10] Integrate canonical revocation freshness semantics (from `10.13`) before risky and dangerous product actions.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md — Section 10.10 (FCP-Inspired Hardening), cross-ref Section 9E.7\n\n## Why This Exists\n\nEnhancement Map 9E.7 requires revocation freshness semantics for product-level execution gates. Section 10.13 defines the canonical revocation freshness primitive (bd-1m8r) that enforces freshness checks before risky and dangerous actions at the trust-primitive level. This bead integrates that primitive into every product-level execution gate in franken_node, ensuring that before any risky action (migration execution, policy deployment, zone boundary change, token delegation to external parties) proceeds, the system verifies that all relevant credentials, keys, and authorizations have not been revoked as of a provably recent timestamp. Without this integration, a revoked operator credential or compromised key could continue to authorize dangerous actions during the window between revocation and the next periodic check — a gap that this bead eliminates by making freshness checks synchronous and mandatory.\n\n## What This Must Do\n\n1. Define a `RevocationFreshnessGate` trait that every risky product action must pass through before execution, with a configurable freshness threshold per safety tier (critical actions: freshness within 1 epoch, standard actions: within 5 epochs, advisory actions: within 10 epochs).\n2. Integrate with 10.13's canonical revocation freshness enforcer (bd-1m8r) to perform the actual freshness check, translating between product-level action categories and 10.13's safety tier classifications.\n3. Classify all product control actions into safety tiers: Tier-1/Critical (migration execution, policy deployment, key rotation, zone deletion), Tier-2/Standard (token delegation, configuration changes, session establishment with elevated privileges), Tier-3/Advisory (read-only queries against sensitive state, audit log exports).\n4. Implement a `FreshnessProof` struct that is returned by the gate and must be threaded through to the action executor as a proof-of-check — the executor refuses to run without a valid, unexpired `FreshnessProof`.\n5. Ensure freshness checks operate within authenticated sessions (from bd-oty): the revocation check itself must be protected against replay and injection.\n6. Implement graceful degradation: if the revocation service is unreachable, Tier-1 actions are blocked (fail-closed), Tier-2 actions are blocked with operator-override capability (signed emergency bypass), and Tier-3 actions proceed with a warning log.\n\n## Context from Enhancement Maps\n\n- 9E.7: \"Revocation freshness semantics for product-level execution gates\"\n- 9E.6 (cross-ref): Session-authenticated channels (bd-oty) protect the freshness check itself from tampering.\n- 9E.4 (cross-ref): Token chain verification (bd-1r2) should include revocation freshness for each token in the chain, but that depth is optional for this bead — the gate operates at the action level.\n- 9A.2 (Observability): Freshness check failures are high-priority structured events for security monitoring.\n\n## Dependencies\n\n- Upstream: bd-1m8r ([10.13] Enforce revocation freshness per safety tier before risky and dangerous actions) — provides the canonical revocation freshness primitive.\n- Upstream: bd-oty ([10.10] Integrate canonical session-authenticated control channel) — freshness checks must occur within authenticated sessions.\n- Downstream: bd-1vp ([10.10] Implement zone/tenant trust segmentation policies) — zone boundary operations require freshness-gated execution.\n- Downstream: bd-1jjq ([10.10] Section-wide verification gate) — aggregates verification evidence.\n\n## Acceptance Criteria\n\n1. Every Tier-1 (critical) product action passes through `RevocationFreshnessGate` before execution — no bypass paths exist (verified by exhaustive action enumeration and code path analysis).\n2. Freshness thresholds are configurable per safety tier and enforced: a Tier-1 action with a freshness proof older than 1 epoch is rejected with `FRESHNESS_STALE`.\n3. `FreshnessProof` is unforgeable: it is signed by the revocation service and includes a timestamp, checked-credentials list, and nonce — replay of an old proof is detected by nonce tracking.\n4. When the revocation service is unreachable: Tier-1 actions are blocked (fail-closed), Tier-2 actions require a signed emergency bypass, Tier-3 actions proceed with `FRESHNESS_DEGRADED` warning.\n5. Emergency bypass for Tier-2 actions requires an owner-signed authorization (not a role key) and is logged as `FRESHNESS_EMERGENCY_BYPASS` with full context.\n6. All freshness checks occur within authenticated sessions — an unauthenticated freshness check request is rejected before processing.\n7. End-to-end latency overhead of freshness gating is under 50ms for cached proofs and under 500ms for fresh checks (benchmark test required).\n8. Verification evidence JSON includes action tier classifications, freshness thresholds, degradation scenarios tested, and bypass audit counts.\n\n## Testing & Logging Requirements\n\n- Unit tests: Test each safety tier with fresh, stale, and borderline-fresh proofs. Test FreshnessProof validation: valid proof accepted, expired proof rejected, tampered proof rejected, replayed nonce rejected. Test tier classification for all known product actions. Test graceful degradation for each tier when revocation service is unavailable.\n- Integration tests: End-to-end: perform a Tier-1 action (e.g., migration execution) with a valid freshness proof — verify success. Repeat with a stale proof — verify rejection before execution begins. Test emergency bypass flow: revocation service down, Tier-2 action attempted, owner signs bypass, action proceeds with audit trail. Verify freshness checks use authenticated sessions from bd-oty.\n- Adversarial tests: Attempt to execute a Tier-1 action without any freshness proof. Attempt to forge a FreshnessProof with a valid-looking but unsigned timestamp. Attempt to replay a freshness proof from a previous epoch. Attempt to downgrade a Tier-1 action to Tier-3 to bypass freshness requirements. Test with a compromised revocation service that returns always-fresh responses for revoked credentials.\n- Structured logs: `FRESHNESS_CHECK_PASSED` (action, tier, proof_age_epochs, credentials_checked). `FRESHNESS_CHECK_FAILED` (action, tier, reason, proof_age_epochs, required_freshness). `FRESHNESS_DEGRADED` (tier, revocation_service_status, fallback_behavior). `FRESHNESS_EMERGENCY_BYPASS` (action, tier, authorizing_owner, bypass_reason). All events include `trace_id`, `epoch_id`, and `session_id`.\n\n## Expected Artifacts\n\n- docs/specs/section_10_10/bd-2sx_contract.md\n- crates/franken-node/src/connector/revocation_gate.rs (or similar module path)\n- scripts/check_revocation_freshness.py with --json flag and self_test()\n- tests/test_check_revocation_freshness.py\n- artifacts/section_10_10/bd-2sx/verification_evidence.json\n- artifacts/section_10_10/bd-2sx/verification_summary.md","acceptance_criteria":"1. Define a RevocationFreshnessCheck struct containing: (a) check_id (unique per invocation), (b) target_key_id or target_token_id being checked, (c) revocation_list_version (monotonic u64 from the canonical revocation registry in 10.13), (d) max_staleness_seconds (configurable per action risk tier), (e) check_timestamp (UTC), (f) result enum (FRESH, STALE, REVOKED, UNAVAILABLE).\n2. Define risk tiers for product actions: DANGEROUS (destructive migration, key deletion) requires max_staleness <= 10s; RISKY (rollback, policy change) requires max_staleness <= 60s; NORMAL (read, query) requires max_staleness <= 300s. These thresholds MUST be configurable but have these defaults.\n3. Implement a pre-action gate: before executing any DANGEROUS or RISKY action, call check_revocation_freshness(target_id, risk_tier) which: (a) fetches the latest revocation list version, (b) compares its age against the tier threshold, (c) returns STALE if the revocation list is older than the threshold, (d) returns REVOKED if the target appears in the revocation list, (e) returns UNAVAILABLE if the revocation source cannot be reached.\n4. Enforce fail-closed semantics: if the freshness check returns STALE, REVOKED, or UNAVAILABLE, the action MUST NOT proceed. Return a typed error (RevocationCheckFailed) with the check struct for audit.\n5. Implement a freshness cache with TTL equal to the risk tier threshold. Cache hits skip the network fetch but still validate staleness against the cached timestamp.\n6. Emit structured log for every freshness check with: check_id, target_id, risk_tier, result, latency_ms, and trace correlation ID.\n7. Unit tests: (a) FRESH result allows action, (b) STALE result blocks action, (c) REVOKED result blocks action, (d) UNAVAILABLE result blocks action (fail-closed), (e) cache hit within TTL, (f) cache miss after TTL expiry, (g) tier threshold configuration override.\n8. Integration test: simulate revocation list update delay, verify DANGEROUS action is blocked while NORMAL action proceeds.\n9. Verification: scripts/check_revocation_freshness.py --json, artifacts at artifacts/section_10_10/bd-2sx/.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:36:49.326230276Z","created_by":"ubuntu","updated_at":"2026-02-20T17:01:30.238170569Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-10","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-2sx","depends_on_id":"bd-1m8r","type":"blocks","created_at":"2026-02-20T14:59:52.112302923Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2t5u","title":"[10.13] Implement predictive pre-staging engine for high-probability offline artifacts.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.13 — FCP Deep-Mined Expansion Execution Track (9I)\n\nWhy This Exists:\nDeep connector/provider contract track: lifecycle FSMs, conformance harnesses, fencing, sandboxing, revocation freshness, and protocol hardening.\n\nTask Objective:\nImplement predictive pre-staging engine for high-probability offline artifacts.\n\nAcceptance Criteria:\n- Pre-staging model raises offline coverage on benchmark scenarios; budget limits prevent prefetch storms; prediction quality is measured and reported.\n\nExpected Artifacts:\n- `docs/specs/predictive_prestaging.md`, `tests/perf/prestaging_coverage_improvement.rs`, `artifacts/10.13/prestaging_model_report.csv`.\n\n- Machine-readable verification artifact at `artifacts/section_10_13/bd-2t5u/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_13/bd-2t5u/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.13] Implement predictive pre-staging engine for high-probability offline artifacts.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.13] Implement predictive pre-staging engine for high-probability offline artifacts.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.13] Implement predictive pre-staging engine for high-probability offline artifacts.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.13] Implement predictive pre-staging engine for high-probability offline artifacts.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.13] Implement predictive pre-staging engine for high-probability offline artifacts.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","status":"closed","priority":1,"issue_type":"task","assignee":"CrimsonCrane","created_at":"2026-02-20T07:36:53.680520990Z","created_by":"ubuntu","updated_at":"2026-02-20T12:32:43.613770964Z","closed_at":"2026-02-20T12:32:43.613743884Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-13","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-2t5u","depends_on_id":"bd-jxgt","type":"blocks","created_at":"2026-02-20T07:43:13.433328627Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2tdi","title":"[10.15] Migrate lifecycle/rollout orchestration to region-owned execution trees.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.15 — Asupersync-First Integration Execution Track (8.4-8.6)\n\nWhy This Exists:\nAsupersync-first control-plane migration track enforcing Cx-first, region ownership, cancellation correctness, and protocol-grade verification gates.\n\nTask Objective:\nMigrate lifecycle/rollout orchestration to region-owned execution trees.\n\nAcceptance Criteria:\n- Lifecycle orchestration runs under region ownership; region close implies quiescence in conformance tests.\n\nExpected Artifacts:\n- `tests/integration/region_owned_lifecycle.rs`, `docs/specs/region_tree_topology.md`, `artifacts/10.15/region_quiescence_trace.jsonl`.\n\n- Machine-readable verification artifact at `artifacts/section_10_15/bd-2tdi/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_15/bd-2tdi/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.15] Migrate lifecycle/rollout orchestration to region-owned execution trees.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.15] Migrate lifecycle/rollout orchestration to region-owned execution trees.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.15] Migrate lifecycle/rollout orchestration to region-owned execution trees.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.15] Migrate lifecycle/rollout orchestration to region-owned execution trees.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.15] Migrate lifecycle/rollout orchestration to region-owned execution trees.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"- Lifecycle orchestration runs under region ownership; region close implies quiescence in conformance tests.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:36:59.809633782Z","created_by":"ubuntu","updated_at":"2026-02-20T17:04:31.375596903Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-15","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-2tdi","depends_on_id":"bd-2177","type":"blocks","created_at":"2026-02-20T17:04:31.375546660Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2tua","title":"[10.16] Implement `frankensqlite` adapter layer for required `franken_node` persistence surfaces.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.16 — Adjacent Substrate Integration Execution Track (8.7)\n\nWhy This Exists:\nAdjacent substrate integration track to enforce deep composition with frankentui, frankensqlite, sqlmodel_rust, and fastapi_rust.\n\nTask Objective:\nImplement `frankensqlite` adapter layer for required `franken_node` persistence surfaces.\n\nAcceptance Criteria:\n- Required persistence APIs route through adapter; conformance tests validate deterministic read/write/replay semantics.\n\nExpected Artifacts:\n- `src/storage/frankensqlite_adapter.rs`, `tests/integration/frankensqlite_adapter_conformance.rs`, `artifacts/10.16/frankensqlite_adapter_report.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_16/bd-2tua/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_16/bd-2tua/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.16] Implement `frankensqlite` adapter layer for required `franken_node` persistence surfaces.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.16] Implement `frankensqlite` adapter layer for required `franken_node` persistence surfaces.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.16] Implement `frankensqlite` adapter layer for required `franken_node` persistence surfaces.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.16] Implement `frankensqlite` adapter layer for required `franken_node` persistence surfaces.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.16] Implement `frankensqlite` adapter layer for required `franken_node` persistence surfaces.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"- Required persistence APIs route through adapter; conformance tests validate deterministic read/write/replay semantics.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:37:02.018964620Z","created_by":"ubuntu","updated_at":"2026-02-20T17:05:17.289826553Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-16","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-2tua","depends_on_id":"bd-1a1j","type":"blocks","created_at":"2026-02-20T17:05:17.289746404Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2twu","title":"[PROGRAM] Enforce canonical evidence-artifact namespace + collision gate","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md (Cross-cutting verification discipline across Sections 10.0–10.21 and 11–16)\nSection: PROGRAM (Cross-cutting verification discipline)\n\nTask Objective:\nDefine and enforce a canonical evidence-artifact namespace contract so every bead produces deterministic, non-colliding, machine-indexable artifacts that can be replayed and audited without ambiguous filenames or overwritten outputs.\n\nWhy This Exists:\nCurrent bead descriptions intentionally preserve capability scope, but many still use generic artifact placeholders. Without explicit namespace rules and automated collision checks, downstream verification can become ambiguous, especially when multiple section gates run concurrently.\n\nAcceptance Criteria:\n- Publish canonical artifact naming schema covering unit, integration, e2e, benchmark, and verification outputs.\n- Define mandatory metadata fields for artifact manifests (bead id, section, scenario id, seed, profile, timestamp, commit, trace id).\n- Add automated collision detector that fails when two beads map to the same canonical artifact path.\n- Require all section/program verification gates to consume the canonical manifest and validate completeness.\n- Document migration rules for legacy/generic artifact placeholders to canonical paths.\n\nExpected Artifacts:\n- docs/verification/ARTIFACT_NAMESPACE_CONTRACT.md\n- schemas/artifact_manifest.schema.json\n- scripts/verify_artifact_namespace.sh\n- artifacts/program/artifact_namespace_validation_report.json\n\nTesting & Logging Requirements:\n- Unit tests for schema validator and path canonicalization logic.\n- E2E tests that execute multi-section verification workflows and assert collision-free artifact emission.\n- Structured logs with stable event codes for namespace resolution, collision checks, and manifest validation outcomes.\n- CPU-intensive checks (full matrix/e2e sweeps) must run via rch offload and include worker metadata in logs.\n\nTask-Specific Clarification:\n- Preserve full feature scope by strengthening evidence rigor only; no capability reduction or gate relaxation is allowed.\n- This bead is additive and must not weaken any existing section-level testing/e2e/logging obligations.\n- Outputs must be deterministic and independently replayable without hidden local context.\n\nWhy This Improves User Outcomes:\n- Prevents false-green verification caused by artifact path collisions or ambiguous outputs.\n- Improves incident forensics by making every result traceable to an exact bead/scenario/seed/profile.\n- Reduces operator confusion and accelerates root-cause analysis in large-scale, parallel execution flows.","status":"closed","priority":1,"issue_type":"task","assignee":"ubuntu","created_at":"2026-02-20T08:20:37.459103037Z","created_by":"ubuntu","updated_at":"2026-02-20T08:37:35.823839955Z","closed_at":"2026-02-20T08:37:35.823749517Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","program-integration","test-obligations","verification"],"dependencies":[{"issue_id":"bd-2twu","depends_on_id":"bd-1dpd","type":"blocks","created_at":"2026-02-20T08:24:27.498877187Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2ut3","title":"[11] No-contract-no-merge gate","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 11\n\nTask Objective:\nImplement hard CI/release gate enforcing 'No contract, no merge'.\n\nExecution Requirements:\n- Make this requirement machine-verifiable and release-gated where applicable.\n- Keep behavior self-documenting so future contributors do not need to re-open the master plan.\n\nTesting & Logging Requirements:\n- Unit tests for rule/contract logic and error conditions.\n- Integration/E2E tests for workflow-level enforcement.\n- Structured logs with stable codes and trace IDs.\n- Reproducible artifacts that prove contract satisfaction.\n\nAcceptance Criteria:\n- Success and failure invariants for [11] No-contract-no-merge gate are explicitly quantified and machine-verifiable.\n- Determinism requirements for [11] No-contract-no-merge gate are validated across repeated runs and adversarial perturbations.\n\n\nExpected Artifacts:\n- Machine-readable verification artifact with explicit canonical path under artifacts/section_11/bd-2ut3/verification_evidence.json, suitable for CI/release gating.\n- Human-readable verification summary with explicit canonical path under artifacts/section_11/bd-2ut3/verification_summary.md, linking implementation intent to measured validation outcomes.\n\nTask-Specific Clarification:\n- The implementation must deliver the exact capability named in the title, with no scope dilution and no silent feature omission.\n- Validation artifacts must include capability-specific thresholds, pass/fail criteria, and reproducible evidence bundles tied to this bead ID.\n- Program/section verification gates must be able to consume this bead's outputs without manual interpretation.\n\nWhy This Improves User Outcomes:\n- \"[11] No-contract-no-merge gate\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[11] No-contract-no-merge gate\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"1. A CI merge-gate check validates that every PR touching production code includes a complete evidence contract.\n2. The gate checks presence and validity of ALL contract fields: change summary, compatibility/threat evidence, EV score+tier, expected-loss model, fallback trigger, rollout wedge, rollback command, benchmark/correctness artifacts.\n3. PRs missing any required contract field are blocked from merge with a clear error message identifying which fields are missing.\n4. The gate is implemented as a pre-merge CI job (not just a linter warning) — merge is physically blocked.\n5. Escape hatch: a designated approver can override the gate with an explicit 'contract-override' label, but this is logged and auditable.\n6. Unit test: a mock PR with complete contract passes the gate; a mock PR missing any single field fails.\n7. Integration test: attempt to merge a PR without contract via CI simulation and verify it is rejected.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:39:33.156047417Z","created_by":"ubuntu","updated_at":"2026-02-20T15:17:12.901647857Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-11","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-2ut3","depends_on_id":"bd-3l8d","type":"blocks","created_at":"2026-02-20T07:43:24.650540061Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2vcg","title":"Epic: Moonshot Disruption Track [10.9]","description":"- type: epic","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-20T07:47:58.072163029Z","updated_at":"2026-02-20T07:49:21.135680718Z","closed_at":"2026-02-20T07:49:21.135658747Z","close_reason":"Superseded by canonical detailed master-plan graph (bd-33v) with full 10.N-10.21 and 11-16 coverage, explicit dependencies, and per-section verification gates.","source_repo":".","compaction_level":0,"original_size":0}
{"id":"bd-2vi","title":"[10.2] Implement L1 lockstep runner integration for Node/Bun/franken_node.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.2 — Compatibility Core\n\nWhy This Exists:\nCompatibility core as strategic wedge: policy-visible behavior, divergence receipts, L1/L2 lockstep, and spec-first fixture governance.\n\nTask Objective:\nImplement L1 lockstep runner integration for Node/Bun/franken_node.\n\nAcceptance Criteria:\n- Implement with explicit success/failure invariants and deterministic behavior under normal, edge, and fault scenarios.\n- Ensure outcomes are verifiable via automated tests and reproducible artifact outputs.\n\nExpected Artifacts:\n- Section-owned design/spec artifact at `docs/specs/section_10_2/bd-2vi_contract.md`, capturing decision rationale, invariants, and interface boundaries.\n- Machine-readable verification artifact at `artifacts/section_10_2/bd-2vi/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_2/bd-2vi/verification_summary.md` linking implementation intent to measured outcomes.\n\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.2] Implement L1 lockstep runner integration for Node/Bun/franken_node.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.2] Implement L1 lockstep runner integration for Node/Bun/franken_node.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.2] Implement L1 lockstep runner integration for Node/Bun/franken_node.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.2] Implement L1 lockstep runner integration for Node/Bun/franken_node.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.2] Implement L1 lockstep runner integration for Node/Bun/franken_node.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","status":"closed","priority":1,"issue_type":"task","assignee":"CrimsonCrane","created_at":"2026-02-20T07:36:44.240551048Z","created_by":"ubuntu","updated_at":"2026-02-20T09:42:40.382499910Z","closed_at":"2026-02-20T09:42:40.382472980Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-2","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-2vi","depends_on_id":"bd-1z3","type":"blocks","created_at":"2026-02-20T07:43:20.261971737Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2vl5","title":"[7] Performance and Developer Velocity Doctrine — core principles, levers, artifact requirements","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md — Section 7\n\n## Why This Exists\nPerformance is a product feature, not a benchmark vanity metric. This doctrine governs ALL performance-related implementation across franken_node.\n\n## Core Principles (7.1)\n1. Low startup overhead for migration and CI loops\n2. Predictable p99 under extension churn\n3. Bounded overhead from security instrumentation\n4. Fast feedback for migration diagnostics and compatibility diffs\n\n## Candidate High-EV Product Levers (7.2, Profile-Gated)\nThese are performance optimization candidates that must pass the 5.1 extreme-software-optimization loop before adoption:\n- Compatibility cache with deterministic invalidation\n- Lockstep differential harness acceleration\n- Zero-copy hostcall bridge paths where safe\n- Batch policy evaluation for high-frequency operations\n- Multi-lane scheduler tuning for cancel/timed/ready workloads\n\n## Required Performance Artifacts (7.3)\nEvery performance-affecting change MUST produce:\n1. Baseline reports with reproducible configs\n2. Profile artifacts (flamegraphs/traces)\n3. Before/after comparison tables\n4. Compatibility correctness proofs for tuned paths\n5. Tail-latency impact notes for security instrumentation\n\n## Implementation Mapping\n- Cold-start and p99 gates: 10.6 (Performance + Packaging)\n- Lockstep harness optimization: 10.6\n- Migration scanner throughput: 10.6\n- Security instrumentation overhead: 10.18 (VEF)\n- Scheduler tuning: 10.15 (Asupersync Integration)\n\n## Acceptance Criteria\n- All 4 core principles have measurable metrics and CI enforcement\n- Performance artifact checklist is enforced in PR review\n- Each optimization lever has before/after evidence and compatibility proof\n- p99 latency budgets are defined and enforced per subsystem\n\n\n## Success Criteria\n- All performance doctrine principles and artifact obligations are represented in relevant downstream performance beads.\n- Optimization decisions remain profile-driven, reproducible, and compatibility-safe per doctrine requirements.\n- Performance gates maintain explicit linkage to user-facing latency, throughput, and safety outcomes.\n\n## Testing & Logging Requirements\n- Unit tests for doctrine-compliance validators covering artifact completeness and metric-policy checks.\n- E2E performance-doctrine audit scripts that confirm required baseline/profile/comparison artifacts exist for each targeted track.\n- Structured logs for doctrine audits, missing-evidence findings, and remediation recommendations.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T16:15:12.720873497Z","created_by":"ubuntu","updated_at":"2026-02-20T17:08:28.256791780Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["doctrine","performance","plan","section-7"]}
{"id":"bd-2vs4","title":"[10.13] Implement deterministic lease coordinator selection and quorum signature verification.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.13 — FCP Deep-Mined Expansion Execution Track (9I)\n\nWhy This Exists:\nDeep connector/provider contract track: lifecycle FSMs, conformance harnesses, fencing, sandboxing, revocation freshness, and protocol hardening.\n\nTask Objective:\nImplement deterministic lease coordinator selection and quorum signature verification.\n\nAcceptance Criteria:\n- Coordinator selection is deterministic for identical inputs; quorum requirements vary by safety tier and are enforced; verification failures are classified.\n\nExpected Artifacts:\n- `tests/conformance/lease_coordinator_selection.rs`, `docs/specs/lease_quorum_rules.md`, `artifacts/10.13/lease_quorum_vectors.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_13/bd-2vs4/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_13/bd-2vs4/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.13] Implement deterministic lease coordinator selection and quorum signature verification.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.13] Implement deterministic lease coordinator selection and quorum signature verification.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.13] Implement deterministic lease coordinator selection and quorum signature verification.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.13] Implement deterministic lease coordinator selection and quorum signature verification.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.13] Implement deterministic lease coordinator selection and quorum signature verification.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","status":"closed","priority":1,"issue_type":"task","assignee":"CrimsonCrane","created_at":"2026-02-20T07:36:53.355836717Z","created_by":"ubuntu","updated_at":"2026-02-20T12:17:55.060711836Z","closed_at":"2026-02-20T12:17:55.060684175Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-13","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-2vs4","depends_on_id":"bd-bq6y","type":"blocks","created_at":"2026-02-20T07:43:13.267723744Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2w4u","title":"[12] Risk control: hardening perf regression","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 12\n\nTask Objective:\nImplement profile-governed tuning plus p99 guardrails against hardening-induced regressions.\n\nExecution Requirements:\n- Make this requirement machine-verifiable and release-gated where applicable.\n- Keep behavior self-documenting so future contributors do not need to re-open the master plan.\n\nTesting & Logging Requirements:\n- Unit tests for rule/contract logic and error conditions.\n- Integration/E2E tests for workflow-level enforcement.\n- Structured logs with stable codes and trace IDs.\n- Reproducible artifacts that prove contract satisfaction.\n\nAcceptance Criteria:\n- Success and failure invariants for [12] Risk control: hardening perf regression are explicitly quantified and machine-verifiable.\n- Determinism requirements for [12] Risk control: hardening perf regression are validated across repeated runs and adversarial perturbations.\n\n\nExpected Artifacts:\n- Machine-readable verification artifact with explicit canonical path under artifacts/section_12/bd-2w4u/verification_evidence.json, suitable for CI/release gating.\n- Human-readable verification summary with explicit canonical path under artifacts/section_12/bd-2w4u/verification_summary.md, linking implementation intent to measured validation outcomes.\n\nTask-Specific Clarification:\n- The implementation must deliver the exact capability named in the title, with no scope dilution and no silent feature omission.\n- Validation artifacts must include capability-specific thresholds, pass/fail criteria, and reproducible evidence bundles tied to this bead ID.\n- Program/section verification gates must be able to consume this bead's outputs without manual interpretation.\n\nWhy This Improves User Outcomes:\n- \"[12] Risk control: hardening perf regression\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[12] Risk control: hardening perf regression\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"RISK: Performance regressions from hardening — security hardening (sandboxing, validation, encryption) introduces unacceptable latency or throughput loss.\nIMPACT: Users disable hardening to meet performance SLAs, negating security benefits; franken_node perceived as slower than Node.js.\nCOUNTERMEASURES:\n  (a) Profile-governed tuning: hardening features have tunable profiles (strict/balanced/permissive) with documented performance tradeoffs.\n  (b) p99 gates: CI enforces that p99 latency under 'balanced' profile does not exceed baseline by more than 15%.\n  (c) Continuous benchmarking: every PR runs performance benchmarks; regressions > 5% on key metrics block merge.\nVERIFICATION:\n  1. At least 3 hardening profiles exist (strict/balanced/permissive) with documented performance characteristics.\n  2. p99 latency gate: CI benchmark suite measures p99 latency; 'balanced' profile stays within 15% of unhardened baseline.\n  3. Throughput gate: requests/sec under 'balanced' profile is >= 85% of unhardened baseline.\n  4. Profile switching is runtime-configurable (no restart required).\nTEST SCENARIOS:\n  - Scenario A: Run benchmark suite under 'strict' profile; document overhead vs baseline (informational, no gate).\n  - Scenario B: Run benchmark under 'balanced' profile; verify p99 latency within 15% of baseline (gate).\n  - Scenario C: Introduce a hardening change that adds 20% latency; verify CI blocks the merge.\n  - Scenario D: Switch profiles at runtime under load; verify no request failures during switch.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:39:33.586113032Z","created_by":"ubuntu","updated_at":"2026-02-20T15:18:47.432469822Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-12","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-2w4u","depends_on_id":"bd-3jc1","type":"blocks","created_at":"2026-02-20T07:43:24.907429550Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2wod","title":"[10.20] Integrate graph-aware quarantine and rollback orchestration with choke-point-first containment strategy.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.20 — Dependency Graph Immune System Execution Track (9N)\n\nWhy This Exists:\nDGIS topological immune system track for dependency contagion modeling, choke-point immunization, and graph-aware containment economics.\n\nTask Objective:\nIntegrate graph-aware quarantine and rollback orchestration with choke-point-first containment strategy.\n\nAcceptance Criteria:\n- Quarantine plans can target upstream choke points and downstream blast zones deterministically; rollback sequencing avoids reintroducing known high-risk paths.\n\nExpected Artifacts:\n- `docs/specs/dgis_quarantine_orchestration.md`, `tests/security/dgis_quarantine_containment.rs`, `artifacts/10.20/dgis_quarantine_drill_results.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_20/bd-2wod/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_20/bd-2wod/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.20] Integrate graph-aware quarantine and rollback orchestration with choke-point-first containment strategy.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.20] Integrate graph-aware quarantine and rollback orchestration with choke-point-first containment strategy.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.20] Integrate graph-aware quarantine and rollback orchestration with choke-point-first containment strategy.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.20] Integrate graph-aware quarantine and rollback orchestration with choke-point-first containment strategy.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.20] Integrate graph-aware quarantine and rollback orchestration with choke-point-first containment strategy.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"- Quarantine plans can target upstream choke points and downstream blast zones deterministically; rollback sequencing avoids reintroducing known high-risk paths.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:37:07.078364648Z","created_by":"ubuntu","updated_at":"2026-02-20T17:05:07.556230592Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-20","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-2wod","depends_on_id":"bd-2fid","type":"blocks","created_at":"2026-02-20T17:05:07.556182343Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2wsm","title":"[10.14] Implement epoch transition barrier protocol across core services with drain requirements.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.14 — FrankenSQLite Deep-Mined Expansion Execution Track (9J)\n\nWhy This Exists:\nFrankenSQLite deep-mined control integrity track: evidence ledgers, proofs, epochs, remote effects, markers/MMR, and deterministic fault labs.\n\nTask Objective:\nImplement epoch transition barrier protocol across core services with drain requirements.\n\nAcceptance Criteria:\n- Barrier requires participant drain acknowledgements; transition commits only on full barrier success; timeout path aborts safely with evidence.\n\nExpected Artifacts:\n- `docs/specs/epoch_barrier_protocol.md`, `tests/integration/epoch_transition_barrier.rs`, `artifacts/10.14/epoch_barrier_transcripts.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_14/bd-2wsm/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_14/bd-2wsm/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.14] Implement epoch transition barrier protocol across core services with drain requirements.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.14] Implement epoch transition barrier protocol across core services with drain requirements.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.14] Implement epoch transition barrier protocol across core services with drain requirements.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.14] Implement epoch transition barrier protocol across core services with drain requirements.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.14] Implement epoch transition barrier protocol across core services with drain requirements.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"Barrier requires participant drain acknowledgements; transition commits only on full barrier success; timeout path aborts safely with evidence.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:36:58.384523616Z","created_by":"ubuntu","updated_at":"2026-02-20T16:24:18.500404644Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-14","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-2wsm","depends_on_id":"bd-2xv8","type":"blocks","created_at":"2026-02-20T16:24:18.500344092Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2wsm","depends_on_id":"bd-3cs3","type":"blocks","created_at":"2026-02-20T07:43:15.893524044Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2wsm","depends_on_id":"bd-3hdv","type":"blocks","created_at":"2026-02-20T16:24:18.331887880Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2wz","title":"[10.2] Define compatibility bands (`core`, `high-value`, `edge`, `unsafe`) with policy defaults.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.2 — Compatibility Core\n\nWhy This Exists:\nCompatibility core as strategic wedge: policy-visible behavior, divergence receipts, L1/L2 lockstep, and spec-first fixture governance.\n\nTask Objective:\nDefine compatibility bands (`core`, `high-value`, `edge`, `unsafe`) with policy defaults.\n\nAcceptance Criteria:\n- Implement with explicit success/failure invariants and deterministic behavior under normal, edge, and fault scenarios.\n- Ensure outcomes are verifiable via automated tests and reproducible artifact outputs.\n\nExpected Artifacts:\n- Section-owned design/spec artifact at `docs/specs/section_10_2/bd-2wz_contract.md`, capturing decision rationale, invariants, and interface boundaries.\n- Machine-readable verification artifact at `artifacts/section_10_2/bd-2wz/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_2/bd-2wz/verification_summary.md` linking implementation intent to measured outcomes.\n\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.2] Define compatibility bands (`core`, `high-value`, `edge`, `unsafe`) with policy defaults.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.2] Define compatibility bands (`core`, `high-value`, `edge`, `unsafe`) with policy defaults.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.2] Define compatibility bands (`core`, `high-value`, `edge`, `unsafe`) with policy defaults.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.2] Define compatibility bands (`core`, `high-value`, `edge`, `unsafe`) with policy defaults.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.2] Define compatibility bands (`core`, `high-value`, `edge`, `unsafe`) with policy defaults.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","status":"closed","priority":1,"issue_type":"task","assignee":"CrimsonCrane","created_at":"2026-02-20T07:36:43.836577935Z","created_by":"ubuntu","updated_at":"2026-02-20T09:31:55.289761153Z","closed_at":"2026-02-20T09:31:55.289738070Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-2","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-2wz","depends_on_id":"bd-1ow","type":"blocks","created_at":"2026-02-20T07:46:34.943916721Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2wz","depends_on_id":"bd-cda","type":"blocks","created_at":"2026-02-20T07:46:34.988209318Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2x1e","title":"[12] Section-wide verification gate: comprehensive unit+e2e+logging","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 12\n\nTask Objective:\nCreate a hard section completion gate that verifies all implemented behavior in this section with comprehensive unit tests, integration/e2e scripts, and detailed structured logging evidence.\n\nAcceptance Criteria:\n- Section test matrix covers happy-path, edge-case, and adversarial/error-path scenarios for all section deliverables.\n- E2E scripts execute representative end-user workflows and policy/control-plane transitions for this section.\n- Structured logs include stable event/error codes, trace correlation IDs, and enough context for deterministic replay triage.\n- Verification report is deterministic and machine-readable for CI/release gating.\n\nExpected Artifacts:\n- Section-level test matrix and coverage summary artifact.\n- E2E script suite with pass/fail outputs and reproducible fixtures/seeds.\n- Structured log validation report and traceability bundle.\n- Gate verdict artifact consumable by release automation.\n\n- Machine-readable verification artifact at `artifacts/section_12/bd-2x1e/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_12/bd-2x1e/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests must validate contracts, invariants, and failure semantics.\n- E2E tests must validate cross-component behavior from user/operator entrypoints to persisted effects.\n- Logging must support root-cause analysis without hidden context requirements.\n\nTask-Specific Clarification:\n- For \"[12] Section-wide verification gate: comprehensive unit+e2e+logging\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[12] Section-wide verification gate: comprehensive unit+e2e+logging\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[12] Section-wide verification gate: comprehensive unit+e2e+logging\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[12] Section-wide verification gate: comprehensive unit+e2e+logging\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[12] Section-wide verification gate: comprehensive unit+e2e+logging\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"1. Section 12 verification gate runs all 12 risk-control check scripts and confirms 100% pass rate.\n2. Gate validates: (a) every risk-control bead has a verification script with self_test(), (b) every risk-control bead has unit tests, (c) all evidence artifacts are present under artifacts/section_12/.\n3. Risk register summary document exists listing all 12 risks with their current status (mitigated/open/monitoring).\n4. Each risk countermeasure has at least one passing test scenario demonstrating effectiveness.\n5. Gate produces section_12_verification_summary.md with per-risk pass/fail matrix.\n6. The gate itself has a unit test verifying correct aggregation of sub-check results.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:48:28.133441922Z","created_by":"ubuntu","updated_at":"2026-02-20T15:20:51.336229276Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-12","test-obligations","verification-gate"],"dependencies":[{"issue_id":"bd-2x1e","depends_on_id":"bd-13yn","type":"blocks","created_at":"2026-02-20T07:48:28.485340643Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2x1e","depends_on_id":"bd-1dpd","type":"blocks","created_at":"2026-02-20T08:24:24.032820826Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2x1e","depends_on_id":"bd-1n1t","type":"blocks","created_at":"2026-02-20T07:48:28.434689061Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2x1e","depends_on_id":"bd-1nab","type":"blocks","created_at":"2026-02-20T07:48:28.532715822Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2x1e","depends_on_id":"bd-1rff","type":"blocks","created_at":"2026-02-20T07:48:28.287856077Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2x1e","depends_on_id":"bd-2twu","type":"blocks","created_at":"2026-02-20T08:20:49.471016940Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2x1e","depends_on_id":"bd-2w4u","type":"blocks","created_at":"2026-02-20T07:48:28.582491612Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2x1e","depends_on_id":"bd-35m7","type":"blocks","created_at":"2026-02-20T07:48:28.231925393Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2x1e","depends_on_id":"bd-38ri","type":"blocks","created_at":"2026-02-20T07:48:28.732748133Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2x1e","depends_on_id":"bd-3jc1","type":"blocks","created_at":"2026-02-20T07:48:28.630944538Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2x1e","depends_on_id":"bd-kiqr","type":"blocks","created_at":"2026-02-20T07:48:28.684961477Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2x1e","depends_on_id":"bd-paui","type":"blocks","created_at":"2026-02-20T07:48:28.387315105Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2x1e","depends_on_id":"bd-s4cu","type":"blocks","created_at":"2026-02-20T07:48:28.782073965Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2x1e","depends_on_id":"bd-v4ps","type":"blocks","created_at":"2026-02-20T07:48:28.339293332Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2xgs","title":"[10.21] Implement deterministic phenotype feature extraction per version from runtime evidence, manifests, and code metadata.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.21 — Behavioral Phenotype Evolution Tracker Execution Track (9O)\n\nWhy This Exists:\nBPET longitudinal phenotype intelligence track for pre-compromise trajectory detection and integration with DGIS/ATC/migration/economic policy.\n\nTask Objective:\nImplement deterministic phenotype feature extraction per version from runtime evidence, manifests, and code metadata.\n\nAcceptance Criteria:\n- Identical inputs produce identical phenotype vectors; extraction records feature provenance and uncertainty; missing fields are typed rather than silently dropped.\n\nExpected Artifacts:\n- `src/security/bpet/phenotype_extractor.rs`, `tests/conformance/bpet_feature_extraction.rs`, `artifacts/10.21/bpet_feature_samples.jsonl`.\n\n- Machine-readable verification artifact at `artifacts/section_10_21/bd-2xgs/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_21/bd-2xgs/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.21] Implement deterministic phenotype feature extraction per version from runtime evidence, manifests, and code metadata.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.21] Implement deterministic phenotype feature extraction per version from runtime evidence, manifests, and code metadata.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.21] Implement deterministic phenotype feature extraction per version from runtime evidence, manifests, and code metadata.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.21] Implement deterministic phenotype feature extraction per version from runtime evidence, manifests, and code metadata.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.21] Implement deterministic phenotype feature extraction per version from runtime evidence, manifests, and code metadata.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"- Identical inputs produce identical phenotype vectors; extraction records feature provenance and uncertainty; missing fields are typed rather than silently dropped.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:37:07.774886105Z","created_by":"ubuntu","updated_at":"2026-02-20T17:05:20.773887439Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-21","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-2xgs","depends_on_id":"bd-39ga","type":"blocks","created_at":"2026-02-20T17:05:20.773836754Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2xv8","title":"[10.14] Implement fail-closed validity window check rejecting future-epoch artifacts.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.14 — FrankenSQLite Deep-Mined Expansion Execution Track (9J)\n\nWhy This Exists:\nFrankenSQLite deep-mined control integrity track: evidence ledgers, proofs, epochs, remote effects, markers/MMR, and deterministic fault labs.\n\nTask Objective:\nImplement fail-closed validity window check rejecting future-epoch artifacts.\n\nAcceptance Criteria:\n- Future-epoch artifacts are rejected before use; validity window policy is explicit and test-covered; rejection telemetry includes epoch context.\n\nExpected Artifacts:\n- `tests/security/future_epoch_rejection.rs`, `docs/specs/validity_window_rules.md`, `artifacts/10.14/epoch_rejection_events.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_14/bd-2xv8/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_14/bd-2xv8/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.14] Implement fail-closed validity window check rejecting future-epoch artifacts.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.14] Implement fail-closed validity window check rejecting future-epoch artifacts.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.14] Implement fail-closed validity window check rejecting future-epoch artifacts.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.14] Implement fail-closed validity window check rejecting future-epoch artifacts.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.14] Implement fail-closed validity window check rejecting future-epoch artifacts.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"Future-epoch artifacts are rejected before use; validity window policy is explicit and test-covered; rejection telemetry includes epoch context.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:36:58.221485027Z","created_by":"ubuntu","updated_at":"2026-02-20T15:44:04.758998194Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-14","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-2xv8","depends_on_id":"bd-3hdv","type":"blocks","created_at":"2026-02-20T07:43:15.807378621Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2yc","title":"[10.5] Implement operator copilot action recommendation API.","description":"# [10.5] Operator Copilot Action Recommendation API\n\n## Why This Exists\n\nThis bead implements Impossible-by-Default capability #8 from Section 3.2 of the franken_node plan: the operator copilot. Section 9A.8 specifies that the copilot must offer live recommended actions with expected-loss rationale, confidence context, and deterministic rollback commands. Section 9B.8 requires VOI-based ranking so operator attention is directed to the highest expected-impact actions. Section 9D.8 mandates that the copilot API must meet interactive latency budgets, since operators rely on it during live incidents where seconds matter.\n\nSection 10.5 (Security + Policy Product Surfaces) positions the operator copilot as the primary human-facing decision support surface. It consumes expected-loss scoring from bd-33b, integrates degraded-mode status from bd-3nr, and provides the context that policy change workflows (bd-sh3) need for informed approval decisions. The copilot does not make autonomous decisions; it presents ranked recommendations with full transparency into the scoring methodology, uncertainty, and rollback options.\n\n## What It Must Do\n\n1. **Action Recommendation Engine**: Given the current system state (trust state, degraded-mode status, pending operations, active incidents), generate a ranked list of recommended actions. Each recommendation includes: (a) the action description, (b) expected-loss vector from bd-33b, (c) uncertainty bands, (d) VOI rank, (e) confidence context explaining why this action is recommended, and (f) a deterministic rollback command.\n\n2. **VOI-Based Ranking**: Actions are ranked by Value-of-Information (Section 9B.8), not by raw expected loss. This ensures operators focus on decisions where their input has the most impact, not on decisions the system can handle autonomously.\n\n3. **Deterministic Rollback Commands**: Every recommended action must include a pre-computed, deterministic rollback command that can be executed atomically to undo the action. Rollback commands must be validated at recommendation time (not just at execution time) to ensure they are executable.\n\n4. **Confidence Context**: Each recommendation includes a structured confidence context explaining: which data sources informed the recommendation, how fresh each source is, which assumptions the scoring depends on, and what would change the recommendation (sensitivity analysis).\n\n5. **Interactive Latency Budget**: The API must respond within 200ms (p99) for up to 50 candidate actions. This is a hard requirement from Section 9D.8 for interactive operator workflows during live incidents.\n\n6. **Degraded-Mode Integration**: When the system is in degraded mode (bd-3nr), the copilot must: (a) surface the degraded-mode status prominently in every response, (b) annotate recommendations that are affected by stale trust data, (c) include the specific stale inputs and their staleness duration, and (d) adjust expected-loss estimates to account for the reduced trust confidence.\n\n7. **Streaming Updates**: Support a streaming mode where the copilot pushes updated recommendations as system state changes, without requiring the operator to poll. Use server-sent events (SSE) or equivalent.\n\n8. **Audit Trail Integration**: Every recommendation served must be recorded in the audit trail with a unique recommendation ID, the full recommendation payload, and the operator identity that requested it.\n\n## Acceptance Criteria\n\n1. The API returns a ranked list of recommendations, each containing: action description, expected-loss vector, uncertainty bands, VOI rank, confidence context, and rollback command.\n2. Recommendations are ranked by VOI, not by raw expected loss. A test with known VOI values verifies the ranking order.\n3. Every rollback command included in a recommendation is validated at recommendation time; if a rollback is not feasible, the recommendation is flagged as non-rollbackable.\n4. Confidence context includes at minimum: data source identifiers, freshness timestamps, key assumptions, and one sensitivity indicator (what change would flip the recommendation).\n5. The API responds within 200ms (p99) for 50 candidate actions, verified by a load test with 1000 sequential requests.\n6. During degraded mode, every API response includes a degraded-mode warning block with stale input details and staleness durations.\n7. Recommendations affected by stale trust data are annotated with a degraded_confidence flag and adjusted uncertainty bands.\n8. Streaming mode delivers updated recommendations within 500ms of a state change, verified by an integration test that injects a state change and measures delivery latency.\n9. Every recommendation served is recorded in the audit trail with recommendation_id, full payload, operator_identity, and trace_id.\n10. The API rejects requests from unauthenticated callers with a structured 401 error (no anonymous copilot access).\n11. All log events use stable codes (COPILOT_RECOMMENDATION_REQUESTED, COPILOT_RECOMMENDATION_SERVED, COPILOT_ROLLBACK_VALIDATED, COPILOT_DEGRADED_WARNING, COPILOT_STREAM_STARTED, COPILOT_STREAM_UPDATED) with trace correlation IDs.\n\n## Key Dependencies\n\n- **Depends on bd-2fa** (counterfactual replay): the copilot can invoke counterfactual simulation to show operators \"what would happen if.\"\n- **Depends on bd-33b** (expected-loss scoring): the copilot consumes expected-loss vectors and VOI rankings from the scoring engine.\n- **Depends on bd-3nr** (degraded-mode policy): the copilot integrates degraded-mode status into every response.\n- **Depended on by bd-33b** (expected-loss scoring depends on copilot API existing as a consumer).\n- **Depended on by bd-1koz** (section-wide verification gate).\n- **Depended on by bd-20a** (section rollup).\n\n## Testing and Logging Requirements\n\n- **Unit tests**: Recommendation generation with mocked scoring engine. VOI ranking correctness. Rollback command validation (valid and invalid rollbacks). Confidence context completeness. Degraded-mode annotation injection.\n- **Integration tests**: Full pipeline from state observation through scoring to recommendation delivery. Verify audit trail entries after serving recommendations. Test degraded-mode integration with mocked stale trust inputs.\n- **E2E tests**: Simulate a live incident scenario: system enters degraded mode, operator requests recommendations, copilot returns degraded-annotated ranked actions, operator executes top action, then executes rollback. Verify full audit trail.\n- **Load tests**: 1000 sequential requests with 50 candidate actions each; assert p99 latency under 200ms. Measure and track streaming update latency.\n- **Adversarial tests**: Request recommendations with no candidate actions (empty state). Request during SUSPENDED mode. Inject a scoring engine timeout and verify graceful degradation of the copilot response (partial results with timeout warning).\n- **Logging**: Recommendation requests at INFO level. Served recommendations at INFO level (with recommendation_id). Degraded-mode warnings at WARN level. Rollback validation failures at WARN level. Streaming events at DEBUG level.\n\n## Expected Artifacts\n\n- `docs/specs/section_10_5/bd-2yc_contract.md` — Design spec with API schema (request/response), VOI ranking algorithm reference, streaming protocol, and latency budget analysis.\n- `artifacts/section_10_5/bd-2yc/verification_evidence.json` — Machine-readable pass/fail evidence.\n- `artifacts/section_10_5/bd-2yc/verification_summary.md` — Human-readable summary.\n- Rust module(s) in `crates/franken-node/src/` implementing the copilot recommendation engine and API handler.\n- Python verification script `scripts/check_operator_copilot.py` with `--json` flag and `self_test()`.\n- Unit tests in `tests/test_check_operator_copilot.py`.\n- Load test harness for latency benchmarking.","acceptance_criteria":"1. Implement an ActionRecommendationEngine that, given current system state and an operator context, returns a ranked Vec<RecommendedAction> ordered by VOI (Value of Information) score descending.\n2. Each RecommendedAction contains: action_id (string), display_name (string), description (string), voi_score (f64, higher is better), expected_loss (ExpectedLossVector), uncertainty_band (ConfidenceInterval with lower_bound, upper_bound, confidence_level fields), preconditions (Vec<String> listing required gate passes), and estimated_duration (Duration).\n3. ExpectedLossVector is a struct with named fields for each loss dimension: availability_loss (f64), integrity_loss (f64), confidentiality_loss (f64), financial_loss (f64), and reputation_loss (f64). All values are non-negative; the engine must validate this invariant.\n4. VOI-based ranking must implement the formula: VOI = expected_gain_if_act - expected_gain_if_wait, using uncertainty bands from the loss vectors. Provide a pure function compute_voi(action: &ActionCandidate, state: &SystemState) -> f64 that is unit-testable in isolation.\n5. The API must return at most top_k recommendations (configurable, default 5) and must complete within 200 ms for up to 100 candidate actions.\n6. Each recommendation must include a human-readable rationale string explaining why this action ranks where it does, referencing the dominant loss dimension.\n7. Verification: scripts/check_copilot_api.py --json exercises the engine with a fixture system state containing at least 10 candidate actions, asserts correct VOI ordering and that uncertainty bands are non-degenerate (upper > lower); unit tests in tests/test_check_copilot_api.py cover ranking stability, edge cases (zero candidates, tied VOI), and the 200 ms latency bound; evidence in artifacts/section_10_5/bd-2yc/.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:36:46.383202614Z","created_by":"ubuntu","updated_at":"2026-02-20T16:59:01.420786244Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-5","self-contained","test-obligations"]}
{"id":"bd-2yc4","title":"[10.13] Implement crash-loop detector with automatic rollback and known-good pin fallback.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.13 — FCP Deep-Mined Expansion Execution Track (9I)\n\nWhy This Exists:\nDeep connector/provider contract track: lifecycle FSMs, conformance harnesses, fencing, sandboxing, revocation freshness, and protocol hardening.\n\nTask Objective:\nImplement crash-loop detector with automatic rollback and known-good pin fallback.\n\nAcceptance Criteria:\n- Crash-loop thresholds are configurable and enforced; rollback to known-good pin is automatic and auditable; rollback cannot bypass trust policy.\n\nExpected Artifacts:\n- `src/runtime/crash_loop_detector.rs`, `tests/integration/crash_loop_rollback.rs`, `artifacts/10.13/crash_loop_incident_bundle.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_13/bd-2yc4/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_13/bd-2yc4/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.13] Implement crash-loop detector with automatic rollback and known-good pin fallback.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.13] Implement crash-loop detector with automatic rollback and known-good pin fallback.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.13] Implement crash-loop detector with automatic rollback and known-good pin fallback.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.13] Implement crash-loop detector with automatic rollback and known-good pin fallback.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.13] Implement crash-loop detector with automatic rollback and known-good pin fallback.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","status":"closed","priority":1,"issue_type":"task","assignee":"CrimsonCrane","created_at":"2026-02-20T07:36:52.950806826Z","created_by":"ubuntu","updated_at":"2026-02-20T11:57:03.050784879Z","closed_at":"2026-02-20T11:57:03.050757678Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-13","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-2yc4","depends_on_id":"bd-1d7n","type":"blocks","created_at":"2026-02-20T07:43:13.054292565Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2yh","title":"[10.4] Implement extension trust-card API and CLI surfaces.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.4 — Extension Ecosystem + Registry\n\nWhy This Exists:\nExtension ecosystem and registry trust foundation: signed artifacts, provenance, trust cards, revocation, and quarantine flows.\n\nTask Objective:\nImplement extension trust-card API and CLI surfaces.\n\nAcceptance Criteria:\n(See structured acceptance_criteria field for domain-specific criteria.)\n\nExpected Artifacts:\n- Section-owned design/spec artifact at `docs/specs/section_10_4/bd-2yh_contract.md`, capturing decision rationale, invariants, and interface boundaries.\n- Machine-readable verification artifact at `artifacts/section_10_4/bd-2yh/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_4/bd-2yh/verification_summary.md` linking implementation intent to measured outcomes.\n\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.4] Implement extension trust-card API and CLI surfaces.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.4] Implement extension trust-card API and CLI surfaces.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.4] Implement extension trust-card API and CLI surfaces.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.4] Implement extension trust-card API and CLI surfaces.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.4] Implement extension trust-card API and CLI surfaces.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"1. Trust card data model combines four dimensions: provenance summary (attestation chain status, source linkage), behavioral telemetry (resource usage histograms, capability exercise frequency, anomaly scores), revocation status (real-time from 10.13 revocation registry), and policy constraints (allowed capabilities, resource envelope limits, required certification level). 2. REST API endpoints: GET /api/v1/extensions/{id}/trust-card (full card), GET /api/v1/extensions/{id}/trust-card/delta (changes since timestamp), POST /api/v1/extensions/{id}/trust-card/evaluate (policy evaluation against a deployment context). 3. CLI commands: 'franken-node ext trust-card show <ext-id>' (human-readable summary), 'franken-node ext trust-card diff <ext-id> --since <timestamp>' (delta view), 'franken-node ext trust-card evaluate <ext-id> --context <policy-file>' (policy check). 4. Trust card deltas are decomposed into posterior components per 9C: each delta field includes a counterfactual_impact score indicating how much that specific change affects the overall trust assessment. 5. Trust card is cryptographically bound to the extension manifest version; manifest version bump requires trust card re-evaluation. 6. API responses include cache-control headers aligned with safety-tier freshness requirements from bd-12q. 7. Trust card rendering supports machine-readable (JSON) and human-readable (terminal-formatted, markdown) output formats. 8. Policy evaluation endpoint returns allow/deny with a structured explanation listing each constraint that passed or failed and the contributing trust card fields. 9. Trust card API requires authentication; write operations (manual trust overrides) require operator role with audit trail. 10. Rate limiting on trust card API: minimum 100 req/s for read operations per node.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:36:45.664911607Z","created_by":"ubuntu","updated_at":"2026-02-20T16:59:00.356894740Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-4","self-contained","test-obligations"]}
{"id":"bd-2yhs","title":"[10.N] Implement duplicate-implementation CI gate","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.N — Execution Normalization Contract\n\nTask Objective:\nImplement a CI gate that detects duplicate implementation semantics across tracks and blocks merges when a non-canonical track attempts to re-implement canonical logic.\n\nAcceptance Criteria:\n- Duplicate-implementation detector identifies prohibited semantic redefinitions with deterministic findings.\n- CI gate blocks violating changes and emits actionable remediation guidance.\n- Waiver flow is explicit, scoped, and auditable when exceptions are necessary.\n\nExpected Artifacts:\n- CI workflow + detector configuration.\n- Duplicate-implementation findings report on representative fixtures.\n\nTesting & Logging Requirements:\n- Unit tests for rule matching and false-positive guardrails.\n- E2E tests covering pass/fail CI scenarios with fixture PRs.\n- Structured violation logs with stable rule IDs.\n\nTask-Specific Clarification:\n- For \"[10.N] Implement duplicate-implementation CI gate\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.N] Implement duplicate-implementation CI gate\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.N] Implement duplicate-implementation CI gate\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.N] Implement duplicate-implementation CI gate\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.N] Implement duplicate-implementation CI gate\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","status":"closed","priority":1,"issue_type":"task","assignee":"ubuntu","created_at":"2026-02-20T07:50:04.132828860Z","created_by":"ubuntu","updated_at":"2026-02-20T08:17:50.478847165Z","closed_at":"2026-02-20T08:17:50.478755294Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-N","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-2yhs","depends_on_id":"bd-zxk8","type":"blocks","created_at":"2026-02-20T07:50:04.258758834Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2ymp","title":"[11] Contract field: rollout wedge","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 11\n\nTask Objective:\nRequire explicit rollout wedge description for staged enablement and safe rollback.\n\nExecution Requirements:\n- Make this requirement machine-verifiable and release-gated where applicable.\n- Keep behavior self-documenting so future contributors do not need to re-open the master plan.\n\nTesting & Logging Requirements:\n- Unit tests for rule/contract logic and error conditions.\n- Integration/E2E tests for workflow-level enforcement.\n- Structured logs with stable codes and trace IDs.\n- Reproducible artifacts that prove contract satisfaction.\n\nAcceptance Criteria:\n- Success and failure invariants for [11] Contract field: rollout wedge are explicitly quantified and machine-verifiable.\n- Determinism requirements for [11] Contract field: rollout wedge are validated across repeated runs and adversarial perturbations.\n\n\nExpected Artifacts:\n- Machine-readable verification artifact with explicit canonical path under artifacts/section_11/bd-2ymp/verification_evidence.json, suitable for CI/release gating.\n- Human-readable verification summary with explicit canonical path under artifacts/section_11/bd-2ymp/verification_summary.md, linking implementation intent to measured validation outcomes.\n\nTask-Specific Clarification:\n- The implementation must deliver the exact capability named in the title, with no scope dilution and no silent feature omission.\n- Validation artifacts must include capability-specific thresholds, pass/fail criteria, and reproducible evidence bundles tied to this bead ID.\n- Program/section verification gates must be able to consume this bead's outputs without manual interpretation.\n\nWhy This Improves User Outcomes:\n- \"[11] Contract field: rollout wedge\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[11] Contract field: rollout wedge\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"1. Every evidence contract specifies a rollout wedge: the incremental deployment strategy with explicit percentage/stage progression.\n2. Wedge must define: (a) stages (e.g., 1% -> 5% -> 25% -> 100%), (b) hold duration per stage (minimum observation window), (c) promotion criteria (metrics that must be green to advance), (d) automatic vs manual promotion decision.\n3. The rollout wedge must reference the fallback trigger — if trigger fires at any stage, rollout halts and rollback executes.\n4. CI rejects contracts missing rollout wedge or with fewer than 2 stages.\n5. Unit test: wedge with 3+ stages, hold durations, and promotion criteria passes; wedge with single stage or no hold duration fails.\n6. The wedge definition must be machine-parseable for integration with deployment automation.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:39:32.903551969Z","created_by":"ubuntu","updated_at":"2026-02-20T15:16:38.347605658Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-11","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-2ymp","depends_on_id":"bd-3v8f","type":"blocks","created_at":"2026-02-20T07:43:24.511155651Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2yvw","title":"[10.19] Implement Sybil-resistant participation controls tied to attestation/staking/reputation evidence.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.19 — Adversarial Trust Commons Execution Track (9M)\n\nWhy This Exists:\nATC federated intelligence track for privacy-preserving cross-deployment threat learning, robust aggregation, and verifier-backed trust metrics.\n\nTask Objective:\nImplement Sybil-resistant participation controls tied to attestation/staking/reputation evidence.\n\nAcceptance Criteria:\n- Participation weighting rejects untrusted identity inflation; weighting policy is auditable and deterministic; attack simulations validate resistance properties.\n\nExpected Artifacts:\n- `src/federation/atc_participation_weighting.rs`, `tests/security/atc_sybil_resistance.rs`, `artifacts/10.19/atc_weighting_audit_report.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_19/bd-2yvw/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_19/bd-2yvw/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.19] Implement Sybil-resistant participation controls tied to attestation/staking/reputation evidence.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.19] Implement Sybil-resistant participation controls tied to attestation/staking/reputation evidence.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.19] Implement Sybil-resistant participation controls tied to attestation/staking/reputation evidence.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.19] Implement Sybil-resistant participation controls tied to attestation/staking/reputation evidence.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.19] Implement Sybil-resistant participation controls tied to attestation/staking/reputation evidence.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"- Participation weighting rejects untrusted identity inflation; weighting policy is auditable and deterministic; attack simulations validate resistance properties.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:37:05.836638959Z","created_by":"ubuntu","updated_at":"2026-02-20T17:00:53.911365976Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-19","self-contained","test-obligations"]}
{"id":"bd-2zip","title":"[10.19] Add verifier APIs and proof artifacts for ATC computations and published ecosystem metrics.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.19 — Adversarial Trust Commons Execution Track (9M)\n\nWhy This Exists:\nATC federated intelligence track for privacy-preserving cross-deployment threat learning, robust aggregation, and verifier-backed trust metrics.\n\nTask Objective:\nAdd verifier APIs and proof artifacts for ATC computations and published ecosystem metrics.\n\nAcceptance Criteria:\n- External verifiers can validate federation computation integrity and metric provenance without private raw participant data; verifier outputs are deterministic.\n\nExpected Artifacts:\n- `docs/specs/atc_verifier_contract.md`, `tests/conformance/atc_verifier_apis.rs`, `artifacts/10.19/atc_verifier_report.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_19/bd-2zip/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_19/bd-2zip/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.19] Add verifier APIs and proof artifacts for ATC computations and published ecosystem metrics.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.19] Add verifier APIs and proof artifacts for ATC computations and published ecosystem metrics.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.19] Add verifier APIs and proof artifacts for ATC computations and published ecosystem metrics.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.19] Add verifier APIs and proof artifacts for ATC computations and published ecosystem metrics.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.19] Add verifier APIs and proof artifacts for ATC computations and published ecosystem metrics.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"- External verifiers can validate federation computation integrity and metric provenance without private raw participant data; verifier outputs are deterministic.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:37:06.170376736Z","created_by":"ubuntu","updated_at":"2026-02-20T17:00:52.003248003Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-19","self-contained","test-obligations"]}
{"id":"bd-2zl","title":"Add transplant hash lockfile for tamper detection","description":"Hash each transplanted file and persist deterministic lockfile with provenance metadata.","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-20T07:26:05.379629417Z","created_by":"ubuntu","updated_at":"2026-02-20T07:27:13.094134728Z","closed_at":"2026-02-20T07:27:13.094114280Z","close_reason":"Duplicate scope; superseded by bd-7rt currently in progress","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-2zl","depends_on_id":"bd-1qz","type":"blocks","created_at":"2026-02-20T07:26:09.609484458Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2zo1","title":"[10.21] Integrate BPET with ATC for privacy-preserving federated temporal intelligence exchange.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.21 — Behavioral Phenotype Evolution Tracker Execution Track (9O)\n\nWhy This Exists:\nBPET longitudinal phenotype intelligence track for pre-compromise trajectory detection and integration with DGIS/ATC/migration/economic policy.\n\nTask Objective:\nIntegrate BPET with ATC for privacy-preserving federated temporal intelligence exchange.\n\nAcceptance Criteria:\n- BPET exports anonymized trajectory summaries and consumes federated temporal priors without raw longitudinal leakage; contracts are verifier-checkable and versioned.\n\nExpected Artifacts:\n- `src/federation/bpet_atc_bridge.rs`, `tests/integration/bpet_atc_temporal_interop.rs`, `artifacts/10.21/bpet_atc_exchange_report.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_21/bd-2zo1/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_21/bd-2zo1/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.21] Integrate BPET with ATC for privacy-preserving federated temporal intelligence exchange.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.21] Integrate BPET with ATC for privacy-preserving federated temporal intelligence exchange.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.21] Integrate BPET with ATC for privacy-preserving federated temporal intelligence exchange.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.21] Integrate BPET with ATC for privacy-preserving federated temporal intelligence exchange.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.21] Integrate BPET with ATC for privacy-preserving federated temporal intelligence exchange.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"- BPET exports anonymized trajectory summaries and consumes federated temporal priors without raw longitudinal leakage; contracts are verifier-checkable and versioned.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:37:08.544511331Z","created_by":"ubuntu","updated_at":"2026-02-20T17:01:06.904968861Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-21","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-2zo1","depends_on_id":"bd-ukh7","type":"blocks","created_at":"2026-02-20T15:01:15.545264479Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2zr4","title":"Epic: Frontier Programs Execution [10.12]","description":"- type: epic","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-20T07:47:58.072163029Z","updated_at":"2026-02-20T07:49:21.153388340Z","closed_at":"2026-02-20T07:49:21.153367661Z","close_reason":"Superseded by canonical detailed master-plan graph (bd-33v) with full 10.N-10.21 and 11-16 coverage, explicit dependencies, and per-section verification gates.","source_repo":".","compaction_level":0,"original_size":0}
{"id":"bd-2zz","title":"[10.1] Add dependency-direction guard preventing local engine crate reintroduction.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.1 — Charter + Split Governance\n\nWhy This Exists:\nProgram charter and governance baseline: split contracts, reproducibility, and anti-regression rules for strategic integrity.\n\nTask Objective:\nAdd dependency-direction guard preventing local engine crate reintroduction.\n\nAcceptance Criteria:\n- Implement with explicit success/failure invariants and deterministic behavior under normal, edge, and fault scenarios.\n- Ensure outcomes are verifiable via automated tests and reproducible artifact outputs.\n\nExpected Artifacts:\n- Section-owned design/spec artifact at `docs/specs/section_10_1/bd-2zz_contract.md`, capturing decision rationale, invariants, and interface boundaries.\n- Machine-readable verification artifact at `artifacts/section_10_1/bd-2zz/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_1/bd-2zz/verification_summary.md` linking implementation intent to measured outcomes.\n\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.1] Add dependency-direction guard preventing local engine crate reintroduction.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.1] Add dependency-direction guard preventing local engine crate reintroduction.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.1] Add dependency-direction guard preventing local engine crate reintroduction.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.1] Add dependency-direction guard preventing local engine crate reintroduction.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.1] Add dependency-direction guard preventing local engine crate reintroduction.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","status":"closed","priority":1,"issue_type":"task","assignee":"CrimsonCrane","created_at":"2026-02-20T07:36:43.448201283Z","created_by":"ubuntu","updated_at":"2026-02-20T09:08:43.244168356Z","closed_at":"2026-02-20T09:08:43.244143199Z","close_reason":"Dependency-direction guard implemented. 4 checks (workspace members, package names, dependency direction, crates dir) all PASS. 9 unit tests all pass. Spec, guard script, and verification artifacts created.","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-1","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-2zz","depends_on_id":"bd-1j2","type":"blocks","created_at":"2026-02-20T07:43:10.617269548Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-3014","title":"[10.15] Integrate canonical remote named-computation registry (from `10.14`) for control-plane distributed actions.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.15 — Asupersync-First Integration Execution Track (8.4-8.6)\n\nWhy This Exists:\nAsupersync-first control-plane migration track enforcing Cx-first, region ownership, cancellation correctness, and protocol-grade verification gates.\n\nTask Objective:\nIntegrate canonical remote named-computation registry (from `10.14`) for control-plane distributed actions.\n\nAcceptance Criteria:\n- Control-plane paths use the same canonical registry semantics as `10.14`; unknown names fail closed with stable error class; no divergent registry behavior is introduced.\n\nExpected Artifacts:\n- `docs/integration/control_remote_registry_adoption.md`, `tests/conformance/named_remote_computations.rs`, `artifacts/10.15/remote_registry_adoption_report.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_15/bd-3014/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_15/bd-3014/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.15] Integrate canonical remote named-computation registry (from `10.14`) for control-plane distributed actions.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.15] Integrate canonical remote named-computation registry (from `10.14`) for control-plane distributed actions.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.15] Integrate canonical remote named-computation registry (from `10.14`) for control-plane distributed actions.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.15] Integrate canonical remote named-computation registry (from `10.14`) for control-plane distributed actions.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.15] Integrate canonical remote named-computation registry (from `10.14`) for control-plane distributed actions.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"- Control-plane paths use the same canonical registry semantics as `10.14`; unknown names fail closed with stable error class; no divergent registry behavior is introduced.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:37:00.138232005Z","created_by":"ubuntu","updated_at":"2026-02-20T17:00:43.688425570Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-15","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-3014","depends_on_id":"bd-ac83","type":"blocks","created_at":"2026-02-20T14:59:47.584707824Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-31tg","title":"[15] Pillar: partner and lighthouse programs","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 15\n\nTask Objective:\nImplement partner/lighthouse adoption programs proving category-shift outcomes.\n\nExecution Requirements:\n- Make this requirement machine-verifiable and release-gated where applicable.\n- Keep behavior self-documenting so future contributors do not need to re-open the master plan.\n\nTesting & Logging Requirements:\n- Unit tests for rule/contract logic and error conditions.\n- Integration/E2E tests for workflow-level enforcement.\n- Structured logs with stable codes and trace IDs.\n- Reproducible artifacts that prove contract satisfaction.\n\nAcceptance Criteria:\n- Success and failure invariants for [15] Pillar: partner and lighthouse programs are explicitly quantified and machine-verifiable.\n- Determinism requirements for [15] Pillar: partner and lighthouse programs are validated across repeated runs and adversarial perturbations.\n\n\nExpected Artifacts:\n- Machine-readable verification artifact with explicit canonical path under artifacts/section_15/bd-31tg/verification_evidence.json, suitable for CI/release gating.\n- Human-readable verification summary with explicit canonical path under artifacts/section_15/bd-31tg/verification_summary.md, linking implementation intent to measured validation outcomes.\n\nTask-Specific Clarification:\n- The implementation must deliver the exact capability named in the title, with no scope dilution and no silent feature omission.\n- Validation artifacts must include capability-specific thresholds, pass/fail criteria, and reproducible evidence bundles tied to this bead ID.\n- Program/section verification gates must be able to consume this bead's outputs without manual interpretation.\n\nWhy This Improves User Outcomes:\n- \"[15] Pillar: partner and lighthouse programs\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[15] Pillar: partner and lighthouse programs\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"1. Partner program defined with tiers: (a) Lighthouse (early adopters, deep integration, direct support channel), (b) Partner (validated migration, co-marketing), (c) Community (self-service, public resources).\n2. Lighthouse program has >= 3 active participants running franken_node in production or staging.\n3. Each lighthouse partner has: (a) documented use case, (b) migration case study (published or in-progress), (c) feedback channel with response SLA <= 48 hours, (d) quarterly review cadence.\n4. Partner program has clear entry criteria: minimum project size, commitment to provide feedback, willingness to publish anonymized results.\n5. Benefits documented: priority bug fixes, early access to features, co-marketing opportunities, benchmark co-design input.\n6. Program produces >= 2 published case studies within first 6 months.\n7. Partner satisfaction tracked via quarterly survey; target NPS >= 30.\n8. Evidence: partner_program_status.json with partner count by tier, case study list, and satisfaction scores.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:39:36.527116261Z","created_by":"ubuntu","updated_at":"2026-02-20T15:26:52.601799077Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-15","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-31tg","depends_on_id":"bd-1961","type":"blocks","created_at":"2026-02-20T07:43:26.385358774Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-329y","title":"Epic: Supply Chain Trust Infrastructure [10.13c]","description":"- type: epic","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-20T07:47:58.072163029Z","updated_at":"2026-02-20T07:49:21.170406598Z","closed_at":"2026-02-20T07:49:21.170388003Z","close_reason":"Superseded by canonical detailed master-plan graph (bd-33v) with full 10.N-10.21 and 11-16 coverage, explicit dependencies, and per-section verification gates.","source_repo":".","compaction_level":0,"original_size":0}
{"id":"bd-32e","title":"Implement init command with profile bootstrapping","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md (Bootstrap bridge for Sections 10.0 and 10.1 operator onboarding)\nSection: BOOTSTRAP (CLI onboarding bridge)\n\nTask Objective:\nImplement `franken-node init` with profile bootstrapping so first-time operators can create deterministic, policy-aligned project configuration safely.\n\nIn Scope:\n- `init` command flow to generate baseline config/profile files.\n- Deterministic profile template selection and safe overwrite semantics.\n- Clear onboarding diagnostics for missing prerequisites and invalid target states.\n\nAcceptance Criteria:\n- `init` produces deterministic output files for equivalent inputs/options.\n- Existing configuration handling is explicit (confirm/abort/backup semantics) and non-destructive by default.\n- Generated profile artifacts are immediately consumable by downstream run/doctor flows.\n\nExpected Artifacts:\n- Init flow contract note with option matrix and file-output expectations.\n- Generated fixture snapshots for representative init scenarios.\n- Machine-readable init summary artifact for CI assertions.\n\n- Machine-readable verification artifact at `artifacts/section_bootstrap/bd-32e/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_bootstrap/bd-32e/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for template rendering, path handling, and overwrite-policy logic.\n- Integration tests validating end-to-end init behavior in clean and pre-existing directories.\n- E2E tests simulating first-run operator onboarding workflows.\n- Structured logs containing init phase events, file actions, and policy decisions with trace IDs.\n\nTask-Specific Clarification:\n- For \"Implement init command with profile bootstrapping\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"Implement init command with profile bootstrapping\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"Implement init command with profile bootstrapping\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"Implement init command with profile bootstrapping\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"Implement init command with profile bootstrapping\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:29:25.296726699Z","created_by":"ubuntu","updated_at":"2026-02-20T08:46:43.431587277Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["cli","config"],"dependencies":[{"issue_id":"bd-32e","depends_on_id":"bd-3rp","type":"blocks","created_at":"2026-02-20T07:29:39.546835453Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-32e","depends_on_id":"bd-3vk","type":"blocks","created_at":"2026-02-20T08:04:16.558688526Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-32e","depends_on_id":"bd-n9r","type":"blocks","created_at":"2026-02-20T07:29:39.613041834Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-32p","title":"[PLAN 10.18] Verifiable Execution Fabric Execution Track (9L)","description":"Section: 10.18 — Verifiable Execution Fabric Execution Track (9L)\n\nStrategic Context:\nVEF track for proof-carrying runtime compliance: receipt chains, commitment checkpoints, proof generation/verification, and claim gating.\n\nExecution Requirements:\n- Preserve all scoped capabilities from the canonical plan.\n- Keep contracts self-contained so future contributors can execute without reopening the master plan.\n- Require explicit testing strategy (unit + integration/e2e) and structured logging/telemetry for every child bead.\n- Require artifact-backed validation for performance, security, and correctness claims.\n\nDependency Semantics:\n- This epic is blocked by its child implementation beads.\n- Cross-epic dependencies encode strategic sequencing and canonical ownership boundaries.\n\n## Success Criteria\n- All child beads for this section are completed with acceptance artifacts attached and no unresolved blockers.\n- Section-level verification gate for comprehensive unit tests, integration/e2e workflows, and detailed structured logging evidence is green.\n- Scope-to-plan traceability is explicit, with no feature loss versus the canonical plan section.\n\n## Optimization Notes\n- User-Outcome Lens: \"[PLAN 10.18] Verifiable Execution Fabric Execution Track (9L)\" must improve operator confidence, safety posture, and deterministic recovery behavior under both normal and adversarial conditions.\n- Cross-Section Coordination: child beads must encode integration assumptions explicitly to avoid local optimizations that degrade system-wide correctness.\n- Verification Non-Negotiable: completion requires reproducible unit/integration/E2E evidence and structured logs suitable for independent replay.\n- Scope Protection: any simplification must preserve canonical-plan feature intent and be justified with explicit tradeoff documentation.","status":"open","priority":2,"issue_type":"epic","created_at":"2026-02-20T07:36:41.705187214Z","created_by":"ubuntu","updated_at":"2026-02-20T08:16:45.857951351Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-18"],"dependencies":[{"issue_id":"bd-32p","depends_on_id":"bd-16fq","type":"blocks","created_at":"2026-02-20T07:37:04.213682609Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-32p","depends_on_id":"bd-1o4v","type":"blocks","created_at":"2026-02-20T07:37:04.663389460Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-32p","depends_on_id":"bd-1u8m","type":"blocks","created_at":"2026-02-20T07:37:04.581684810Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-32p","depends_on_id":"bd-1wz","type":"blocks","created_at":"2026-02-20T07:37:11.557148743Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-32p","depends_on_id":"bd-28u0","type":"blocks","created_at":"2026-02-20T07:37:04.496043147Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-32p","depends_on_id":"bd-2hjg","type":"blocks","created_at":"2026-02-20T07:48:18.733878534Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-32p","depends_on_id":"bd-3g4k","type":"blocks","created_at":"2026-02-20T07:37:04.413295084Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-32p","depends_on_id":"bd-3go4","type":"blocks","created_at":"2026-02-20T07:37:04.992192425Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-32p","depends_on_id":"bd-3lzk","type":"blocks","created_at":"2026-02-20T07:37:05.238696122Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-32p","depends_on_id":"bd-3pds","type":"blocks","created_at":"2026-02-20T07:37:04.909485368Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-32p","depends_on_id":"bd-3ptu","type":"blocks","created_at":"2026-02-20T07:37:05.073989607Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-32p","depends_on_id":"bd-3qo","type":"blocks","created_at":"2026-02-20T07:37:11.516760954Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-32p","depends_on_id":"bd-4jh9","type":"blocks","created_at":"2026-02-20T07:37:04.827567831Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-32p","depends_on_id":"bd-5rh","type":"blocks","created_at":"2026-02-20T07:37:11.475902728Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-32p","depends_on_id":"bd-8qlj","type":"blocks","created_at":"2026-02-20T07:37:04.745789915Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-32p","depends_on_id":"bd-cda","type":"blocks","created_at":"2026-02-20T07:37:09.741081780Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-32p","depends_on_id":"bd-p73r","type":"blocks","created_at":"2026-02-20T07:37:04.327089580Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-32p","depends_on_id":"bd-ufk5","type":"blocks","created_at":"2026-02-20T07:37:05.155969188Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-32v","title":"[10.2] Implement minimized divergence fixture generation.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.2 — Compatibility Core\n\nWhy This Exists:\nCompatibility core as strategic wedge: policy-visible behavior, divergence receipts, L1/L2 lockstep, and spec-first fixture governance.\n\nTask Objective:\nImplement minimized divergence fixture generation.\n\nAcceptance Criteria:\n- Implement with explicit success/failure invariants and deterministic behavior under normal, edge, and fault scenarios.\n- Ensure outcomes are verifiable via automated tests and reproducible artifact outputs.\n\nExpected Artifacts:\n- Section-owned design/spec artifact at `docs/specs/section_10_2/bd-32v_contract.md`, capturing decision rationale, invariants, and interface boundaries.\n- Machine-readable verification artifact at `artifacts/section_10_2/bd-32v/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_2/bd-32v/verification_summary.md` linking implementation intent to measured outcomes.\n\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.2] Implement minimized divergence fixture generation.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.2] Implement minimized divergence fixture generation.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.2] Implement minimized divergence fixture generation.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.2] Implement minimized divergence fixture generation.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.2] Implement minimized divergence fixture generation.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","status":"closed","priority":1,"issue_type":"task","assignee":"CrimsonCrane","created_at":"2026-02-20T07:36:44.318888576Z","created_by":"ubuntu","updated_at":"2026-02-20T09:44:43.563344477Z","closed_at":"2026-02-20T09:44:43.563317597Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-2","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-32v","depends_on_id":"bd-2vi","type":"blocks","created_at":"2026-02-20T07:43:20.304057300Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-33b","title":"[10.5] Implement expected-loss action scoring with explicit loss matrices.","description":"# [10.5] Expected-Loss Action Scoring with Explicit Loss Matrices\n\n## Why This Exists\n\nThis bead implements the mathematical core of operator decision support in franken_node. Section 9C.8 (alien-artifact expected-loss vectors) requires that every recommended action be accompanied by an expected-loss vector and uncertainty band, so operators never make decisions based on opaque confidence scores. Section 9B.8 (VOI-based ranking) requires that actions be ranked by value-of-information so operator attention is directed to the highest-impact decisions first. Section 10.5 (Security + Policy Product Surfaces) positions this as the quantitative foundation that the operator copilot (bd-2yc), degraded-mode policy (bd-3nr), and policy change workflows (bd-sh3) all build upon.\n\nThe economic trust layer described in Section 10.0.9 depends on this scoring engine to translate trust-state observations into actionable economic quantities. Without explicit, configurable, and auditable loss matrices, operators cannot reason about trade-offs between competing actions, and the system cannot provide defensible recommendations. The plan mandates that loss matrices are never hard-coded: they must be operator-configurable, version-controlled, and included in the audit trail of every scored action.\n\n## What It Must Do\n\n1. **Loss Matrix Definition and Management**: Define a schema for loss matrices that map (action, outcome) pairs to scalar loss values. Support multiple loss dimensions (e.g., availability loss, security loss, compliance loss, financial loss). Matrices must be loadable from versioned configuration files and hot-reloadable without restart.\n\n2. **Expected-Loss Computation**: For each candidate action, compute the expected loss as the probability-weighted sum of losses across all possible outcomes. The computation must be deterministic for the same inputs (no floating-point non-determinism from parallel reduction).\n\n3. **Uncertainty Band Calculation**: For each expected-loss value, compute and report uncertainty bands (e.g., 5th/25th/50th/75th/95th percentiles) derived from the uncertainty in the input probability estimates. Use propagation of uncertainty or Monte Carlo sampling (configurable).\n\n4. **VOI-Based Ranking**: Implement Value-of-Information ranking per Section 9B.8. For each action, compute the VOI: the expected reduction in loss from obtaining additional information before deciding. Rank actions by VOI so the operator copilot can present the highest-impact decisions first.\n\n5. **Multi-Dimensional Loss Vectors**: Return loss as a vector across all configured dimensions, not a single scalar. The operator copilot and policy engine must be able to filter or weight dimensions according to operator preferences.\n\n6. **Scoring Audit Trail**: Every scoring invocation must produce an audit record containing: the input state, the loss matrix version used, the probability estimates, the computed expected-loss vector, the uncertainty bands, the VOI ranking, and a trace correlation ID.\n\n7. **Configurable Probability Sources**: The probability estimates feeding into expected-loss computation must come from pluggable sources (e.g., historical incident rates, real-time telemetry, operator priors). Each source must be identified in the audit record.\n\n## Acceptance Criteria\n\n1. Loss matrices are loaded from versioned TOML/JSON configuration files and can be hot-reloaded; the system logs the matrix version on every reload and on every scoring invocation.\n2. Expected-loss computation is deterministic: the same inputs always produce the same output, verified by a reproducibility test that runs the same scoring 1000 times and asserts bitwise equality.\n3. Uncertainty bands are computed for every expected-loss value; the method (analytical propagation or Monte Carlo with seed) is configurable and recorded in the audit record.\n4. VOI ranking is computed for all candidate actions and the ranking order is included in the scoring response.\n5. Loss vectors are multi-dimensional; the API returns per-dimension values, not a single aggregated scalar (aggregation is the caller's responsibility).\n6. Every scoring invocation writes an audit record to the scoring audit trail with all required fields (input state, matrix version, probabilities, expected-loss vector, uncertainty bands, VOI, trace ID).\n7. The scoring API responds within 10ms for up to 100 candidate actions with a 4-dimensional loss matrix and 10 possible outcomes per action (benchmark test required).\n8. Loss matrices with invalid schemas (missing dimensions, non-numeric values, inconsistent outcome sets) are rejected at load time with structured validation errors.\n9. The system exposes a /scoring/health endpoint that reports the current loss matrix version, the number of configured probability sources, and the last scoring invocation timestamp.\n10. All log events use stable codes (SCORING_INVOKED, SCORING_COMPLETED, MATRIX_LOADED, MATRIX_RELOAD, MATRIX_VALIDATION_FAILED, VOI_COMPUTED) with trace correlation IDs.\n\n## Key Dependencies\n\n- **Depends on bd-2yc** (operator copilot): the copilot consumes scoring results to build recommendations.\n- **Depends on probability source interfaces**: real-time telemetry and historical incident data feeds.\n- **Depended on by bd-3nr** (degraded-mode policy): degraded-mode notifications include expected-loss context.\n- **Depended on by bd-sh3** (policy change workflows): policy change impact assessment uses expected-loss scoring.\n- **Depended on by Section 10.0.9** (economic trust layer): trust economics are denominated in expected-loss units.\n- **Depended on by bd-1koz** (section-wide verification gate).\n\n## Testing and Logging Requirements\n\n- **Unit tests**: Loss matrix loading and validation (valid, invalid, partial). Expected-loss computation with known inputs and hand-verified expected outputs. Uncertainty band computation (analytical and Monte Carlo modes). VOI ranking with known VOI ordering. Determinism test (1000 identical invocations).\n- **Integration tests**: End-to-end scoring with multiple probability sources, verify audit trail completeness. Hot-reload loss matrix mid-operation and verify new matrix is used.\n- **E2E tests**: Simulate operator copilot requesting scores for a set of candidate actions during a degraded-mode scenario. Verify the full pipeline from probability input through scoring to copilot-formatted output.\n- **Adversarial tests**: Supply a loss matrix with NaN values, negative probabilities, probabilities summing to > 1.0, and verify graceful rejection. Supply conflicting probability sources and verify the system selects according to configured priority.\n- **Benchmark tests**: Measure and assert scoring latency for the target workload (100 actions, 4 dimensions, 10 outcomes). Track regression across builds.\n- **Logging**: Scoring invocations at INFO level. Matrix loads at WARN level (since they change system behavior). Validation failures at ERROR level. Per-action scoring details at DEBUG level.\n\n## Expected Artifacts\n\n- `docs/specs/section_10_5/bd-33b_contract.md` — Design spec with loss matrix schema, expected-loss algebra, VOI ranking algorithm, and uncertainty propagation method.\n- `artifacts/section_10_5/bd-33b/verification_evidence.json` — Machine-readable pass/fail evidence.\n- `artifacts/section_10_5/bd-33b/verification_summary.md` — Human-readable summary.\n- Rust module(s) in `crates/franken-node/src/` implementing the scoring engine, loss matrix loader, and audit trail emitter.\n- Python verification script `scripts/check_expected_loss_scoring.py` with `--json` flag and `self_test()`.\n- Unit tests in `tests/test_check_expected_loss_scoring.py`.\n- Benchmark fixture in `tests/bench_expected_loss_scoring.rs` or equivalent.","acceptance_criteria":"1. Define a LossMatrix struct as a named 2D matrix where rows are possible actions (including 'do nothing') and columns are possible outcome states; each cell holds an f64 loss value. The matrix must be explicitly constructed (no implicit defaults) so that every action-outcome pair has a deliberate loss assignment.\n2. Implement score_action(action: &str, loss_matrix: &LossMatrix, state_probabilities: &[f64]) -> ExpectedLossScore where ExpectedLossScore contains: action (string), expected_loss (f64, computed as dot product of the action's row with state_probabilities), dominant_outcome (the outcome state contributing the most to expected loss), and breakdown (Vec<(String, f64)> mapping each outcome to its weighted loss contribution).\n3. State probabilities must sum to 1.0 (within epsilon 1e-9); return Err if they do not.\n4. Provide a compare_actions(actions: &[&str], matrix: &LossMatrix, probs: &[f64]) -> Vec<ExpectedLossScore> that returns all actions sorted by expected_loss ascending (lowest loss = best action).\n5. Support sensitivity analysis: vary each state probability by +/- delta (configurable, default 0.05) and report which actions change rank, returned as a Vec<SensitivityRecord> with fields: parameter_name, delta, original_rank, perturbed_rank.\n6. Loss matrices must be serializable to/from JSON and must include a schema_version field for forward compatibility.\n7. Verification: scripts/check_loss_scoring.py --json constructs a 4-action x 5-outcome loss matrix from plan Section 9A.8 scenarios, computes scores, and asserts the lowest-loss action matches the analytically known answer; unit tests in tests/test_check_loss_scoring.py cover degenerate matrices (single action, single outcome), probability validation, and sensitivity analysis; evidence in artifacts/section_10_5/bd-33b/.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:36:46.464089771Z","created_by":"ubuntu","updated_at":"2026-02-20T16:59:01.247598088Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-5","self-contained","test-obligations"]}
{"id":"bd-33kj","title":"[10.15] Define claim-language policy tying trust/replay claims to asupersync-backed invariant evidence.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.15 — Asupersync-First Integration Execution Track (8.4-8.6)\n\nWhy This Exists:\nAsupersync-first control-plane migration track enforcing Cx-first, region ownership, cancellation correctness, and protocol-grade verification gates.\n\nTask Objective:\nDefine claim-language policy tying trust/replay claims to asupersync-backed invariant evidence.\n\nAcceptance Criteria:\n- Public claim templates enforce evidence references; unverifiable claim text is blocked by documentation gate.\n\nExpected Artifacts:\n- `docs/policy/claim_language_asupersync_requirements.md`, `tests/conformance/claim_language_gate.rs`, `artifacts/10.15/claim_language_gate_report.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_15/bd-33kj/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_15/bd-33kj/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.15] Define claim-language policy tying trust/replay claims to asupersync-backed invariant evidence.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.15] Define claim-language policy tying trust/replay claims to asupersync-backed invariant evidence.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.15] Define claim-language policy tying trust/replay claims to asupersync-backed invariant evidence.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.15] Define claim-language policy tying trust/replay claims to asupersync-backed invariant evidence.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.15] Define claim-language policy tying trust/replay claims to asupersync-backed invariant evidence.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"- Public claim templates enforce evidence references; unverifiable claim text is blocked by documentation gate.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:37:01.445380165Z","created_by":"ubuntu","updated_at":"2026-02-20T17:04:48.828754019Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-15","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-33kj","depends_on_id":"bd-1id0","type":"blocks","created_at":"2026-02-20T17:04:48.828703274Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-33u2","title":"[16] Output contract: widely used verifier/benchmark releases","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 16\n\nTask Objective:\nDeliver widely used open verifier or benchmark tool releases.\n\nExecution Requirements:\n- Make this requirement machine-verifiable and release-gated where applicable.\n- Keep behavior self-documenting so future contributors do not need to re-open the master plan.\n\nTesting & Logging Requirements:\n- Unit tests for rule/contract logic and error conditions.\n- Integration/E2E tests for workflow-level enforcement.\n- Structured logs with stable codes and trace IDs.\n- Reproducible artifacts that prove contract satisfaction.\n\nAcceptance Criteria:\n- Success and failure invariants for [16] Output contract: widely used verifier/benchmark releases are explicitly quantified and machine-verifiable.\n- Determinism requirements for [16] Output contract: widely used verifier/benchmark releases are validated across repeated runs and adversarial perturbations.\n\n\nExpected Artifacts:\n- Machine-readable verification artifact with explicit canonical path under artifacts/section_16/bd-33u2/verification_evidence.json, suitable for CI/release gating.\n- Human-readable verification summary with explicit canonical path under artifacts/section_16/bd-33u2/verification_summary.md, linking implementation intent to measured validation outcomes.\n\nTask-Specific Clarification:\n- The implementation must deliver the exact capability named in the title, with no scope dilution and no silent feature omission.\n- Validation artifacts must include capability-specific thresholds, pass/fail criteria, and reproducible evidence bundles tied to this bead ID.\n- Program/section verification gates must be able to consume this bead's outputs without manual interpretation.\n\nWhy This Improves User Outcomes:\n- \"[16] Output contract: widely used verifier/benchmark releases\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[16] Output contract: widely used verifier/benchmark releases\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"1. Verifier toolkit has >= 100 downloads/installs across all distribution channels (npm, cargo, Docker) within 6 months of release.\n2. Benchmark suite has >= 50 external runs (tracked via telemetry opt-in or usage reports) within 6 months of release.\n3. At least 3 external projects or organizations have adopted the verifier or benchmark tools (documented via case studies, blog posts, or direct feedback).\n4. Tools are maintained with: (a) bug fixes within 14 days of report, (b) compatibility updates within 30 days of major dependency changes, (c) documentation updates with each release.\n5. External contribution: at least 2 external pull requests or issues from non-team members (indicating community engagement).\n6. Tools are listed/indexed in relevant package registries and discovery platforms.\n7. User feedback is collected and acted upon: at least 1 feature or improvement driven by external user feedback per quarter.\n8. Evidence: tool_adoption_metrics.json with download counts, known users, external contributions, and feedback-driven improvements.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:39:37.389351490Z","created_by":"ubuntu","updated_at":"2026-02-20T15:29:25.575123721Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-16","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-33u2","depends_on_id":"bd-e5cz","type":"blocks","created_at":"2026-02-20T07:43:26.856869179Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-33v","title":"[PLAN] franken_node master execution graph (full 10.x)","description":"Master execution graph for PLAN_TO_CREATE_FRANKEN_NODE.md.\n\nPurpose:\n- Preserve full feature scope and ambition of the canonical plan.\n- Organize all 10.x execution tasks into self-contained, dependency-aware beads.\n- Ensure every capability has explicit implementation, testing, observability, and evidence obligations.\n\nDelivery doctrine:\n- No oversimplification and no silent feature loss.\n- Unit tests + integration/e2e + detailed structured logging are mandatory for every implementation family.\n- Claims must be verifier-backed with reproducible artifacts.\n\n## Success Criteria\n- All child beads for this section are completed with acceptance artifacts attached and no unresolved blockers.\n- Section-level verification gate for comprehensive unit tests, integration/e2e workflows, and detailed structured logging evidence is green.\n- Scope-to-plan traceability is explicit, with no feature loss versus the canonical plan section.\n\n## Optimization Notes\n- User-Outcome Lens: \"[PLAN] franken_node master execution graph (full 10.x)\" must improve operator confidence, safety posture, and deterministic recovery behavior under both normal and adversarial conditions.\n- Cross-Section Coordination: child beads must encode integration assumptions explicitly to avoid local optimizations that degrade system-wide correctness.\n- Verification Non-Negotiable: completion requires reproducible unit/integration/E2E evidence and structured logs suitable for independent replay.\n- Scope Protection: any simplification must preserve canonical-plan feature intent and be justified with explicit tradeoff documentation.","status":"open","priority":0,"issue_type":"epic","created_at":"2026-02-20T07:35:22.836795409Z","created_by":"ubuntu","updated_at":"2026-02-20T16:17:12.517082169Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["execution-graph","master","plan"],"dependencies":[{"issue_id":"bd-33v","depends_on_id":"bd-10zx","type":"blocks","created_at":"2026-02-20T07:56:11.666208988Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-33v","depends_on_id":"bd-1hf","type":"blocks","created_at":"2026-02-20T07:36:41.067060798Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-33v","depends_on_id":"bd-1ow","type":"blocks","created_at":"2026-02-20T07:36:40.333074686Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-33v","depends_on_id":"bd-1ps","type":"blocks","created_at":"2026-02-20T07:36:42.199819366Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-33v","depends_on_id":"bd-1ta","type":"blocks","created_at":"2026-02-20T07:36:41.320945114Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-33v","depends_on_id":"bd-1u9","type":"blocks","created_at":"2026-02-20T07:36:40.740950950Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-33v","depends_on_id":"bd-1wz","type":"blocks","created_at":"2026-02-20T07:36:41.660104369Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-33v","depends_on_id":"bd-1xg","type":"blocks","created_at":"2026-02-20T07:36:40.581261539Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-33v","depends_on_id":"bd-20a","type":"blocks","created_at":"2026-02-20T07:36:40.660962957Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-33v","depends_on_id":"bd-20z","type":"blocks","created_at":"2026-02-20T07:36:41.148065604Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-33v","depends_on_id":"bd-229","type":"blocks","created_at":"2026-02-20T07:36:40.499974958Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-33v","depends_on_id":"bd-22e7","type":"blocks","created_at":"2026-02-20T16:17:11.840722161Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-33v","depends_on_id":"bd-26k","type":"blocks","created_at":"2026-02-20T07:36:40.986774751Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-33v","depends_on_id":"bd-28wj","type":"blocks","created_at":"2026-02-20T16:17:11.669998700Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-33v","depends_on_id":"bd-2g8","type":"blocks","created_at":"2026-02-20T07:36:42.128788568Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-33v","depends_on_id":"bd-2hrg","type":"blocks","created_at":"2026-02-20T16:17:11.501178014Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-33v","depends_on_id":"bd-2j9w","type":"blocks","created_at":"2026-02-20T08:08:10.938497041Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-33v","depends_on_id":"bd-2ke","type":"blocks","created_at":"2026-02-20T07:36:42.293903525Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-33v","depends_on_id":"bd-2vl5","type":"blocks","created_at":"2026-02-20T16:17:12.180252732Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-33v","depends_on_id":"bd-32p","type":"blocks","created_at":"2026-02-20T07:36:41.740397650Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-33v","depends_on_id":"bd-37i","type":"blocks","created_at":"2026-02-20T07:36:41.985807540Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-33v","depends_on_id":"bd-39a","type":"blocks","created_at":"2026-02-20T07:36:41.823195546Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-33v","depends_on_id":"bd-3fo","type":"blocks","created_at":"2026-02-20T07:36:40.251374936Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-33v","depends_on_id":"bd-3hig","type":"blocks","created_at":"2026-02-20T16:17:12.517002160Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-33v","depends_on_id":"bd-3hyk","type":"blocks","created_at":"2026-02-20T16:17:11.329674419Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-33v","depends_on_id":"bd-3il","type":"blocks","created_at":"2026-02-20T07:36:40.417428981Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-33v","depends_on_id":"bd-3qo","type":"blocks","created_at":"2026-02-20T07:36:41.489494708Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-33v","depends_on_id":"bd-3rc","type":"blocks","created_at":"2026-02-20T07:36:40.824379771Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-33v","depends_on_id":"bd-4ou","type":"blocks","created_at":"2026-02-20T07:36:42.057208007Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-33v","depends_on_id":"bd-5rh","type":"blocks","created_at":"2026-02-20T07:36:41.405409033Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-33v","depends_on_id":"bd-c4f","type":"blocks","created_at":"2026-02-20T07:36:40.905079449Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-33v","depends_on_id":"bd-cda","type":"blocks","created_at":"2026-02-20T07:36:40.169253630Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-33v","depends_on_id":"bd-go4","type":"blocks","created_at":"2026-02-20T07:36:41.232660257Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-33v","depends_on_id":"bd-k25j","type":"blocks","created_at":"2026-02-20T16:17:12.346641235Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-33v","depends_on_id":"bd-n71","type":"blocks","created_at":"2026-02-20T07:36:41.576712857Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-33v","depends_on_id":"bd-r6i","type":"blocks","created_at":"2026-02-20T07:36:42.440815415Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-33v","depends_on_id":"bd-t8m","type":"blocks","created_at":"2026-02-20T07:36:42.367219649Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-33v","depends_on_id":"bd-ud5h","type":"blocks","created_at":"2026-02-20T16:17:12.013567616Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-33v","depends_on_id":"bd-ybe","type":"blocks","created_at":"2026-02-20T07:36:41.904704050Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-33x","title":"[10.3] Build migration risk scoring model with explainable features.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.3 — Migration System\n\nWhy This Exists:\nMigration autopilot pipeline from audit to rollout, designed to collapse migration friction with deterministic confidence and replayability.\n\nTask Objective:\nBuild migration risk scoring model with explainable features.\n\nAcceptance Criteria:\n- Implement with explicit success/failure invariants and deterministic behavior under normal, edge, and fault scenarios.\n- Ensure outcomes are verifiable via automated tests and reproducible artifact outputs.\n\nExpected Artifacts:\n- Section-owned design/spec artifact at `docs/specs/section_10_3/bd-33x_contract.md`, capturing decision rationale, invariants, and interface boundaries.\n- Machine-readable verification artifact at `artifacts/section_10_3/bd-33x/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_3/bd-33x/verification_summary.md` linking implementation intent to measured outcomes.\n\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.3] Build migration risk scoring model with explainable features.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.3] Build migration risk scoring model with explainable features.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.3] Build migration risk scoring model with explainable features.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.3] Build migration risk scoring model with explainable features.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.3] Build migration risk scoring model with explainable features.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","status":"closed","priority":1,"issue_type":"task","assignee":"CrimsonCrane","created_at":"2026-02-20T07:36:44.877076923Z","created_by":"ubuntu","updated_at":"2026-02-20T10:10:42.465633073Z","closed_at":"2026-02-20T10:10:42.465607315Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-3","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-33x","depends_on_id":"bd-2a0","type":"blocks","created_at":"2026-02-20T07:43:22.054752439Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-3429","title":"Epic: Evidence Ledger System [10.14a]","description":"- type: epic","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-20T07:47:58.072163029Z","updated_at":"2026-02-20T07:49:21.198491417Z","closed_at":"2026-02-20T07:49:21.198474215Z","close_reason":"Superseded by canonical detailed master-plan graph (bd-33v) with full 10.N-10.21 and 11-16 coverage, explicit dependencies, and per-section verification gates.","source_repo":".","compaction_level":0,"original_size":0}
{"id":"bd-34d5","title":"[13] Concrete target gate: friction-minimized install-to-first-safe-production pathway for representative setups","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 13 — Program Success Criteria\n\nWhy This Exists:\nSection 13 defines 6 qualitative success criteria and 6 concrete quantitative target gates. This bead covers the quantitative target requiring a friction-minimized install-to-first-safe-production pathway that works for representative project setups.\n\nTask Objective:\nDefine and validate a friction-minimized install-to-first-safe-production pathway for representative project setups. The onboarding experience from curl install through franken-node init through franken-node run --policy balanced must complete with minimal manual steps, clear feedback, and no silent failures for common Node/Bun project archetypes.\n\nAcceptance Criteria:\n- Define representative setups cohort (minimum 5 project archetypes).\n- Measure end-to-end install-to-production time for each archetype.\n- Gate requires completion under defined time budget.\n- Zero manual file edits required for balanced-profile onboarding on standard archetypes.\n- All steps emit structured progress/error telemetry.\n- Gate failure blocks release.\n\nExpected Artifacts:\n- tests/e2e/install_to_production_pathway.sh\n- docs/success_criteria/friction_minimized_pathway.md\n- artifacts/13/friction_pathway_report.json\n\n- Machine-readable verification artifact at `artifacts/section_13/bd-34d5/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_13/bd-34d5/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- E2E test scripts exercising full install-init-audit-run pipeline per archetype.\n- Structured logging with stable codes and trace correlation IDs.\n- Deterministic failure reproduction via captured telemetry bundles.\n\nTask-Specific Clarification:\n- For \"[13] Concrete target gate: friction-minimized install-to-first-safe-production pathway for representative setups\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[13] Concrete target gate: friction-minimized install-to-first-safe-production pathway for representative setups\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[13] Concrete target gate: friction-minimized install-to-first-safe-production pathway for representative setups\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[13] Concrete target gate: friction-minimized install-to-first-safe-production pathway for representative setups\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[13] Concrete target gate: friction-minimized install-to-first-safe-production pathway for representative setups\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"1. A representative set of >= 5 setup configurations is defined: (a) macOS + npm, (b) Linux + npm, (c) Windows + npm, (d) Docker container, (e) CI environment (GitHub Actions).\n2. For each setup, the install-to-first-safe-production pathway is documented step-by-step.\n3. Total time from 'npm install franken-node' to first production-safe process running is <= 10 minutes for each setup.\n4. 'Production-safe' means: hardening profile active, trust system initialized, compatibility checks passing, health endpoint responding.\n5. No step in the pathway requires manual configuration beyond environment variables or a single config file.\n6. Pathway is tested in CI for at least 3 of the 5 setups (macOS, Linux, Docker).\n7. Failure modes are documented: if any step fails, the user gets an actionable error message with resolution steps.\n8. Evidence artifact: onboarding_timing_report.json with per-setup step timings and total elapsed time.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T08:01:30.881812092Z","created_by":"ubuntu","updated_at":"2026-02-20T16:08:17.871753515Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-13","self-contained","test-obligations"]}
{"id":"bd-34ll","title":"[10.16] Define `frankentui` integration contract for all relevant console/TUI surfaces.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.16 — Adjacent Substrate Integration Execution Track (8.7)\n\nWhy This Exists:\nAdjacent substrate integration track to enforce deep composition with frankentui, frankensqlite, sqlmodel_rust, and fastapi_rust.\n\nTask Objective:\nDefine `frankentui` integration contract for all relevant console/TUI surfaces.\n\nAcceptance Criteria:\n- Contract specifies component boundaries, styling/token strategy, and rendering/event-loop integration expectations.\n\nExpected Artifacts:\n- `docs/specs/frankentui_integration_contract.md`, `artifacts/10.16/frankentui_contract_checklist.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_16/bd-34ll/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_16/bd-34ll/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.16] Define `frankentui` integration contract for all relevant console/TUI surfaces.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.16] Define `frankentui` integration contract for all relevant console/TUI surfaces.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.16] Define `frankentui` integration contract for all relevant console/TUI surfaces.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.16] Define `frankentui` integration contract for all relevant console/TUI surfaces.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.16] Define `frankentui` integration contract for all relevant console/TUI surfaces.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"- Contract specifies component boundaries, styling/token strategy, and rendering/event-loop integration expectations.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:37:01.689778611Z","created_by":"ubuntu","updated_at":"2026-02-20T17:00:56.408554369Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-16","self-contained","test-obligations"]}
{"id":"bd-351r","title":"[10.20] Add ATC interoperability for topology indicators and federated cascade priors.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.20 — Dependency Graph Immune System Execution Track (9N)\n\nWhy This Exists:\nDGIS topological immune system track for dependency contagion modeling, choke-point immunization, and graph-aware containment economics.\n\nTask Objective:\nAdd ATC interoperability for topology indicators and federated cascade priors.\n\nAcceptance Criteria:\n- DGIS emits privacy-preserving topology indicators to ATC and consumes federated priors without raw dependency disclosure; ingestion/output contracts are versioned and verifier-checkable.\n\nExpected Artifacts:\n- `src/federation/dgis_atc_bridge.rs`, `tests/integration/dgis_atc_interop.rs`, `artifacts/10.20/dgis_atc_exchange_report.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_20/bd-351r/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_20/bd-351r/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.20] Add ATC interoperability for topology indicators and federated cascade priors.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.20] Add ATC interoperability for topology indicators and federated cascade priors.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.20] Add ATC interoperability for topology indicators and federated cascade priors.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.20] Add ATC interoperability for topology indicators and federated cascade priors.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.20] Add ATC interoperability for topology indicators and federated cascade priors.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"- DGIS emits privacy-preserving topology indicators to ATC and consumes federated priors without raw dependency disclosure; ingestion/output contracts are versioned and verifier-checkable.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:37:07.161493170Z","created_by":"ubuntu","updated_at":"2026-02-20T17:00:54.828767087Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-20","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-351r","depends_on_id":"bd-3aqy","type":"blocks","created_at":"2026-02-20T15:01:15.095236299Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-35by","title":"[10.13] Build mandatory serialization/object-id/signature/revocation/source-diversity interop suites.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.13 — FCP Deep-Mined Expansion Execution Track (9I)\n\nWhy This Exists:\nDeep connector/provider contract track: lifecycle FSMs, conformance harnesses, fencing, sandboxing, revocation freshness, and protocol hardening.\n\nTask Objective:\nBuild mandatory serialization/object-id/signature/revocation/source-diversity interop suites.\n\nAcceptance Criteria:\n- Interop suite covers all mandatory classes and passes across independent implementations; failures include minimal reproducer fixtures.\n\nExpected Artifacts:\n- `tests/interop/*.rs`, `fixtures/interop/*.json`, `artifacts/10.13/interop_results_matrix.csv`.\n\n- Machine-readable verification artifact at `artifacts/section_10_13/bd-35by/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_13/bd-35by/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.13] Build mandatory serialization/object-id/signature/revocation/source-diversity interop suites.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.13] Build mandatory serialization/object-id/signature/revocation/source-diversity interop suites.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.13] Build mandatory serialization/object-id/signature/revocation/source-diversity interop suites.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.13] Build mandatory serialization/object-id/signature/revocation/source-diversity interop suites.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.13] Build mandatory serialization/object-id/signature/revocation/source-diversity interop suites.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","status":"closed","priority":1,"issue_type":"task","assignee":"CrimsonCrane","created_at":"2026-02-20T07:36:54.899465986Z","created_by":"ubuntu","updated_at":"2026-02-20T13:32:26.457742510Z","closed_at":"2026-02-20T13:32:26.457715409Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-13","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-35by","depends_on_id":"bd-ck2h","type":"blocks","created_at":"2026-02-20T07:43:14.062691295Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-35l5","title":"[10.16] Add performance overhead guardrails for adjacent substrate integrations.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.16 — Adjacent Substrate Integration Execution Track (8.7)\n\nWhy This Exists:\nAdjacent substrate integration track to enforce deep composition with frankentui, frankensqlite, sqlmodel_rust, and fastapi_rust.\n\nTask Objective:\nAdd performance overhead guardrails for adjacent substrate integrations.\n\nAcceptance Criteria:\n- Integration overhead budgets are defined and enforced; regressions fail perf gate with before/after evidence.\n\nExpected Artifacts:\n- `tests/perf/adjacent_substrate_overhead_gate.rs`, `artifacts/10.16/adjacent_substrate_overhead_report.csv`.\n\n- Machine-readable verification artifact at `artifacts/section_10_16/bd-35l5/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_16/bd-35l5/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.16] Add performance overhead guardrails for adjacent substrate integrations.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.16] Add performance overhead guardrails for adjacent substrate integrations.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.16] Add performance overhead guardrails for adjacent substrate integrations.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.16] Add performance overhead guardrails for adjacent substrate integrations.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.16] Add performance overhead guardrails for adjacent substrate integrations.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"- Integration overhead budgets are defined and enforced; regressions fail perf gate with before/after evidence.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:37:02.764016223Z","created_by":"ubuntu","updated_at":"2026-02-20T17:00:56.721818044Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-16","self-contained","test-obligations"]}
{"id":"bd-35m7","title":"[12] Risk control: trajectory-gaming camouflage","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 12\n\nTask Objective:\nImplement mimicry corpus tests, motif randomization stress tests, and hybrid signal fusion safeguards.\n\nExecution Requirements:\n- Make this requirement machine-verifiable and release-gated where applicable.\n- Keep behavior self-documenting so future contributors do not need to re-open the master plan.\n\nTesting & Logging Requirements:\n- Unit tests for rule/contract logic and error conditions.\n- Integration/E2E tests for workflow-level enforcement.\n- Structured logs with stable codes and trace IDs.\n- Reproducible artifacts that prove contract satisfaction.\n\nAcceptance Criteria:\n- Success and failure invariants for [12] Risk control: trajectory-gaming camouflage are explicitly quantified and machine-verifiable.\n- Determinism requirements for [12] Risk control: trajectory-gaming camouflage are validated across repeated runs and adversarial perturbations.\n\n\nExpected Artifacts:\n- Machine-readable verification artifact with explicit canonical path under artifacts/section_12/bd-35m7/verification_evidence.json, suitable for CI/release gating.\n- Human-readable verification summary with explicit canonical path under artifacts/section_12/bd-35m7/verification_summary.md, linking implementation intent to measured validation outcomes.\n\nTask-Specific Clarification:\n- The implementation must deliver the exact capability named in the title, with no scope dilution and no silent feature omission.\n- Validation artifacts must include capability-specific thresholds, pass/fail criteria, and reproducible evidence bundles tied to this bead ID.\n- Program/section verification gates must be able to consume this bead's outputs without manual interpretation.\n\nWhy This Improves User Outcomes:\n- \"[12] Risk control: trajectory-gaming camouflage\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[12] Risk control: trajectory-gaming camouflage\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"RISK: Trajectory-gaming camouflage — malicious actors craft behavioral trajectories that mimic legitimate patterns to evade detection.\nIMPACT: Malicious extensions pass trust checks by mimicking benign behavior patterns, undermining the entire behavioral trust system.\nCOUNTERMEASURES:\n  (a) Adversarial mimicry corpus: maintain a dataset of known mimicry patterns; trust models are trained to detect them.\n  (b) Motif randomization: detection features include randomized behavioral motifs that are hard for attackers to predict and replicate.\n  (c) Hybrid signal fusion: trust decisions fuse behavioral signals with non-behavioral signals (provenance, code analysis, reputation) so gaming one channel is insufficient.\nVERIFICATION:\n  1. Adversarial mimicry corpus contains >= 100 mimicry patterns, updated at least quarterly.\n  2. Trust model detects >= 90% of known mimicry patterns in the corpus (measured by recall).\n  3. Hybrid fusion: a node gaming behavioral signals but failing provenance or code-analysis checks is still correctly flagged.\n  4. Motif randomization: two consecutive evaluations of the same trajectory use different feature subsets (verified by feature-set logging).\nTEST SCENARIOS:\n  - Scenario A: Submit a trajectory matching a known mimicry pattern; verify trust system flags it with >= 90% confidence.\n  - Scenario B: Submit a trajectory that games behavioral signals perfectly but has suspicious provenance; verify hybrid fusion catches it.\n  - Scenario C: Run the same trajectory through detection twice; verify different randomized motifs are used each time.\n  - Scenario D: Add a new mimicry pattern to the corpus; retrain model; verify detection rate remains >= 90%.\n  - Scenario E: Adaptive adversary: evolve mimicry across 10 rounds; verify detection rate stays >= 80% even against evolving attacks.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:39:34.218605784Z","created_by":"ubuntu","updated_at":"2026-02-20T15:20:40.084263243Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-12","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-35m7","depends_on_id":"bd-1rff","type":"blocks","created_at":"2026-02-20T07:43:25.215361876Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-35q1","title":"[10.13] Implement threshold signature verification for connector publication artifacts.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.13 — FCP Deep-Mined Expansion Execution Track (9I)\n\nWhy This Exists:\nDeep connector/provider contract track: lifecycle FSMs, conformance harnesses, fencing, sandboxing, revocation freshness, and protocol hardening.\n\nTask Objective:\nImplement threshold signature verification for connector publication artifacts.\n\nAcceptance Criteria:\n- Publication requires configured threshold quorum; partial signature sets are rejected; verification failures produce stable failure reasons.\n\nExpected Artifacts:\n- `docs/specs/threshold_signatures.md`, `tests/security/threshold_signature_verification.rs`, `artifacts/10.13/threshold_signature_vectors.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_13/bd-35q1/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_13/bd-35q1/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.13] Implement threshold signature verification for connector publication artifacts.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.13] Implement threshold signature verification for connector publication artifacts.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.13] Implement threshold signature verification for connector publication artifacts.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.13] Implement threshold signature verification for connector publication artifacts.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.13] Implement threshold signature verification for connector publication artifacts.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","status":"closed","priority":1,"issue_type":"task","assignee":"CrimsonCrane","created_at":"2026-02-20T07:36:52.618388717Z","created_by":"ubuntu","updated_at":"2026-02-20T11:39:28.801633636Z","closed_at":"2026-02-20T11:39:28.801608279Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-13","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-35q1","depends_on_id":"bd-3n58","type":"blocks","created_at":"2026-02-20T07:43:12.887467919Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-364","title":"[10.10] Implement key-role separation for control-plane signing/encryption/issuance.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md — Section 10.10 (FCP-Inspired Hardening), cross-ref Section 9E.5\n\n## Why This Exists\n\nEnhancement Map 9E.5 requires key-role separation and owner-signed operational attestations for the control plane. In a system where a single key handles signing, encryption, and token issuance, compromise of that key grants the attacker full control over all three functions simultaneously. By enforcing strict key-role separation — distinct keys for signing (authenticity), encryption (confidentiality), and issuance (authority) — the blast radius of any single key compromise is contained to one operational domain. This is essential for the three-kernel architecture where each kernel must independently verify that control-plane messages were signed by the correct role-specific key, not merely any key belonging to the sender. This bead establishes the key-role registry and enforcement layer that all downstream trust operations (session auth, revocation, zone segmentation) depend upon.\n\n## What This Must Do\n\n1. Define a `KeyRole` enum with at least three mandatory roles: `Signing` (authenticates control-plane messages and attestations), `Encryption` (protects confidential control-plane payloads in transit and at rest), and `Issuance` (creates delegation tokens and authority certificates).\n2. Implement a `KeyRoleRegistry` that binds each cryptographic key to exactly one role, with a one-to-one invariant: a key can serve only one role, and each active role must have exactly one bound key at any time.\n3. Provide `attest(key_role, payload) -> SignedAttestation` API that requires the caller to specify the intended role, retrieves the role-bound key, and produces a signed attestation with explicit role metadata in the signature preimage (via bd-jjm's canonical serializer).\n4. Implement key rotation primitives: `rotate_key(role, new_key, owner_authorization)` that requires an owner-signed authorization (distinct from the role keys themselves) to replace a role key, ensuring no role key can self-rotate.\n5. Add compile-time or runtime guards preventing any code path from using a key outside its registered role — e.g., attempting to encrypt with a signing key produces `KEY_ROLE_MISMATCH`.\n6. Integrate role metadata into the `AudienceBoundToken` from bd-1r2: token issuance must use the `Issuance` key, token verification checks that the issuer used the correct role key.\n\n## Context from Enhancement Maps\n\n- 9E.5: \"Key-role separation and owner-signed operational attestations\"\n- 9E.4 (cross-ref): Token delegation chains (bd-1r2) depend on role-separated issuance keys to prevent signing/issuance confusion.\n- 9E.6 (cross-ref): Session-authenticated control channels (bd-oty) use the encryption role key for channel establishment and the signing role key for message authentication.\n- 9E.8 (cross-ref): Zone trust boundaries (bd-1vp) use role-specific keys to sign zone boundary claims.\n\n## Dependencies\n\n- Upstream: bd-1r2 ([10.10] Implement audience-bound token chains for control actions) — key-role separation refines the token issuance path established by the token chain system.\n- Downstream: bd-oty ([10.10] Integrate canonical session-authenticated control channel) — session establishment requires role-separated keys.\n- Downstream: bd-1jjq ([10.10] Section-wide verification gate) — aggregates verification evidence.\n\n## Acceptance Criteria\n\n1. `KeyRole` enum defines at minimum `Signing`, `Encryption`, and `Issuance` roles with documented security semantics for each.\n2. `KeyRoleRegistry` enforces the one-to-one invariant: binding a key already bound to another role is rejected with `KEY_ALREADY_BOUND`; binding a second key to an already-filled role is rejected with `ROLE_ALREADY_FILLED`.\n3. Using a key outside its registered role is rejected with `KEY_ROLE_MISMATCH` at the call site — zero bypass paths verified by exhaustive code path analysis.\n4. Key rotation requires owner-signed authorization: attempting to rotate without valid owner signature is rejected with `ROTATION_UNAUTHORIZED`.\n5. Role key self-rotation is impossible: a signing key cannot authorize its own replacement (the owner key is a separate, higher-authority key).\n6. All attestation outputs include role metadata in the canonical preimage, and an attestation signed with the wrong role key fails verification with a clear diagnostic.\n7. Integration with bd-1r2's token chain is verified: tokens issued with a non-`Issuance` key are rejected during chain verification.\n8. Verification evidence JSON includes role configurations tested, rotation scenarios, and mismatch rejection counts.\n\n## Testing & Logging Requirements\n\n- Unit tests: Register keys for all three roles and verify correct binding. Attempt to use each key for a non-matching role and verify rejection. Test key rotation with valid and invalid owner authorization. Test that after rotation, the old key is no longer usable for its former role. Test concurrent access to the registry under multiple threads.\n- Integration tests: End-to-end flow: issue a delegation token using the Issuance key, sign a control message using the Signing key, encrypt a payload using the Encryption key — verify each operation uses the correct role key and cross-role usage is impossible. Verify that role metadata in attestations survives canonical serialization round-trip.\n- Adversarial tests: Attempt to register the same key material for two different roles. Attempt owner-key impersonation by signing a rotation request with a role key instead of the owner key. Attempt to forge attestation role metadata after signing. Test behavior when the registry is in a partially-rotated state (one role rotated, others not).\n- Structured logs: `KEY_ROLE_BOUND` (role, key_fingerprint, bound_by_owner). `KEY_ROLE_ROTATED` (role, old_key_fingerprint, new_key_fingerprint, authorized_by). `KEY_ROLE_MISMATCH` (attempted_role, actual_role, key_fingerprint, caller_location). `ATTESTATION_CREATED` (role, payload_type, payload_hash_prefix). All events include `trace_id` and `epoch_id`.\n\n## Expected Artifacts\n\n- docs/specs/section_10_10/bd-364_contract.md\n- crates/franken-node/src/connector/key_role_registry.rs (or similar module path)\n- scripts/check_key_role_separation.py with --json flag and self_test()\n- tests/test_check_key_role_separation.py\n- artifacts/section_10_10/bd-364/verification_evidence.json\n- artifacts/section_10_10/bd-364/verification_summary.md","acceptance_criteria":"1. Define a KeyRole enum with exactly four variants: SIGNING (Ed25519/ECDSA for authentication), ENCRYPTION (X25519/AES for confidentiality), ISSUANCE (dedicated key for minting tokens/certificates), ATTESTATION (dedicated key for operator attestation signatures). Each variant has a fixed 2-byte role tag.\n2. Define a KeyRoleBinding struct containing: (a) key_id (TrustObjectId with KEY domain), (b) role (KeyRole), (c) public_key_bytes, (d) bound_at (UTC timestamp), (e) bound_by (TrustObjectId of the authority that approved the binding), (f) max_validity_seconds (u64).\n3. Enforce role exclusivity: a single key_id MUST NOT be bound to more than one role. Attempting to bind the same key_id to a second role returns RoleSeparationViolation error.\n4. Implement a KeyRoleRegistry that stores active bindings and supports: (a) bind(key_id, role, public_key, authority) -> Result, (b) lookup(key_id) -> Option<KeyRoleBinding>, (c) lookup_by_role(role) -> Vec<KeyRoleBinding>, (d) revoke(key_id, authority) that moves the binding to a revoked set.\n5. Enforce that cryptographic operations check role before use: a SIGNING key MUST NOT be used for encryption operations, and vice versa. Provide a guard function verify_role(key_id, expected_role) -> Result.\n6. Implement key rotation: bind a new key_id to the same role while revoking the old one, atomically (both operations succeed or neither does).\n7. Emit structured log events for: bind, revoke, rotation, and role-violation-attempt, each with trace correlation ID and severity.\n8. Unit tests: (a) bind each role type, (b) role exclusivity violation, (c) lookup by ID and by role, (d) revoke and re-lookup returns None, (e) rotation atomicity (verify old key is revoked and new key is bound after rotation), (f) verify_role guard pass/fail.\n9. Verification: scripts/check_key_role_separation.py --json, artifacts at artifacts/section_10_10/bd-364/.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:36:49.167021491Z","created_by":"ubuntu","updated_at":"2026-02-20T17:01:30.544643096Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-10","self-contained","test-obligations"]}
{"id":"bd-36wa","title":"[11] Contract field: compatibility and threat evidence","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 11\n\nTask Objective:\nRequire compatibility and threat evidence payloads for each major subsystem proposal.\n\nExecution Requirements:\n- Make this requirement machine-verifiable and release-gated where applicable.\n- Keep behavior self-documenting so future contributors do not need to re-open the master plan.\n\nTesting & Logging Requirements:\n- Unit tests for rule/contract logic and error conditions.\n- Integration/E2E tests for workflow-level enforcement.\n- Structured logs with stable codes and trace IDs.\n- Reproducible artifacts that prove contract satisfaction.\n\nAcceptance Criteria:\n- Success and failure invariants for [11] Contract field: compatibility and threat evidence are explicitly quantified and machine-verifiable.\n- Determinism requirements for [11] Contract field: compatibility and threat evidence are validated across repeated runs and adversarial perturbations.\n\n\nExpected Artifacts:\n- Machine-readable verification artifact with explicit canonical path under artifacts/section_11/bd-36wa/verification_evidence.json, suitable for CI/release gating.\n- Human-readable verification summary with explicit canonical path under artifacts/section_11/bd-36wa/verification_summary.md, linking implementation intent to measured validation outcomes.\n\nTask-Specific Clarification:\n- The implementation must deliver the exact capability named in the title, with no scope dilution and no silent feature omission.\n- Validation artifacts must include capability-specific thresholds, pass/fail criteria, and reproducible evidence bundles tied to this bead ID.\n- Program/section verification gates must be able to consume this bead's outputs without manual interpretation.\n\nWhy This Improves User Outcomes:\n- \"[11] Contract field: compatibility and threat evidence\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[11] Contract field: compatibility and threat evidence\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"1. Every evidence contract includes a compatibility-and-threat-evidence section with: (a) list of compatibility test suites exercised and pass/fail counts, (b) threat model delta — new attack surfaces introduced or closed, (c) regression risk assessment citing specific API families.\n2. Compatibility evidence must reference actual test run artifact paths (verification_evidence.json) not just claims.\n3. Threat evidence must enumerate at least: privilege escalation, data exfiltration, and denial-of-service vectors with mitigations.\n4. CI validation rejects contracts where compatibility evidence references zero test suites or threat section is empty.\n5. Unit test: contract with full evidence passes; contract missing threat model or test references fails.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:39:32.564264525Z","created_by":"ubuntu","updated_at":"2026-02-20T15:15:53.889613653Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-11","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-36wa","depends_on_id":"bd-3se1","type":"blocks","created_at":"2026-02-20T07:43:24.327391786Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-37i","title":"[PLAN 10.21] Behavioral Phenotype Evolution Tracker Execution Track (9O)","description":"Section: 10.21 — Behavioral Phenotype Evolution Tracker Execution Track (9O)\n\nStrategic Context:\nBPET longitudinal phenotype intelligence track for pre-compromise trajectory detection and integration with DGIS/ATC/migration/economic policy.\n\nExecution Requirements:\n- Preserve all scoped capabilities from the canonical plan.\n- Keep contracts self-contained so future contributors can execute without reopening the master plan.\n- Require explicit testing strategy (unit + integration/e2e) and structured logging/telemetry for every child bead.\n- Require artifact-backed validation for performance, security, and correctness claims.\n\nDependency Semantics:\n- This epic is blocked by its child implementation beads.\n- Cross-epic dependencies encode strategic sequencing and canonical ownership boundaries.\n\n## Success Criteria\n- All child beads for this section are completed with acceptance artifacts attached and no unresolved blockers.\n- Section-level verification gate for comprehensive unit tests, integration/e2e workflows, and detailed structured logging evidence is green.\n- Scope-to-plan traceability is explicit, with no feature loss versus the canonical plan section.\n\n## Optimization Notes\n- User-Outcome Lens: \"[PLAN 10.21] Behavioral Phenotype Evolution Tracker Execution Track (9O)\" must improve operator confidence, safety posture, and deterministic recovery behavior under both normal and adversarial conditions.\n- Cross-Section Coordination: child beads must encode integration assumptions explicitly to avoid local optimizations that degrade system-wide correctness.\n- Verification Non-Negotiable: completion requires reproducible unit/integration/E2E evidence and structured logs suitable for independent replay.\n- Scope Protection: any simplification must preserve canonical-plan feature intent and be justified with explicit tradeoff documentation.","status":"open","priority":2,"issue_type":"epic","created_at":"2026-02-20T07:36:41.949926696Z","created_by":"ubuntu","updated_at":"2026-02-20T08:16:44.191036940Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-21"],"dependencies":[{"issue_id":"bd-37i","depends_on_id":"bd-1b9x","type":"blocks","created_at":"2026-02-20T07:37:08.245687528Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-37i","depends_on_id":"bd-1ga5","type":"blocks","created_at":"2026-02-20T07:37:07.983305173Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-37i","depends_on_id":"bd-1jpc","type":"blocks","created_at":"2026-02-20T07:37:08.330982325Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-37i","depends_on_id":"bd-1naf","type":"blocks","created_at":"2026-02-20T07:37:08.916309542Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-37i","depends_on_id":"bd-1wz","type":"blocks","created_at":"2026-02-20T07:37:11.751750073Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-37i","depends_on_id":"bd-232t","type":"blocks","created_at":"2026-02-20T07:37:08.414777809Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-37i","depends_on_id":"bd-2ao3","type":"blocks","created_at":"2026-02-20T07:37:08.072795746Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-37i","depends_on_id":"bd-2lll","type":"blocks","created_at":"2026-02-20T07:37:08.158043515Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-37i","depends_on_id":"bd-2xgs","type":"blocks","created_at":"2026-02-20T07:37:07.812739454Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-37i","depends_on_id":"bd-2zo1","type":"blocks","created_at":"2026-02-20T07:37:08.582873568Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-37i","depends_on_id":"bd-39a","type":"blocks","created_at":"2026-02-20T07:37:11.790632409Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-37i","depends_on_id":"bd-39ga","type":"blocks","created_at":"2026-02-20T07:37:07.728748295Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-37i","depends_on_id":"bd-3cbi","type":"blocks","created_at":"2026-02-20T07:37:08.748979390Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-37i","depends_on_id":"bd-3rai","type":"blocks","created_at":"2026-02-20T07:37:07.894941800Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-37i","depends_on_id":"bd-3v9l","type":"blocks","created_at":"2026-02-20T07:37:08.999649869Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-37i","depends_on_id":"bd-aoq6","type":"blocks","created_at":"2026-02-20T07:37:08.664956492Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-37i","depends_on_id":"bd-cda","type":"blocks","created_at":"2026-02-20T07:37:09.855676864Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-37i","depends_on_id":"bd-kwwg","type":"blocks","created_at":"2026-02-20T07:37:08.498553707Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-37i","depends_on_id":"bd-ybe","type":"blocks","created_at":"2026-02-20T07:37:11.829668260Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-37i","depends_on_id":"bd-ye4m","type":"blocks","created_at":"2026-02-20T07:37:08.831448764Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-37i","depends_on_id":"bd-zm5b","type":"blocks","created_at":"2026-02-20T07:48:22.644441684Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-383z","title":"[10.17] Build counterfactual incident lab and mitigation synthesis workflow.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.17 — Radical Expansion Execution Track (9K)\n\nWhy This Exists:\nRadical expansion track for proof-carrying speculation, Bayesian adversary control, time-travel replay, ZK attestations, and claim compiler systems.\n\nTask Objective:\nBuild counterfactual incident lab and mitigation synthesis workflow.\n\nAcceptance Criteria:\n- Real incident traces can be replayed and compared against synthesized mitigations with expected-loss deltas; promoted mitigations require signed rollout and rollback contracts.\n\nExpected Artifacts:\n- `docs/specs/counterfactual_incident_lab.md`, `tests/lab/counterfactual_mitigation_eval.rs`, `src/ops/mitigation_synthesis.rs`, `artifacts/10.17/counterfactual_eval_report.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_17/bd-383z/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_17/bd-383z/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.17] Build counterfactual incident lab and mitigation synthesis workflow.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.17] Build counterfactual incident lab and mitigation synthesis workflow.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.17] Build counterfactual incident lab and mitigation synthesis workflow.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.17] Build counterfactual incident lab and mitigation synthesis workflow.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.17] Build counterfactual incident lab and mitigation synthesis workflow.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"- Real incident traces can be replayed and compared against synthesized mitigations with expected-loss deltas; promoted mitigations require signed rollout and rollback contracts.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:37:04.007645237Z","created_by":"ubuntu","updated_at":"2026-02-20T15:45:58.139525461Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-17","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-383z","depends_on_id":"bd-2o8b","type":"blocks","created_at":"2026-02-20T07:43:18.825966610Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-38l","title":"[10.2] Implement divergence ledger with signed rationale entries.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.2 — Compatibility Core\n\nWhy This Exists:\nCompatibility core as strategic wedge: policy-visible behavior, divergence receipts, L1/L2 lockstep, and spec-first fixture governance.\n\nTask Objective:\nImplement divergence ledger with signed rationale entries.\n\nAcceptance Criteria:\n- Implement with explicit success/failure invariants and deterministic behavior under normal, edge, and fault scenarios.\n- Ensure outcomes are verifiable via automated tests and reproducible artifact outputs.\n\nExpected Artifacts:\n- Section-owned design/spec artifact at `docs/specs/section_10_2/bd-38l_contract.md`, capturing decision rationale, invariants, and interface boundaries.\n- Machine-readable verification artifact at `artifacts/section_10_2/bd-38l/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_2/bd-38l/verification_summary.md` linking implementation intent to measured outcomes.\n\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.2] Implement divergence ledger with signed rationale entries.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.2] Implement divergence ledger with signed rationale entries.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.2] Implement divergence ledger with signed rationale entries.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.2] Implement divergence ledger with signed rationale entries.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.2] Implement divergence ledger with signed rationale entries.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","status":"closed","priority":1,"issue_type":"task","assignee":"CrimsonCrane","created_at":"2026-02-20T07:36:43.998141036Z","created_by":"ubuntu","updated_at":"2026-02-20T09:36:09.242921857Z","closed_at":"2026-02-20T09:36:09.242897141Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-2","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-38l","depends_on_id":"bd-2qf","type":"blocks","created_at":"2026-02-20T07:43:20.137245348Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-38m","title":"[10.6] Optimize lockstep harness throughput and memory profile.","description":"## [10.6] Optimize Lockstep Harness Throughput and Memory Profile\n\n### Why This Exists\n\nThe lockstep differential harness (10.2) runs Node, Bun, and franken_node in parallel to verify compatibility by comparing outputs. As the fixture corpus grows, harness execution time and memory consumption become bottlenecks for CI and local development. Section 9D.4 explicitly calls for \"reducing differential harness cost with streaming normalization and parallel fixture evaluation.\" This bead ensures the harness scales to large corpus sizes without degrading CI cycle time or exhausting memory.\n\n### What It Must Do\n\nOptimize four critical phases of the lockstep harness: (1) harness startup — reduce process spawning overhead by reusing warm runtime pools where safe, (2) fixture loading — switch from bulk-load to streaming fixture ingestion so memory stays bounded regardless of corpus size, (3) result comparison — implement streaming normalization of outputs so diffs are computed incrementally rather than buffering entire outputs, and (4) memory management during large corpus runs — enforce a configurable memory ceiling and spill to disk when exceeded.\n\nThe optimization must not change the harness's correctness guarantees. Every fixture that passed or failed before must produce the identical verdict after optimization. A \"correctness canary\" test runs the full corpus through both old and new code paths and asserts identical verdicts.\n\nBefore/after benchmark evidence is mandatory per Section 7.3. Benchmarks must measure: total wall-clock time for the full corpus, peak resident memory, per-fixture p50/p99 comparison latency, and startup time. Results are recorded in structured JSON for trend tracking.\n\nStreaming normalization must handle the output normalization rules already defined in 10.2 (timestamp stripping, PID masking, path canonicalization) without requiring the full output to be in memory.\n\n### Acceptance Criteria\n\n1. Harness fixture loading uses streaming ingestion; peak memory during a 5000-fixture corpus run stays below 512 MB (configurable ceiling).\n2. Streaming normalization produces byte-identical normalized output compared to the existing bulk normalizer for every fixture in the corpus.\n3. Before/after benchmarks are recorded in `artifacts/section_10_6/bd-38m/benchmark_comparison.json` showing wall-clock time, peak RSS, and per-fixture latency.\n4. Total harness throughput improves by at least 20% on the reference corpus (measured wall-clock).\n5. A correctness canary test validates identical verdicts between old and new paths across the full corpus.\n6. Memory spill-to-disk activates when the ceiling is reached, and a warning is logged.\n7. Verification script `scripts/check_harness_throughput.py` with `--json` flag validates benchmarks meet targets.\n8. Unit tests in `tests/test_check_harness_throughput.py` cover streaming normalization correctness, memory ceiling enforcement, and benchmark parsing.\n\n### Key Dependencies\n\n- Lockstep differential harness from 10.2 (bd-3rp and related beads).\n- Fixture corpus (compatibility test fixtures).\n- Output normalization rules defined in 10.2.\n\n### Testing & Logging Requirements\n\n- Correctness canary: run full corpus through both bulk and streaming paths, assert identical verdicts.\n- Benchmark test: run on reference corpus, record structured results, compare against baseline.\n- Memory ceiling test: run with artificially low ceiling, verify spill-to-disk activates.\n- Structured JSON logs for each optimization phase: startup time, fixtures loaded, normalization throughput, comparison throughput, peak memory.\n\n### Expected Artifacts\n\n- Optimized harness code in `crates/franken-node/src/` or harness runner script.\n- `scripts/check_harness_throughput.py` — verification script.\n- `tests/test_check_harness_throughput.py` — unit tests.\n- `artifacts/section_10_6/bd-38m/benchmark_comparison.json` — before/after benchmarks.\n- `artifacts/section_10_6/bd-38m/verification_evidence.json` — gate results.\n- `artifacts/section_10_6/bd-38m/verification_summary.md` — human-readable summary.\n\n### Additional E2E Requirement\n\n- E2E test scripts must execute full end-to-end workflows from clean fixtures, emit deterministic machine-readable pass/fail evidence, and capture stepwise structured logs for root-cause triage.","acceptance_criteria":"1. Lockstep harness throughput improves by at least 2x over current baseline, measured in operations/second on the standard compatibility test corpus.\n2. Peak memory usage of the lockstep harness does not exceed a defined ceiling (documented in spec) during full-corpus runs.\n3. Memory profile artifact (heap snapshot or allocation trace) is generated and persisted under artifacts/.\n4. Before/after table comparing throughput and memory is produced per Section 7 doctrine.\n5. Optimization does not regress correctness: all existing lockstep oracle tests continue to pass.\n6. Compatibility proof: output diff between pre- and post-optimization runs on the golden corpus is empty.\n7. Profile-driven optimization loop per Section 9D: at least one profiling pass informs each optimization, with evidence recorded.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:36:46.858398443Z","created_by":"ubuntu","updated_at":"2026-02-20T17:08:36.478025724Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-6","self-contained","test-obligations"]}
{"id":"bd-38ri","title":"[12] Risk control: scope explosion","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 12\n\nTask Objective:\nImplement capability gates and artifact-gated delivery controls to contain scope explosion.\n\nExecution Requirements:\n- Make this requirement machine-verifiable and release-gated where applicable.\n- Keep behavior self-documenting so future contributors do not need to re-open the master plan.\n\nTesting & Logging Requirements:\n- Unit tests for rule/contract logic and error conditions.\n- Integration/E2E tests for workflow-level enforcement.\n- Structured logs with stable codes and trace IDs.\n- Reproducible artifacts that prove contract satisfaction.\n\nAcceptance Criteria:\n- Success and failure invariants for [12] Risk control: scope explosion are explicitly quantified and machine-verifiable.\n- Determinism requirements for [12] Risk control: scope explosion are validated across repeated runs and adversarial perturbations.\n\n\nExpected Artifacts:\n- Machine-readable verification artifact with explicit canonical path under artifacts/section_12/bd-38ri/verification_evidence.json, suitable for CI/release gating.\n- Human-readable verification summary with explicit canonical path under artifacts/section_12/bd-38ri/verification_summary.md, linking implementation intent to measured validation outcomes.\n\nTask-Specific Clarification:\n- The implementation must deliver the exact capability named in the title, with no scope dilution and no silent feature omission.\n- Validation artifacts must include capability-specific thresholds, pass/fail criteria, and reproducible evidence bundles tied to this bead ID.\n- Program/section verification gates must be able to consume this bead's outputs without manual interpretation.\n\nWhy This Improves User Outcomes:\n- \"[12] Risk control: scope explosion\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[12] Risk control: scope explosion\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"RISK: Scope explosion — unbounded feature creep expanding the project surface area beyond what can be verified and maintained.\nIMPACT: Missed deadlines, incomplete verification coverage, accumulated technical debt, inability to close sections.\nCOUNTERMEASURES:\n  (a) Capability gates: each of the 16 capabilities has a defined boundary; new work must map to an existing capability or go through formal scope-change review.\n  (b) Artifact-gated delivery: no capability is considered complete without its full bead delivery pattern (spec, impl, verification, evidence, tests).\n  (c) Bead budget: each section has a maximum bead count; exceeding it requires explicit justification and approval.\nVERIFICATION:\n  1. Every bead maps to exactly one of the 16 capabilities via its section tag.\n  2. A CI check validates that new beads include a capability mapping and do not exceed section bead budget.\n  3. Scope-change proposals are tracked as dedicated beads with explicit justification.\n  4. Quarterly audit: actual bead count vs planned bead count per section, with variance report.\nTEST SCENARIOS:\n  - Scenario A: Attempt to create a bead outside any capability boundary; verify it is flagged for review.\n  - Scenario B: Section reaches bead budget; verify new bead creation requires explicit override.\n  - Scenario C: Verify all existing beads have valid capability mappings (no orphans).","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:39:33.329588509Z","created_by":"ubuntu","updated_at":"2026-02-20T15:17:59.112604233Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-12","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-38ri","depends_on_id":"bd-s4cu","type":"blocks","created_at":"2026-02-20T07:43:24.776786171Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-38yt","title":"[10.20] Add performance/scale budgets and release claim gates for DGIS-derived security assertions.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.20 — Dependency Graph Immune System Execution Track (9N)\n\nWhy This Exists:\nDGIS topological immune system track for dependency contagion modeling, choke-point immunization, and graph-aware containment economics.\n\nTask Objective:\nAdd performance/scale budgets and release claim gates for DGIS-derived security assertions.\n\nAcceptance Criteria:\n- DGIS computation overhead and decision latency remain within p95/p99 budgets at target graph scales; release pipeline blocks topology-security claims lacking signed DGIS evidence artifacts.\n\nExpected Artifacts:\n- `tests/perf/dgis_budget_gate.rs`, `.github/workflows/dgis-claim-gate.yml`, `artifacts/10.20/dgis_release_gate_report.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_20/bd-38yt/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_20/bd-38yt/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.20] Add performance/scale budgets and release claim gates for DGIS-derived security assertions.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.20] Add performance/scale budgets and release claim gates for DGIS-derived security assertions.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.20] Add performance/scale budgets and release claim gates for DGIS-derived security assertions.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.20] Add performance/scale budgets and release claim gates for DGIS-derived security assertions.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.20] Add performance/scale budgets and release claim gates for DGIS-derived security assertions.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"- DGIS computation overhead and decision latency remain within p95/p99 budgets at target graph scales; release pipeline blocks topology-security claims lacking signed DGIS evidence artifacts.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:37:07.609887205Z","created_by":"ubuntu","updated_at":"2026-02-20T17:00:52.067268458Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-20","self-contained","test-obligations"]}
{"id":"bd-390","title":"[10.11] Implement anti-entropy reconciliation for distributed product trust state.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md — Section 10.11 (FrankenSQLite-Inspired Runtime Systems), cross-ref Section 9G.10, 9G.9\n\n## Why This Exists\n\nDistributed product trust state (epoch metadata, trust receipts, quarantine verdicts, capability grants) must converge across all nodes participating in a franken_node cluster without requiring full state transfers on every sync cycle. Enhancement Map 9G.10 mandates an O(delta) anti-entropy reconciliation mechanism that exchanges only the missing or divergent portions of the trust state, along with proof-carrying recovery artifacts that let any receiving node verify the authenticity and ordering of each reconciled record. Without this, network partitions or node restarts could leave nodes with stale trust decisions, silently admitting revoked artifacts or rejecting valid ones — a category-defining failure for a product that promises deterministic, evidence-backed trust management.\n\nThis bead adapts the append-only marker stream (10.14, bd-126h), fork/divergence detection (10.14, bd-xwk5), and MMR checkpoint primitives (10.14, bd-1dar) into a product-level reconciliation service that operates within the three-kernel architecture, respecting Cx-first control, epoch boundaries, and cancellation protocol semantics.\n\n## What This Must Do\n\n1. Implement an `AntiEntropyReconciler` struct that accepts two trust-state digests (local and remote) and computes the minimal O(delta) diff using Merkle-Mountain-Range prefix comparison from 10.14 primitives.\n2. Produce proof-carrying recovery artifacts for each reconciled record: each artifact bundles the record payload, its marker-stream position, an MMR inclusion proof, and the epoch in which it was created.\n3. Apply reconciled records through the existing two-phase obligation channel (bd-2ah) so that partial reconciliation failures do not corrupt local state.\n4. Enforce epoch-scoped validity: reject any incoming record whose epoch exceeds the local node's current epoch (fail-closed, per 9G.6 / bd-2gr).\n5. Emit structured reconciliation events to the append-only decision stream (per 9G.9) including delta size, records accepted, records rejected, and elapsed time.\n6. Support cancellation at every await point during reconciliation (per bd-7om cancel-drain-finalize protocol), ensuring partial progress is safely abandoned without state corruption.\n\n## Context from Enhancement Maps\n\n- 9G.10: \"O(delta) anti-entropy reconciliation + proof-carrying recovery artifacts\"\n- 9G.9: \"Three-tier integrity strategy + append-only tamper-evident decision stream\"\n- 9J.12: \"Epoch-scoped validity windows for trust artifacts\" — reconciled records must respect epoch boundaries.\n- 9J.19: \"Cancellation-complete protocol discipline with obligation closure proofs\" — reconciliation loops must be cancellation-safe.\n- Architecture invariant #3 (8.5): Cancellation protocol semantics — all long-running reconciliation must honor cancel signals.\n- Architecture invariant #8 (8.5): Evidence-by-default — every reconciliation cycle must produce auditable evidence.\n\n## Dependencies\n\n- Upstream: bd-126h (append-only marker stream), bd-xwk5 (fork/divergence detection), bd-1dar (MMR checkpoints/inclusion proofs), bd-2gr (epoch-scoped validity windows), bd-2ah (obligation-tracked two-phase channels), bd-7om (cancel-drain-finalize protocol)\n- Downstream: bd-1jpo (10.11 section-wide verification gate)\n\n## Acceptance Criteria\n\n1. Unit test confirms O(delta) behavior: reconciling two states with N total records and K differing records completes in O(K) comparisons, not O(N).\n2. Every reconciled record includes a verifiable MMR inclusion proof that passes independent validation.\n3. Records from a future epoch (epoch > local_current) are rejected with a structured error event logged to the decision stream.\n4. A crash or cancellation mid-reconciliation leaves local trust state unchanged (atomic rollback via two-phase channel).\n5. Structured log events include: `anti_entropy.cycle_started`, `anti_entropy.delta_computed`, `anti_entropy.record_accepted`, `anti_entropy.record_rejected`, `anti_entropy.cycle_completed` — each with trace correlation ID and epoch tag.\n6. Reconciliation under simulated 50% packet loss completes correctly (possibly after retries) without data corruption.\n7. Verification evidence JSON includes delta_size, records_accepted, records_rejected, proof_verification_pass_rate, and elapsed_ms fields.\n8. Fork detection (divergent histories) triggers a structured alert event and halts reconciliation rather than silently merging.\n\n## Testing & Logging Requirements\n\n- Unit tests: (a) Identical states produce zero-delta; (b) Single-record divergence produces delta=1; (c) Bulk divergence (1000 records) completes in bounded time; (d) Future-epoch record rejection; (e) Invalid MMR proof rejection.\n- Integration tests: (a) Two-node reconciliation through obligation channels with simulated network delay; (b) Three-node transitive convergence (A syncs with B, B syncs with C, verify A and C converge); (c) Reconciliation during concurrent local writes.\n- Adversarial tests: (a) Tampered proof payloads are detected and rejected; (b) Cancellation signal mid-reconciliation leaves state clean; (c) Simulated fork/divergence triggers halt-and-alert; (d) Replay of already-reconciled records is idempotent.\n- Structured logs: All events use stable event codes (FN-AE-001 through FN-AE-008), include `trace_id`, `epoch`, `delta_size`, and are JSON-formatted for machine parsing.\n\n## Expected Artifacts\n\n- docs/specs/section_10_11/bd-390_contract.md\n- crates/franken-node/src/runtime/anti_entropy.rs (or equivalent module path)\n- scripts/check_anti_entropy_reconciliation.py (with --json flag and self_test())\n- tests/test_check_anti_entropy_reconciliation.py\n- artifacts/section_10_11/bd-390/verification_evidence.json\n- artifacts/section_10_11/bd-390/verification_summary.md","acceptance_criteria":"AC for bd-390:\n1. Implement an anti-entropy reconciliation protocol for distributed product trust state: each node maintains a local trust state (capability grants, epoch bindings, obligation tokens) and periodically synchronizes with peers to detect and resolve divergence.\n2. The reconciliation uses a Merkle-tree digest of the local trust state; peers exchange root hashes and, on mismatch, perform a tree-diff to identify the minimal set of divergent entries.\n3. Conflict resolution follows a deterministic policy: (a) higher epoch_id wins, (b) within the same epoch, the entry with the later timestamp wins, (c) ties are broken by lexicographic ordering of the node_id. This ensures all nodes converge to the same state without coordination.\n4. Reconciliation runs on a configurable interval (default: 30 seconds) and also triggers immediately on epoch transition events (bd-2gr) to accelerate convergence during security-critical transitions.\n5. A convergence SLA is enforced: after a state mutation, all nodes must agree within max_convergence_time (configurable, default: 3 reconciliation intervals). A RECONCILIATION_SLA_BREACH event fires if convergence is not achieved within the deadline.\n6. The reconciliation protocol is bandwidth-efficient: only divergent entries are transferred, not the full state; a test verifies that reconciling 1 divergent entry out of 10,000 transfers O(log N) data, not O(N).\n7. Unit tests verify: (a) identical states produce matching Merkle roots (no false divergence), (b) single-entry divergence is detected and resolved per conflict policy, (c) epoch-based conflict resolution picks higher epoch, (d) immediate reconciliation triggers on epoch transition, (e) SLA breach event fires when convergence deadline is exceeded.\n8. Integration test: a 3-node cluster with injected network partition heals after partition removal and converges within the SLA.\n9. Structured log events: RECONCILIATION_START / RECONCILIATION_DIVERGENCE_DETECTED / RECONCILIATION_RESOLVED / RECONCILIATION_CONVERGED / RECONCILIATION_SLA_BREACH with node_id, peer_id, divergent_entry_count, and merkle_root_hash.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:36:50.721442902Z","created_by":"ubuntu","updated_at":"2026-02-20T17:01:39.813256007Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-11","self-contained","test-obligations"]}
{"id":"bd-39a","title":"[PLAN 10.19] Adversarial Trust Commons Execution Track (9M)","description":"Section: 10.19 — Adversarial Trust Commons Execution Track (9M)\n\nStrategic Context:\nATC federated intelligence track for privacy-preserving cross-deployment threat learning, robust aggregation, and verifier-backed trust metrics.\n\nExecution Requirements:\n- Preserve all scoped capabilities from the canonical plan.\n- Keep contracts self-contained so future contributors can execute without reopening the master plan.\n- Require explicit testing strategy (unit + integration/e2e) and structured logging/telemetry for every child bead.\n- Require artifact-backed validation for performance, security, and correctness claims.\n\nDependency Semantics:\n- This epic is blocked by its child implementation beads.\n- Cross-epic dependencies encode strategic sequencing and canonical ownership boundaries.\n\n## Success Criteria\n- All child beads for this section are completed with acceptance artifacts attached and no unresolved blockers.\n- Section-level verification gate for comprehensive unit tests, integration/e2e workflows, and detailed structured logging evidence is green.\n- Scope-to-plan traceability is explicit, with no feature loss versus the canonical plan section.\n\n## Optimization Notes\n- User-Outcome Lens: \"[PLAN 10.19] Adversarial Trust Commons Execution Track (9M)\" must improve operator confidence, safety posture, and deterministic recovery behavior under both normal and adversarial conditions.\n- Cross-Section Coordination: child beads must encode integration assumptions explicitly to avoid local optimizations that degrade system-wide correctness.\n- Verification Non-Negotiable: completion requires reproducible unit/integration/E2E evidence and structured logs suitable for independent replay.\n- Scope Protection: any simplification must preserve canonical-plan feature intent and be justified with explicit tradeoff documentation.","status":"open","priority":2,"issue_type":"epic","created_at":"2026-02-20T07:36:41.786603797Z","created_by":"ubuntu","updated_at":"2026-02-20T08:16:44.998351215Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-19"],"dependencies":[{"issue_id":"bd-39a","depends_on_id":"bd-11rz","type":"blocks","created_at":"2026-02-20T07:37:06.374039234Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-39a","depends_on_id":"bd-1eot","type":"blocks","created_at":"2026-02-20T07:37:06.124908784Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-39a","depends_on_id":"bd-1hj3","type":"blocks","created_at":"2026-02-20T07:37:05.540023168Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-39a","depends_on_id":"bd-1wz","type":"blocks","created_at":"2026-02-20T07:37:11.596212456Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-39a","depends_on_id":"bd-24du","type":"blocks","created_at":"2026-02-20T07:37:06.289533878Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-39a","depends_on_id":"bd-253o","type":"blocks","created_at":"2026-02-20T07:37:06.043103837Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-39a","depends_on_id":"bd-293y","type":"blocks","created_at":"2026-02-20T07:37:05.371611762Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-39a","depends_on_id":"bd-2ozr","type":"blocks","created_at":"2026-02-20T07:37:05.792612622Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-39a","depends_on_id":"bd-2yvw","type":"blocks","created_at":"2026-02-20T07:37:05.873914251Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-39a","depends_on_id":"bd-2zip","type":"blocks","created_at":"2026-02-20T07:37:06.208110201Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-39a","depends_on_id":"bd-32p","type":"blocks","created_at":"2026-02-20T07:37:11.635592238Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-39a","depends_on_id":"bd-3aqy","type":"blocks","created_at":"2026-02-20T07:37:05.454103307Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-39a","depends_on_id":"bd-3gwi","type":"blocks","created_at":"2026-02-20T07:37:05.956512235Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-39a","depends_on_id":"bd-3hr2","type":"blocks","created_at":"2026-02-20T07:48:19.686800741Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-39a","depends_on_id":"bd-3ps8","type":"blocks","created_at":"2026-02-20T07:37:05.710130284Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-39a","depends_on_id":"bd-cda","type":"blocks","created_at":"2026-02-20T07:37:09.779371902Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-39a","depends_on_id":"bd-ukh7","type":"blocks","created_at":"2026-02-20T07:37:05.623215900Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-39ga","title":"[10.21] Define canonical `BehavioralGenome` schema and version-lineage contract for extension phenotypes.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.21 — Behavioral Phenotype Evolution Tracker Execution Track (9O)\n\nWhy This Exists:\nBPET longitudinal phenotype intelligence track for pre-compromise trajectory detection and integration with DGIS/ATC/migration/economic policy.\n\nTask Objective:\nDefine canonical `BehavioralGenome` schema and version-lineage contract for extension phenotypes.\n\nAcceptance Criteria:\n- Schema encodes capability usage, dependency reach, API-surface traits, resource/network envelopes, complexity signals, maintainer/build events, and provenance bindings; serialization is deterministic and signed.\n\nExpected Artifacts:\n- `docs/specs/bpet_behavioral_genome_schema.md`, `spec/bpet_behavioral_genome_v1.json`, `artifacts/10.21/bpet_genome_schema_vectors.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_21/bd-39ga/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_21/bd-39ga/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.21] Define canonical `BehavioralGenome` schema and version-lineage contract for extension phenotypes.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.21] Define canonical `BehavioralGenome` schema and version-lineage contract for extension phenotypes.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.21] Define canonical `BehavioralGenome` schema and version-lineage contract for extension phenotypes.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.21] Define canonical `BehavioralGenome` schema and version-lineage contract for extension phenotypes.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.21] Define canonical `BehavioralGenome` schema and version-lineage contract for extension phenotypes.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"- Schema encodes capability usage, dependency reach, API-surface traits, resource/network envelopes, complexity signals, maintainer/build events, and provenance bindings; serialization is deterministic and signed.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:37:07.690649218Z","created_by":"ubuntu","updated_at":"2026-02-20T15:45:23.837548010Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-21","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-39ga","depends_on_id":"bd-1wz","type":"blocks","created_at":"2026-02-20T07:46:35.470819834Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-39ga","depends_on_id":"bd-39a","type":"blocks","created_at":"2026-02-20T07:46:35.516070424Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-39ga","depends_on_id":"bd-cda","type":"blocks","created_at":"2026-02-20T07:46:35.560633625Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-39ga","depends_on_id":"bd-ybe","type":"blocks","created_at":"2026-02-20T07:46:35.609054219Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-3a3q","title":"[10.14] Implement anytime-valid guardrail monitor set for security/durability-critical budgets.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.14 — FrankenSQLite Deep-Mined Expansion Execution Track (9J)\n\nWhy This Exists:\nFrankenSQLite deep-mined control integrity track: evidence ledgers, proofs, epochs, remote effects, markers/MMR, and deterministic fault labs.\n\nTask Objective:\nImplement anytime-valid guardrail monitor set for security/durability-critical budgets.\n\nAcceptance Criteria:\n- Guardrails are always-on for critical budgets; monitor outputs remain valid under optional stopping; alert thresholds are policy-configurable.\n\nExpected Artifacts:\n- `docs/specs/anytime_valid_guardrails.md`, `tests/conformance/anytime_guardrail_monitors.rs`, `artifacts/10.14/guardrail_monitor_telemetry.csv`.\n\n- Machine-readable verification artifact at `artifacts/section_10_14/bd-3a3q/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_14/bd-3a3q/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.14] Implement anytime-valid guardrail monitor set for security/durability-critical budgets.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.14] Implement anytime-valid guardrail monitor set for security/durability-critical budgets.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.14] Implement anytime-valid guardrail monitor set for security/durability-critical budgets.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.14] Implement anytime-valid guardrail monitor set for security/durability-critical budgets.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.14] Implement anytime-valid guardrail monitor set for security/durability-critical budgets.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"Guardrails are always-on for critical budgets; monitor outputs remain valid under optional stopping; alert thresholds are policy-configurable.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:36:55.712438394Z","created_by":"ubuntu","updated_at":"2026-02-20T16:23:28.170041692Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-14","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-3a3q","depends_on_id":"bd-sddz","type":"blocks","created_at":"2026-02-20T16:23:28.169993882Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-3agp","title":"[13] Concrete target gate: >=3x migration velocity","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 13\n\nTask Objective:\nBuild KPI gate for migration velocity improvement threshold >=3x.\n\nExecution Requirements:\n- Make this requirement machine-verifiable and release-gated where applicable.\n- Keep behavior self-documenting so future contributors do not need to re-open the master plan.\n\nTesting & Logging Requirements:\n- Unit tests for rule/contract logic and error conditions.\n- Integration/E2E tests for workflow-level enforcement.\n- Structured logs with stable codes and trace IDs.\n- Reproducible artifacts that prove contract satisfaction.\n\nAcceptance Criteria:\n- Success and failure invariants for [13] Concrete target gate: >=3x migration velocity are explicitly quantified and machine-verifiable.\n- Determinism requirements for [13] Concrete target gate: >=3x migration velocity are validated across repeated runs and adversarial perturbations.\n\n\nExpected Artifacts:\n- Machine-readable verification artifact with explicit canonical path under artifacts/section_13/bd-3agp/verification_evidence.json, suitable for CI/release gating.\n- Human-readable verification summary with explicit canonical path under artifacts/section_13/bd-3agp/verification_summary.md, linking implementation intent to measured validation outcomes.\n\nTask-Specific Clarification:\n- The implementation must deliver the exact capability named in the title, with no scope dilution and no silent feature omission.\n- Validation artifacts must include capability-specific thresholds, pass/fail criteria, and reproducible evidence bundles tied to this bead ID.\n- Program/section verification gates must be able to consume this bead's outputs without manual interpretation.\n\nWhy This Improves User Outcomes:\n- \"[13] Concrete target gate: >=3x migration velocity\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[13] Concrete target gate: >=3x migration velocity\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"1. Migration velocity is measured as: (time to migrate a representative project cohort with franken_node tools) / (time to migrate the same cohort manually or with baseline tools).\n2. The representative cohort includes >= 10 projects spanning: Express.js app, Fastify app, Next.js app, CLI tool, library package, worker service, WebSocket server, monorepo, project with native addons (expected partial), project with custom build pipeline.\n3. franken_node migration tools achieve >= 3x velocity improvement: if manual migration takes T hours, tooled migration takes <= T/3 hours.\n4. Velocity is measured end-to-end: from initial analysis to first passing test suite on franken_node.\n5. Each cohort project's migration is documented with: start time, end time, manual intervention points, blockers encountered.\n6. CI runs the velocity benchmark on at least 3 cohort projects per release.\n7. Evidence artifact: migration_velocity_report.json with per-project timings and overall velocity ratio.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:39:34.958919372Z","created_by":"ubuntu","updated_at":"2026-02-20T15:21:15.815621130Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-13","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-3agp","depends_on_id":"bd-28sz","type":"blocks","created_at":"2026-02-20T07:43:25.576288252Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-3aqy","title":"[10.19] Define canonical federated signal schema for anomaly/trust/revocation/quarantine intelligence.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.19 — Adversarial Trust Commons Execution Track (9M)\n\nWhy This Exists:\nATC federated intelligence track for privacy-preserving cross-deployment threat learning, robust aggregation, and verifier-backed trust metrics.\n\nTask Objective:\nDefine canonical federated signal schema for anomaly/trust/revocation/quarantine intelligence.\n\nAcceptance Criteria:\n- Schema covers required signal classes with stable typing, provenance fields, confidence semantics, and expiry windows; schema validation is enforced in CI.\n\nExpected Artifacts:\n- `docs/specs/atc_signal_schema.md`, `spec/atc_signal_schema_v1.json`, `artifacts/10.19/atc_signal_vectors.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_19/bd-3aqy/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_19/bd-3aqy/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.19] Define canonical federated signal schema for anomaly/trust/revocation/quarantine intelligence.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.19] Define canonical federated signal schema for anomaly/trust/revocation/quarantine intelligence.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.19] Define canonical federated signal schema for anomaly/trust/revocation/quarantine intelligence.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.19] Define canonical federated signal schema for anomaly/trust/revocation/quarantine intelligence.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.19] Define canonical federated signal schema for anomaly/trust/revocation/quarantine intelligence.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"- Schema covers required signal classes with stable typing, provenance fields, confidence semantics, and expiry windows; schema validation is enforced in CI.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:37:05.416432328Z","created_by":"ubuntu","updated_at":"2026-02-20T17:00:55.582301599Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-19","self-contained","test-obligations"]}
{"id":"bd-3b8m","title":"[10.13] Implement anti-amplification response bounds for retrieval/sync traffic and test with adversarial traffic harness.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.13 — FCP Deep-Mined Expansion Execution Track (9I)\n\nWhy This Exists:\nDeep connector/provider contract track: lifecycle FSMs, conformance harnesses, fencing, sandboxing, revocation freshness, and protocol hardening.\n\nTask Objective:\nImplement anti-amplification response bounds for retrieval/sync traffic and test with adversarial traffic harness.\n\nAcceptance Criteria:\n- Response payloads never exceed request-declared bounds under adversarial inputs; unauthenticated limits are stricter and enforced; harness reproduces attacks deterministically.\n\nExpected Artifacts:\n- `tests/security/anti_amplification_harness.rs`, `docs/specs/anti_amplification_rules.md`, `artifacts/10.13/anti_amplification_test_results.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_13/bd-3b8m/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_13/bd-3b8m/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.13] Implement anti-amplification response bounds for retrieval/sync traffic and test with adversarial traffic harness.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.13] Implement anti-amplification response bounds for retrieval/sync traffic and test with adversarial traffic harness.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.13] Implement anti-amplification response bounds for retrieval/sync traffic and test with adversarial traffic harness.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.13] Implement anti-amplification response bounds for retrieval/sync traffic and test with adversarial traffic harness.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.13] Implement anti-amplification response bounds for retrieval/sync traffic and test with adversarial traffic harness.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","status":"closed","priority":1,"issue_type":"task","assignee":"CrimsonCrane","created_at":"2026-02-20T07:36:54.003652591Z","created_by":"ubuntu","updated_at":"2026-02-20T12:48:56.769978383Z","closed_at":"2026-02-20T12:48:56.769950862Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-13","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-3b8m","depends_on_id":"bd-2k74","type":"blocks","created_at":"2026-02-20T07:43:13.601557508Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-3c2","title":"[10.12] Implement verifier-economy SDK with independent validation workflows.","description":"[10.12] Implement verifier-economy SDK with independent validation workflows.\n\n## Why This Exists\n\nSection 9H.3 and Impossible-by-Default principle #10 require that verification is not a privilege of the system operator alone — it must be accessible to independent third parties. The verifier economy is the ecosystem where external auditors, compliance teams, partner organizations, and automated verification agents can independently validate claims made by the franken_node system. This bead provides the SDK — libraries, APIs, and tooling — that makes independent verification easy, reliable, and the default path. Without this SDK, verification requires deep system knowledge and ad-hoc tooling, which makes it effectively impossible for external parties.\n\n## What It Must Do\n\n1. **Verification SDK core library.** Implement a Rust library (`verifier-sdk`) exposing core verification operations:\n   - `verify_claim(claim: &Claim, evidence: &Evidence) -> VerificationResult` — verify a single claim against its evidence bundle.\n   - `verify_migration_artifact(artifact: &MigrationArtifact) -> VerificationResult` — verify a migration artifact (from bd-3hm) including signature, schema, preconditions, and rollback receipt.\n   - `verify_trust_state(state: &TrustStateVector, anchor: &TrustAnchor) -> VerificationResult` — verify a trust state vector's chain of trust back to a known anchor.\n   - `replay_capsule(capsule: &ReplayCapsule) -> ReplayResult` — replay a capsule (from 10.17) and verify that the replay produces the claimed output.\n\n2. **Multi-language bindings.** Provide at minimum:\n   - Rust (native) — the core library.\n   - Python (via PyO3 or FFI) — for scripting and CI integration.\n   - CLI tool (`franken-verify`) — for operator and CI usage.\n   The Python binding and CLI tool must expose all core verification operations.\n\n3. **Evidence bundle format.** Define the evidence bundle format that accompanies every verifiable claim. The bundle contains: the claim itself, supporting artifacts (signatures, hashes, timestamps), verification procedure reference, and expected outcome. Bundles are self-contained — a verifier needs only the bundle and the SDK to validate.\n\n4. **Independent validation workflows.** Define standard validation workflows for common scenarios:\n   - **Release validation**: verify all golden vectors, migration artifacts, and trust state for a release.\n   - **Incident validation**: verify the trust state and revocation chain during/after an incident.\n   - **Compliance audit**: verify a set of claims against their evidence bundles for regulatory compliance.\n   Each workflow is documented with step-by-step instructions and executable scripts.\n\n5. **Verification result format.** All verification operations return a structured `VerificationResult` containing: verdict (pass/fail/inconclusive), confidence score, checked assertions with individual pass/fail, execution timestamp, verifier identity, and cryptographic binding to the verified artifacts. Results are themselves signed by the verifier.\n\n6. **Offline verification.** The SDK must support fully offline verification — no network access required. All necessary data is contained in the evidence bundle. This is critical for air-gapped environments and ensures verification cannot be blocked by network manipulation.\n\n7. **Verification transparency log.** The SDK optionally publishes verification results to a transparency log (append-only, Merkle-tree-backed). This creates a public record of what was verified, by whom, and when. The log format is compatible with RFC 6962 (Certificate Transparency) style structures.\n\n## Acceptance Criteria\n\n1. Rust verifier SDK library implemented in `crates/franken-node/src/connector/verifier_sdk.rs` with all four core operations.\n2. Python bindings or CLI wrapper expose all core operations with `--json` output.\n3. Evidence bundle format defined with JSON Schema at `spec/evidence_bundle_schema.json`.\n4. At least 3 standard validation workflows documented with executable scripts.\n5. `VerificationResult` format includes verdict, confidence, per-assertion results, and cryptographic binding.\n6. Offline verification works without network access (tested in sandboxed environment).\n7. Transparency log format defined and append-only property tested.\n8. Verification script `scripts/check_verifier_sdk.py` with `--json` flag confirms all criteria.\n9. Evidence artifacts written to `artifacts/section_10_12/bd-3c2/`.\n\n## Key Dependencies\n\n- bd-3hm (migration artifact contract) — SDK verifies migration artifacts.\n- bd-1l5 (trust object IDs) — claims and evidence reference canonical IDs.\n- bd-5si (trust fabric convergence) — SDK verifies trust state vectors.\n- 10.17 replay capsules — SDK replays capsules for validation.\n- 10.13 stable error namespace — SDK errors use registered codes.\n- 10.13 golden vectors — release validation workflow runs golden vector suites.\n\n## Testing & Logging Requirements\n\n- Unit tests for each core verification operation with valid and invalid inputs.\n- Integration test executing the release validation workflow against the current codebase.\n- Offline verification test in a network-isolated sandbox.\n- Round-trip test: generate evidence bundle, verify, check signed result, append to transparency log.\n- Fuzz test: mutate evidence bundles and confirm the SDK rejects them.\n- Self-test mode that generates a claim, bundles evidence, verifies, and confirms pass.\n- Structured logging: `verifier.claim_verified`, `verifier.claim_failed`, `verifier.migration_verified`, `verifier.trust_state_verified`, `verifier.replay_completed`, `verifier.result_signed`, `verifier.transparency_log_appended` events.\n\n## Expected Artifacts\n\n- `docs/specs/section_10_12/bd-3c2_contract.md` — specification document.\n- `crates/franken-node/src/connector/verifier_sdk.rs` — Rust SDK implementation.\n- `spec/evidence_bundle_schema.json` — evidence bundle JSON Schema.\n- `scripts/check_verifier_sdk.py` — verification script.\n- `tests/test_check_verifier_sdk.py` — unit tests.\n- `artifacts/section_10_12/bd-3c2/verification_evidence.json` — evidence.\n- `artifacts/section_10_12/bd-3c2/verification_summary.md` — summary.","acceptance_criteria":"1. Define a VerifierSDK module providing: (a) verify_migration_artifact(artifact_json: &str) -> VerificationResult that validates a MigrationSingularityArtifact (from bd-3hm) without access to the source system, (b) verify_rollback_receipt(receipt_json: &str, public_key: &[u8]) -> VerificationResult that checks receipt signature and deadline, (c) verify_release_gate(manifest_json: &str) -> VerificationResult that validates a ReleaseGateManifest (from bd-1hd).\n2. Define VerificationResult as a struct: (a) verdict (PASS, FAIL, INCONCLUSIVE), (b) checked_fields (list of {field_name, check, pass/fail}), (c) verifier_version (SDK semver), (d) verification_timestamp, (e) details (human-readable summary).\n3. The SDK MUST be usable as a standalone library with zero runtime dependencies on the franken_node system. It should depend only on cryptographic primitives and JSON parsing.\n4. Implement schema validation: the SDK MUST reject artifacts whose schema_version it does not recognize, returning INCONCLUSIVE with a message indicating the required SDK version.\n5. Implement independent hash verification: the SDK MUST recompute all hashes (migration_plan_hash, state fingerprints) from the artifact's embedded data and compare against the declared values. Any mismatch produces FAIL.\n6. Implement signature verification: the SDK MUST verify all signatures in the artifact using the provided public keys. Support Ed25519 signatures at minimum.\n7. Provide a CLI wrapper: `franken-verify <artifact.json> [--public-key <key.pem>]` that prints the VerificationResult as JSON to stdout and exits with code 0 (PASS), 1 (FAIL), or 2 (INCONCLUSIVE).\n8. Unit tests: (a) valid artifact passes, (b) tampered hash fails, (c) invalid signature fails, (d) unknown schema version returns INCONCLUSIVE, (e) missing required field returns FAIL, (f) CLI exit codes match verdict.\n9. Provide example artifacts in fixtures/verifier_sdk/ with known-good and known-bad samples for third-party testing.\n10. Verification: scripts/check_verifier_sdk.py --json, artifacts at artifacts/section_10_12/bd-3c2/.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:36:51.075158591Z","created_by":"ubuntu","updated_at":"2026-02-20T17:01:50.449681121Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-12","self-contained","test-obligations"]}
{"id":"bd-3cbi","title":"[10.21] Integrate BPET risk into economic trust layer and operator copilot recommendation engine.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.21 — Behavioral Phenotype Evolution Tracker Execution Track (9O)\n\nWhy This Exists:\nBPET longitudinal phenotype intelligence track for pre-compromise trajectory detection and integration with DGIS/ATC/migration/economic policy.\n\nTask Objective:\nIntegrate BPET risk into economic trust layer and operator copilot recommendation engine.\n\nAcceptance Criteria:\n- Economic models price trajectory-derived compromise propensity and intervention ROI; operator guidance includes historical motif matches and mitigation playbooks.\n\nExpected Artifacts:\n- `src/security/bpet/economic_integration.rs`, `src/ops/bpet_operator_copilot.rs`, `artifacts/10.21/bpet_economic_guidance_report.csv`.\n\n- Machine-readable verification artifact at `artifacts/section_10_21/bd-3cbi/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_21/bd-3cbi/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.21] Integrate BPET risk into economic trust layer and operator copilot recommendation engine.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.21] Integrate BPET risk into economic trust layer and operator copilot recommendation engine.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.21] Integrate BPET risk into economic trust layer and operator copilot recommendation engine.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.21] Integrate BPET risk into economic trust layer and operator copilot recommendation engine.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.21] Integrate BPET risk into economic trust layer and operator copilot recommendation engine.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"- Economic models price trajectory-derived compromise propensity and intervention ROI; operator guidance includes historical motif matches and mitigation playbooks.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:37:08.711484589Z","created_by":"ubuntu","updated_at":"2026-02-20T17:01:06.486867180Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-21","self-contained","test-obligations"]}
{"id":"bd-3cm3","title":"[10.13] Implement schema-gated quarantine promotion rules and promotion provenance receipts.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.13 — FCP Deep-Mined Expansion Execution Track (9I)\n\nWhy This Exists:\nDeep connector/provider contract track: lifecycle FSMs, conformance harnesses, fencing, sandboxing, revocation freshness, and protocol hardening.\n\nTask Objective:\nImplement schema-gated quarantine promotion rules and promotion provenance receipts.\n\nAcceptance Criteria:\n- Promotion requires reachability/authenticated request/pin plus schema validation; promotion emits provenance receipt with promotion reason; invalid promotions fail closed.\n\nExpected Artifacts:\n- `docs/specs/quarantine_promotion_rules.md`, `tests/security/quarantine_promotion_gate.rs`, `artifacts/10.13/quarantine_promotion_receipts.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_13/bd-3cm3/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_13/bd-3cm3/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.13] Implement schema-gated quarantine promotion rules and promotion provenance receipts.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.13] Implement schema-gated quarantine promotion rules and promotion provenance receipts.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.13] Implement schema-gated quarantine promotion rules and promotion provenance receipts.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.13] Implement schema-gated quarantine promotion rules and promotion provenance receipts.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.13] Implement schema-gated quarantine promotion rules and promotion provenance receipts.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","status":"closed","priority":1,"issue_type":"task","assignee":"CrimsonCrane","created_at":"2026-02-20T07:36:54.171128726Z","created_by":"ubuntu","updated_at":"2026-02-20T12:54:59.637303286Z","closed_at":"2026-02-20T12:54:59.637274472Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-13","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-3cm3","depends_on_id":"bd-2eun","type":"blocks","created_at":"2026-02-20T07:43:13.684421630Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-3cpa","title":"[13] Concrete target gate: >=10x compromise reduction","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 13\n\nTask Objective:\nBuild KPI gate for host-compromise reduction threshold >=10x under adversarial campaigns.\n\nExecution Requirements:\n- Make this requirement machine-verifiable and release-gated where applicable.\n- Keep behavior self-documenting so future contributors do not need to re-open the master plan.\n\nTesting & Logging Requirements:\n- Unit tests for rule/contract logic and error conditions.\n- Integration/E2E tests for workflow-level enforcement.\n- Structured logs with stable codes and trace IDs.\n- Reproducible artifacts that prove contract satisfaction.\n\nAcceptance Criteria:\n- Success and failure invariants for [13] Concrete target gate: >=10x compromise reduction are explicitly quantified and machine-verifiable.\n- Determinism requirements for [13] Concrete target gate: >=10x compromise reduction are validated across repeated runs and adversarial perturbations.\n\n\nExpected Artifacts:\n- Machine-readable verification artifact with explicit canonical path under artifacts/section_13/bd-3cpa/verification_evidence.json, suitable for CI/release gating.\n- Human-readable verification summary with explicit canonical path under artifacts/section_13/bd-3cpa/verification_summary.md, linking implementation intent to measured validation outcomes.\n\nTask-Specific Clarification:\n- The implementation must deliver the exact capability named in the title, with no scope dilution and no silent feature omission.\n- Validation artifacts must include capability-specific thresholds, pass/fail criteria, and reproducible evidence bundles tied to this bead ID.\n- Program/section verification gates must be able to consume this bead's outputs without manual interpretation.\n\nWhy This Improves User Outcomes:\n- \"[13] Concrete target gate: >=10x compromise reduction\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[13] Concrete target gate: >=10x compromise reduction\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"1. Host-compromise reduction is measured by comparing: (successful compromises in hardened franken_node) vs (successful compromises in unhardened baseline) under identical attack campaigns.\n2. Attack campaign includes >= 20 distinct attack vectors: RCE via dependency, prototype pollution, path traversal, SSRF, deserialization, supply-chain injection, privilege escalation, sandbox escape, memory corruption, etc.\n3. franken_node achieves >= 10x reduction: if baseline is compromised by N attacks, franken_node is compromised by <= N/10.\n4. Each attack vector is documented with: attack description, baseline outcome (compromised/not), franken_node outcome (compromised/not), mitigation that blocked it.\n5. The attack campaign is reproducible: scripted attacks that can be rerun on any version.\n6. At least 3 attack vectors must demonstrate containment (attack detected and isolated, not just prevented).\n7. Evidence artifact: compromise_reduction_report.json with per-attack results and overall reduction ratio.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:39:35.043905043Z","created_by":"ubuntu","updated_at":"2026-02-20T15:21:27.797871831Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-13","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-3cpa","depends_on_id":"bd-3agp","type":"blocks","created_at":"2026-02-20T07:43:25.620145926Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-3cs3","title":"[10.14] Implement epoch-scoped key derivation for trust artifact authentication.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.14 — FrankenSQLite Deep-Mined Expansion Execution Track (9J)\n\nWhy This Exists:\nFrankenSQLite deep-mined control integrity track: evidence ledgers, proofs, epochs, remote effects, markers/MMR, and deterministic fault labs.\n\nTask Objective:\nImplement epoch-scoped key derivation for trust artifact authentication.\n\nAcceptance Criteria:\n- Authentication key derivation binds to epoch and domain; cross-epoch key reuse is impossible by construction; verification vectors are published.\n\nExpected Artifacts:\n- `src/security/epoch_scoped_keys.rs`, `tests/conformance/epoch_key_derivation.rs`, `artifacts/10.14/epoch_key_vectors.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_14/bd-3cs3/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_14/bd-3cs3/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.14] Implement epoch-scoped key derivation for trust artifact authentication.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.14] Implement epoch-scoped key derivation for trust artifact authentication.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.14] Implement epoch-scoped key derivation for trust artifact authentication.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.14] Implement epoch-scoped key derivation for trust artifact authentication.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.14] Implement epoch-scoped key derivation for trust artifact authentication.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"Authentication key derivation binds to epoch and domain; cross-epoch key reuse is impossible by construction; verification vectors are published.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:36:58.302630425Z","created_by":"ubuntu","updated_at":"2026-02-20T16:24:18.167418777Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-14","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-3cs3","depends_on_id":"bd-3hdv","type":"blocks","created_at":"2026-02-20T16:24:18.167343306Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-3dn","title":"[10.3] Build rollout planner (`shadow -> canary -> ramp -> default`) per project.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.3 — Migration System\n\nWhy This Exists:\nMigration autopilot pipeline from audit to rollout, designed to collapse migration friction with deterministic confidence and replayability.\n\nTask Objective:\nBuild rollout planner (`shadow -> canary -> ramp -> default`) per project.\n\nAcceptance Criteria:\n- Implement with explicit success/failure invariants and deterministic behavior under normal, edge, and fault scenarios.\n- Ensure outcomes are verifiable via automated tests and reproducible artifact outputs.\n\nExpected Artifacts:\n- Section-owned design/spec artifact at `docs/specs/section_10_3/bd-3dn_contract.md`, capturing decision rationale, invariants, and interface boundaries.\n- Machine-readable verification artifact at `artifacts/section_10_3/bd-3dn/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_3/bd-3dn/verification_summary.md` linking implementation intent to measured outcomes.\n\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.3] Build rollout planner (`shadow -> canary -> ramp -> default`) per project.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.3] Build rollout planner (`shadow -> canary -> ramp -> default`) per project.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.3] Build rollout planner (`shadow -> canary -> ramp -> default`) per project.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.3] Build rollout planner (`shadow -> canary -> ramp -> default`) per project.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.3] Build rollout planner (`shadow -> canary -> ramp -> default`) per project.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","status":"closed","priority":1,"issue_type":"task","assignee":"CrimsonCrane","created_at":"2026-02-20T07:36:45.113759296Z","created_by":"ubuntu","updated_at":"2026-02-20T10:16:34.652745740Z","closed_at":"2026-02-20T10:16:34.652721064Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-3","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-3dn","depends_on_id":"bd-2st","type":"blocks","created_at":"2026-02-20T07:43:22.190282530Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-3e74","title":"[13] Success criterion: benchmark/verifier external usage","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 13\n\nTask Objective:\nTrack and enforce external usage targets for benchmark and verifier standards.\n\nExecution Requirements:\n- Make this requirement machine-verifiable and release-gated where applicable.\n- Keep behavior self-documenting so future contributors do not need to re-open the master plan.\n\nTesting & Logging Requirements:\n- Unit tests for rule/contract logic and error conditions.\n- Integration/E2E tests for workflow-level enforcement.\n- Structured logs with stable codes and trace IDs.\n- Reproducible artifacts that prove contract satisfaction.\n\nAcceptance Criteria:\n- Success and failure invariants for [13] Success criterion: benchmark/verifier external usage are explicitly quantified and machine-verifiable.\n- Determinism requirements for [13] Success criterion: benchmark/verifier external usage are validated across repeated runs and adversarial perturbations.\n\n\nExpected Artifacts:\n- Machine-readable verification artifact with explicit canonical path under artifacts/section_13/bd-3e74/verification_evidence.json, suitable for CI/release gating.\n- Human-readable verification summary with explicit canonical path under artifacts/section_13/bd-3e74/verification_summary.md, linking implementation intent to measured validation outcomes.\n\nTask-Specific Clarification:\n- The implementation must deliver the exact capability named in the title, with no scope dilution and no silent feature omission.\n- Validation artifacts must include capability-specific thresholds, pass/fail criteria, and reproducible evidence bundles tied to this bead ID.\n- Program/section verification gates must be able to consume this bead's outputs without manual interpretation.\n\nWhy This Improves User Outcomes:\n- \"[13] Success criterion: benchmark/verifier external usage\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[13] Success criterion: benchmark/verifier external usage\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"1. Published benchmark suite is used by >= 3 external projects or organizations (tracked via download/citation metrics).\n2. Published verifier toolkit is used by >= 2 external parties for independent validation (tracked via usage reports).\n3. Benchmark results are cited in >= 1 external publication, blog post, or conference presentation.\n4. Benchmark and verifier are packaged for easy external consumption: npm package, Docker image, or standalone binary.\n5. External usage is tracked via: (a) npm download counts, (b) Docker pull counts, (c) GitHub stars/forks on benchmark repo, (d) citation tracking.\n6. Documentation includes 'getting started' guide for external users that enables first benchmark run in <= 15 minutes.\n7. Evidence: external_usage_report.json with download counts, known external users, and citation list.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:39:34.752176035Z","created_by":"ubuntu","updated_at":"2026-02-20T15:23:17.432118837Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-13","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-3e74","depends_on_id":"bd-1xao","type":"blocks","created_at":"2026-02-20T07:43:25.487730766Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-3en","title":"[10.13] Build connector protocol conformance harness and block registry publication on failures.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.13 — FCP Deep-Mined Expansion Execution Track (9I)\n\nWhy This Exists:\nDeep connector/provider contract track: lifecycle FSMs, conformance harnesses, fencing, sandboxing, revocation freshness, and protocol hardening.\n\nTask Objective:\nBuild connector protocol conformance harness and block registry publication on failures.\n\nAcceptance Criteria:\n- CI gate fails publication for non-conformant connectors; harness emits deterministic pass/fail reasons; bypass requires explicit policy override artifact.\n\nExpected Artifacts:\n- `tests/conformance/connector_protocol_harness.rs`, `.github/workflows/connector-conformance.yml`, `artifacts/10.13/publication_gate_evidence.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_13/bd-3en/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_13/bd-3en/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.13] Build connector protocol conformance harness and block registry publication on failures.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.13] Build connector protocol conformance harness and block registry publication on failures.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.13] Build connector protocol conformance harness and block registry publication on failures.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.13] Build connector protocol conformance harness and block registry publication on failures.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.13] Build connector protocol conformance harness and block registry publication on failures.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","status":"closed","priority":1,"issue_type":"task","assignee":"CrimsonCrane","created_at":"2026-02-20T07:36:51.637471661Z","created_by":"ubuntu","updated_at":"2026-02-20T10:45:02.089794576Z","closed_at":"2026-02-20T10:45:02.089769249Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-13","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-3en","depends_on_id":"bd-1h6","type":"blocks","created_at":"2026-02-20T07:43:12.356825845Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-3enl","title":"[10.3] Section-wide verification gate: comprehensive unit+e2e+logging","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.3\n\nTask Objective:\nCreate a hard section completion gate that verifies all implemented behavior in this section with comprehensive unit tests, integration/e2e scripts, and detailed structured logging evidence.\n\nAcceptance Criteria:\n- Section test matrix covers happy-path, edge-case, and adversarial/error-path scenarios for all section deliverables.\n- E2E scripts execute representative end-user workflows and policy/control-plane transitions for this section.\n- Structured logs include stable event/error codes, trace correlation IDs, and enough context for deterministic replay triage.\n- Verification report is deterministic and machine-readable for CI/release gating.\n\nExpected Artifacts:\n- Section-level test matrix and coverage summary artifact.\n- E2E script suite with pass/fail outputs and reproducible fixtures/seeds.\n- Structured log validation report and traceability bundle.\n- Gate verdict artifact consumable by release automation.\n\n- Machine-readable verification artifact at `artifacts/section_10_3/bd-3enl/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_3/bd-3enl/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests must validate contracts, invariants, and failure semantics.\n- E2E tests must validate cross-component behavior from user/operator entrypoints to persisted effects.\n- Logging must support root-cause analysis without hidden context requirements.\n\nTask-Specific Clarification:\n- For \"[10.3] Section-wide verification gate: comprehensive unit+e2e+logging\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.3] Section-wide verification gate: comprehensive unit+e2e+logging\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.3] Section-wide verification gate: comprehensive unit+e2e+logging\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.3] Section-wide verification gate: comprehensive unit+e2e+logging\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.3] Section-wide verification gate: comprehensive unit+e2e+logging\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","status":"closed","priority":1,"issue_type":"task","assignee":"CrimsonCrane","created_at":"2026-02-20T07:48:22.811975525Z","created_by":"ubuntu","updated_at":"2026-02-20T10:23:05.394460058Z","closed_at":"2026-02-20T10:23:05.394434400Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-3","test-obligations","verification-gate"],"dependencies":[{"issue_id":"bd-3enl","depends_on_id":"bd-12f","type":"blocks","created_at":"2026-02-20T07:48:23.006448591Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3enl","depends_on_id":"bd-1dpd","type":"blocks","created_at":"2026-02-20T08:24:25.309475159Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3enl","depends_on_id":"bd-2a0","type":"blocks","created_at":"2026-02-20T07:48:23.263782169Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3enl","depends_on_id":"bd-2ew","type":"blocks","created_at":"2026-02-20T07:48:23.153016291Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3enl","depends_on_id":"bd-2st","type":"blocks","created_at":"2026-02-20T07:48:23.103783062Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3enl","depends_on_id":"bd-2twu","type":"blocks","created_at":"2026-02-20T08:20:50.967786989Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3enl","depends_on_id":"bd-33x","type":"blocks","created_at":"2026-02-20T07:48:23.206554850Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3enl","depends_on_id":"bd-3dn","type":"blocks","created_at":"2026-02-20T07:48:23.053801569Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3enl","depends_on_id":"bd-3f9","type":"blocks","created_at":"2026-02-20T07:48:22.909400214Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3enl","depends_on_id":"bd-hg1","type":"blocks","created_at":"2026-02-20T07:48:22.957592705Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-3epz","title":"[10.14] Section-wide verification gate: comprehensive unit+e2e+logging","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.14\n\nTask Objective:\nCreate a hard section completion gate that verifies all implemented behavior in this section with comprehensive unit tests, integration/e2e scripts, and detailed structured logging evidence.\n\nAcceptance Criteria:\n- Section test matrix covers happy-path, edge-case, and adversarial/error-path scenarios for all section deliverables.\n- E2E scripts execute representative end-user workflows and policy/control-plane transitions for this section.\n- Structured logs include stable event/error codes, trace correlation IDs, and enough context for deterministic replay triage.\n- Verification report is deterministic and machine-readable for CI/release gating.\n\nExpected Artifacts:\n- Section-level test matrix and coverage summary artifact.\n- E2E script suite with pass/fail outputs and reproducible fixtures/seeds.\n- Structured log validation report and traceability bundle.\n- Gate verdict artifact consumable by release automation.\n\n- Machine-readable verification artifact at `artifacts/section_10_14/bd-3epz/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_14/bd-3epz/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests must validate contracts, invariants, and failure semantics.\n- E2E tests must validate cross-component behavior from user/operator entrypoints to persisted effects.\n- Logging must support root-cause analysis without hidden context requirements.\n\nTask-Specific Clarification:\n- For \"[10.14] Section-wide verification gate: comprehensive unit+e2e+logging\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.14] Section-wide verification gate: comprehensive unit+e2e+logging\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.14] Section-wide verification gate: comprehensive unit+e2e+logging\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.14] Section-wide verification gate: comprehensive unit+e2e+logging\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.14] Section-wide verification gate: comprehensive unit+e2e+logging\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"Section test matrix covers happy-path, edge-case, and adversarial/error-path scenarios for all section deliverables.; E2E scripts execute representative end-user workflows and policy/control-plane transitions for this section.; Structured logs include stable event/error codes, trace correlation IDs, and enough context for deterministic replay triage.; Verification report is deterministic and machine-readable for CI/release gating.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:48:11.517555268Z","created_by":"ubuntu","updated_at":"2026-02-20T15:44:01.562913438Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-14","test-obligations","verification-gate"],"dependencies":[{"issue_id":"bd-3epz","depends_on_id":"bd-126h","type":"blocks","created_at":"2026-02-20T07:48:12.099669987Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3epz","depends_on_id":"bd-129f","type":"blocks","created_at":"2026-02-20T07:48:12.050677055Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3epz","depends_on_id":"bd-12n3","type":"blocks","created_at":"2026-02-20T07:48:12.579463687Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3epz","depends_on_id":"bd-15u3","type":"blocks","created_at":"2026-02-20T07:48:13.726723938Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3epz","depends_on_id":"bd-18ud","type":"blocks","created_at":"2026-02-20T07:48:12.844617560Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3epz","depends_on_id":"bd-1ayu","type":"blocks","created_at":"2026-02-20T07:48:13.537978771Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3epz","depends_on_id":"bd-1dar","type":"blocks","created_at":"2026-02-20T07:48:11.954977660Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3epz","depends_on_id":"bd-1daz","type":"blocks","created_at":"2026-02-20T07:48:13.483134591Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3epz","depends_on_id":"bd-1dpd","type":"blocks","created_at":"2026-02-20T08:24:26.552106678Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3epz","depends_on_id":"bd-1fck","type":"blocks","created_at":"2026-02-20T07:48:12.778348751Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3epz","depends_on_id":"bd-1fp4","type":"blocks","created_at":"2026-02-20T07:48:13.432440680Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3epz","depends_on_id":"bd-1iyx","type":"blocks","created_at":"2026-02-20T07:48:13.091380138Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3epz","depends_on_id":"bd-1l62","type":"blocks","created_at":"2026-02-20T07:48:13.336980962Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3epz","depends_on_id":"bd-1nfu","type":"blocks","created_at":"2026-02-20T07:48:12.674028840Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3epz","depends_on_id":"bd-1oof","type":"blocks","created_at":"2026-02-20T07:48:14.022270479Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3epz","depends_on_id":"bd-1ru2","type":"blocks","created_at":"2026-02-20T07:48:12.724543917Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3epz","depends_on_id":"bd-1vsr","type":"blocks","created_at":"2026-02-20T07:48:12.147581655Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3epz","depends_on_id":"bd-1zym","type":"blocks","created_at":"2026-02-20T07:48:13.585081732Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3epz","depends_on_id":"bd-206h","type":"blocks","created_at":"2026-02-20T07:48:12.531769845Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3epz","depends_on_id":"bd-20uo","type":"blocks","created_at":"2026-02-20T07:48:13.289068563Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3epz","depends_on_id":"bd-22yy","type":"blocks","created_at":"2026-02-20T07:48:11.658851089Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3epz","depends_on_id":"bd-2573","type":"blocks","created_at":"2026-02-20T07:48:13.044507204Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3epz","depends_on_id":"bd-25nl","type":"blocks","created_at":"2026-02-20T07:48:11.855103519Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3epz","depends_on_id":"bd-27o2","type":"blocks","created_at":"2026-02-20T07:48:12.950567006Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3epz","depends_on_id":"bd-2808","type":"blocks","created_at":"2026-02-20T07:48:11.806344072Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3epz","depends_on_id":"bd-29r6","type":"blocks","created_at":"2026-02-20T07:48:13.142600498Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3epz","depends_on_id":"bd-29yx","type":"blocks","created_at":"2026-02-20T07:48:13.241977493Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3epz","depends_on_id":"bd-2e73","type":"blocks","created_at":"2026-02-20T07:48:14.118909774Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3epz","depends_on_id":"bd-2igi","type":"blocks","created_at":"2026-02-20T07:48:13.774165601Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3epz","depends_on_id":"bd-2ona","type":"blocks","created_at":"2026-02-20T07:48:13.974737346Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3epz","depends_on_id":"bd-2qqu","type":"blocks","created_at":"2026-02-20T07:48:11.756050698Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3epz","depends_on_id":"bd-2twu","type":"blocks","created_at":"2026-02-20T08:20:52.427491455Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3epz","depends_on_id":"bd-2wsm","type":"blocks","created_at":"2026-02-20T07:48:12.203158600Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3epz","depends_on_id":"bd-2xv8","type":"blocks","created_at":"2026-02-20T07:48:12.308091603Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3epz","depends_on_id":"bd-3a3q","type":"blocks","created_at":"2026-02-20T07:48:13.820935352Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3epz","depends_on_id":"bd-3cs3","type":"blocks","created_at":"2026-02-20T07:48:12.253010762Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3epz","depends_on_id":"bd-3hdv","type":"blocks","created_at":"2026-02-20T07:48:12.365237861Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3epz","depends_on_id":"bd-3i6c","type":"blocks","created_at":"2026-02-20T07:48:11.613358106Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3epz","depends_on_id":"bd-3ort","type":"blocks","created_at":"2026-02-20T07:48:13.190861787Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3epz","depends_on_id":"bd-3rya","type":"blocks","created_at":"2026-02-20T07:48:13.632148927Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3epz","depends_on_id":"bd-876n","type":"blocks","created_at":"2026-02-20T07:48:11.707461478Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3epz","depends_on_id":"bd-8tvs","type":"blocks","created_at":"2026-02-20T07:48:12.998070203Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3epz","depends_on_id":"bd-ac83","type":"blocks","created_at":"2026-02-20T07:48:12.626672606Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3epz","depends_on_id":"bd-b9b6","type":"blocks","created_at":"2026-02-20T07:48:13.384860049Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3epz","depends_on_id":"bd-bq4p","type":"blocks","created_at":"2026-02-20T07:48:13.881264508Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3epz","depends_on_id":"bd-mwvn","type":"blocks","created_at":"2026-02-20T07:48:13.680901782Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3epz","depends_on_id":"bd-nupr","type":"blocks","created_at":"2026-02-20T07:48:14.168391136Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3epz","depends_on_id":"bd-nwhn","type":"blocks","created_at":"2026-02-20T07:48:11.902054718Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3epz","depends_on_id":"bd-okqy","type":"blocks","created_at":"2026-02-20T07:48:12.903638970Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3epz","depends_on_id":"bd-oolt","type":"blocks","created_at":"2026-02-20T07:48:14.068854233Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3epz","depends_on_id":"bd-qlc6","type":"blocks","created_at":"2026-02-20T07:48:12.414930826Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3epz","depends_on_id":"bd-sddz","type":"blocks","created_at":"2026-02-20T07:48:13.928230394Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3epz","depends_on_id":"bd-v4l0","type":"blocks","created_at":"2026-02-20T07:48:12.473044486Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3epz","depends_on_id":"bd-xwk5","type":"blocks","created_at":"2026-02-20T07:48:12.003363050Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-3ex","title":"[10.7] Add verifier CLI conformance contract tests.","description":"## [10.7] Verifier CLI Conformance Contract Tests\n\n### Why This Exists\n\nThe verifier CLI (`franken-node verify`) is a public interface used by external verifiers, CI pipelines, and the verifier economy (10.17, 10.12). Its behavior must be stable and predictable across versions — breaking changes to input formats, output schemas, exit codes, or error messages would break downstream consumers silently. This bead establishes contract tests that pin the CLI's observable behavior, detect unintended breaking changes, and require explicit version bumps when intentional breaking changes are made.\n\n### What It Must Do\n\n**Contract definition**: Define the verifier CLI's contract in a machine-readable format (`spec/verifier_cli_contract.toml` or similar). The contract specifies: (1) accepted input formats (file paths, stdin, glob patterns), (2) output JSON schema for success and error cases, (3) exit codes and their meanings (0 = all checks pass, 1 = some checks fail, 2 = invalid input, 3 = internal error), (4) error message format (structured JSON with error code, human message, and remediation hint), and (5) command-line flag inventory with types and defaults.\n\n**Contract test suite**: A test suite that exercises every aspect of the contract with specific, pinned inputs and expected outputs. Tests use snapshot-style assertions — the expected output is stored alongside the test and compared byte-for-byte (after normalization of timestamps and paths). Adding a new flag or output field is non-breaking; removing or changing one is breaking.\n\n**Breaking change detection**: When a contract test fails, the failure message clearly distinguishes between \"new field added\" (non-breaking, auto-update snapshot) and \"existing field changed/removed\" (breaking, requires version bump). A `--update-snapshots` flag regenerates expected outputs for non-breaking changes.\n\n**Version enforcement**: The CLI embeds a contract version (semver). Breaking changes increment the major version. The contract test suite validates that the embedded version matches the contract definition's version. If a breaking change is detected without a version bump, the gate fails.\n\n**Backward compatibility layer**: When a major version bump occurs, the CLI supports `--compat-version=<N>` to produce output in the previous format for one major version back, giving consumers time to migrate.\n\n### Acceptance Criteria\n\n1. Verifier CLI contract is defined in a machine-readable file specifying input formats, output schemas, exit codes, error formats, and flag inventory.\n2. Contract test suite covers every exit code, every output field, every error code, and every accepted input format with pinned expected outputs.\n3. Breaking vs. non-breaking change detection works: new fields pass, changed/removed fields fail with clear breaking-change message.\n4. Contract version is embedded in CLI output and validated against the contract definition.\n5. `--update-snapshots` regenerates expected outputs for non-breaking changes only.\n6. `--compat-version=<N>` produces output in the previous major version's format.\n7. Verification script `scripts/check_verifier_contract.py` with `--json` flag validates the contract test suite.\n8. Unit tests in `tests/test_check_verifier_contract.py` cover contract parsing, snapshot comparison, breaking change detection, version validation, and compat-version output.\n\n### Key Dependencies\n\n- Verifier CLI implementation (`franken-node verify` subcommand).\n- CLI framework from bd-n9r (cli.rs).\n- JSON Schema for output validation.\n- Existing verification scripts that the CLI orchestrates.\n\n### Testing & Logging Requirements\n\n- Stability test: run contract tests against current CLI, assert all pass.\n- Breaking change test: modify an output field, run tests, assert breaking-change failure.\n- Non-breaking change test: add a new output field, run tests, assert pass (or auto-update).\n- Compat-version test: request previous version output, assert format matches previous contract.\n- Structured JSON logs for each contract test execution: test name, expected vs. actual, pass/fail, breaking/non-breaking classification.\n\n### Expected Artifacts\n\n- `spec/verifier_cli_contract.toml` — contract definition.\n- `tests/contract/snapshots/` — pinned expected outputs.\n- `scripts/check_verifier_contract.py` — verification script.\n- `tests/test_check_verifier_contract.py` — unit tests.\n- `artifacts/section_10_7/bd-3ex/verification_evidence.json` — gate results.\n- `artifacts/section_10_7/bd-3ex/verification_summary.md` — human-readable summary.\n\n### Additional E2E Requirement\n\n- E2E test scripts must execute full end-to-end workflows from clean fixtures, emit deterministic machine-readable pass/fail evidence, and capture stepwise structured logs for root-cause triage.","acceptance_criteria":"1. Verifier CLI exposes at least: verify-module, verify-migration, verify-compatibility, and verify-corpus subcommands.\n2. Each subcommand has a conformance contract defined in a spec document under docs/specs/ specifying inputs, outputs, exit codes, and error formats.\n3. Contract tests exercise every specified input/output combination including edge cases and error paths.\n4. Tests validate that CLI output conforms to the documented JSON schema (no undocumented fields, no missing required fields).\n5. Exit codes follow a documented taxonomy: 0=pass, 1=fail, 2=error, 3=skipped.\n6. Per Section 3.2 capability #10 (public verifier toolkit): CLI is usable by an external party with no internal knowledge — contract tests verify this by running in an isolated environment with no access to internal state.\n7. Contract tests are generated from the spec documents (not hand-written) to ensure spec and tests stay in sync.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:36:47.589413741Z","created_by":"ubuntu","updated_at":"2026-02-20T17:08:35.023433315Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-7","self-contained","test-obligations"]}
{"id":"bd-3f9","title":"[10.3] Build deterministic migration failure replay tooling.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.3 — Migration System\n\nWhy This Exists:\nMigration autopilot pipeline from audit to rollout, designed to collapse migration friction with deterministic confidence and replayability.\n\nTask Objective:\nBuild deterministic migration failure replay tooling.\n\nAcceptance Criteria:\n- Implement with explicit success/failure invariants and deterministic behavior under normal, edge, and fault scenarios.\n- Ensure outcomes are verifiable via automated tests and reproducible artifact outputs.\n\nExpected Artifacts:\n- Section-owned design/spec artifact at `docs/specs/section_10_3/bd-3f9_contract.md`, capturing decision rationale, invariants, and interface boundaries.\n- Machine-readable verification artifact at `artifacts/section_10_3/bd-3f9/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_3/bd-3f9/verification_summary.md` linking implementation intent to measured outcomes.\n\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.3] Build deterministic migration failure replay tooling.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.3] Build deterministic migration failure replay tooling.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.3] Build deterministic migration failure replay tooling.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.3] Build deterministic migration failure replay tooling.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.3] Build deterministic migration failure replay tooling.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","status":"closed","priority":1,"issue_type":"task","assignee":"CrimsonCrane","created_at":"2026-02-20T07:36:45.346743011Z","created_by":"ubuntu","updated_at":"2026-02-20T10:21:45.798113045Z","closed_at":"2026-02-20T10:21:45.798088439Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-3","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-3f9","depends_on_id":"bd-hg1","type":"blocks","created_at":"2026-02-20T07:43:22.321503315Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-3f91","title":"Epic: Lease Service + Execution Planner [10.13e]","description":"- type: epic","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-20T07:47:58.072163029Z","updated_at":"2026-02-20T07:49:21.181692969Z","closed_at":"2026-02-20T07:49:21.181674655Z","close_reason":"Superseded by canonical detailed master-plan graph (bd-33v) with full 10.N-10.21 and 11-16 coverage, explicit dependencies, and per-section verification gates.","source_repo":".","compaction_level":0,"original_size":0}
{"id":"bd-3fo","title":"[PLAN 10.0] Top 10 Initiative Tracking","description":"Section: 10.0 — Top 10 Initiative Tracking\n\nStrategic Context:\nTop-10 initiative tracking layer that ensures category-defining capabilities are delivered as a complete system, not isolated features.\n\nExecution Requirements:\n- Preserve all scoped capabilities from the canonical plan.\n- Keep contracts self-contained so future contributors can execute without reopening the master plan.\n- Require explicit testing strategy (unit + integration/e2e) and structured logging/telemetry for every child bead.\n- Require artifact-backed validation for performance, security, and correctness claims.\n\nDependency Semantics:\n- This epic is blocked by its child implementation beads.\n- Cross-epic dependencies encode strategic sequencing and canonical ownership boundaries.\n\n## Success Criteria\n- All child beads for this section are completed with acceptance artifacts attached and no unresolved blockers.\n- Section-level verification gate for comprehensive unit tests, integration/e2e workflows, and detailed structured logging evidence is green.\n- Scope-to-plan traceability is explicit, with no feature loss versus the canonical plan section.\n\n## Optimization Notes\n- User-Outcome Lens: \"[PLAN 10.0] Top 10 Initiative Tracking\" must improve operator confidence, safety posture, and deterministic recovery behavior under both normal and adversarial conditions.\n- Cross-Section Coordination: child beads must encode integration assumptions explicitly to avoid local optimizations that degrade system-wide correctness.\n- Verification Non-Negotiable: completion requires reproducible unit/integration/E2E evidence and structured logs suitable for independent replay.\n- Scope Protection: any simplification must preserve canonical-plan feature intent and be justified with explicit tradeoff documentation.","status":"open","priority":1,"issue_type":"epic","created_at":"2026-02-20T07:36:40.215374799Z","created_by":"ubuntu","updated_at":"2026-02-20T16:17:13.035972875Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-0"],"dependencies":[{"issue_id":"bd-3fo","depends_on_id":"bd-1hf","type":"blocks","created_at":"2026-02-20T07:37:12.221401584Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3fo","depends_on_id":"bd-1nf","type":"blocks","created_at":"2026-02-20T07:36:43.079963183Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3fo","depends_on_id":"bd-1ow","type":"blocks","created_at":"2026-02-20T07:37:11.867966377Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3fo","depends_on_id":"bd-1qp","type":"blocks","created_at":"2026-02-20T07:36:42.521695479Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3fo","depends_on_id":"bd-1ta","type":"blocks","created_at":"2026-02-20T07:37:12.340046742Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3fo","depends_on_id":"bd-1u9","type":"blocks","created_at":"2026-02-20T07:37:12.067810643Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3fo","depends_on_id":"bd-1wz","type":"blocks","created_at":"2026-02-20T07:37:12.519035922Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3fo","depends_on_id":"bd-1xg","type":"blocks","created_at":"2026-02-20T07:37:11.989087818Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3fo","depends_on_id":"bd-20a","type":"blocks","created_at":"2026-02-20T07:37:12.028202556Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3fo","depends_on_id":"bd-20z","type":"blocks","created_at":"2026-02-20T07:37:12.260081623Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3fo","depends_on_id":"bd-229","type":"blocks","created_at":"2026-02-20T07:37:11.949951780Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3fo","depends_on_id":"bd-26k","type":"blocks","created_at":"2026-02-20T07:37:12.183548386Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3fo","depends_on_id":"bd-2ac","type":"blocks","created_at":"2026-02-20T07:36:42.999829460Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3fo","depends_on_id":"bd-2de","type":"blocks","created_at":"2026-02-20T07:36:42.600501940Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3fo","depends_on_id":"bd-2g0","type":"blocks","created_at":"2026-02-20T07:36:43.164646441Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3fo","depends_on_id":"bd-2hrg","type":"blocks","created_at":"2026-02-20T16:17:13.035913334Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3fo","depends_on_id":"bd-32p","type":"blocks","created_at":"2026-02-20T07:37:12.558528684Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3fo","depends_on_id":"bd-37i","type":"blocks","created_at":"2026-02-20T07:37:12.675325580Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3fo","depends_on_id":"bd-39a","type":"blocks","created_at":"2026-02-20T07:37:12.597436407Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3fo","depends_on_id":"bd-3il","type":"blocks","created_at":"2026-02-20T07:37:11.911697915Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3fo","depends_on_id":"bd-3qo","type":"blocks","created_at":"2026-02-20T07:37:12.416970617Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3fo","depends_on_id":"bd-3qsp","type":"blocks","created_at":"2026-02-20T07:48:05.895248058Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3fo","depends_on_id":"bd-3rc","type":"blocks","created_at":"2026-02-20T07:37:12.106806771Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3fo","depends_on_id":"bd-5rh","type":"blocks","created_at":"2026-02-20T07:37:12.378255974Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3fo","depends_on_id":"bd-c4f","type":"blocks","created_at":"2026-02-20T07:37:12.145428631Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3fo","depends_on_id":"bd-cda","type":"blocks","created_at":"2026-02-20T07:37:09.037682421Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3fo","depends_on_id":"bd-go4","type":"blocks","created_at":"2026-02-20T07:37:12.301336487Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3fo","depends_on_id":"bd-khy","type":"blocks","created_at":"2026-02-20T07:36:43.244272449Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3fo","depends_on_id":"bd-mwf","type":"blocks","created_at":"2026-02-20T07:36:42.840729637Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3fo","depends_on_id":"bd-n71","type":"blocks","created_at":"2026-02-20T07:37:12.456495840Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3fo","depends_on_id":"bd-uo4","type":"blocks","created_at":"2026-02-20T07:36:42.762488900Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3fo","depends_on_id":"bd-y4g","type":"blocks","created_at":"2026-02-20T07:36:42.681380220Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3fo","depends_on_id":"bd-ybe","type":"blocks","created_at":"2026-02-20T07:37:12.636396016Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3fo","depends_on_id":"bd-yqz","type":"blocks","created_at":"2026-02-20T07:36:42.919101499Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-3fr6","title":"Epic: Migration System [10.3]","description":"- type: epic","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-20T07:47:58.072163029Z","updated_at":"2026-02-20T07:49:21.101468746Z","closed_at":"2026-02-20T07:49:21.101451594Z","close_reason":"Superseded by canonical detailed master-plan graph (bd-33v) with full 10.N-10.21 and 11-16 coverage, explicit dependencies, and per-section verification gates.","source_repo":".","compaction_level":0,"original_size":0}
{"id":"bd-3g4k","title":"[10.18] Implement hash-chained receipt stream with periodic commitment checkpoints.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.18 — Verifiable Execution Fabric Execution Track (9L)\n\nWhy This Exists:\nVEF track for proof-carrying runtime compliance: receipt chains, commitment checkpoints, proof generation/verification, and claim gating.\n\nTask Objective:\nImplement hash-chained receipt stream with periodic commitment checkpoints.\n\nAcceptance Criteria:\n- Receipt stream is append-only with deterministic chain linkage; checkpoint commitments are reproducible; tamper detection is fail-closed.\n\nExpected Artifacts:\n- `src/trust/vef_receipt_chain.rs`, `tests/conformance/vef_receipt_chain_integrity.rs`, `artifacts/10.18/vef_receipt_commitment_log.jsonl`.\n\n- Machine-readable verification artifact at `artifacts/section_10_18/bd-3g4k/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_18/bd-3g4k/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.18] Implement hash-chained receipt stream with periodic commitment checkpoints.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.18] Implement hash-chained receipt stream with periodic commitment checkpoints.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.18] Implement hash-chained receipt stream with periodic commitment checkpoints.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.18] Implement hash-chained receipt stream with periodic commitment checkpoints.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.18] Implement hash-chained receipt stream with periodic commitment checkpoints.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"- Receipt stream is append-only with deterministic chain linkage; checkpoint commitments are reproducible; tamper detection is fail-closed.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:37:04.375699004Z","created_by":"ubuntu","updated_at":"2026-02-20T17:05:40.919250618Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-18","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-3g4k","depends_on_id":"bd-p73r","type":"blocks","created_at":"2026-02-20T17:05:40.919195716Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-3ghc","title":"Epic: Asupersync Remote + Evidence Integration [10.15c]","description":"- type: epic","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-20T07:47:58.072163029Z","updated_at":"2026-02-20T07:49:21.267153425Z","closed_at":"2026-02-20T07:49:21.267132776Z","close_reason":"Superseded by canonical detailed master-plan graph (bd-33v) with full 10.N-10.21 and 11-16 coverage, explicit dependencies, and per-section verification gates.","source_repo":".","compaction_level":0,"original_size":0}
{"id":"bd-3gnh","title":"[10.15] Add observability dashboards for region health, obligation health, lane pressure, and cancel latency.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.15 — Asupersync-First Integration Execution Track (8.4-8.6)\n\nWhy This Exists:\nAsupersync-first control-plane migration track enforcing Cx-first, region ownership, cancellation correctness, and protocol-grade verification gates.\n\nTask Objective:\nAdd observability dashboards for region health, obligation health, lane pressure, and cancel latency.\n\nAcceptance Criteria:\n- Dashboards expose core runtime health invariants with alert thresholds; metrics are mapped to runbook actions.\n\nExpected Artifacts:\n- `docs/observability/asupersync_control_dashboards.md`, `artifacts/10.15/dashboard_snapshot.json`, `artifacts/10.15/alert_policy_map.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_15/bd-3gnh/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_15/bd-3gnh/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.15] Add observability dashboards for region health, obligation health, lane pressure, and cancel latency.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.15] Add observability dashboards for region health, obligation health, lane pressure, and cancel latency.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.15] Add observability dashboards for region health, obligation health, lane pressure, and cancel latency.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.15] Add observability dashboards for region health, obligation health, lane pressure, and cancel latency.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.15] Add observability dashboards for region health, obligation health, lane pressure, and cancel latency.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"- Dashboards expose core runtime health invariants with alert thresholds; metrics are mapped to runbook actions.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:37:01.118361043Z","created_by":"ubuntu","updated_at":"2026-02-20T17:00:43.967336837Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-15","self-contained","test-obligations"]}
{"id":"bd-3go4","title":"[10.18] Integrate VEF coverage and proof-validity metrics into claim compiler and public trust scoreboard.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.18 — Verifiable Execution Fabric Execution Track (9L)\n\nWhy This Exists:\nVEF track for proof-carrying runtime compliance: receipt chains, commitment checkpoints, proof generation/verification, and claim gating.\n\nTask Objective:\nIntegrate VEF coverage and proof-validity metrics into claim compiler and public trust scoreboard.\n\nAcceptance Criteria:\n- Claim compiler can require VEF-backed evidence for security/compliance claims; scoreboard publishes VEF coverage/validity stats with signed evidence links.\n\nExpected Artifacts:\n- `docs/specs/vef_claim_integration.md`, `tests/conformance/vef_claim_gate.rs`, `artifacts/10.18/vef_claim_coverage_snapshot.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_18/bd-3go4/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_18/bd-3go4/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.18] Integrate VEF coverage and proof-validity metrics into claim compiler and public trust scoreboard.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.18] Integrate VEF coverage and proof-validity metrics into claim compiler and public trust scoreboard.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.18] Integrate VEF coverage and proof-validity metrics into claim compiler and public trust scoreboard.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.18] Integrate VEF coverage and proof-validity metrics into claim compiler and public trust scoreboard.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.18] Integrate VEF coverage and proof-validity metrics into claim compiler and public trust scoreboard.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"- Claim compiler can require VEF-backed evidence for security/compliance claims; scoreboard publishes VEF coverage/validity stats with signed evidence links.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:37:04.954326332Z","created_by":"ubuntu","updated_at":"2026-02-20T17:00:47.026209158Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-18","self-contained","test-obligations"]}
{"id":"bd-3gwi","title":"[10.19] Implement contribution-weighted intelligence access policy and reciprocity controls.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.19 — Adversarial Trust Commons Execution Track (9M)\n\nWhy This Exists:\nATC federated intelligence track for privacy-preserving cross-deployment threat learning, robust aggregation, and verifier-backed trust metrics.\n\nTask Objective:\nImplement contribution-weighted intelligence access policy and reciprocity controls.\n\nAcceptance Criteria:\n- Intelligence access tiers map to measured contribution quality/quantity by policy; free-rider limits and exception paths are explicit and auditable.\n\nExpected Artifacts:\n- `docs/specs/atc_reciprocity_policy.md`, `tests/conformance/atc_reciprocity_enforcement.rs`, `artifacts/10.19/atc_reciprocity_matrix.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_19/bd-3gwi/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_19/bd-3gwi/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.19] Implement contribution-weighted intelligence access policy and reciprocity controls.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.19] Implement contribution-weighted intelligence access policy and reciprocity controls.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.19] Implement contribution-weighted intelligence access policy and reciprocity controls.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.19] Implement contribution-weighted intelligence access policy and reciprocity controls.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.19] Implement contribution-weighted intelligence access policy and reciprocity controls.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"- Intelligence access tiers map to measured contribution quality/quantity by policy; free-rider limits and exception paths are explicit and auditable.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:37:05.919149500Z","created_by":"ubuntu","updated_at":"2026-02-20T17:00:53.527438221Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-19","self-contained","test-obligations"]}
{"id":"bd-3h1g","title":"[14] Publish benchmark specs/harness/datasets/scoring formulas","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 14\n\nTask Objective:\nPublish full benchmark package and scoring semantics for public reproducibility.\n\nExecution Requirements:\n- Make this requirement machine-verifiable and release-gated where applicable.\n- Keep behavior self-documenting so future contributors do not need to re-open the master plan.\n\nTesting & Logging Requirements:\n- Unit tests for rule/contract logic and error conditions.\n- Integration/E2E tests for workflow-level enforcement.\n- Structured logs with stable codes and trace IDs.\n- Reproducible artifacts that prove contract satisfaction.\n\nAcceptance Criteria:\n- Success and failure invariants for [14] Publish benchmark specs/harness/datasets/scoring formulas are explicitly quantified and machine-verifiable.\n- Determinism requirements for [14] Publish benchmark specs/harness/datasets/scoring formulas are validated across repeated runs and adversarial perturbations.\n\n\nExpected Artifacts:\n- Machine-readable verification artifact with explicit canonical path under artifacts/section_14/bd-3h1g/verification_evidence.json, suitable for CI/release gating.\n- Human-readable verification summary with explicit canonical path under artifacts/section_14/bd-3h1g/verification_summary.md, linking implementation intent to measured validation outcomes.\n\nTask-Specific Clarification:\n- The implementation must deliver the exact capability named in the title, with no scope dilution and no silent feature omission.\n- Validation artifacts must include capability-specific thresholds, pass/fail criteria, and reproducible evidence bundles tied to this bead ID.\n- Program/section verification gates must be able to consume this bead's outputs without manual interpretation.\n\nWhy This Improves User Outcomes:\n- \"[14] Publish benchmark specs/harness/datasets/scoring formulas\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[14] Publish benchmark specs/harness/datasets/scoring formulas\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"1. Benchmark specification document published covering: scope, methodology, metrics, scoring formulas, and interpretation guide.\n2. Benchmark harness is open-source, installable via npm or cargo, and runs on Linux/macOS/Windows.\n3. Benchmark datasets are versioned, hosted publicly, and include: (a) API compatibility corpus, (b) performance workloads (throughput, latency, cold start), (c) security attack scenarios.\n4. Scoring formulas are transparent: each metric has a defined formula, input sources, normalization method, and aggregation into overall score.\n5. Harness produces machine-readable output (JSON) and human-readable report (Markdown).\n6. Benchmark is reproducible: same harness + same dataset + same system produces results within 5% variance.\n7. Publication checklist: spec reviewed by >= 2 external reviewers, harness CI-tested on 3 platforms, datasets validated for completeness.\n8. Evidence: benchmark_publication_checklist.json with per-item status.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:39:35.301199420Z","created_by":"ubuntu","updated_at":"2026-02-20T16:08:37.463156629Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-14","self-contained","test-obligations"]}
{"id":"bd-3h63","title":"[10.15] Add saga wrappers with deterministic compensations for multi-step remote+local workflows.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.15 — Asupersync-First Integration Execution Track (8.4-8.6)\n\nWhy This Exists:\nAsupersync-first control-plane migration track enforcing Cx-first, region ownership, cancellation correctness, and protocol-grade verification gates.\n\nTask Objective:\nAdd saga wrappers with deterministic compensations for multi-step remote+local workflows.\n\nAcceptance Criteria:\n- Cancellation/crash at any step leaves equivalent \"never happened\" state or committed terminal state; compensation traces are replay-stable.\n\nExpected Artifacts:\n- `docs/specs/control_sagas.md`, `tests/integration/control_saga_compensation.rs`, `artifacts/10.15/control_saga_traces.jsonl`.\n\n- Machine-readable verification artifact at `artifacts/section_10_15/bd-3h63/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_15/bd-3h63/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.15] Add saga wrappers with deterministic compensations for multi-step remote+local workflows.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.15] Add saga wrappers with deterministic compensations for multi-step remote+local workflows.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.15] Add saga wrappers with deterministic compensations for multi-step remote+local workflows.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.15] Add saga wrappers with deterministic compensations for multi-step remote+local workflows.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.15] Add saga wrappers with deterministic compensations for multi-step remote+local workflows.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"- Cancellation/crash at any step leaves equivalent \"never happened\" state or committed terminal state; compensation traces are replay-stable.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:37:00.301222545Z","created_by":"ubuntu","updated_at":"2026-02-20T17:00:44.108843672Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-15","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-3h63","depends_on_id":"bd-12n3","type":"blocks","created_at":"2026-02-20T14:59:48.113662257Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3h63","depends_on_id":"bd-ac83","type":"blocks","created_at":"2026-02-20T14:59:47.982183828Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-3hdv","title":"[10.14] Define monotonic control epoch in canonical manifest state.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.14 — FrankenSQLite Deep-Mined Expansion Execution Track (9J)\n\nWhy This Exists:\nFrankenSQLite deep-mined control integrity track: evidence ledgers, proofs, epochs, remote effects, markers/MMR, and deterministic fault labs.\n\nTask Objective:\nDefine monotonic control epoch in canonical manifest state.\n\nAcceptance Criteria:\n- Epoch value is monotonic and durable; regressions are rejected; epoch changes produce signed control events.\n\nExpected Artifacts:\n- `docs/specs/control_epoch_contract.md`, `tests/conformance/control_epoch_monotonicity.rs`, `artifacts/10.14/control_epoch_history.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_14/bd-3hdv/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_14/bd-3hdv/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.14] Define monotonic control epoch in canonical manifest state.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.14] Define monotonic control epoch in canonical manifest state.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.14] Define monotonic control epoch in canonical manifest state.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.14] Define monotonic control epoch in canonical manifest state.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.14] Define monotonic control epoch in canonical manifest state.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"Epoch value is monotonic and durable; regressions are rejected; epoch changes produce signed control events.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:36:58.142004330Z","created_by":"ubuntu","updated_at":"2026-02-20T16:22:27.930899939Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-14","self-contained","test-obligations"]}
{"id":"bd-3he","title":"[10.11] Implement supervision tree with restart budgets and escalation policies.","description":"[10.11] Implement supervision tree with restart budgets and escalation policies.\n\n## Why This Exists\n\nReliable systems require deterministic failure containment. When a component fails, the system must decide: restart it, escalate to a parent supervisor, or shut down gracefully. Without a structured supervision tree, failure handling is ad-hoc — some components retry infinitely, others crash the entire process, and failure cascades are unpredictable. This bead implements an Erlang-inspired supervision tree for franken_node's product services, providing configurable restart budgets, escalation policies, and bounded recovery behavior. This is foundational for the reliability guarantees referenced throughout sections 10.11 and 10.12.\n\n## What It Must Do\n\n1. **Supervisor abstraction.** Implement a `Supervisor` struct that manages a set of child workers or nested supervisors. Each supervisor has a `SupervisionStrategy`: `OneForOne` (restart only the failed child), `OneForAll` (restart all children if any fails), or `RestForOne` (restart the failed child and all children started after it).\n\n2. **Restart budgets.** Each supervisor has a restart budget: `max_restarts` within a `time_window` (e.g., 5 restarts in 60 seconds). When the budget is exhausted, the supervisor does not restart the child — instead it escalates to its own parent supervisor. Budget tracking uses a sliding window, not a fixed window.\n\n3. **Escalation policies.** When a supervisor exhausts its budget, it reports the failure to its parent. The parent can: absorb the failure (reset child supervisor's budget), propagate the escalation upward, or trigger a graceful shutdown of the entire subtree. Escalation chains are bounded — a maximum escalation depth is configurable (default: 5 levels).\n\n4. **Child specifications.** Each child is defined by a `ChildSpec` that includes: name, start function, restart type (`Permanent` — always restart, `Transient` — restart only on abnormal exit, `Temporary` — never restart), shutdown timeout, and capability profile reference (from bd-cvt).\n\n5. **Graceful shutdown sequencing.** When a supervisor shuts down, it stops children in reverse start order, respecting each child's shutdown timeout. If a child doesn't stop within its timeout, it is forcefully terminated. Shutdown progress is logged.\n\n6. **Health reporting integration.** Each supervisor exposes health status: number of active children, restart count within current window, budget remaining, and escalation state. This feeds into the health gate system (health_gate.rs).\n\n7. **Deterministic testing.** The supervision tree must be testable with deterministic simulated failures. A test harness allows injecting failures at specific children and asserting the resulting restart/escalation sequence.\n\n## Acceptance Criteria\n\n1. `Supervisor`, `ChildSpec`, and `SupervisionStrategy` types implemented in `crates/franken-node/src/connector/supervision.rs`.\n2. All three strategies (`OneForOne`, `OneForAll`, `RestForOne`) are implemented with tests.\n3. Restart budget with sliding window correctly limits restarts and triggers escalation.\n4. Escalation chain terminates at configurable max depth with graceful shutdown.\n5. Graceful shutdown respects reverse-order and per-child timeouts.\n6. Health reporting exposes active children, restart count, and budget remaining.\n7. Deterministic test harness exercises at least 10 failure scenarios.\n8. Verification script `scripts/check_supervision_tree.py` with `--json` flag confirms all criteria.\n9. Evidence artifacts written to `artifacts/section_10_11/bd-3he/`.\n\n## Key Dependencies\n\n- bd-cvt (capability profiles) — child specs reference capability profiles.\n- health_gate.rs — health reporting integration.\n- Tokio runtime for async supervision (if async workers are used).\n- 10.13 stable error namespace for supervision error codes.\n\n## Testing & Logging Requirements\n\n- Unit tests covering each supervision strategy, budget exhaustion, escalation, and shutdown sequencing.\n- Integration test with a 3-level supervision tree where leaf workers are injected with failures.\n- Property-based tests confirming that restart count never exceeds budget within any sliding window.\n- Structured logging: `supervisor.child_started`, `supervisor.child_failed`, `supervisor.child_restarted`, `supervisor.budget_exhausted`, `supervisor.escalation`, `supervisor.shutdown_started`, `supervisor.shutdown_complete` events.\n\n## Expected Artifacts\n\n- `docs/specs/section_10_11/bd-3he_contract.md` — specification document.\n- `crates/franken-node/src/connector/supervision.rs` — Rust implementation.\n- `scripts/check_supervision_tree.py` — verification script.\n- `tests/test_check_supervision_tree.py` — unit tests.\n- `artifacts/section_10_11/bd-3he/verification_evidence.json` — evidence.\n- `artifacts/section_10_11/bd-3he/verification_summary.md` — summary.","acceptance_criteria":"AC for bd-3he:\n1. A SupervisionTree structure models parent-child relationships between supervised actors/tasks, where each node has a configurable RestartBudget (max_restarts: u32, window: Duration).\n2. When a child fails, the supervisor applies the restart policy: if restarts remaining in the current window > 0, restart the child and decrement the budget; if budget is exhausted, escalate to the parent supervisor.\n3. Escalation policies are configurable per node: RestartChild (default), RestartAllChildren (one-for-all), EscalateToParent, and ShutdownSubtree.\n4. The root supervisor has no parent; budget exhaustion at the root triggers a controlled system shutdown with SUPERVISION_ROOT_EXHAUSTED event and a full tree status dump.\n5. Restart budgets use a sliding window (not fixed intervals): the window tracks the timestamp of each restart and expires old entries, so bursts are correctly detected.\n6. Each supervisor node exposes metrics: restart_count (counter), active_children (gauge), budget_remaining (gauge), and escalation_count (counter).\n7. The supervision tree integrates with the cancellation protocol (bd-7om): shutting down a subtree sends cancel -> drain -> finalize to all children in reverse dependency order (leaves first, then parents).\n8. Unit tests verify: (a) child restart within budget succeeds, (b) budget exhaustion triggers escalation, (c) one-for-all policy restarts all siblings, (d) root exhaustion triggers shutdown, (e) sliding window correctly expires old restarts, (f) subtree shutdown follows cancel -> drain -> finalize order.\n9. Structured log events: CHILD_STARTED / CHILD_FAILED / CHILD_RESTARTED / BUDGET_EXHAUSTED / ESCALATION_TRIGGERED / SUBTREE_SHUTDOWN with supervisor path and child ID.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:36:50.140473702Z","created_by":"ubuntu","updated_at":"2026-02-20T17:01:40.783409610Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-11","self-contained","test-obligations"]}
{"id":"bd-3hig","title":"[9] Multi-Track Build Program — Tracks A-E + Enhancement Maps 9A-9O genesis context","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md — Section 9\n\n## Why This Exists\nThis captures the multi-track build program structure (Tracks A-E) and the enhancement maps (9A-9O) that generated the detailed execution tracks (10.x). Understanding this context is essential for knowing WHY each 10.x track exists and what it must deliver.\n\n## Build Tracks A-E (Section 9)\n\n### Track A: Product Substrate and Split Governance\nExit gate: split contract enforced by CI, baseline compatibility harness green, initial migration report artifacts reproducible.\nImplementation: 10.1 (Charter) + 10.2 (Compatibility Core)\n\n### Track B: Compatibility Superset + Migration Singularity\nExit gate: targeted compatibility threshold met, migration pipeline produces actionable reports, divergence receipts reproducible.\nImplementation: 10.2, 10.3, 10.7\n\n### Track C: Trust-Native Ecosystem Layer\nExit gate: trust-native workflows operational, replay/audit pass external verifier checks, revocation/quarantine drills pass gates.\nImplementation: 10.4, 10.5, 10.8, 10.13\n\n### Track D: Category Benchmark + Market Capture\nExit gate: benchmark publicly consumable, independent external replication, enterprise pilot shows measurable lift.\nImplementation: 10.9, 10.12, 10.14\n\n### Track E: Frontier Industrialization\nExit gate: impossible-by-default capabilities adopted, category benchmark external adoption, sustained advantages.\nImplementation: 10.17, 10.18, 10.19, 10.20, 10.21\n\n## Enhancement Maps (Section 9A-9O) — Genesis of 10.x Tracks\n\n### Per-Initiative Enhancement Maps\n- 9A (Idea-Wizard Top 10): Generated 10.0 initiative tracking. Dependency order: compatibility envelope -> migration autopilot -> lockstep oracle -> policy shims -> trust cards -> distribution -> quarantine -> copilot -> economics -> benchmark.\n- 9B (Alien-Graveyard): High-EV primitives per initiative. Session-type checks, authenticated data structures, delta-debugging, policy-as-data signatures, anti-entropy reconciliation, key-transparency, VOI ranking, decision-theoretic expected-loss, conformance vectors.\n- 9C (Alien-Artifact): Mathematical rigor per initiative. Proof-carrying claims, hypothesis-tested transformations, posterior decomposition, causal trace equivalence, non-interference checks, probabilistic SLO proofs, cryptographic receipts, expected-loss vectors, posterior attacker ROI, statistical rigor.\n- 9D (Extreme-Software-Optimization): Performance discipline per initiative. Precompiled decision DAGs, deterministic batching, incremental updates, streaming normalization, deterministic rule order, propagation fast paths, batched pipelines, interactive budgets, hot path scoring, benchmark runner optimization.\n- 9E (FCP-Spec-Inspired): Generated 10.10 (FCP-Inspired Hardening) and parts of 10.13.\n- 9F (Moonshot Bets): Generated 10.9 and 10.12. 15 category-shift initiatives.\n- 9G (FrankenSQLite-Inspired): Generated 10.11 (Runtime Systems).\n- 9H (Frontier Programs): Generated 10.12. 5 adopted programs: Migration Singularity, Trust Fabric, Verifier Economy, Operator Intelligence, Ecosystem Network Effects.\n- 9I (FCP Deep-Mined): Generated 10.13. 20 expansion items covering connector lifecycle, state management, sandbox, network guard, admission, control channel, telemetry, conformance, golden vectors.\n- 9J (FrankenSQLite Deep-Mined): Generated 10.14. 20 expansion items covering evidence ledger, correctness envelope, dual-statistics, monotonic hardening, proof-carrying repair, deterministic encoding, object-class profiles, tiered storage, durability modes, remote effects, bulkheads, epochs, markers, MMR proofs, root pointers, caches, fault labs, cancellation, lane scheduling.\n- 9K (Radical Expansion): Generated 10.17. 15 items including proof-carrying speculation, adversary graph, time-travel, capability artifacts, isolation mesh, ZK attestation, N-version oracle, staking/slashing, optimization governor, intent firewall, exfiltration sentinel, verifier SDK, hardware planner, counterfactual lab, claim compiler.\n- 9L (VEF): Generated 10.18. Verifiable Execution Fabric.\n- 9M (ATC): Generated 10.19. Adversarial Trust Commons.\n- 9N (DGIS): Generated 10.20. Dependency Graph Immune System.\n- 9O (BPET): Generated 10.21. Behavioral Phenotype Evolution Tracker.\n\n## Acceptance Criteria\n- All 5 build tracks have measurable exit gates\n- Enhancement maps are traceable to 10.x implementation beads\n- No enhancement map item is lost in translation to execution tracks\n\n\n## Success Criteria\n- Tracks A-E maintain explicit, complete linkage to their 10.x execution beads and exit gates.\n- Enhancement maps 9A-9O remain fully represented in implementation planning with no dropped high-impact items.\n- Build-program sequencing remains dependency-sound and reproducible for multi-agent execution.\n\n## Testing & Logging Requirements\n- Unit tests for map-to-track linkage validators and exit-gate coverage checks.\n- E2E build-program trace scripts that verify every enhancement map item resolves into concrete execution beads and verification gates.\n- Structured logs/artifacts capturing traceability scans, uncovered gaps, and deterministic remediation guidance.","status":"open","priority":1,"issue_type":"epic","created_at":"2026-02-20T16:16:20.068242610Z","created_by":"ubuntu","updated_at":"2026-02-20T17:08:27.842141799Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["build-program","enhancement-maps","plan","section-9"],"dependencies":[{"issue_id":"bd-3hig","depends_on_id":"bd-3hyk","type":"blocks","created_at":"2026-02-20T16:17:12.859166529Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-3hm","title":"[10.12] Define migration singularity artifact contract and verifier format.","description":"[10.12] Define migration singularity artifact contract and verifier format.\n\n## Why This Exists\n\nSection 9H.1 defines the migration singularity as the point where migration operations become fully machine-checkable and self-verifying. The migration autopilot (10.3) generates migration plans, but those plans currently lack a standardized artifact format that external verifiers can consume. This bead defines the artifact contract: a structured, versioned format for migration outputs that includes rollback receipts, confidence intervals, precondition proofs, and verifier-friendly validation metadata. This is the bridge between the migration system (10.3) and the verifier economy (10.17), enabling independent parties to validate migration correctness without trusting the migration engine.\n\n## What It Must Do\n\n1. **Migration artifact schema.** Define a JSON Schema for migration singularity artifacts. The schema must include:\n   - `plan_id`: deterministic ID derived from plan content (using trust object ID scheme from bd-1l5).\n   - `plan_version`: semantic version of the plan format.\n   - `preconditions`: array of machine-checkable precondition assertions with verification methods.\n   - `steps`: ordered array of migration steps, each with: action type, target resource, expected pre-state hash, expected post-state hash, rollback action, and estimated duration.\n   - `rollback_receipt`: signed receipt proving that a rollback path exists and was validated before execution.\n   - `confidence_interval`: statistical confidence that the migration will succeed, derived from dry-run results and historical migration data.\n   - `verifier_metadata`: information needed by external verifiers — replay capsule references, golden vector suite IDs, and assertion schemas.\n\n2. **Rollback receipt format.** Define a signed rollback receipt containing: original state snapshot reference, rollback procedure hash, maximum rollback time, and signer identity. The receipt is generated before migration execution and is independently verifiable.\n\n3. **Confidence interval computation.** Define how migration confidence is computed: dry-run success rate, similarity to historically successful migrations, precondition coverage, and rollback path validation. The confidence score must be a calibrated probability (not a raw score).\n\n4. **Verifier-friendly validation metadata.** The artifact must contain everything an independent verifier needs to validate the migration without access to the production system: replay capsule references (from 10.17), expected state hashes, assertion schemas, and verification procedure descriptions.\n\n5. **Artifact versioning and backward compatibility.** The artifact format is versioned. Verifiers must be able to validate artifacts from the current and previous major version. Breaking changes require a new major version with a migration guide.\n\n6. **Artifact signing.** Migration artifacts must be signed by the migration engine's identity key. Signatures use the trust protocol's canonical signing scheme. Unsigned artifacts are rejected by verifiers.\n\n7. **Reference artifact generator.** Implement a tool that generates well-formed sample artifacts for testing. The generator produces artifacts with valid signatures, realistic confidence intervals, and complete verifier metadata.\n\n## Acceptance Criteria\n\n1. JSON Schema for migration artifacts exists at `spec/migration_artifact_schema.json` and validates against JSON Schema Draft 2020-12.\n2. Rollback receipt format is defined with signing and independent verification.\n3. Confidence interval computation is documented with calibration requirements.\n4. Verifier metadata includes replay capsule references, state hashes, and assertion schemas.\n5. Artifact versioning supports current and previous major version validation.\n6. Artifact signing using canonical trust protocol scheme is implemented.\n7. Reference artifact generator produces valid sample artifacts.\n8. Verification script `scripts/check_migration_artifacts.py` with `--json` flag validates schema compliance, signature validity, and completeness.\n9. Evidence artifacts written to `artifacts/section_10_12/bd-3hm/`.\n\n## Key Dependencies\n\n- bd-1l5 (trust object IDs) — plan_id derivation scheme.\n- 10.3 migration system — source of migration plans.\n- 10.17 replay capsules / verifier economy — verifier metadata references.\n- Trust protocol signing scheme — artifact signatures.\n- 10.13 stable error namespace — validation error codes.\n\n## Testing & Logging Requirements\n\n- Unit tests validating schema compliance of generated artifacts.\n- Round-trip test: generate artifact, sign, serialize, deserialize, verify signature, validate schema.\n- Backward compatibility test: validate a v1 artifact with a v2 verifier.\n- Fuzz test: mutate valid artifacts and confirm schema validation rejects them.\n- Self-test mode generates a reference artifact and validates it end-to-end.\n- Structured logging: `migration_artifact.generated`, `migration_artifact.signed`, `migration_artifact.validated`, `migration_artifact.schema_violation`, `migration_artifact.signature_invalid` events.\n\n## Expected Artifacts\n\n- `docs/specs/section_10_12/bd-3hm_contract.md` — specification document.\n- `spec/migration_artifact_schema.json` — JSON Schema.\n- `crates/franken-node/src/connector/migration_artifact.rs` — Rust implementation.\n- `scripts/check_migration_artifacts.py` — verification script.\n- `tests/test_check_migration_artifacts.py` — unit tests.\n- `vectors/migration_artifacts.json` — reference artifacts.\n- `artifacts/section_10_12/bd-3hm/verification_evidence.json` — evidence.\n- `artifacts/section_10_12/bd-3hm/verification_summary.md` — summary.","acceptance_criteria":"1. Define a MigrationSingularityArtifact struct containing: (a) artifact_id (TrustObjectId with unique domain tag MIGRATION or reuse POLICY), (b) source_system_fingerprint (hash of the pre-migration system state), (c) target_system_fingerprint (hash of the expected post-migration state), (d) migration_plan_hash (SHA-256 of the canonical-serialized migration plan), (e) rollback_receipt (a signed proof that the migration can be reversed), (f) precondition_checks (list of {check_name, pass/fail, evidence_hash}), (g) postcondition_checks (same structure), (h) created_at, (i) signer_key_id.\n2. Define a VerifierFormat spec: the artifact MUST be serializable to a self-contained JSON document that an independent verifier can validate without access to the source system. The JSON schema MUST be versioned (schema_version field) and published in docs/specs/section_10_12/migration_artifact_schema.json.\n3. Implement MigrationSingularityArtifact::validate_structure() that checks: (a) all required fields are present, (b) artifact_id is well-formed, (c) at least one precondition and one postcondition check exist, (d) rollback_receipt is parseable and signature-verifiable.\n4. Implement a rollback receipt format: RollbackReceipt containing (a) original_artifact_id, (b) rollback_plan_hash, (c) rollback_deadline (UTC timestamp after which rollback is no longer guaranteed), (d) signer_key_id, (e) signature. The receipt MUST be verifiable using the signer's public key from the key-role registry (bd-364).\n5. Implement schema evolution rules: (a) new fields may be added as optional, (b) existing required fields MUST NOT be removed, (c) field types MUST NOT change. Provide a schema_compatible(old_version, new_version) check.\n6. Unit tests: (a) valid artifact construction and serialization, (b) missing required field rejection, (c) rollback receipt signature verification, (d) schema compatibility check for additive change, (e) schema compatibility check rejects breaking change, (f) round-trip serialize/deserialize identity.\n7. Golden fixture: a complete migration artifact with rollback receipt in vectors/migration_singularity_artifact.json.\n8. Verification: scripts/check_migration_artifact.py --json, artifacts at artifacts/section_10_12/bd-3hm/.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:36:50.834582916Z","created_by":"ubuntu","updated_at":"2026-02-20T16:24:26.580872904Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-12","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-3hm","depends_on_id":"bd-1wz","type":"blocks","created_at":"2026-02-20T07:46:32.054913239Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3hm","depends_on_id":"bd-1xg","type":"blocks","created_at":"2026-02-20T07:46:32.116003832Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3hm","depends_on_id":"bd-20a","type":"blocks","created_at":"2026-02-20T07:46:32.182601201Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3hm","depends_on_id":"bd-229","type":"blocks","created_at":"2026-02-20T07:46:32.261976220Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3hm","depends_on_id":"bd-cda","type":"blocks","created_at":"2026-02-20T07:46:32.322273695Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-3hr2","title":"[10.19] Section-wide verification gate: comprehensive unit+e2e+logging","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.19\n\nTask Objective:\nCreate a hard section completion gate that verifies all implemented behavior in this section with comprehensive unit tests, integration/e2e scripts, and detailed structured logging evidence.\n\nAcceptance Criteria:\n- Section test matrix covers happy-path, edge-case, and adversarial/error-path scenarios for all section deliverables.\n- E2E scripts execute representative end-user workflows and policy/control-plane transitions for this section.\n- Structured logs include stable event/error codes, trace correlation IDs, and enough context for deterministic replay triage.\n- Verification report is deterministic and machine-readable for CI/release gating.\n\nExpected Artifacts:\n- Section-level test matrix and coverage summary artifact.\n- E2E script suite with pass/fail outputs and reproducible fixtures/seeds.\n- Structured log validation report and traceability bundle.\n- Gate verdict artifact consumable by release automation.\n\n- Machine-readable verification artifact at `artifacts/section_10_19/bd-3hr2/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_19/bd-3hr2/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests must validate contracts, invariants, and failure semantics.\n- E2E tests must validate cross-component behavior from user/operator entrypoints to persisted effects.\n- Logging must support root-cause analysis without hidden context requirements.\n\nTask-Specific Clarification:\n- For \"[10.19] Section-wide verification gate: comprehensive unit+e2e+logging\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.19] Section-wide verification gate: comprehensive unit+e2e+logging\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.19] Section-wide verification gate: comprehensive unit+e2e+logging\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.19] Section-wide verification gate: comprehensive unit+e2e+logging\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.19] Section-wide verification gate: comprehensive unit+e2e+logging\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"- Section test matrix covers happy-path, edge-case, and adversarial/error-path scenarios for all section deliverables.\n- E2E scripts execute representative end-user workflows and policy/control-plane transitions for this section.\n- Structured logs include stable event/error codes, trace correlation IDs, and enough context for deterministic replay triage.\n- Verification report is deterministic and machine-readable for CI/release gating.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:48:18.906875449Z","created_by":"ubuntu","updated_at":"2026-02-20T15:45:24.751612699Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-19","test-obligations","verification-gate"],"dependencies":[{"issue_id":"bd-3hr2","depends_on_id":"bd-11rz","type":"blocks","created_at":"2026-02-20T07:48:19.020748614Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3hr2","depends_on_id":"bd-1dpd","type":"blocks","created_at":"2026-02-20T08:24:25.877635349Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3hr2","depends_on_id":"bd-1eot","type":"blocks","created_at":"2026-02-20T07:48:19.166917882Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3hr2","depends_on_id":"bd-1hj3","type":"blocks","created_at":"2026-02-20T07:48:19.511872298Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3hr2","depends_on_id":"bd-24du","type":"blocks","created_at":"2026-02-20T07:48:19.069739152Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3hr2","depends_on_id":"bd-253o","type":"blocks","created_at":"2026-02-20T07:48:19.217751683Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3hr2","depends_on_id":"bd-293y","type":"blocks","created_at":"2026-02-20T07:48:19.639611058Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3hr2","depends_on_id":"bd-2ozr","type":"blocks","created_at":"2026-02-20T07:48:19.360949867Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3hr2","depends_on_id":"bd-2twu","type":"blocks","created_at":"2026-02-20T08:20:51.636842036Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3hr2","depends_on_id":"bd-2yvw","type":"blocks","created_at":"2026-02-20T07:48:19.312954673Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3hr2","depends_on_id":"bd-2zip","type":"blocks","created_at":"2026-02-20T07:48:19.118664658Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3hr2","depends_on_id":"bd-3aqy","type":"blocks","created_at":"2026-02-20T07:48:19.592815869Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3hr2","depends_on_id":"bd-3gwi","type":"blocks","created_at":"2026-02-20T07:48:19.265546934Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3hr2","depends_on_id":"bd-3ps8","type":"blocks","created_at":"2026-02-20T07:48:19.408224588Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3hr2","depends_on_id":"bd-ukh7","type":"blocks","created_at":"2026-02-20T07:48:19.455939690Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-3hw","title":"[10.11] Integrate canonical remote idempotency + saga semantics (from `10.14`) for multi-step workflows.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md — Section 10.11 (FrankenSQLite-Inspired Runtime Systems), cross-ref Section 9G.7, 9J.10\n\n## Why This Exists\n\nMulti-step product workflows in franken_node — such as publish-verify-promote, trust-rotation-with-rollback, and cross-region migration — involve sequences of remote effects where partial completion leaves the system in an inconsistent state. Enhancement Map 9G.7 mandates explicit remote-effects contracts with idempotency and saga semantics so that every multi-step workflow can be safely retried (idempotency) and safely reversed (compensating saga). 9J.10 further requires that all remote effects be capability-gated, named, idempotent, and saga-safe, establishing a strict contract that prevents ad hoc remote calls from bypassing the safety framework.\n\nThis bead integrates the canonical idempotency primitives from 10.14 (bd-12n3 key derivation, bd-206h dedupe store, bd-ac83 named computation registry, bd-1nfu RemoteCap requirement) and saga wrappers from 10.15 (bd-3h63) into franken_node's product-layer workflow engine, ensuring every remote effect in a multi-step workflow is named, idempotent, capability-gated, and wrapped in a saga with deterministic compensations.\n\n## What This Must Do\n\n1. Implement a `SagaOrchestrator` that manages multi-step workflows as a sequence of named remote effects, each with a forward action and a deterministic compensation action.\n2. Integrate idempotency-key derivation (from bd-12n3): every remote effect call includes an idempotency key derived from the request payload bytes and the current epoch, ensuring safe retries.\n3. Integrate the named computation registry (from bd-ac83): every remote effect must be registered by name; calls to unregistered computations are rejected at compile time or startup.\n4. Enforce `RemoteCap` gating (from bd-1nfu): no remote effect can be dispatched without a valid `RemoteCap` token derived from the caller's `CapabilityContext`.\n5. On forward-action failure, execute compensations in reverse order with at-least-once delivery semantics, logging each compensation to the decision stream.\n6. Integrate with the global remote bulkhead (bd-lus / bd-v4l0): all saga remote effects pass through the bulkhead, and bulkhead exhaustion triggers saga pause (not abort) with structured backpressure event.\n\n## Context from Enhancement Maps\n\n- 9G.7: \"Explicit remote-effects contracts with idempotency and sagas\"\n- 9J.10: \"Remote effects must be capability-gated, named, idempotent, and saga-safe\"\n- 9J.11: \"Global remote bulkhead to prevent retry-storm self-DoS\" — saga retries must respect the bulkhead.\n- Architecture invariant #1 (8.5): Cx-first control — RemoteCap must derive from CapabilityContext.\n- Architecture invariant #6 (8.5): Remote effects contract — no ambient remote calls.\n- Architecture invariant #10 (8.5): No ambient authority — remote effects require explicit capability tokens.\n\n## Dependencies\n\n- Upstream: bd-12n3 (10.14 idempotency key derivation), bd-206h (10.14 idempotency dedupe store), bd-ac83 (10.14 named computation registry), bd-1nfu (10.14 RemoteCap requirement), bd-3h63 (10.15 saga wrappers), bd-v4l0 (10.14 global remote bulkhead), bd-lus (scheduler lanes + bulkhead product integration)\n- Downstream: bd-390 (anti-entropy reconciliation uses saga for multi-record sync), bd-2ah (obligation channels compose with sagas for critical flows), bd-1jpo (10.11 section-wide verification gate)\n\n## Acceptance Criteria\n\n1. Every remote effect in a multi-step workflow is registered by name in the computation registry; an attempt to call an unregistered effect produces a compile-time error or a startup-time panic with clear diagnostics.\n2. Idempotency keys are deterministically derived from (request_payload_bytes, epoch); replaying the same request in the same epoch returns the cached result without re-execution.\n3. RemoteCap is required for every remote dispatch; a call without RemoteCap is rejected with a structured `MissingRemoteCap` error.\n4. Saga compensation executes in reverse order on forward failure; each compensation is logged to the append-only decision stream with its outcome (success/fail/retry).\n5. Compensation actions are themselves idempotent: executing the same compensation twice produces the same result.\n6. Saga retries respect the global bulkhead: if bulkhead is exhausted, the saga pauses and emits a `saga.backpressure` event rather than spinning.\n7. End-to-end test: a 5-step saga with a failure injected at step 3 produces compensations for steps 2 and 1, verified via decision stream replay.\n8. Verification evidence JSON includes saga_steps_total, saga_steps_completed, compensations_executed, idempotency_cache_hits, and remote_cap_violations fields.\n\n## Testing & Logging Requirements\n\n- Unit tests: (a) Idempotency key derivation is deterministic for same input; (b) Different payloads produce different keys; (c) Named computation registry rejects unknown names; (d) RemoteCap validation accepts valid tokens, rejects expired/forged; (e) Compensation ordering is strictly reverse.\n- Integration tests: (a) Full saga lifecycle: all steps succeed; (b) Saga with mid-flight failure and full compensation; (c) Saga retry after transient failure with idempotency dedup; (d) Saga under bulkhead pressure with pause/resume behavior.\n- Adversarial tests: (a) Compensation action fails — verify retry with exponential backoff and eventual logging of permanent failure; (b) Concurrent identical sagas — verify idempotency prevents double-execution; (c) Epoch change mid-saga — verify saga aborts cleanly rather than mixing epoch contexts; (d) RemoteCap revocation mid-saga — verify remaining steps are blocked.\n- Structured logs: Events use stable codes (FN-SG-001 through FN-SG-012), include `saga_id`, `step_name`, `trace_id`, `epoch`, `idempotency_key`, `outcome`. JSON-formatted.\n\n## Expected Artifacts\n\n- docs/specs/section_10_11/bd-3hw_contract.md\n- crates/franken-node/src/runtime/saga_orchestrator.rs (or equivalent module path)\n- crates/franken-node/src/runtime/remote_effect.rs (named effect + idempotency integration)\n- scripts/check_remote_idempotency_saga.py (with --json flag and self_test())\n- tests/test_check_remote_idempotency_saga.py\n- artifacts/section_10_11/bd-3hw/verification_evidence.json\n- artifacts/section_10_11/bd-3hw/verification_summary.md","acceptance_criteria":"AC for bd-3hw:\n1. Integrate the canonical remote idempotency model from 10.14: every remote-effect operation derives an IdempotencyKey from (request_bytes, epoch_id) using a deterministic hash; replayed requests with matching key+payload return the cached outcome without re-execution.\n2. An IdempotencyStore trait provides: lookup(key) -> Option<CachedOutcome>, insert(key, payload_hash, outcome), and expire(older_than: Duration). The store rejects key reuse with a different payload_hash via IDEMPOTENCY_CONFLICT error.\n3. Saga semantics: multi-step workflows are modeled as a Saga<S> with a sequence of Steps, each having a forward action and a compensating action. If step N fails, compensations for steps N-1..0 are executed in reverse order.\n4. The saga coordinator persists step completion state so that crash recovery resumes from the last completed step (forward) or the current compensation step (backward); this integrates with the checkpoint contract (bd-93k).\n5. Compensation actions are idempotent: re-executing a compensation for an already-compensated step is a no-op (verified by idempotency key).\n6. Saga execution is epoch-bound (bd-2gr): a saga started in epoch N that is still in-flight when epoch N+1 activates is either drained to completion or compensated, depending on configurable policy (drain_on_epoch_change vs compensate_on_epoch_change).\n7. Unit tests verify: (a) idempotent replay returns cached outcome, (b) payload mismatch on same key returns IDEMPOTENCY_CONFLICT, (c) saga forward path completes all steps, (d) saga failure at step 3 of 5 compensates steps 2,1,0 in order, (e) crash recovery resumes saga from persisted state, (f) compensation idempotency (double-compensate is no-op).\n8. Integration test: a 5-step saga with injected failure at step 3 demonstrates full compensation and verified resource release.\n9. Structured log events: IDEMPOTENCY_HIT / IDEMPOTENCY_MISS / IDEMPOTENCY_CONFLICT / SAGA_STEP_FORWARD / SAGA_STEP_COMPENSATE / SAGA_COMPLETED / SAGA_COMPENSATED with saga_id, step_index, idempotency_key, and epoch_id.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:36:50.553299304Z","created_by":"ubuntu","updated_at":"2026-02-20T17:01:40.088346563Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-11","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-3hw","depends_on_id":"bd-12n3","type":"blocks","created_at":"2026-02-20T15:00:18.646602108Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3hw","depends_on_id":"bd-206h","type":"blocks","created_at":"2026-02-20T15:00:18.834054666Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-3hyk","title":"[1-3] Strategic Foundations — mission, thesis, category-creation doctrine, build strategy","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md — Sections 1, 2, 3\n\n## Why This Exists\nThis bead captures the foundational strategic context: what franken_node IS, why it exists, and what it must become. All implementation beads derive their purpose from this strategic foundation. Without this context, execution becomes disconnected from mission.\n\n## Section 1: Background and Role\nfranken_node is the product and ecosystem surface built on franken_engine.\n- franken_engine: native runtime internals, policy semantics, trust primitives\n- franken_node owns: compatibility capture, migration/operator experience, extension ecosystem, packaging/rollout, enterprise control planes\n- Strategic role: turn engine breakthroughs into mass adoption and category capture\n\n## Section 2: Core Thesis\nfranken_node must become the default choice for extension-heavy JS/TS execution where teams need ALL of:\n- Node/Bun-level developer ergonomics\n- Materially stronger security outcomes\n- Deterministic explainability for high-impact decisions\n- Operational confidence at fleet scale\n\nCore proposition:\n- Compatibility is table stakes\n- Trust-native operations are the differentiator\n- Migration velocity is the growth engine\n\n## Section 3: Strategic Objective\nBuild franken_node into the category-defining runtime product layer that functionally obsoletes Node/Bun for high-trust extension ecosystems.\n\nCategory-defining disruptive floor (non-optional):\n- >= 95% pass on targeted compatibility corpus for high-value Node/Bun usage bands\n- >= 3x migration throughput and confidence quality versus baseline patterns\n- >= 10x reduction in successful host compromise under adversarial extension campaigns\n- Friction-minimized, automation-first path from install to policy-governed safe extension workloads\n- 100% deterministic replay artifact availability for high-severity incidents\n- >= 3 impossible-by-default product capabilities broadly adopted by production users\n\n## Section 3.1: Category-Creation Doctrine\nfranken_node is NOT a \"better Node clone.\" It is the category bridge between JS/TS ecosystem scale and zero-illusion trust operations.\n\nDoctrine rules:\n- Treat compatibility as a strategic wedge, not final destination\n- Ship trust-native workflows that incumbents cannot provide by default\n- Define benchmark language and verification standards for the category\n- Own migration ergonomics so adoption feels inevitable, not costly\n- Turn operator trust from intuition into cryptographically and statistically grounded evidence\n\n## Section 3.3: Baseline Build Strategy (Hybrid, Spec-First)\nDECISION: franken_node will NOT begin with a full clean-room Bun reimplementation.\n\nCanonical strategy:\n- Use Node/Bun as behavioral reference systems and oracle targets, not architecture templates\n- Execute spec-first compatibility capture (Essence Extraction) for prioritized API/runtime bands\n- Implement natively on franken_engine + asupersync with trust/migration architecture from day one\n- Reuse patterns from /dp/pi_agent_rust where accretive, avoiding architecture lock-in\n\nRationale: A Bun-first clone path creates architecture lock-in and delays category-defining differentiators.\n\n## Acceptance Criteria\n- All teams can articulate the core thesis and strategic objective\n- Implementation decisions reference this strategic context in design docs\n- Category-creation doctrine tests are applied to every major feature proposal\n\n\n## Success Criteria\n- Strategic foundations are explicitly referenced by dependent architecture and execution-track beads, with traceable linkage to Sections 1-3 goals.\n- Category-creation doctrine checks are enforceable as repeatable planning and release-review gates.\n- Quantitative objectives from Section 3 remain represented in downstream verification and cannot be silently dropped by local optimization decisions.\n\n## Testing & Logging Requirements\n- Unit tests for any metadata validation helpers that enforce strategic-linkage and doctrine-reference requirements.\n- E2E graph-validation scripts that traverse dependent beads and confirm complete mapping back to strategic objectives.\n- Structured logging/artifacts for strategy-compliance scans, including missing-link diagnostics and remediation hints.","status":"open","priority":0,"issue_type":"epic","created_at":"2026-02-20T16:16:54.111219824Z","created_by":"ubuntu","updated_at":"2026-02-20T17:08:27.408031213Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-1","section-2","section-3","strategic-foundations"]}
{"id":"bd-3i6c","title":"[10.14] Add conformance suite for ledger determinism, idempotency, epoch validity, and marker/MMR proof correctness.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.14 — FrankenSQLite Deep-Mined Expansion Execution Track (9J)\n\nWhy This Exists:\nFrankenSQLite deep-mined control integrity track: evidence ledgers, proofs, epochs, remote effects, markers/MMR, and deterministic fault labs.\n\nTask Objective:\nAdd conformance suite for ledger determinism, idempotency, epoch validity, and marker/MMR proof correctness.\n\nAcceptance Criteria:\n- Suite includes normative fixtures for all four domains; suite is required for release profile claim; failures map to stable conformance IDs.\n\nExpected Artifacts:\n- `tests/conformance/fsqlite_inspired_suite.rs`, `fixtures/conformance/fsqlite_inspired/*`, `artifacts/10.14/fsqlite_inspired_conformance_report.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_14/bd-3i6c/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_14/bd-3i6c/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.14] Add conformance suite for ledger determinism, idempotency, epoch validity, and marker/MMR proof correctness.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.14] Add conformance suite for ledger determinism, idempotency, epoch validity, and marker/MMR proof correctness.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.14] Add conformance suite for ledger determinism, idempotency, epoch validity, and marker/MMR proof correctness.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.14] Add conformance suite for ledger determinism, idempotency, epoch validity, and marker/MMR proof correctness.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.14] Add conformance suite for ledger determinism, idempotency, epoch validity, and marker/MMR proof correctness.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"Suite includes normative fixtures for all four domains; suite is required for release profile claim; failures map to stable conformance IDs.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:36:59.393612417Z","created_by":"ubuntu","updated_at":"2026-02-20T16:24:31.059895968Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-14","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-3i6c","depends_on_id":"bd-22yy","type":"blocks","created_at":"2026-02-20T07:43:16.400324425Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3i6c","depends_on_id":"bd-2808","type":"blocks","created_at":"2026-02-20T16:24:30.685821770Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3i6c","depends_on_id":"bd-2qqu","type":"blocks","created_at":"2026-02-20T16:24:30.869148921Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3i6c","depends_on_id":"bd-876n","type":"blocks","created_at":"2026-02-20T16:24:31.059844682Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-3i9o","title":"[10.13] Implement provenance/attestation policy gates (required attestation types, minimum build assurance, trusted builders).","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.13 — FCP Deep-Mined Expansion Execution Track (9I)\n\nWhy This Exists:\nDeep connector/provider contract track: lifecycle FSMs, conformance harnesses, fencing, sandboxing, revocation freshness, and protocol hardening.\n\nTask Objective:\nImplement provenance/attestation policy gates (required attestation types, minimum build assurance, trusted builders).\n\nAcceptance Criteria:\n- Policy engine enforces required attestations and builder trust constraints; non-compliant artifacts are blocked pre-activation; gate results are signed.\n\nExpected Artifacts:\n- `docs/specs/provenance_policy.md`, `tests/security/attestation_gate.rs`, `artifacts/10.13/provenance_gate_decisions.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_13/bd-3i9o/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_13/bd-3i9o/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.13] Implement provenance/attestation policy gates (required attestation types, minimum build assurance, trusted builders).\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.13] Implement provenance/attestation policy gates (required attestation types, minimum build assurance, trusted builders).\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.13] Implement provenance/attestation policy gates (required attestation types, minimum build assurance, trusted builders).\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.13] Implement provenance/attestation policy gates (required attestation types, minimum build assurance, trusted builders).\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.13] Implement provenance/attestation policy gates (required attestation types, minimum build assurance, trusted builders).\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","status":"closed","priority":1,"issue_type":"task","assignee":"CrimsonCrane","created_at":"2026-02-20T07:36:52.780708056Z","created_by":"ubuntu","updated_at":"2026-02-20T11:46:42.246827311Z","closed_at":"2026-02-20T11:46:42.246798919Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-13","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-3i9o","depends_on_id":"bd-1z9s","type":"blocks","created_at":"2026-02-20T07:43:12.970190878Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-3id1","title":"[16] Contribution: external red-team and independent evaluations","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 16\n\nTask Objective:\nPublish external red-team and independent evaluation reports.\n\nExecution Requirements:\n- Make this requirement machine-verifiable and release-gated where applicable.\n- Keep behavior self-documenting so future contributors do not need to re-open the master plan.\n\nTesting & Logging Requirements:\n- Unit tests for rule/contract logic and error conditions.\n- Integration/E2E tests for workflow-level enforcement.\n- Structured logs with stable codes and trace IDs.\n- Reproducible artifacts that prove contract satisfaction.\n\nAcceptance Criteria:\n- Success and failure invariants for [16] Contribution: external red-team and independent evaluations are explicitly quantified and machine-verifiable.\n- Determinism requirements for [16] Contribution: external red-team and independent evaluations are validated across repeated runs and adversarial perturbations.\n\n\nExpected Artifacts:\n- Machine-readable verification artifact with explicit canonical path under artifacts/section_16/bd-3id1/verification_evidence.json, suitable for CI/release gating.\n- Human-readable verification summary with explicit canonical path under artifacts/section_16/bd-3id1/verification_summary.md, linking implementation intent to measured validation outcomes.\n\nTask-Specific Clarification:\n- The implementation must deliver the exact capability named in the title, with no scope dilution and no silent feature omission.\n- Validation artifacts must include capability-specific thresholds, pass/fail criteria, and reproducible evidence bundles tied to this bead ID.\n- Program/section verification gates must be able to consume this bead's outputs without manual interpretation.\n\nWhy This Improves User Outcomes:\n- \"[16] Contribution: external red-team and independent evaluations\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[16] Contribution: external red-team and independent evaluations\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"1. At least 2 external red-team exercises conducted by independent security firms or research groups.\n2. Each red-team engagement includes: (a) defined scope (which subsystems are in-scope), (b) rules of engagement, (c) findings report with severity ratings (critical/high/medium/low/informational), (d) remediation timeline for each finding, (e) re-test verification after remediation.\n3. At least 1 independent evaluation (non-red-team) assessing: trust system correctness, compatibility claim validity, or benchmark methodology soundness.\n4. All critical and high findings are remediated within 30 days; medium within 90 days.\n5. Red-team reports are published (with responsible disclosure timeline if needed) including: findings summary, methodology, and franken_node's response.\n6. Independent evaluations are published with evaluator's permission.\n7. Findings from red-team and evaluations are tracked as beads and closed when remediated.\n8. Evidence: external_evaluation_registry.json with per-engagement: evaluator, scope, finding count by severity, remediation status, and publication URL.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:39:37.044915395Z","created_by":"ubuntu","updated_at":"2026-02-20T15:28:33.756811169Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-16","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-3id1","depends_on_id":"bd-nbh7","type":"blocks","created_at":"2026-02-20T07:43:26.680501950Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-3il","title":"[PLAN 10.2] Compatibility Core","description":"Section: 10.2 — Compatibility Core\n\nStrategic Context:\nCompatibility core as strategic wedge: policy-visible behavior, divergence receipts, L1/L2 lockstep, and spec-first fixture governance.\n\nExecution Requirements:\n- Preserve all scoped capabilities from the canonical plan.\n- Keep contracts self-contained so future contributors can execute without reopening the master plan.\n- Require explicit testing strategy (unit + integration/e2e) and structured logging/telemetry for every child bead.\n- Require artifact-backed validation for performance, security, and correctness claims.\n\nDependency Semantics:\n- This epic is blocked by its child implementation beads.\n- Cross-epic dependencies encode strategic sequencing and canonical ownership boundaries.\n\n## Success Criteria\n- All child beads for this section are completed with acceptance artifacts attached and no unresolved blockers.\n- Section-level verification gate for comprehensive unit tests, integration/e2e workflows, and detailed structured logging evidence is green.\n- Scope-to-plan traceability is explicit, with no feature loss versus the canonical plan section.\n\n## Optimization Notes\n- User-Outcome Lens: \"[PLAN 10.2] Compatibility Core\" must improve operator confidence, safety posture, and deterministic recovery behavior under both normal and adversarial conditions.\n- Cross-Section Coordination: child beads must encode integration assumptions explicitly to avoid local optimizations that degrade system-wide correctness.\n- Verification Non-Negotiable: completion requires reproducible unit/integration/E2E evidence and structured logs suitable for independent replay.\n- Scope Protection: any simplification must preserve canonical-plan feature intent and be justified with explicit tradeoff documentation.","status":"closed","priority":2,"issue_type":"epic","created_at":"2026-02-20T07:36:40.377571480Z","created_by":"ubuntu","updated_at":"2026-02-20T10:05:27.181170544Z","closed_at":"2026-02-20T10:05:27.181144235Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-2"],"dependencies":[{"issue_id":"bd-3il","depends_on_id":"bd-1ck","type":"blocks","created_at":"2026-02-20T07:36:44.438983595Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3il","depends_on_id":"bd-1ow","type":"blocks","created_at":"2026-02-20T07:37:09.893838928Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3il","depends_on_id":"bd-1z3","type":"blocks","created_at":"2026-02-20T07:36:44.198729738Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3il","depends_on_id":"bd-23ys","type":"blocks","created_at":"2026-02-20T07:48:20.504504177Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3il","depends_on_id":"bd-240","type":"blocks","created_at":"2026-02-20T07:36:44.519440841Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3il","depends_on_id":"bd-2hs","type":"blocks","created_at":"2026-02-20T07:36:44.598252001Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3il","depends_on_id":"bd-2kf","type":"blocks","created_at":"2026-02-20T07:36:44.116856715Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3il","depends_on_id":"bd-2qf","type":"blocks","created_at":"2026-02-20T07:36:43.954455543Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3il","depends_on_id":"bd-2vi","type":"blocks","created_at":"2026-02-20T07:36:44.276333539Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3il","depends_on_id":"bd-2wz","type":"blocks","created_at":"2026-02-20T07:36:43.872334648Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3il","depends_on_id":"bd-32v","type":"blocks","created_at":"2026-02-20T07:36:44.355131936Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3il","depends_on_id":"bd-38l","type":"blocks","created_at":"2026-02-20T07:36:44.034546618Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3il","depends_on_id":"bd-7mt","type":"blocks","created_at":"2026-02-20T07:36:44.755766771Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3il","depends_on_id":"bd-80g","type":"blocks","created_at":"2026-02-20T07:36:44.677334126Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3il","depends_on_id":"bd-cda","type":"blocks","created_at":"2026-02-20T07:37:09.114585838Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-3j4","title":"[10.12] Implement end-to-end migration singularity pipeline for pilot cohorts.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md — Section 10.12 (Frontier Programs), cross-ref Section 9H.1 (Migration Singularity Program), 9F.1 (Migration singularity engine)\n\n## Why This Exists\nfranken_node's growth engine thesis is that migration velocity from Node/Bun is the primary adoption driver — and the Migration Singularity Program (9H.1) aims to make that migration so deterministic, so low-friction, and so machine-checked that it becomes the obvious default choice. This bead implements the end-to-end migration singularity pipeline for pilot cohorts: the complete automation chain from compatibility analysis through migration plan generation, execution, verification, and rollback receipt issuance. It directly advances the category-defining floor target of \">= 3x migration throughput and confidence quality\" by eliminating manual migration steps and producing machine-checked migration plans with cryptographic rollback guarantees. It also advances \">= 95% pass on targeted compatibility corpus\" by integrating compatibility verification as a mandatory pipeline stage.\n\n## What This Must Do\n1. Implement a `MigrationSingularityPipeline` module (`crates/franken-node/src/connector/migration_pipeline.rs`) that orchestrates the full migration lifecycle as a deterministic, restartable state machine with stages: INTAKE -> ANALYSIS -> PLAN_GENERATION -> PLAN_REVIEW -> EXECUTION -> VERIFICATION -> RECEIPT_ISSUANCE -> COMPLETE (with ROLLBACK reachable from any post-INTAKE stage).\n2. Implement the ANALYSIS stage: ingest a pilot cohort definition (list of extension packages with version constraints), run compatibility analysis against the 10.2 compatibility core, identify migration blockers, and produce a machine-readable compatibility report with per-extension pass/fail/partial status.\n3. Implement the PLAN_GENERATION stage: for each extension in the cohort, generate a deterministic migration plan that specifies: source version, target version, transformation steps (API shimming, polyfill injection, dependency rewiring), estimated risk score (from operator intelligence bd-y0v expected-loss model), and rollback specification.\n4. Implement the EXECUTION stage: apply migration plans in dependency order with fencing (from 10.13 fencing infrastructure), producing per-extension execution traces that capture every state transition, file mutation, and dependency resolution. Execution must be idempotent — re-running the same plan on the same input produces identical output.\n5. Implement the VERIFICATION stage: after execution, run the full compatibility test suite (10.2) against migrated extensions, run trust verification (10.4/10.13) against new artifacts, and produce a migration verification report with pass/fail per extension and per test.\n6. Implement the RECEIPT_ISSUANCE stage: for each successfully migrated extension, produce a signed migration receipt containing: pre-migration state hash, migration plan fingerprint, post-migration state hash, verification report summary, rollback proof (content-addressed pre-migration snapshot + deterministic undo sequence), and timestamp. Receipts must be independently verifiable.\n7. Implement pilot cohort management: define cohort selection criteria (extension popularity rank, dependency complexity score, risk tier), track cohort progress through the pipeline, and produce cohort-level summary metrics (throughput, success rate, mean time to migrate, rollback rate).\n\n## Context from Enhancement Maps\n- 9H.1: \"Drive mass migration by converting compatibility uncertainty into deterministic, machine-checked migration plans with rollback receipts.\" This bead is the direct end-to-end implementation of that program for pilot-scale execution.\n- 9F.1: \"Migration singularity engine (automation-first, low-friction execution)\" — the pipeline is the automation-first engine; pilot cohorts are the proving ground before mass-scale rollout.\n- Category-defining targets (Section 3.2): \">= 3x migration throughput and confidence quality\" — the pipeline must demonstrate >= 3x throughput improvement over manual migration for pilot cohorts, measured in extensions migrated per hour with >= 95% first-pass success rate. \">= 95% pass on targeted compatibility corpus\" — the VERIFICATION stage enforces this target as a hard gate. \"100% deterministic replay artifact availability\" — migration receipts with rollback proofs satisfy this for every migration action.\n\n## Dependencies\n- Upstream: bd-3hm (migration singularity artifact contract and verifier format — defines the receipt and verification schema this pipeline produces), Section 10.2 (compatibility core — compatibility analysis and verification test suites), Section 10.3 (migration system — foundational migration primitives this pipeline orchestrates), Section 10.4/10.13 (trust/security — trust verification for migrated artifacts, fencing infrastructure for execution isolation).\n- Downstream: bd-5si (trust fabric convergence — consumes migration receipts as trust evidence), bd-2aj (ecosystem network-effect APIs — migration outcomes feed into reputation and compliance evidence), bd-n1w (frontier demo gates — migration singularity must register a demo gate), bd-1d6x (section-wide verification gate), bd-go4 (section 10.12 plan rollup).\n\n## Acceptance Criteria\n1. The `MigrationSingularityPipeline` implements all seven stages (INTAKE through COMPLETE) as a deterministic state machine, with ROLLBACK reachable from any post-INTAKE stage.\n2. Pipeline execution is idempotent: re-running the same migration plan on the same input state produces byte-identical outputs and receipts.\n3. The ANALYSIS stage correctly identifies migration blockers using 10.2 compatibility analysis and produces a machine-readable compatibility report.\n4. The PLAN_GENERATION stage produces deterministic migration plans that include transformation steps, risk scores, and rollback specifications for every extension in the cohort.\n5. The VERIFICATION stage enforces >= 95% compatibility pass rate as a hard gate; cohorts below this threshold are not advanced to RECEIPT_ISSUANCE.\n6. Migration receipts are signed, content-addressed, and independently verifiable — an external party can verify the receipt by checking pre/post state hashes and the rollback proof.\n7. Pilot cohort throughput demonstrates >= 3x improvement over baseline manual migration (measured in extensions successfully migrated per hour), validated by benchmark test.\n8. Rollback proofs are exercised in tests: executing the rollback sequence from a completed migration restores the pre-migration state hash exactly.\n\n## Testing & Logging Requirements\n- Unit tests: Test state machine transitions for all valid and invalid stage sequences; test ANALYSIS stage with known-blocker extensions; test PLAN_GENERATION determinism with fixed inputs; test idempotency by double-executing a plan; test rollback proof generation and verification round-trip.\n- Integration tests: End-to-end pipeline execution for a small pilot cohort (3-5 extensions) from INTAKE through RECEIPT_ISSUANCE; test ROLLBACK from each stage; test VERIFICATION gate enforcement (inject a failing extension and confirm pipeline halts); test receipt signing and independent verification.\n- E2E tests: Full pilot cohort migration workflow — define cohort, run pipeline, verify all receipts, execute rollback for one extension, confirm state restoration, re-migrate and confirm idempotent result.\n- External reproducibility tests: An independent verifier receives migration receipts and a pinned input corpus, re-executes the migration pipeline, and confirms output state hashes match the receipts.\n- Structured logs: Emit `PIPELINE_STAGE_ENTER`, `PIPELINE_STAGE_EXIT`, `ANALYSIS_BLOCKER_FOUND`, `PLAN_GENERATED`, `EXECUTION_STEP`, `EXECUTION_IDEMPOTENT_CHECK`, `VERIFICATION_PASS`, `VERIFICATION_FAIL`, `RECEIPT_ISSUED`, `RECEIPT_VERIFIED`, `ROLLBACK_INITIATED`, `ROLLBACK_COMPLETE`, `COHORT_SUMMARY` events with trace correlation IDs, cohort identifiers, extension identifiers, stage durations, and risk scores.\n\n## Expected Artifacts\n- docs/specs/section_10_12/bd-3j4_contract.md\n- artifacts/section_10_12/bd-3j4/verification_evidence.json\n- artifacts/section_10_12/bd-3j4/verification_summary.md","acceptance_criteria":"1. Implement a MigrationSingularityPipeline that orchestrates the full lifecycle: (a) plan_generation: accept a source system descriptor and produce a machine-checked migration plan, (b) precondition_evaluation: run all precondition checks and collect evidence, (c) execution: apply migration steps with progress tracking, (d) postcondition_evaluation: run all postcondition checks, (e) artifact_emission: produce a MigrationSingularityArtifact (from bd-3hm) with all evidence, (f) rollback_readiness: generate and attach a RollbackReceipt.\n2. Implement pilot cohort management: define a PilotCohort struct with (a) cohort_id, (b) member_count (1-100 for pilot), (c) migration_artifact_ids (one per member), (d) cohort_status (PENDING, IN_PROGRESS, COMPLETED, ROLLED_BACK), (e) started_at, completed_at.\n3. Implement cohort-level rollback: if any member migration fails postcondition checks, the entire cohort MUST be rolled back. Track per-member rollback status. Provide a cohort_rollback(cohort_id) function that invokes rollback for all members.\n4. Implement progress telemetry: emit structured events for each pipeline stage transition (plan_generated, preconditions_passed, executing, postconditions_passed, artifact_emitted, rolled_back) with cohort_id, member_id, duration_ms, and trace correlation ID.\n5. Implement idempotency: re-running the pipeline for an already-completed member (same source fingerprint and plan hash) MUST return the existing artifact rather than re-executing. Detect via the idempotency store from 10.14.\n6. Implement a dry-run mode: execute all stages except the actual migration execution step, producing a DryRunReport with predicted outcome, estimated duration, and identified risks.\n7. Unit tests: (a) full pipeline happy path for a single member, (b) precondition failure aborts before execution, (c) postcondition failure triggers rollback, (d) cohort rollback on partial failure, (e) idempotent re-run returns cached artifact, (f) dry-run mode produces report without side effects.\n8. Integration test: simulate a 5-member pilot cohort, inject one failure, verify cohort-level rollback and telemetry.\n9. Verification: scripts/check_migration_pipeline.py --json, artifacts at artifacts/section_10_12/bd-3j4/.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:36:50.916052848Z","created_by":"ubuntu","updated_at":"2026-02-20T17:01:50.725738569Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-12","self-contained","test-obligations"]}
{"id":"bd-3jc1","title":"[12] Risk control: migration friction persistence","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 12\n\nTask Objective:\nImplement migration autopilot and confidence reporting guardrails to prevent persistent migration friction.\n\nExecution Requirements:\n- Make this requirement machine-verifiable and release-gated where applicable.\n- Keep behavior self-documenting so future contributors do not need to re-open the master plan.\n\nTesting & Logging Requirements:\n- Unit tests for rule/contract logic and error conditions.\n- Integration/E2E tests for workflow-level enforcement.\n- Structured logs with stable codes and trace IDs.\n- Reproducible artifacts that prove contract satisfaction.\n\nAcceptance Criteria:\n- Success and failure invariants for [12] Risk control: migration friction persistence are explicitly quantified and machine-verifiable.\n- Determinism requirements for [12] Risk control: migration friction persistence are validated across repeated runs and adversarial perturbations.\n\n\nExpected Artifacts:\n- Machine-readable verification artifact with explicit canonical path under artifacts/section_12/bd-3jc1/verification_evidence.json, suitable for CI/release gating.\n- Human-readable verification summary with explicit canonical path under artifacts/section_12/bd-3jc1/verification_summary.md, linking implementation intent to measured validation outcomes.\n\nTask-Specific Clarification:\n- The implementation must deliver the exact capability named in the title, with no scope dilution and no silent feature omission.\n- Validation artifacts must include capability-specific thresholds, pass/fail criteria, and reproducible evidence bundles tied to this bead ID.\n- Program/section verification gates must be able to consume this bead's outputs without manual interpretation.\n\nWhy This Improves User Outcomes:\n- \"[12] Risk control: migration friction persistence\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[12] Risk control: migration friction persistence\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"RISK: Migration friction persistence — migration from Node.js/Bun remains painful despite tooling, discouraging adoption.\nIMPACT: Low adoption rates, failed migrations returning to original runtime, negative community perception.\nCOUNTERMEASURES:\n  (a) Migration autopilot: automated tool that handles >= 80% of migration steps without manual intervention.\n  (b) Confidence reporting: migration tool produces a confidence score (0-100) with specific blockers listed.\n  (c) Incremental migration: support running mixed-mode (partial migration) so users are not forced into all-or-nothing.\nVERIFICATION:\n  1. Migration autopilot successfully migrates >= 80% of steps in a representative 10-project cohort without manual intervention.\n  2. Confidence report is generated for every migration attempt, with score and ranked blocker list.\n  3. Confidence score correlates with actual migration success: score >= 80 predicts success >= 90% of the time.\n  4. Mixed-mode operation demonstrated: partially migrated project runs correctly with both runtimes active.\nTEST SCENARIOS:\n  - Scenario A: Run autopilot on Express.js starter app; verify fully automated migration with confidence >= 90.\n  - Scenario B: Run autopilot on project with native C++ addons; verify confidence score < 50 and blockers list includes 'native addon'.\n  - Scenario C: Run mixed-mode on a project with 50% migrated modules; verify both migrated and unmigrated modules function correctly.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:39:33.501591325Z","created_by":"ubuntu","updated_at":"2026-02-20T15:18:30.329786774Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-12","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-3jc1","depends_on_id":"bd-kiqr","type":"blocks","created_at":"2026-02-20T07:43:24.863920726Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-3k6h","title":"E2E Test Scripts + Logging Infrastructure","description":"- type: task","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-20T07:47:58.072163029Z","updated_at":"2026-02-20T07:49:21.333563548Z","closed_at":"2026-02-20T07:49:21.333541487Z","close_reason":"Superseded by canonical detailed master-plan graph (bd-33v) with full 10.N-10.21 and 11-16 coverage, explicit dependencies, and per-section verification gates.","source_repo":".","compaction_level":0,"original_size":0}
{"id":"bd-3k9t","title":"[BOOTSTRAP] Implement foundation e2e scripts with structured log bundles","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md (Bootstrap integration quality overlay for Sections 10.0–10.5)\nSection: BOOTSTRAP (Foundation E2E and structured logging)\n\nTask Objective:\nImplement bootstrap E2E script suite that exercises command, config, and transplant workflows end-to-end with rich structured logging and reproducible evidence bundles.\n\nAcceptance Criteria:\n- E2E suite covers representative user/operator journeys (`init`, `run`, `doctor`, transplant integrity checks).\n- Scripts emit deterministic machine-readable pass/fail summaries and attach replay inputs.\n- Logs include stable event/error codes, stage markers, and trace-correlation IDs across the full flow.\n\nExpected Artifacts:\n- Bootstrap E2E script suite and execution harness docs.\n- Evidence bundle format containing command outputs, logs, and replay fixtures.\n- CI integration note for running suite in gated pipelines.\n\n- Machine-readable verification artifact at `artifacts/section_bootstrap/bd-3k9t/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_bootstrap/bd-3k9t/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for script helpers/parsers where applicable.\n- E2E self-check runs against fixture environments (clean + degraded + drifted states).\n- Detailed structured logs with per-stage timestamps and failure taxonomy.\n\nTask-Specific Clarification:\n- For \"[BOOTSTRAP] Implement foundation e2e scripts with structured log bundles\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[BOOTSTRAP] Implement foundation e2e scripts with structured log bundles\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[BOOTSTRAP] Implement foundation e2e scripts with structured log bundles\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[BOOTSTRAP] Implement foundation e2e scripts with structured log bundles\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[BOOTSTRAP] Implement foundation e2e scripts with structured log bundles\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T08:03:10.391869457Z","created_by":"ubuntu","updated_at":"2026-02-20T08:46:08.372682252Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["bootstrap","e2e","logging","verification"],"dependencies":[{"issue_id":"bd-3k9t","depends_on_id":"bd-1dpd","type":"blocks","created_at":"2026-02-20T08:24:22.946868179Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3k9t","depends_on_id":"bd-1pk","type":"blocks","created_at":"2026-02-20T08:03:11.274964953Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3k9t","depends_on_id":"bd-1qz","type":"blocks","created_at":"2026-02-20T08:03:11.387735624Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3k9t","depends_on_id":"bd-29q","type":"blocks","created_at":"2026-02-20T08:03:11.647306751Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3k9t","depends_on_id":"bd-2lb","type":"blocks","created_at":"2026-02-20T08:03:10.773258145Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3k9t","depends_on_id":"bd-2nd","type":"blocks","created_at":"2026-02-20T08:03:11.793987344Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3k9t","depends_on_id":"bd-2twu","type":"blocks","created_at":"2026-02-20T08:20:48.131253791Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3k9t","depends_on_id":"bd-32e","type":"blocks","created_at":"2026-02-20T08:03:11.149658132Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3k9t","depends_on_id":"bd-3vk","type":"blocks","created_at":"2026-02-20T08:03:10.905798220Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3k9t","depends_on_id":"bd-7rt","type":"blocks","created_at":"2026-02-20T08:03:11.514369948Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3k9t","depends_on_id":"bd-jvzc","type":"blocks","created_at":"2026-02-20T08:03:10.638125612Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3k9t","depends_on_id":"bd-n9r","type":"blocks","created_at":"2026-02-20T08:03:11.026067868Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-3kn","title":"[10.6] Add packaging profiles for local/dev/enterprise deployments.","description":"## [10.6] Packaging Profiles for Local/Dev/Enterprise Deployments\n\n### Why This Exists\n\nfranken_node targets three distinct deployment contexts with very different requirements: local development (minimal size, fast startup, no ceremony), CI/dev pipelines (debug symbols, verbose logging, reproducibility tooling), and enterprise production (signed binaries, audit logging, compliance evidence bundled). A single undifferentiated build cannot serve all three well. This bead defines packaging profiles that tailor the build output, default configuration, and bundled assets to each deployment target, as required by the 10.6 Performance + Packaging track.\n\n### What It Must Do\n\nDefine three packaging profiles — `local`, `dev`, and `enterprise` — each specifying:\n\n**Included components**: The `local` profile includes only the core binary and minimal runtime. The `dev` profile adds debug symbols, the lockstep harness, fixture generators, and verbose log configuration. The `enterprise` profile adds signed binary verification, audit log infrastructure, compliance evidence bundles (verification_evidence.json files from all gates), and telemetry export configuration.\n\n**Default policy settings**: Each profile sets sensible defaults. `local` disables telemetry and audit logging. `dev` enables verbose logging and disables binary signing verification. `enterprise` enables all security features, strict policy evaluation, and mandatory audit logging.\n\n**Startup behavior**: `local` optimizes for fastest possible cold-start (lazy initialization, deferred policy loading). `dev` eagerly loads all modules for comprehensive error checking at startup. `enterprise` performs full integrity self-check at startup (binary signature, configuration checksum, policy schema validation).\n\n**Telemetry level**: `local` = off, `dev` = debug-level local-only, `enterprise` = structured export to configured endpoint with PII filtering.\n\nProfiles are selected via `--profile <name>` CLI flag or `FRANKEN_NODE_PROFILE` environment variable. Unknown profile names produce a clear error listing valid options.\n\nProfile definitions are stored in a checked-in configuration file (`packaging/profiles.toml`) so they are auditable and versionable. The build system reads this file to determine what to include in the output artifact.\n\n### Acceptance Criteria\n\n1. Three profiles (`local`, `dev`, `enterprise`) are defined in `packaging/profiles.toml` with component lists, default policies, startup behavior, and telemetry settings.\n2. `--profile <name>` CLI flag and `FRANKEN_NODE_PROFILE` env var select the active profile; CLI flag takes precedence.\n3. `local` profile produces a binary at least 30% smaller than `enterprise` (measured by stripping debug symbols and excluding compliance bundles).\n4. `enterprise` profile bundles all verification evidence JSON files and performs startup integrity self-check.\n5. `dev` profile includes debug symbols and enables verbose logging by default.\n6. Unknown profile name produces a clear error listing valid options and exits non-zero.\n7. Verification script `scripts/check_packaging_profiles.py` with `--json` flag validates each profile's properties.\n8. Unit tests in `tests/test_check_packaging_profiles.py` cover profile loading, flag/env precedence, component inclusion, and error handling for invalid profiles.\n\n### Key Dependencies\n\n- CLI framework from bd-n9r (cli.rs, config.rs).\n- Binary signing from bd-2pw (enterprise profile depends on signing).\n- Telemetry namespace from 10.13 telemetry work.\n- Verification evidence from all prior gate beads.\n\n### Testing & Logging Requirements\n\n- Profile selection test: verify CLI flag overrides env var.\n- Component inclusion test: verify each profile bundles exactly the expected components.\n- Startup behavior test: verify `enterprise` self-check runs and `local` skips it.\n- Structured JSON log at startup records: active profile, components loaded, policy defaults applied.\n\n### Expected Artifacts\n\n- `packaging/profiles.toml` — profile definitions.\n- Profile-aware build logic in `crates/franken-node/` or build scripts.\n- `scripts/check_packaging_profiles.py` — verification script.\n- `tests/test_check_packaging_profiles.py` — unit tests.\n- `artifacts/section_10_6/bd-3kn/verification_evidence.json` — gate results.\n- `artifacts/section_10_6/bd-3kn/verification_summary.md` — human-readable summary.\n\n### Additional E2E Requirement\n\n- E2E test scripts must execute full end-to-end workflows from clean fixtures, emit deterministic machine-readable pass/fail evidence, and capture stepwise structured logs for root-cause triage.","acceptance_criteria":"1. At least three packaging profiles are defined: local (dev laptop), dev (CI/staging), and enterprise (production/air-gapped).\n2. Each profile is specified in a declarative config file (TOML or JSON) listing included binaries, feature flags, default configs, and resource limits.\n3. Local profile produces a minimal binary with debug symbols and hot-reload support.\n4. Enterprise profile produces a hardened binary with stripped symbols, static linking where possible, and no dev-only dependencies.\n5. A single build command (e.g., cargo build --profile <name> or scripts/package.sh <name>) produces the correct artifact for each profile.\n6. Each profile's output artifact is checksummed (SHA-256) and the checksum is recorded in the build manifest.\n7. Profile selection is documented in a packaging guide under docs/ with a comparison matrix of what each profile includes/excludes.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:36:47.017498866Z","created_by":"ubuntu","updated_at":"2026-02-20T17:08:36.067791950Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-6","self-contained","test-obligations"]}
{"id":"bd-3ku8","title":"[10.17] Define and enforce capability-carrying extension artifact format.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.17 — Radical Expansion Execution Track (9K)\n\nWhy This Exists:\nRadical expansion track for proof-carrying speculation, Bayesian adversary control, time-travel replay, ZK attestations, and claim compiler systems.\n\nTask Objective:\nDefine and enforce capability-carrying extension artifact format.\n\nAcceptance Criteria:\n- Artifact admission fails closed on missing/invalid capability contracts; runtime enforcement matches admitted capability envelope without drift.\n\nExpected Artifacts:\n- `docs/specs/capability_artifact_format.md`, `src/extensions/artifact_contract.rs`, `tests/conformance/capability_artifact_admission.rs`, `artifacts/10.17/capability_artifact_vectors.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_17/bd-3ku8/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_17/bd-3ku8/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.17] Define and enforce capability-carrying extension artifact format.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.17] Define and enforce capability-carrying extension artifact format.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.17] Define and enforce capability-carrying extension artifact format.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.17] Define and enforce capability-carrying extension artifact format.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.17] Define and enforce capability-carrying extension artifact format.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"- Artifact admission fails closed on missing/invalid capability contracts; runtime enforcement matches admitted capability envelope without drift.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:37:03.182308178Z","created_by":"ubuntu","updated_at":"2026-02-20T15:46:00.380398370Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-17","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-3ku8","depends_on_id":"bd-1xbc","type":"blocks","created_at":"2026-02-20T07:43:18.391173150Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-3l2p","title":"[10.17] Ship intent-aware remote effects firewall for extension-originated traffic.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.17 — Radical Expansion Execution Track (9K)\n\nWhy This Exists:\nRadical expansion track for proof-carrying speculation, Bayesian adversary control, time-travel replay, ZK attestations, and claim compiler systems.\n\nTask Objective:\nShip intent-aware remote effects firewall for extension-originated traffic.\n\nAcceptance Criteria:\n- Requests receive stable intent classification and policy verdicts; risky intent categories trigger challenge/simulate/deny/quarantine pathways with deterministic receipts.\n\nExpected Artifacts:\n- `src/security/intent_firewall.rs`, `docs/specs/intent_effects_policy.md`, `tests/security/intent_firewall_conformance.rs`, `artifacts/10.17/intent_firewall_eval_report.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_17/bd-3l2p/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_17/bd-3l2p/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.17] Ship intent-aware remote effects firewall for extension-originated traffic.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.17] Ship intent-aware remote effects firewall for extension-originated traffic.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.17] Ship intent-aware remote effects firewall for extension-originated traffic.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.17] Ship intent-aware remote effects firewall for extension-originated traffic.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.17] Ship intent-aware remote effects firewall for extension-originated traffic.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"- Requests receive stable intent classification and policy verdicts; risky intent categories trigger challenge/simulate/deny/quarantine pathways with deterministic receipts.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:37:03.680391608Z","created_by":"ubuntu","updated_at":"2026-02-20T15:45:59.097319509Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-17","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-3l2p","depends_on_id":"bd-21fo","type":"blocks","created_at":"2026-02-20T07:43:18.644614327Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-3l8d","title":"[11] Contract field: benchmark and correctness artifacts","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 11\n\nTask Objective:\nRequire benchmark and correctness artifacts before merge for all major subsystem changes.\n\nExecution Requirements:\n- Make this requirement machine-verifiable and release-gated where applicable.\n- Keep behavior self-documenting so future contributors do not need to re-open the master plan.\n\nTesting & Logging Requirements:\n- Unit tests for rule/contract logic and error conditions.\n- Integration/E2E tests for workflow-level enforcement.\n- Structured logs with stable codes and trace IDs.\n- Reproducible artifacts that prove contract satisfaction.\n\nAcceptance Criteria:\n- Success and failure invariants for [11] Contract field: benchmark and correctness artifacts are explicitly quantified and machine-verifiable.\n- Determinism requirements for [11] Contract field: benchmark and correctness artifacts are validated across repeated runs and adversarial perturbations.\n\n\nExpected Artifacts:\n- Machine-readable verification artifact with explicit canonical path under artifacts/section_11/bd-3l8d/verification_evidence.json, suitable for CI/release gating.\n- Human-readable verification summary with explicit canonical path under artifacts/section_11/bd-3l8d/verification_summary.md, linking implementation intent to measured validation outcomes.\n\nTask-Specific Clarification:\n- The implementation must deliver the exact capability named in the title, with no scope dilution and no silent feature omission.\n- Validation artifacts must include capability-specific thresholds, pass/fail criteria, and reproducible evidence bundles tied to this bead ID.\n- Program/section verification gates must be able to consume this bead's outputs without manual interpretation.\n\nWhy This Improves User Outcomes:\n- \"[11] Contract field: benchmark and correctness artifacts\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[11] Contract field: benchmark and correctness artifacts\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"1. Every evidence contract includes benchmark and correctness artifacts: (a) benchmark results with metric names, values, and baseline comparisons, (b) correctness proof artifacts (test results, formal verification outputs, or fuzzing coverage reports).\n2. Benchmark artifacts must include: metric name, unit, measured value, baseline value, delta, and whether delta is within acceptable bounds.\n3. Correctness artifacts must include: test suite name, pass/fail counts, coverage percentage, and links to raw test output files.\n4. All artifacts must be persisted under artifacts/section_N/bd-XXX/ with stable filenames.\n5. CI rejects contracts where benchmark section references zero metrics or correctness section references zero test suites.\n6. Unit test: contract with both benchmark table and correctness references passes; contract missing either section fails.\n7. Artifact files referenced in the contract must actually exist at the specified paths (CI checks file existence).","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:39:33.071851427Z","created_by":"ubuntu","updated_at":"2026-02-20T15:16:59.628277804Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-11","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-3l8d","depends_on_id":"bd-nglx","type":"blocks","created_at":"2026-02-20T07:43:24.604980107Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-3lh","title":"[10.6] Add cold-start and p99 latency gates for core workflows.","description":"## [10.6] Cold-Start and P99 Latency Gates for Core Workflows\n\n### Why This Exists\n\nSection 7.1 of the franken_node plan mandates \"low startup overhead for migration and CI loops\" and \"predictable p99 under extension churn.\" Without enforceable performance budgets, regressions silently accumulate across releases. This bead establishes hard CI gates that block merges when cold-start time or p99 latency exceeds profile-specific thresholds, ensuring every core workflow meets its performance contract before shipping.\n\n### What It Must Do\n\nDefine and enforce performance budgets for cold-start time and p99 latency across all core workflows: migration scan, compatibility check, policy evaluation, trust card lookup, and incident replay. Each workflow gets a budget table with three deployment profiles (dev/local, CI/dev, enterprise) since acceptable latency differs by context — a local developer tolerates 200ms cold-start but an enterprise CI pipeline with thousands of runs per day cannot.\n\nThe gate runs as a post-build CI step. It executes each workflow against a standardized fixture set, collects timing samples (minimum 30 iterations after warmup discard), computes p99 from the sample distribution, and compares against the budget table. If any workflow exceeds its budget for the active profile, the gate fails with a structured JSON report identifying the regression, the measured value, and the budget.\n\nWhen a regression is detected, the gate must also produce flamegraph evidence (via `cargo flamegraph` or equivalent) so developers can immediately see where time is being spent. Flamegraph SVGs are persisted as verification artifacts alongside the numeric results.\n\nBudget values are stored in a checked-in TOML configuration (`perf/budgets.toml`) so they are versioned and auditable. Updating a budget requires a justification comment in the commit message.\n\n### Acceptance Criteria\n\n1. A `perf/budgets.toml` file defines cold-start and p99 latency budgets for each core workflow, broken out by profile (dev/local, CI/dev, enterprise).\n2. A verification script `scripts/check_latency_gates.py` with `--json` flag executes benchmarks, computes p99, and compares against budgets.\n3. The script exits non-zero and emits structured JSON when any budget is exceeded.\n4. Flamegraph SVGs are generated automatically for any workflow that exceeds 80% of its budget (early warning) or 100% (failure).\n5. Unit tests in `tests/test_check_latency_gates.py` cover budget parsing, p99 computation, profile selection, and gate pass/fail logic with mock timing data.\n6. Evidence artifacts are written to `artifacts/section_10_6/bd-3lh/` including `verification_evidence.json` and `verification_summary.md`.\n7. Budget updates require a changelog entry documenting the justification.\n8. The gate integrates with the existing CI pipeline and respects the `--profile` flag for selecting deployment context.\n\n### Key Dependencies\n\n- Core workflow implementations from 10.2 (compatibility), 10.3 (migration), 10.4 (policy), 10.5 (trust cards), 10.9 (incident replay).\n- Benchmark fixture set (standardized inputs for each workflow).\n- Flamegraph tooling (`cargo-flamegraph` or perf-based equivalent).\n\n### Testing & Logging Requirements\n\n- Unit tests must achieve full branch coverage on budget comparison logic.\n- Integration test runs the gate against a known-good fixture and verifies pass.\n- Integration test with an artificially slow fixture verifies gate failure and flamegraph generation.\n- All runs emit structured JSON logs with workflow name, iteration count, p50/p95/p99 values, and pass/fail status.\n\n### Expected Artifacts\n\n- `perf/budgets.toml` — budget definitions.\n- `scripts/check_latency_gates.py` — verification script.\n- `tests/test_check_latency_gates.py` — unit tests.\n- `artifacts/section_10_6/bd-3lh/verification_evidence.json` — benchmark results.\n- `artifacts/section_10_6/bd-3lh/verification_summary.md` — human-readable summary.\n- Flamegraph SVGs for any regressions detected.","acceptance_criteria":"1. Cold-start gate: franken_node process must reach 'ready' state within a defined SLA threshold (documented in spec), measured from exec to first successful health-check response.\n2. p99 latency gate: core workflows (module-load, require-resolve, compatibility-shim dispatch) must each have a p99 latency ceiling defined in a TOML/JSON config file.\n3. CI pipeline enforces gates: build fails if cold-start or any p99 ceiling is exceeded.\n4. Measurement harness runs at least 1000 iterations per workflow to produce statistically significant tail-latency numbers.\n5. Results include tail-latency notes per Section 7 doctrine: p99, p99.9, and max with jitter analysis.\n6. Before/after comparison table is generated automatically when baselines change.\n7. Gate thresholds are parameterized per deployment profile (local/dev/enterprise) so each tier has appropriate ceilings.","notes":"Plan-space QA addendum (2026-02-20):\n1) Preserve full canonical-plan scope; no feature compression or silent omission is allowed.\n2) Completion requires comprehensive verification coverage appropriate to scope: unit tests, integration tests, and E2E scripts/workflows with detailed structured logging (stable event/error codes + trace correlation IDs).\n3) Completion requires reproducible evidence artifacts (machine-readable + human-readable) that allow independent replay and root-cause triage.\n4) Any implementation PR for this bead must include explicit links to the above test/logging artifacts.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:36:46.779051685Z","created_by":"ubuntu","updated_at":"2026-02-20T17:09:12.816895014Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-6","self-contained","test-obligations"]}
{"id":"bd-3lzk","title":"[10.18] Add release gate requiring VEF-backed evidence for designated high-impact security and compliance claims.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.18 — Verifiable Execution Fabric Execution Track (9L)\n\nWhy This Exists:\nVEF track for proof-carrying runtime compliance: receipt chains, commitment checkpoints, proof generation/verification, and claim gating.\n\nTask Objective:\nAdd release gate requiring VEF-backed evidence for designated high-impact security and compliance claims.\n\nAcceptance Criteria:\n- Release pipeline blocks designated claims without VEF evidence coverage; gate output is machine-readable, signed, and externally verifiable.\n\nExpected Artifacts:\n- `.github/workflows/vef-claim-gate.yml`, `docs/conformance/vef_release_claim_gate.md`, `artifacts/10.18/vef_release_gate_report.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_18/bd-3lzk/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_18/bd-3lzk/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.18] Add release gate requiring VEF-backed evidence for designated high-impact security and compliance claims.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.18] Add release gate requiring VEF-backed evidence for designated high-impact security and compliance claims.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.18] Add release gate requiring VEF-backed evidence for designated high-impact security and compliance claims.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.18] Add release gate requiring VEF-backed evidence for designated high-impact security and compliance claims.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.18] Add release gate requiring VEF-backed evidence for designated high-impact security and compliance claims.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"- Release pipeline blocks designated claims without VEF evidence coverage; gate output is machine-readable, signed, and externally verifiable.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:37:05.201133996Z","created_by":"ubuntu","updated_at":"2026-02-20T17:00:46.510938433Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-18","self-contained","test-obligations"]}
{"id":"bd-3m6","title":"[10.8] Implement disaster-recovery drills for control-plane failures.","description":"## [10.8] Implement disaster-recovery drills for control-plane failures\n\n### Why This Exists\n\nSection 10.15 (invariant-breach runbooks) and Section 9J.18 (deterministic fault labs) require that disaster-recovery procedures are not merely documented but regularly exercised through automated drills. Runbooks rot when they are not tested — infrastructure changes, code paths evolve, and assumptions become invalid. Automated DR drills ensure that recovery procedures remain current, that recovery time meets SLOs, and that drill results serve as compliance evidence. This bead builds the drill execution infrastructure and the initial set of control-plane failure simulations.\n\n### What It Must Do\n\nBuild an automated disaster-recovery drill framework and implement drill scenarios for all major control-plane failure modes:\n\n- **Drill framework**: A scheduling and execution engine that runs configured drill scenarios on a recurring basis (configurable interval, default: weekly for high-severity, monthly for medium-severity). The framework injects faults, executes the corresponding runbook steps (from bd-nr4), measures recovery time and correctness, and persists results as compliance evidence.\n- **Fault injection**: Each drill scenario uses deterministic fault injection — not random chaos engineering. Faults are precisely specified (which component fails, how it fails, when it fails) so that drill results are reproducible. Fault injection must be safe for production-adjacent environments (no accidental production impact).\n- **Drill scenarios** (minimum set):\n  1. Evidence ledger loss — simulate complete ledger data loss, verify recovery from backup/snapshot.\n  2. Trust artifact corruption — inject bitflip in trust artifact, verify detection and repair.\n  3. Epoch barrier failure — simulate epoch transition timeout, verify hold-and-retry procedure.\n  4. Federation partition — simulate network partition between federation peers, verify split-brain prevention and reconciliation.\n  5. Proof pipeline outage — kill proof generation service, verify queue-and-recover behavior.\n- **Recovery time measurement**: Each drill measures wall-clock time from fault injection to verified recovery. Results are compared against configured SLOs (e.g., evidence ledger recovery < 5 minutes, trust artifact repair < 2 minutes).\n- **Compliance evidence**: Drill results are persisted as structured JSON with: drill ID, scenario name, timestamp, fault description, recovery steps executed, recovery time, SLO met (bool), and any anomalies observed. These artifacts fall under the `required` retention class (bd-f2y).\n- **Drill health dashboard**: A summary report showing drill history, pass/fail trends, SLO adherence, and staleness (time since last drill per scenario).\n- **Abort safety**: If a drill detects unexpected state (e.g., fault injection affected something outside the drill scope), it must abort immediately, log the anomaly, and alert operators. Drills must never leave the system in a worse state than before the drill.\n\n### Acceptance Criteria\n\n1. Drill framework can schedule and execute drill scenarios on configurable recurring intervals.\n2. All five minimum drill scenarios are implemented with deterministic, reproducible fault injection.\n3. Each drill scenario exercises the corresponding runbook (bd-nr4) and verifies that runbook steps produce correct recovery.\n4. Recovery time is measured for each drill and compared against configurable SLOs; SLO violations are flagged as failures.\n5. Drill results are persisted as structured JSON compliance evidence under the `required` retention class.\n6. A drill summary report shows pass/fail trends and identifies any scenario that has not been drilled within its configured freshness window.\n7. Drill abort safety is verified: a test simulates unexpected state during drill and confirms clean abort with operator alert.\n8. Drill execution is idempotent — running the same drill twice produces consistent results and does not leave residual state.\n\n### Key Dependencies\n\n- Operator runbooks (bd-nr4) — drills execute runbook procedures\n- Incident bundle retention (bd-f2y) — drill results are compliance evidence\n- Safe mode (bd-k6o) — some drill scenarios trigger safe-mode entry\n- Evidence ledger, trust artifact, epoch, and proof pipeline infrastructure\n\n### Testing & Logging Requirements\n\n- Unit tests for drill scheduling, fault injection, and result persistence.\n- Integration tests that execute each drill scenario in an isolated test environment.\n- Verification script (`scripts/check_dr_drills.py`) with `--json` and `self_test()`.\n- All drill operations logged at INFO; fault injection logged at WARN; abort conditions logged at ERROR.\n\n### Expected Artifacts\n\n- `docs/specs/section_10_8/bd-3m6_contract.md` — drill framework specification\n- `scripts/check_dr_drills.py` — verification script\n- `tests/test_check_dr_drills.py` — unit tests\n- `fixtures/drills/` — drill scenario definitions (JSON)\n- `artifacts/section_10_8/bd-3m6/verification_evidence.json`\n- `artifacts/section_10_8/bd-3m6/verification_summary.md`","acceptance_criteria":"1. DR drill covers at minimum: complete control-plane failure, network partition between fleet segments, loss of trust-anchor store, and simultaneous quarantine of >50% of fleet.\n2. Each drill has a written scenario document specifying: preconditions, injected failure, expected system behavior, success criteria, and maximum allowable recovery time.\n3. Drills are executable via automation scripts (scripts/dr_drill_*.sh) that inject failures into a test environment and measure recovery.\n4. Recovery verification is automated: scripts assert that the system returns to a defined healthy state within the time ceiling.\n5. Drill results are captured in a structured JSON report with: scenario ID, start time, recovery time, success/failure status, and any deviations from expected behavior.\n6. At least one drill validates the anti-entropy reconciliation from the fleet control API (cross-reference bd-tg2).\n7. Drill cadence is documented: drills must run at least once per release cycle, with results archived under artifacts/.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:36:48.206127362Z","created_by":"ubuntu","updated_at":"2026-02-20T16:59:04.059346011Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-8","self-contained","test-obligations"]}
{"id":"bd-3mj9","title":"[15] Pillar: enterprise governance integrations","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 15\n\nTask Objective:\nImplement enterprise policy/audit/compliance integration pillar.\n\nExecution Requirements:\n- Make this requirement machine-verifiable and release-gated where applicable.\n- Keep behavior self-documenting so future contributors do not need to re-open the master plan.\n\nTesting & Logging Requirements:\n- Unit tests for rule/contract logic and error conditions.\n- Integration/E2E tests for workflow-level enforcement.\n- Structured logs with stable codes and trace IDs.\n- Reproducible artifacts that prove contract satisfaction.\n\nAcceptance Criteria:\n- Success and failure invariants for [15] Pillar: enterprise governance integrations are explicitly quantified and machine-verifiable.\n- Determinism requirements for [15] Pillar: enterprise governance integrations are validated across repeated runs and adversarial perturbations.\n\n\nExpected Artifacts:\n- Machine-readable verification artifact with explicit canonical path under artifacts/section_15/bd-3mj9/verification_evidence.json, suitable for CI/release gating.\n- Human-readable verification summary with explicit canonical path under artifacts/section_15/bd-3mj9/verification_summary.md, linking implementation intent to measured validation outcomes.\n\nTask-Specific Clarification:\n- The implementation must deliver the exact capability named in the title, with no scope dilution and no silent feature omission.\n- Validation artifacts must include capability-specific thresholds, pass/fail criteria, and reproducible evidence bundles tied to this bead ID.\n- Program/section verification gates must be able to consume this bead's outputs without manual interpretation.\n\nWhy This Improves User Outcomes:\n- \"[15] Pillar: enterprise governance integrations\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[15] Pillar: enterprise governance integrations\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"1. Enterprise governance integration supports >= 3 enterprise platforms: (a) LDAP/Active Directory for identity, (b) SIEM integration (Splunk, ELK, or equivalent) for security event forwarding, (c) policy engine integration (OPA or equivalent) for custom trust policies.\n2. Each integration has: (a) configuration guide, (b) authentication/authorization flow documented, (c) data format specification, (d) test suite validating the integration.\n3. SIEM integration: all security-relevant events (trust decisions, containment actions, revocations) are forwarded in CEF or equivalent standard format.\n4. Policy engine integration: custom policies can override default trust decisions with audit trail.\n5. SSO support: enterprise users authenticate via SAML/OIDC without creating separate credentials.\n6. Compliance reporting: generate reports compatible with SOC2, ISO27001, or equivalent frameworks.\n7. Enterprise integration test suite passes against mock enterprise services in CI.\n8. Evidence: enterprise_integration_status.json with per-platform integration status and test results.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:39:36.353660538Z","created_by":"ubuntu","updated_at":"2026-02-20T15:26:23.932273023Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-15","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-3mj9","depends_on_id":"bd-wpck","type":"blocks","created_at":"2026-02-20T07:43:26.298478993Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-3n2u","title":"[10.13] Publish formal schema spec files and golden vectors for serialization, signatures, and control-channel frames.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.13 — FCP Deep-Mined Expansion Execution Track (9I)\n\nWhy This Exists:\nDeep connector/provider contract track: lifecycle FSMs, conformance harnesses, fencing, sandboxing, revocation freshness, and protocol hardening.\n\nTask Objective:\nPublish formal schema spec files and golden vectors for serialization, signatures, and control-channel frames.\n\nAcceptance Criteria:\n- Normative schema files and golden vectors are versioned and release-published; verification CLI passes full vector suite; vector changes require explicit changelog entry.\n\nExpected Artifacts:\n- `spec/FNODE_TRUST_SCHEMA_V1.cddl`, `vectors/fnode_trust_vectors_v1.json`, `artifacts/10.13/vector_verification_report.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_13/bd-3n2u/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_13/bd-3n2u/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.13] Publish formal schema spec files and golden vectors for serialization, signatures, and control-channel frames.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.13] Publish formal schema spec files and golden vectors for serialization, signatures, and control-channel frames.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.13] Publish formal schema spec files and golden vectors for serialization, signatures, and control-channel frames.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.13] Publish formal schema spec files and golden vectors for serialization, signatures, and control-channel frames.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.13] Publish formal schema spec files and golden vectors for serialization, signatures, and control-channel frames.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","status":"closed","priority":1,"issue_type":"task","assignee":"CrimsonCrane","created_at":"2026-02-20T07:36:55.059574888Z","created_by":"ubuntu","updated_at":"2026-02-20T14:55:48.209691291Z","closed_at":"2026-02-20T14:55:48.209665232Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-13","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-3n2u","depends_on_id":"bd-29ct","type":"blocks","created_at":"2026-02-20T07:43:14.149707100Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-3n58","title":"[10.13] Add domain-separated interface-hash verification and admission failure telemetry.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.13 — FCP Deep-Mined Expansion Execution Track (9I)\n\nWhy This Exists:\nDeep connector/provider contract track: lifecycle FSMs, conformance harnesses, fencing, sandboxing, revocation freshness, and protocol hardening.\n\nTask Objective:\nAdd domain-separated interface-hash verification and admission failure telemetry.\n\nAcceptance Criteria:\n- Interface hash uses domain-separated derivation; invalid hashes block admission; telemetry exposes rejection code distribution.\n\nExpected Artifacts:\n- `src/security/interface_hash.rs`, `tests/conformance/interface_hash_verification.rs`, `artifacts/10.13/interface_hash_rejection_metrics.csv`.\n\n- Machine-readable verification artifact at `artifacts/section_10_13/bd-3n58/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_13/bd-3n58/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.13] Add domain-separated interface-hash verification and admission failure telemetry.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.13] Add domain-separated interface-hash verification and admission failure telemetry.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.13] Add domain-separated interface-hash verification and admission failure telemetry.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.13] Add domain-separated interface-hash verification and admission failure telemetry.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.13] Add domain-separated interface-hash verification and admission failure telemetry.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","status":"closed","priority":1,"issue_type":"task","assignee":"CrimsonCrane","created_at":"2026-02-20T07:36:52.535107430Z","created_by":"ubuntu","updated_at":"2026-02-20T11:35:56.503105126Z","closed_at":"2026-02-20T11:35:56.503078356Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-13","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-3n58","depends_on_id":"bd-17mb","type":"blocks","created_at":"2026-02-20T07:43:12.846096877Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-3ndj","title":"[10.16] Define `fastapi_rust` control-plane service integration contract.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.16 — Adjacent Substrate Integration Execution Track (8.7)\n\nWhy This Exists:\nAdjacent substrate integration track to enforce deep composition with frankentui, frankensqlite, sqlmodel_rust, and fastapi_rust.\n\nTask Objective:\nDefine `fastapi_rust` control-plane service integration contract.\n\nAcceptance Criteria:\n- Contract defines endpoint lifecycle, auth/policy hooks, error contract mapping, and observability requirements.\n\nExpected Artifacts:\n- `docs/specs/fastapi_rust_integration_contract.md`, `artifacts/10.16/fastapi_contract_checklist.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_16/bd-3ndj/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_16/bd-3ndj/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.16] Define `fastapi_rust` control-plane service integration contract.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.16] Define `fastapi_rust` control-plane service integration contract.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.16] Define `fastapi_rust` control-plane service integration contract.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.16] Define `fastapi_rust` control-plane service integration contract.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.16] Define `fastapi_rust` control-plane service integration contract.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"- Contract defines endpoint lifecycle, auth/policy hooks, error contract mapping, and observability requirements.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:37:02.348772938Z","created_by":"ubuntu","updated_at":"2026-02-20T17:00:56.976606112Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-16","self-contained","test-obligations"]}
{"id":"bd-3nr","title":"[10.5] Implement degraded-mode policy behavior with mandatory audit events.","description":"# [10.5] Degraded-Mode Policy Behavior with Mandatory Audit Events\n\n## Why This Exists\n\nDistributed trust systems inevitably encounter states where trust inputs become stale or unavailable: revocation frontiers lag, federation peers go offline, proof pipelines stall, or upstream certificate authorities become unreachable. Section 6.5 of the Security Doctrine requires \"auditable degraded-mode semantics when trust state is stale.\" Rather than failing closed (blocking all operations) or failing open (silently continuing without trust verification), franken_node must operate in an explicit, bounded degraded mode that restricts high-risk actions while preserving safe operations.\n\nThis bead implements the degraded-mode policy engine within Section 10.5 (Security + Policy Product Surfaces). It is a critical safety mechanism: without explicit degraded-mode handling, operators have no visibility into whether the system is making decisions based on fresh or stale trust data. Every entry into and exit from degraded mode must emit mandatory, structured audit events so that incident replay (capability #4, Section 3.2) and the operator copilot (capability #8, Section 3.2, and Section 9A.8) can reason about trust-state transitions.\n\n## What It Must Do\n\n1. **Trust Staleness Detection**: Monitor all trust-state inputs (revocation frontier timestamps, federation heartbeats, proof pipeline watermarks, certificate freshness) and detect when any input exceeds its configured staleness threshold. Each trust input has an independent threshold.\n\n2. **Degraded-Mode State Machine**: Implement a state machine with at least three states: NORMAL, DEGRADED (with sub-states per stale input), and SUSPENDED (when degradation exceeds critical thresholds). Transitions must be deterministic and driven solely by trust-state observations.\n\n3. **Action Restriction Policy**: In DEGRADED mode, classify all pending and incoming actions into risk tiers. High-risk actions (e.g., policy changes, key rotations, envelope modifications) are blocked with a structured rejection. Medium-risk actions are allowed but annotated with a degraded-mode flag. Low-risk actions proceed normally. The tier classification must be configurable and auditable.\n\n4. **Mandatory Audit Events**: Every state transition (NORMAL->DEGRADED, DEGRADED->SUSPENDED, DEGRADED->NORMAL, etc.) must emit a structured audit event containing: (a) the previous and new state, (b) which trust inputs triggered the transition, (c) the staleness duration of each stale input, (d) the set of actions that are now restricted or unrestricted, and (e) a trace correlation ID linking to the broader operation context.\n\n5. **Operator Notification**: On degraded-mode entry, emit a high-priority notification to the operator copilot (bd-2yc) with recommended remediation actions and expected-loss context from bd-33b.\n\n6. **Automatic Recovery**: When stale trust inputs are refreshed, the system must automatically transition back to NORMAL mode, emitting a recovery audit event. Recovery must not require manual intervention unless the system entered SUSPENDED state.\n\n7. **Degraded-Mode Duration Limits**: Configurable maximum duration for DEGRADED mode. If the duration is exceeded without recovery, the system escalates to SUSPENDED and blocks all non-essential operations.\n\n## Acceptance Criteria\n\n1. When any trust-state input exceeds its staleness threshold, the system transitions to DEGRADED mode within one monitoring cycle (configurable, default 1 second).\n2. In DEGRADED mode, high-risk actions are rejected with a structured error that includes the degraded-mode reason and the specific stale trust inputs.\n3. In DEGRADED mode, medium-risk actions succeed but carry a degraded-mode annotation in their response and in the audit trail.\n4. Every state transition emits a mandatory audit event with all required fields (previous state, new state, triggering inputs, staleness durations, affected action set, trace ID).\n5. No state transition can occur without emitting the corresponding audit event; the audit event emission is atomic with the state transition (same transaction boundary).\n6. The SUSPENDED state blocks all non-essential operations and requires explicit operator acknowledgment (or automatic recovery of all trust inputs) to exit.\n7. Degraded-mode duration limits are enforced: exceeding the configured maximum triggers SUSPENDED escalation.\n8. Automatic recovery from DEGRADED to NORMAL occurs within one monitoring cycle after all trust inputs are refreshed, and emits a recovery audit event.\n9. The action risk-tier classification is loaded from a configuration file, supports hot-reload without restart, and logs every classification decision at DEBUG level.\n10. All audit events use stable event codes (DEGRADED_MODE_ENTERED, DEGRADED_MODE_EXITED, DEGRADED_MODE_SUSPENDED, DEGRADED_ACTION_BLOCKED, DEGRADED_ACTION_ANNOTATED, TRUST_INPUT_STALE, TRUST_INPUT_REFRESHED) with trace correlation IDs.\n11. The degraded-mode state is queryable via a health endpoint that returns the current state, stale inputs, and time-in-state.\n\n## Key Dependencies\n\n- **Depends on bd-33b** (expected-loss scoring): degraded-mode notifications to the operator copilot include expected-loss context.\n- **Depends on trust-state monitoring infrastructure** (Section 6 Security Doctrine): trust input freshness signals.\n- **Depended on by bd-sh3** (policy change workflows): policy change workflows must check degraded-mode state and block elevated changes during degradation.\n- **Depended on by bd-1koz** (section-wide verification gate).\n- **Depended on by bd-20a** (section rollup).\n\n## Testing and Logging Requirements\n\n- **Unit tests**: State machine transitions for all valid paths (NORMAL->DEGRADED, DEGRADED->SUSPENDED, DEGRADED->NORMAL, SUSPENDED->NORMAL). Action classification for each risk tier. Staleness threshold detection. Duration limit enforcement.\n- **Integration tests**: Simulate trust input going stale (mock federation offline, mock revocation frontier lag), verify degraded-mode entry, action blocking, and automatic recovery when inputs refresh.\n- **E2E tests**: Full lifecycle: start in NORMAL, inject staleness, verify DEGRADED behavior, let duration limit expire, verify SUSPENDED escalation, refresh inputs, verify recovery.\n- **Adversarial tests**: Attempt high-risk action during DEGRADED mode and verify rejection. Attempt to suppress audit events and verify they are mandatory (cannot be disabled by configuration). Attempt to skip DEGRADED and go directly to NORMAL from SUSPENDED without input recovery.\n- **Logging**: All audit events must be structured JSON with stable codes. Monitoring cycle timing must be logged at TRACE level. State transitions logged at WARN level. Action blocks logged at ERROR level.\n\n## Expected Artifacts\n\n- `docs/specs/section_10_5/bd-3nr_contract.md` — Design spec with state machine diagram, trust input catalog, action risk-tier schema, and audit event schema.\n- `artifacts/section_10_5/bd-3nr/verification_evidence.json` — Machine-readable pass/fail evidence.\n- `artifacts/section_10_5/bd-3nr/verification_summary.md` — Human-readable summary.\n- Rust module(s) in `crates/franken-node/src/` implementing the degraded-mode state machine and audit emitter.\n- Python verification script `scripts/check_degraded_mode_policy.py` with `--json` flag and `self_test()`.\n- Unit tests in `tests/test_check_degraded_mode_policy.py`.","acceptance_criteria":"1. Define a DegradedModePolicy struct containing: mode_name (string), trigger_conditions (Vec<TriggerCondition>), permitted_actions (HashSet<String>), denied_actions (HashSet<String>), mandatory_audit_events (Vec<AuditEventSpec>), and auto_recovery_criteria (Vec<RecoveryCriterion>).\n2. TriggerCondition is an enum with variants: HealthGateFailed(gate_name: String), CapabilityUnavailable(capability_id: String), ErrorRateExceeded { threshold: f64, window_secs: u64 }, and ManualActivation(operator_id: String).\n3. When degraded mode activates, the system must emit a structured DegradedModeEntered audit event containing: timestamp, mode_name, triggering_condition, active_policy_version, and list of denied_actions. This event must be emitted before any action processing resumes.\n4. Every action attempted during degraded mode must produce a DegradedModeActionAudit event containing: timestamp, action_name, actor, permitted (bool), and if denied, the denial_reason referencing the specific denied_actions entry.\n5. Mandatory audit events defined in mandatory_audit_events must fire at configurable intervals (default: every 60s) while degraded mode is active; missing a mandatory event must trigger an alert (separate AuditEventMissed event).\n6. Recovery: when all auto_recovery_criteria are met, emit a DegradedModeExited event and restore normal policy. Recovery must require all criteria satisfied for a stabilization_window (default 300s) before exiting.\n7. Verification: scripts/check_degraded_mode.py --json simulates mode entry, action denial, mandatory audit ticks, and recovery; asserts correct event ordering and completeness; unit tests in tests/test_check_degraded_mode.py cover each trigger variant, the denied-action path, the missed-audit alert, and the stabilization window; evidence in artifacts/section_10_5/bd-3nr/.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:36:46.543807640Z","created_by":"ubuntu","updated_at":"2026-02-20T16:59:01.072955623Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-5","self-contained","test-obligations"]}
{"id":"bd-3o6","title":"[10.8] Adopt canonical structured observability + stable error taxonomy contracts (from `10.13`) across operational surfaces.","description":"## [10.8] Adopt canonical structured observability + stable error taxonomy contracts across operational surfaces\n\n### Why This Exists\n\nSection 9I.18 mandates a stable telemetry namespace and AI-recovery error contract that spans every operational surface of franken_node. The 10.13 chain delivered the foundational contracts — `telemetry_namespace.md` defines canonical metric names, dimensions, and cardinality budgets, while `error_code_registry.md` establishes the stable error taxonomy with machine-readable recovery hints. This bead enforces adoption of those contracts everywhere: CLI commands, operational APIs, dashboard integrations, health endpoints, and log output. Without uniform adoption, operators face a patchwork of ad-hoc metric names and opaque error codes that defeat both human triage and autonomous operator agents.\n\n### What It Must Do\n\nEvery operational surface that emits metrics, errors, or structured logs must be audited against the 10.13 canonical contracts and brought into compliance. Specifically:\n\n- **Metric names**: All emitted metrics must use the canonical namespace prefixes and dimension keys defined in `telemetry_namespace.md`. No ad-hoc or legacy metric names may be emitted in production builds.\n- **Error codes**: All user-facing and operator-facing errors must carry a registered error code from the error code registry. Each error must include the associated severity, category, and machine-readable recovery hint.\n- **Recovery hints**: Recovery hints must be structured JSON objects (not free-text) so that autonomous operator agents can parse and act on them without NLP. The hint schema must include: `action` (enum), `target` (resource identifier), `confidence` (float 0-1), and optional `escalation_path`.\n- **CLI output**: All CLI commands that report operational state must emit structured JSON (with `--json` flag) using canonical metric and error schemas.\n- **Dashboard contracts**: Any dashboard integration surface must document which canonical metrics it consumes, ensuring dashboards break visibly (not silently) if a metric is renamed or removed.\n- **Log correlation**: Structured log entries must carry trace context (trace_id, span_id) and reference canonical error codes where applicable, enabling end-to-end correlation from log line to error taxonomy to recovery hint.\n\n### Acceptance Criteria\n\n1. A compliance audit script (`scripts/check_observability_adoption.py`) scans all operational surfaces and reports any metric name, error code, or log field not present in the canonical contracts. Zero violations required for gate passage.\n2. Every error emitted by CLI commands, APIs, and health endpoints carries a registered error code with structured recovery hint conforming to the hint schema.\n3. All emitted metric names conform to the canonical namespace; no ad-hoc or legacy metric names appear in production code paths.\n4. Machine-readable recovery hints are parseable by a reference autonomous-agent consumer (test harness simulates agent consuming hints and verifies actionability).\n5. A backward-compatibility check ensures that removing or renaming a canonical metric or error code requires a deprecation cycle (at least one release with both old and new).\n6. Dashboard contract files enumerate consumed metrics per integration surface, and a CI check validates that all referenced metrics exist in the canonical namespace.\n7. Structured log output includes trace context fields and canonical error code references where applicable.\n\n### Key Dependencies\n\n- `telemetry_namespace.md` contract from 10.13 chain (bd-1cm or predecessor)\n- `error_code_registry.md` contract from 10.13 chain\n- Trace context propagation (bd-3tzl / trace_context.rs)\n- CLI structured output infrastructure (cli.rs)\n\n### Testing & Logging Requirements\n\n- Unit tests for recovery hint schema validation (round-trip serialize/deserialize).\n- Integration test that exercises every CLI command with `--json` and validates output against canonical schemas.\n- Verification script must support `--json` output and `self_test()` per bead delivery pattern.\n- All compliance violations logged at WARN level with specific file/line and suggested fix.\n\n### Expected Artifacts\n\n- `scripts/check_observability_adoption.py` — compliance audit script\n- `tests/test_check_observability_adoption.py` — unit tests for the audit script\n- `artifacts/section_10_8/bd-3o6/verification_evidence.json` — audit results\n- `artifacts/section_10_8/bd-3o6/verification_summary.md` — human-readable summary\n- `docs/specs/section_10_8/bd-3o6_contract.md` — adoption contract and hint schema spec","acceptance_criteria":"1. All log output uses structured format (JSON lines) with mandatory fields: timestamp, level, component, event_type, and correlation_id.\n2. Error taxonomy defines a stable, versioned catalog of error codes organized by domain (connectivity, compatibility, migration, trust, fleet).\n3. Each error code has: unique identifier, severity level, human-readable description, suggested remediation, and a stability guarantee (codes are never reused or silently changed).\n4. Observability contracts from 10.13 (telemetry namespace, trace context) are adopted and all emitted metrics/traces conform to those schemas.\n5. Log schema and error taxonomy are published as JSON Schema files under spec/ for external tooling consumption.\n6. Breaking changes to the error taxonomy require a semver-major bump and a migration note.\n7. Integration test verifies that every code path that produces an error emits a cataloged error code — uncataloged errors cause test failure.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:36:47.864218865Z","created_by":"ubuntu","updated_at":"2026-02-20T16:59:04.620638874Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-8","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-3o6","depends_on_id":"bd-1ugy","type":"blocks","created_at":"2026-02-20T15:00:23.655745001Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3o6","depends_on_id":"bd-novi","type":"blocks","created_at":"2026-02-20T15:00:23.475314471Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-3ohj","title":"[BOOTSTRAP] Foundation verification gate: comprehensive unit+e2e+logging","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md (Bootstrap integration quality overlay)\nSection: BOOTSTRAP (Foundation verification overlay)\n\nTask Objective:\nCreate a hard bootstrap completion gate requiring comprehensive unit/integration/E2E validation and detailed structured logging evidence before bootstrap epic closure.\n\nAcceptance Criteria:\n- Gate consumes matrix coverage, E2E outcomes, baseline check artifacts, and docs-navigation validation.\n- Gate fails closed on missing evidence, unstable logs, or nondeterministic outcomes.\n- Gate outputs deterministic machine-readable verdict consumable by downstream planning/release automation.\n\nExpected Artifacts:\n- Bootstrap gate policy contract and verdict schema.\n- Gate pass/fail artifact samples with remediation guidance.\n- Traceability report linking each bootstrap bead to verification evidence.\n\n- Machine-readable verification artifact at `artifacts/section_bootstrap/bd-3ohj/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_bootstrap/bd-3ohj/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for gate aggregation/evaluation logic.\n- E2E tests for gate behavior under green, partial, and failing evidence sets.\n- Structured gate logs with explicit failing dimension tags and trace correlation IDs.\n\nTask-Specific Clarification:\n- For \"[BOOTSTRAP] Foundation verification gate: comprehensive unit+e2e+logging\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[BOOTSTRAP] Foundation verification gate: comprehensive unit+e2e+logging\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[BOOTSTRAP] Foundation verification gate: comprehensive unit+e2e+logging\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[BOOTSTRAP] Foundation verification gate: comprehensive unit+e2e+logging\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[BOOTSTRAP] Foundation verification gate: comprehensive unit+e2e+logging\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T08:03:10.519600884Z","created_by":"ubuntu","updated_at":"2026-02-20T08:46:08.183091660Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["bootstrap","test-obligations","verification-gate"],"dependencies":[{"issue_id":"bd-3ohj","depends_on_id":"bd-1dpd","type":"blocks","created_at":"2026-02-20T08:24:23.078691855Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3ohj","depends_on_id":"bd-2a3","type":"blocks","created_at":"2026-02-20T08:03:12.074478046Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3ohj","depends_on_id":"bd-2twu","type":"blocks","created_at":"2026-02-20T08:20:48.304308367Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3ohj","depends_on_id":"bd-3k9t","type":"blocks","created_at":"2026-02-20T08:03:11.943618042Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-3ort","title":"[10.14] Add proof-presence requirement for quarantine promotion in high-assurance modes.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.14 — FrankenSQLite Deep-Mined Expansion Execution Track (9J)\n\nWhy This Exists:\nFrankenSQLite deep-mined control integrity track: evidence ledgers, proofs, epochs, remote effects, markers/MMR, and deterministic fault labs.\n\nTask Objective:\nAdd proof-presence requirement for quarantine promotion in high-assurance modes.\n\nAcceptance Criteria:\n- High-assurance mode promotion fails without required proof bundle; mode toggle is policy-controlled; conformance covers both assurance modes.\n\nExpected Artifacts:\n- `tests/conformance/high_assurance_quarantine_promotion.rs`, `docs/specs/high_assurance_promotion.md`, `artifacts/10.14/high_assurance_promotion_matrix.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_14/bd-3ort/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_14/bd-3ort/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.14] Add proof-presence requirement for quarantine promotion in high-assurance modes.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.14] Add proof-presence requirement for quarantine promotion in high-assurance modes.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.14] Add proof-presence requirement for quarantine promotion in high-assurance modes.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.14] Add proof-presence requirement for quarantine promotion in high-assurance modes.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.14] Add proof-presence requirement for quarantine promotion in high-assurance modes.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"High-assurance mode promotion fails without required proof bundle; mode toggle is policy-controlled; conformance covers both assurance modes.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:36:56.803588168Z","created_by":"ubuntu","updated_at":"2026-02-20T16:24:04.314136125Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-14","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-3ort","depends_on_id":"bd-1l62","type":"blocks","created_at":"2026-02-20T16:24:04.314054002Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-3p74","title":"Epic: Radical Expansion - Time-Travel + Artifacts [10.17b]","description":"- type: epic","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-20T07:47:58.072163029Z","updated_at":"2026-02-20T07:49:21.289235835Z","closed_at":"2026-02-20T07:49:21.289215857Z","close_reason":"Superseded by canonical detailed master-plan graph (bd-33v) with full 10.N-10.21 and 11-16 coverage, explicit dependencies, and per-section verification gates.","source_repo":".","compaction_level":0,"original_size":0}
{"id":"bd-3p9n","title":"[10.6] Section-wide verification gate: comprehensive unit+e2e+logging","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.6\n\nTask Objective:\nCreate a hard section completion gate that verifies all implemented behavior in this section with comprehensive unit tests, integration/e2e scripts, and detailed structured logging evidence.\n\nAcceptance Criteria:\n- Section test matrix covers happy-path, edge-case, and adversarial/error-path scenarios for all section deliverables.\n- E2E scripts execute representative end-user workflows and policy/control-plane transitions for this section.\n- Structured logs include stable event/error codes, trace correlation IDs, and enough context for deterministic replay triage.\n- Verification report is deterministic and machine-readable for CI/release gating.\n\nExpected Artifacts:\n- Section-level test matrix and coverage summary artifact.\n- E2E script suite with pass/fail outputs and reproducible fixtures/seeds.\n- Structured log validation report and traceability bundle.\n- Gate verdict artifact consumable by release automation.\n\n- Machine-readable verification artifact at `artifacts/section_10_6/bd-3p9n/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_6/bd-3p9n/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests must validate contracts, invariants, and failure semantics.\n- E2E tests must validate cross-component behavior from user/operator entrypoints to persisted effects.\n- Logging must support root-cause analysis without hidden context requirements.\n\nTask-Specific Clarification:\n- For \"[10.6] Section-wide verification gate: comprehensive unit+e2e+logging\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.6] Section-wide verification gate: comprehensive unit+e2e+logging\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.6] Section-wide verification gate: comprehensive unit+e2e+logging\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.6] Section-wide verification gate: comprehensive unit+e2e+logging\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.6] Section-wide verification gate: comprehensive unit+e2e+logging\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"1. Section 10.6 gate aggregates pass/fail status from all sibling beads (bd-k4s, bd-3lh, bd-38m, bd-2q5, bd-3kn, bd-2pw, bd-3q9).\n2. Gate script (scripts/check_section_10_6_gate.py) runs all section verification scripts and produces a unified JSON report.\n3. Gate fails if any sibling bead's verification evidence is missing or shows a failure.\n4. Unit tests cover gate logic: all-pass, single-fail, and missing-evidence scenarios.\n5. Gate produces a section-level summary under artifacts/ with per-bead status, timestamps, and links to individual evidence files.\n6. E2E logging: gate execution emits structured log lines for each sub-check with bead ID, result, and duration.\n7. Gate is idempotent: running it twice in succession with no changes produces identical output.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:48:24.783780697Z","created_by":"ubuntu","updated_at":"2026-02-20T15:16:24.462716885Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-6","test-obligations","verification-gate"],"dependencies":[{"issue_id":"bd-3p9n","depends_on_id":"bd-1dpd","type":"blocks","created_at":"2026-02-20T08:24:24.876702944Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3p9n","depends_on_id":"bd-2pw","type":"blocks","created_at":"2026-02-20T07:48:24.933749843Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3p9n","depends_on_id":"bd-2q5","type":"blocks","created_at":"2026-02-20T07:48:25.029904196Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3p9n","depends_on_id":"bd-2twu","type":"blocks","created_at":"2026-02-20T08:20:50.468307824Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3p9n","depends_on_id":"bd-38m","type":"blocks","created_at":"2026-02-20T07:48:25.078010626Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3p9n","depends_on_id":"bd-3kn","type":"blocks","created_at":"2026-02-20T07:48:24.981114262Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3p9n","depends_on_id":"bd-3lh","type":"blocks","created_at":"2026-02-20T07:48:25.126851335Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3p9n","depends_on_id":"bd-3q9","type":"blocks","created_at":"2026-02-20T07:48:24.885587438Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3p9n","depends_on_id":"bd-k4s","type":"blocks","created_at":"2026-02-20T07:48:25.175550019Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-3pds","title":"[10.18] Integrate VEF evidence into verifier SDK replay capsules and external verification APIs.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.18 — Verifiable Execution Fabric Execution Track (9L)\n\nWhy This Exists:\nVEF track for proof-carrying runtime compliance: receipt chains, commitment checkpoints, proof generation/verification, and claim gating.\n\nTask Objective:\nIntegrate VEF evidence into verifier SDK replay capsules and external verification APIs.\n\nAcceptance Criteria:\n- Replay capsules include receipt commitments, proof references, and verifier-friendly validation metadata; external verifiers can independently validate VEF claims.\n\nExpected Artifacts:\n- `docs/specs/vef_capsule_extension.md`, `tests/conformance/vef_verifier_sdk_integration.rs`, `artifacts/10.18/vef_external_verification_report.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_18/bd-3pds/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_18/bd-3pds/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.18] Integrate VEF evidence into verifier SDK replay capsules and external verification APIs.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.18] Integrate VEF evidence into verifier SDK replay capsules and external verification APIs.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.18] Integrate VEF evidence into verifier SDK replay capsules and external verification APIs.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.18] Integrate VEF evidence into verifier SDK replay capsules and external verification APIs.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.18] Integrate VEF evidence into verifier SDK replay capsules and external verification APIs.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"- Replay capsules include receipt commitments, proof references, and verifier-friendly validation metadata; external verifiers can independently validate VEF claims.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:37:04.872225515Z","created_by":"ubuntu","updated_at":"2026-02-20T17:05:57.857210671Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-18","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-3pds","depends_on_id":"bd-1o4v","type":"blocks","created_at":"2026-02-20T17:05:57.857161129Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-3po7","title":"[10.20] Section-wide verification gate: comprehensive unit+e2e+logging","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.20\n\nTask Objective:\nCreate a hard section completion gate that verifies all implemented behavior in this section with comprehensive unit tests, integration/e2e scripts, and detailed structured logging evidence.\n\nAcceptance Criteria:\n- Section test matrix covers happy-path, edge-case, and adversarial/error-path scenarios for all section deliverables.\n- E2E scripts execute representative end-user workflows and policy/control-plane transitions for this section.\n- Structured logs include stable event/error codes, trace correlation IDs, and enough context for deterministic replay triage.\n- Verification report is deterministic and machine-readable for CI/release gating.\n\nExpected Artifacts:\n- Section-level test matrix and coverage summary artifact.\n- E2E script suite with pass/fail outputs and reproducible fixtures/seeds.\n- Structured log validation report and traceability bundle.\n- Gate verdict artifact consumable by release automation.\n\n- Machine-readable verification artifact at `artifacts/section_10_20/bd-3po7/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_20/bd-3po7/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests must validate contracts, invariants, and failure semantics.\n- E2E tests must validate cross-component behavior from user/operator entrypoints to persisted effects.\n- Logging must support root-cause analysis without hidden context requirements.\n\nTask-Specific Clarification:\n- For \"[10.20] Section-wide verification gate: comprehensive unit+e2e+logging\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.20] Section-wide verification gate: comprehensive unit+e2e+logging\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.20] Section-wide verification gate: comprehensive unit+e2e+logging\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.20] Section-wide verification gate: comprehensive unit+e2e+logging\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.20] Section-wide verification gate: comprehensive unit+e2e+logging\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"- Section test matrix covers happy-path, edge-case, and adversarial/error-path scenarios for all section deliverables.\n- E2E scripts execute representative end-user workflows and policy/control-plane transitions for this section.\n- Structured logs include stable event/error codes, trace correlation IDs, and enough context for deterministic replay triage.\n- Verification report is deterministic and machine-readable for CI/release gating.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:48:20.709409404Z","created_by":"ubuntu","updated_at":"2026-02-20T15:45:24.980353011Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-20","test-obligations","verification-gate"],"dependencies":[{"issue_id":"bd-3po7","depends_on_id":"bd-19k2","type":"blocks","created_at":"2026-02-20T07:48:21.061016901Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3po7","depends_on_id":"bd-1dpd","type":"blocks","created_at":"2026-02-20T08:24:25.599914748Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3po7","depends_on_id":"bd-1f8v","type":"blocks","created_at":"2026-02-20T07:48:21.003099437Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3po7","depends_on_id":"bd-1q38","type":"blocks","created_at":"2026-02-20T07:48:21.355393153Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3po7","depends_on_id":"bd-1tnu","type":"blocks","created_at":"2026-02-20T07:48:21.252698508Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3po7","depends_on_id":"bd-2bj4","type":"blocks","created_at":"2026-02-20T07:48:21.501526153Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3po7","depends_on_id":"bd-2d17","type":"blocks","created_at":"2026-02-20T07:48:20.947378384Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3po7","depends_on_id":"bd-2fid","type":"blocks","created_at":"2026-02-20T07:48:21.299719187Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3po7","depends_on_id":"bd-2jns","type":"blocks","created_at":"2026-02-20T07:48:21.404396414Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3po7","depends_on_id":"bd-2twu","type":"blocks","created_at":"2026-02-20T08:20:51.305266418Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3po7","depends_on_id":"bd-2wod","type":"blocks","created_at":"2026-02-20T07:48:21.155362746Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3po7","depends_on_id":"bd-351r","type":"blocks","created_at":"2026-02-20T07:48:21.108297404Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3po7","depends_on_id":"bd-38yt","type":"blocks","created_at":"2026-02-20T07:48:20.834483641Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3po7","depends_on_id":"bd-b541","type":"blocks","created_at":"2026-02-20T07:48:21.550562296Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3po7","depends_on_id":"bd-c97l","type":"blocks","created_at":"2026-02-20T07:48:21.202861935Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3po7","depends_on_id":"bd-cclm","type":"blocks","created_at":"2026-02-20T07:48:20.894532395Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3po7","depends_on_id":"bd-t89w","type":"blocks","created_at":"2026-02-20T07:48:21.452341534Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-3ps8","title":"[10.19] Implement mergeable sketch system for scalable ecosystem pattern sharing.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.19 — Adversarial Trust Commons Execution Track (9M)\n\nWhy This Exists:\nATC federated intelligence track for privacy-preserving cross-deployment threat learning, robust aggregation, and verifier-backed trust metrics.\n\nTask Objective:\nImplement mergeable sketch system for scalable ecosystem pattern sharing.\n\nAcceptance Criteria:\n- Sketch merge semantics are deterministic and bounded-error; bandwidth and compute costs stay within configured budgets under large participant counts.\n\nExpected Artifacts:\n- `src/federation/atc_sketches.rs`, `tests/perf/atc_sketch_scaling.rs`, `artifacts/10.19/atc_sketch_accuracy_report.csv`.\n\n- Machine-readable verification artifact at `artifacts/section_10_19/bd-3ps8/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_19/bd-3ps8/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.19] Implement mergeable sketch system for scalable ecosystem pattern sharing.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.19] Implement mergeable sketch system for scalable ecosystem pattern sharing.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.19] Implement mergeable sketch system for scalable ecosystem pattern sharing.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.19] Implement mergeable sketch system for scalable ecosystem pattern sharing.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.19] Implement mergeable sketch system for scalable ecosystem pattern sharing.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"- Sketch merge semantics are deterministic and bounded-error; bandwidth and compute costs stay within configured budgets under large participant counts.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:37:05.669165059Z","created_by":"ubuntu","updated_at":"2026-02-20T17:00:54.522053506Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-19","self-contained","test-obligations"]}
{"id":"bd-3ptu","title":"[10.18] Add adversarial test suite for receipt tampering, proof replay, stale-policy proofs, and commitment mismatch.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.18 — Verifiable Execution Fabric Execution Track (9L)\n\nWhy This Exists:\nVEF track for proof-carrying runtime compliance: receipt chains, commitment checkpoints, proof generation/verification, and claim gating.\n\nTask Objective:\nAdd adversarial test suite for receipt tampering, proof replay, stale-policy proofs, and commitment mismatch.\n\nAcceptance Criteria:\n- Adversarial scenarios are deterministic and fail closed; mismatch classes map to stable error codes and remediation hints.\n\nExpected Artifacts:\n- `tests/security/vef_adversarial_suite.rs`, `docs/security/vef_adversarial_testing.md`, `artifacts/10.18/vef_adversarial_results.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_18/bd-3ptu/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_18/bd-3ptu/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.18] Add adversarial test suite for receipt tampering, proof replay, stale-policy proofs, and commitment mismatch.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.18] Add adversarial test suite for receipt tampering, proof replay, stale-policy proofs, and commitment mismatch.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.18] Add adversarial test suite for receipt tampering, proof replay, stale-policy proofs, and commitment mismatch.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.18] Add adversarial test suite for receipt tampering, proof replay, stale-policy proofs, and commitment mismatch.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.18] Add adversarial test suite for receipt tampering, proof replay, stale-policy proofs, and commitment mismatch.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"- Adversarial scenarios are deterministic and fail closed; mismatch classes map to stable error codes and remediation hints.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:37:05.036498944Z","created_by":"ubuntu","updated_at":"2026-02-20T17:06:00.917915944Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-18","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-3ptu","depends_on_id":"bd-3g4k","type":"blocks","created_at":"2026-02-20T17:06:00.917866432Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-3q9","title":"[10.6] Add release rollback bundles with deterministic restore checks.","description":"## [10.6] Release Rollback Bundles with Deterministic Restore Checks\n\n### Why This Exists\n\nSection 9I.10 requires crash-loop rollback capability, and Section 8.5 invariant #4 mandates two-phase effects — every state change must be reversible. When a release causes problems in production, operators need a guaranteed path back to the previous known-good state. Ad-hoc rollback is error-prone and often incomplete (binary reverted but configuration left in new format, or state migrations left half-applied). This bead ensures every release ships with a rollback bundle that deterministically restores the previous version, including binary, configuration, and state.\n\n### What It Must Do\n\n**Rollback bundle generation**: During the release build, a rollback bundle is created alongside the release artifacts. The bundle contains: (1) the previous version's binary, (2) a configuration diff that reverses any configuration schema changes, (3) state migration reversal scripts that undo any data format changes introduced by the new version, and (4) a health check sequence that validates the rollback succeeded.\n\n**Deterministic restore**: Applying the rollback bundle produces a state that is byte-identical (where applicable) to the pre-upgrade state. For state that cannot be byte-identical (e.g., logs, timestamps), the rollback verifies semantic equivalence via health checks. The rollback process is idempotent — applying it multiple times produces the same result.\n\n**Pre/post consistency verification**: Before rollback, the bundle captures a state snapshot (configuration checksums, active policy set, schema version). After rollback, it captures another snapshot and compares against the expected pre-upgrade state. Mismatches are reported as structured errors with specific remediation guidance.\n\n**Health check sequence**: After rollback, a sequence of health checks runs: binary version verification, configuration schema validation, state integrity check, core workflow smoke tests (migration scan, compatibility check, policy evaluation). All must pass for rollback to be considered successful.\n\n**Rollback CLI**: `franken-node rollback <bundle-path>` executes the full rollback sequence. It supports `--dry-run` to preview what would change without applying. Exit codes distinguish success (0), partial rollback (1, with details), and failure (2).\n\n### Acceptance Criteria\n\n1. Every release build produces a rollback bundle alongside the release artifacts, containing previous binary, config diff, state migration reversal, and health check definitions.\n2. `franken-node rollback <bundle-path>` applies the bundle and runs the health check sequence, reporting structured JSON results.\n3. `--dry-run` previews rollback actions without applying changes.\n4. Pre/post state snapshots are compared after rollback; mismatches produce structured error reports with remediation guidance.\n5. Rollback is idempotent — applying the same bundle twice produces identical state.\n6. Health check sequence covers: binary version, config schema, state integrity, and core workflow smoke tests.\n7. Verification script `scripts/check_rollback_bundles.py` with `--json` flag validates bundle generation and restore correctness.\n8. Unit tests in `tests/test_check_rollback_bundles.py` cover bundle generation, config diff application, state reversal, health check execution, idempotency, and dry-run mode.\n\n### Key Dependencies\n\n- Release pipeline (generates bundles during build).\n- State migration system from 10.3 (for migration reversal scripts).\n- Configuration system from bd-n9r (for config diff generation).\n- Health gate from 10.13 (for health check infrastructure).\n\n### Testing & Logging Requirements\n\n- Round-trip test: upgrade from version A to B, rollback to A, verify state matches pre-upgrade snapshot.\n- Idempotency test: apply rollback twice, verify identical state after both applications.\n- Dry-run test: run with `--dry-run`, verify no state changes occurred.\n- Partial failure test: simulate health check failure mid-rollback, verify structured error report.\n- Structured JSON logs for each rollback step: action taken, pre/post checksums, health check results, overall status.\n\n### Expected Artifacts\n\n- Rollback bundle generator in build pipeline or `scripts/`.\n- Rollback CLI subcommand in `crates/franken-node/src/`.\n- `scripts/check_rollback_bundles.py` — verification script.\n- `tests/test_check_rollback_bundles.py` — unit tests.\n- `artifacts/section_10_6/bd-3q9/verification_evidence.json` — gate results.\n- `artifacts/section_10_6/bd-3q9/verification_summary.md` — human-readable summary.\n\n### Additional E2E Requirement\n\n- E2E test scripts must execute full end-to-end workflows from clean fixtures, emit deterministic machine-readable pass/fail evidence, and capture stepwise structured logs for root-cause triage.","acceptance_criteria":"1. Rollback bundle is a self-contained archive containing the previous release binary, its config, migration state snapshot, and a restore manifest.\n2. Deterministic restore: applying a rollback bundle to a clean environment produces a byte-identical state to the pre-upgrade state (verified by checksum comparison).\n3. Restore check script (scripts/verify_rollback.sh) validates bundle integrity, applies it to a temp environment, and confirms deterministic restoration.\n4. Rollback bundles are generated automatically during every release build and stored alongside release artifacts.\n5. Bundle includes a compatibility proof: a record of which versions it can safely roll back from/to.\n6. Restore procedure completes within a defined time ceiling (documented in spec) for the standard deployment size.\n7. CI pipeline includes a rollback-and-verify integration test that upgrades, then rolls back, then asserts state equivalence.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:36:47.183210313Z","created_by":"ubuntu","updated_at":"2026-02-20T17:08:35.847219444Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-6","self-contained","test-obligations"]}
{"id":"bd-3qo","title":"[PLAN 10.15] Asupersync-First Integration Execution Track (8.4-8.6)","description":"Section: 10.15 — Asupersync-First Integration Execution Track (8.4-8.6)\n\nStrategic Context:\nAsupersync-first control-plane migration track enforcing Cx-first, region ownership, cancellation correctness, and protocol-grade verification gates.\n\nExecution Requirements:\n- Preserve all scoped capabilities from the canonical plan.\n- Keep contracts self-contained so future contributors can execute without reopening the master plan.\n- Require explicit testing strategy (unit + integration/e2e) and structured logging/telemetry for every child bead.\n- Require artifact-backed validation for performance, security, and correctness claims.\n\nDependency Semantics:\n- This epic is blocked by its child implementation beads.\n- Cross-epic dependencies encode strategic sequencing and canonical ownership boundaries.\n\n## Success Criteria\n- All child beads for this section are completed with acceptance artifacts attached and no unresolved blockers.\n- Section-level verification gate for comprehensive unit tests, integration/e2e workflows, and detailed structured logging evidence is green.\n- Scope-to-plan traceability is explicit, with no feature loss versus the canonical plan section.\n\n## Optimization Notes\n- User-Outcome Lens: \"[PLAN 10.15] Asupersync-First Integration Execution Track (8.4-8.6)\" must improve operator confidence, safety posture, and deterministic recovery behavior under both normal and adversarial conditions.\n- Cross-Section Coordination: child beads must encode integration assumptions explicitly to avoid local optimizations that degrade system-wide correctness.\n- Verification Non-Negotiable: completion requires reproducible unit/integration/E2E evidence and structured logs suitable for independent replay.\n- Scope Protection: any simplification must preserve canonical-plan feature intent and be justified with explicit tradeoff documentation.","status":"open","priority":1,"issue_type":"epic","created_at":"2026-02-20T07:36:41.453506694Z","created_by":"ubuntu","updated_at":"2026-02-20T08:16:40.108162887Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-15"],"dependencies":[{"issue_id":"bd-3qo","depends_on_id":"bd-145n","type":"blocks","created_at":"2026-02-20T07:37:00.746589551Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3qo","depends_on_id":"bd-15j6","type":"blocks","created_at":"2026-02-20T07:37:00.585278490Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3qo","depends_on_id":"bd-181w","type":"blocks","created_at":"2026-02-20T07:37:00.421192691Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3qo","depends_on_id":"bd-1cs7","type":"blocks","created_at":"2026-02-20T07:36:59.928358869Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3qo","depends_on_id":"bd-1cwp","type":"blocks","created_at":"2026-02-20T07:37:00.257980619Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3qo","depends_on_id":"bd-1f8m","type":"blocks","created_at":"2026-02-20T07:37:01.238015361Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3qo","depends_on_id":"bd-1hbw","type":"blocks","created_at":"2026-02-20T07:37:00.502995994Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3qo","depends_on_id":"bd-1id0","type":"blocks","created_at":"2026-02-20T07:36:59.511565245Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3qo","depends_on_id":"bd-1n5p","type":"blocks","created_at":"2026-02-20T07:37:00.009686516Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3qo","depends_on_id":"bd-1ow","type":"blocks","created_at":"2026-02-20T07:37:11.087554779Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3qo","depends_on_id":"bd-1xwz","type":"blocks","created_at":"2026-02-20T07:37:01.401126034Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3qo","depends_on_id":"bd-20eg","type":"blocks","created_at":"2026-02-20T07:48:15.670063999Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3qo","depends_on_id":"bd-2177","type":"blocks","created_at":"2026-02-20T07:36:59.599731671Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3qo","depends_on_id":"bd-25oa","type":"blocks","created_at":"2026-02-20T07:37:00.992189165Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3qo","depends_on_id":"bd-2g6r","type":"blocks","created_at":"2026-02-20T07:36:59.682462893Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3qo","depends_on_id":"bd-2h2s","type":"blocks","created_at":"2026-02-20T07:37:01.319393552Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3qo","depends_on_id":"bd-2tdi","type":"blocks","created_at":"2026-02-20T07:36:59.846593025Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3qo","depends_on_id":"bd-3014","type":"blocks","created_at":"2026-02-20T07:37:00.175342100Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3qo","depends_on_id":"bd-33kj","type":"blocks","created_at":"2026-02-20T07:37:01.482583323Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3qo","depends_on_id":"bd-3gnh","type":"blocks","created_at":"2026-02-20T07:37:01.155642186Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3qo","depends_on_id":"bd-3h63","type":"blocks","created_at":"2026-02-20T07:37:00.338797355Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3qo","depends_on_id":"bd-3tpg","type":"blocks","created_at":"2026-02-20T07:37:00.828209183Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3qo","depends_on_id":"bd-3u6o","type":"blocks","created_at":"2026-02-20T07:37:00.909187610Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3qo","depends_on_id":"bd-5rh","type":"blocks","created_at":"2026-02-20T07:37:11.126045505Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3qo","depends_on_id":"bd-721z","type":"blocks","created_at":"2026-02-20T07:36:59.766480300Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3qo","depends_on_id":"bd-cda","type":"blocks","created_at":"2026-02-20T07:37:09.623066235Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3qo","depends_on_id":"bd-cuut","type":"blocks","created_at":"2026-02-20T07:37:00.093553323Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3qo","depends_on_id":"bd-h93z","type":"blocks","created_at":"2026-02-20T07:37:01.072709589Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3qo","depends_on_id":"bd-tyr2","type":"blocks","created_at":"2026-02-20T07:37:00.665876499Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-3qsp","title":"[10.0] Section-wide verification gate: comprehensive unit+e2e+logging","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.0\n\nTask Objective:\nCreate a hard section completion gate that verifies all implemented behavior in this section with comprehensive unit tests, integration/e2e scripts, and detailed structured logging evidence.\n\nAcceptance Criteria:\n- Section test matrix covers happy-path, edge-case, and adversarial/error-path scenarios for all section deliverables.\n- E2E scripts execute representative end-user workflows and policy/control-plane transitions for this section.\n- Structured logs include stable event/error codes, trace correlation IDs, and enough context for deterministic replay triage.\n- Verification report is deterministic and machine-readable for CI/release gating.\n\nExpected Artifacts:\n- Section-level test matrix and coverage summary artifact.\n- E2E script suite with pass/fail outputs and reproducible fixtures/seeds.\n- Structured log validation report and traceability bundle.\n- Gate verdict artifact consumable by release automation.\n\n- Machine-readable verification artifact at `artifacts/section_10_0/bd-3qsp/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_0/bd-3qsp/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests must validate contracts, invariants, and failure semantics.\n- E2E tests must validate cross-component behavior from user/operator entrypoints to persisted effects.\n- Logging must support root-cause analysis without hidden context requirements.\n\nTask-Specific Clarification:\n- For \"[10.0] Section-wide verification gate: comprehensive unit+e2e+logging\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.0] Section-wide verification gate: comprehensive unit+e2e+logging\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.0] Section-wide verification gate: comprehensive unit+e2e+logging\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.0] Section-wide verification gate: comprehensive unit+e2e+logging\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.0] Section-wide verification gate: comprehensive unit+e2e+logging\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:48:05.309065965Z","created_by":"ubuntu","updated_at":"2026-02-20T08:43:53.562426330Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-0","test-obligations","verification-gate"],"dependencies":[{"issue_id":"bd-3qsp","depends_on_id":"bd-1dpd","type":"blocks","created_at":"2026-02-20T08:24:27.368025603Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3qsp","depends_on_id":"bd-1nf","type":"blocks","created_at":"2026-02-20T07:48:05.499625522Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3qsp","depends_on_id":"bd-1qp","type":"blocks","created_at":"2026-02-20T07:48:05.843287579Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3qsp","depends_on_id":"bd-2ac","type":"blocks","created_at":"2026-02-20T07:48:05.546095194Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3qsp","depends_on_id":"bd-2de","type":"blocks","created_at":"2026-02-20T07:48:05.793624600Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3qsp","depends_on_id":"bd-2g0","type":"blocks","created_at":"2026-02-20T07:48:05.453113871Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3qsp","depends_on_id":"bd-2twu","type":"blocks","created_at":"2026-02-20T08:20:53.412645689Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3qsp","depends_on_id":"bd-khy","type":"blocks","created_at":"2026-02-20T07:48:05.405752829Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3qsp","depends_on_id":"bd-mwf","type":"blocks","created_at":"2026-02-20T07:48:05.644826295Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3qsp","depends_on_id":"bd-uo4","type":"blocks","created_at":"2026-02-20T07:48:05.694144253Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3qsp","depends_on_id":"bd-y4g","type":"blocks","created_at":"2026-02-20T07:48:05.744199643Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3qsp","depends_on_id":"bd-yqz","type":"blocks","created_at":"2026-02-20T07:48:05.594045694Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-3rai","title":"[10.21] Implement signed lineage graph builder linking versions, maintainers, dependency graph deltas, and build pipeline transitions.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.21 — Behavioral Phenotype Evolution Tracker Execution Track (9O)\n\nWhy This Exists:\nBPET longitudinal phenotype intelligence track for pre-compromise trajectory detection and integration with DGIS/ATC/migration/economic policy.\n\nTask Objective:\nImplement signed lineage graph builder linking versions, maintainers, dependency graph deltas, and build pipeline transitions.\n\nAcceptance Criteria:\n- Lineage graph is replayable and tamper-evident; version ancestry, handoff events, and dependency pivot points are queryable with stable identifiers.\n\nExpected Artifacts:\n- `src/security/bpet/lineage_graph.rs`, `docs/specs/bpet_lineage_contract.md`, `artifacts/10.21/bpet_lineage_snapshot.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_21/bd-3rai/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_21/bd-3rai/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.21] Implement signed lineage graph builder linking versions, maintainers, dependency graph deltas, and build pipeline transitions.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.21] Implement signed lineage graph builder linking versions, maintainers, dependency graph deltas, and build pipeline transitions.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.21] Implement signed lineage graph builder linking versions, maintainers, dependency graph deltas, and build pipeline transitions.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.21] Implement signed lineage graph builder linking versions, maintainers, dependency graph deltas, and build pipeline transitions.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.21] Implement signed lineage graph builder linking versions, maintainers, dependency graph deltas, and build pipeline transitions.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"- Lineage graph is replayable and tamper-evident; version ancestry, handoff events, and dependency pivot points are queryable with stable identifiers.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:37:07.857879515Z","created_by":"ubuntu","updated_at":"2026-02-20T17:05:24.029450255Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-21","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-3rai","depends_on_id":"bd-39ga","type":"blocks","created_at":"2026-02-20T17:05:24.029400712Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-3rc","title":"[PLAN 10.7] Conformance + Verification","description":"Section: 10.7 — Conformance + Verification\n\nStrategic Context:\nConformance and verification evidence stack for compatibility, trust protocols, fuzzing, and external reproducibility.\n\nExecution Requirements:\n- Preserve all scoped capabilities from the canonical plan.\n- Keep contracts self-contained so future contributors can execute without reopening the master plan.\n- Require explicit testing strategy (unit + integration/e2e) and structured logging/telemetry for every child bead.\n- Require artifact-backed validation for performance, security, and correctness claims.\n\nDependency Semantics:\n- This epic is blocked by its child implementation beads.\n- Cross-epic dependencies encode strategic sequencing and canonical ownership boundaries.\n\n## Success Criteria\n- All child beads for this section are completed with acceptance artifacts attached and no unresolved blockers.\n- Section-level verification gate for comprehensive unit tests, integration/e2e workflows, and detailed structured logging evidence is green.\n- Scope-to-plan traceability is explicit, with no feature loss versus the canonical plan section.\n\n## Optimization Notes\n- User-Outcome Lens: \"[PLAN 10.7] Conformance + Verification\" must improve operator confidence, safety posture, and deterministic recovery behavior under both normal and adversarial conditions.\n- Cross-Section Coordination: child beads must encode integration assumptions explicitly to avoid local optimizations that degrade system-wide correctness.\n- Verification Non-Negotiable: completion requires reproducible unit/integration/E2E evidence and structured logs suitable for independent replay.\n- Scope Protection: any simplification must preserve canonical-plan feature intent and be justified with explicit tradeoff documentation.","status":"open","priority":2,"issue_type":"epic","created_at":"2026-02-20T07:36:40.788808523Z","created_by":"ubuntu","updated_at":"2026-02-20T08:16:48.822854764Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-7"],"dependencies":[{"issue_id":"bd-3rc","depends_on_id":"bd-1rwq","type":"blocks","created_at":"2026-02-20T07:48:25.814626403Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3rc","depends_on_id":"bd-1ta","type":"blocks","created_at":"2026-02-20T07:37:10.356942993Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3rc","depends_on_id":"bd-1u4","type":"blocks","created_at":"2026-02-20T07:36:47.543636392Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3rc","depends_on_id":"bd-1ul","type":"blocks","created_at":"2026-02-20T07:36:47.464082679Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3rc","depends_on_id":"bd-229","type":"blocks","created_at":"2026-02-20T07:37:10.318911362Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3rc","depends_on_id":"bd-2ja","type":"blocks","created_at":"2026-02-20T07:36:47.301407958Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3rc","depends_on_id":"bd-2pu","type":"blocks","created_at":"2026-02-20T07:36:47.706582098Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3rc","depends_on_id":"bd-3ex","type":"blocks","created_at":"2026-02-20T07:36:47.627010682Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3rc","depends_on_id":"bd-3il","type":"blocks","created_at":"2026-02-20T07:37:10.280281627Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3rc","depends_on_id":"bd-5rh","type":"blocks","created_at":"2026-02-20T07:37:10.395352628Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3rc","depends_on_id":"bd-cda","type":"blocks","created_at":"2026-02-20T07:37:09.309377883Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3rc","depends_on_id":"bd-s6y","type":"blocks","created_at":"2026-02-20T07:36:47.381263934Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-3rp","title":"Build CLI scaffold with clap for franken-node binary","status":"closed","priority":1,"issue_type":"task","assignee":"CoralReef","created_at":"2026-02-20T07:29:16.130665777Z","created_by":"ubuntu","updated_at":"2026-02-20T07:32:07.416297382Z","closed_at":"2026-02-20T07:32:07.416275331Z","close_reason":"CLI scaffold implemented: 10 top-level commands (init, run, migrate, verify, trust, fleet, incident, registry, bench, doctor) with full argument parsing via clap derive. All subcommands match README CLI reference. Compiles clean, clippy clean, fmt clean.","source_repo":".","compaction_level":0,"original_size":0,"labels":["cli","foundation"]}
{"id":"bd-3rya","title":"[10.14] Implement monotonic hardening mode state machine with one-way escalation semantics.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.14 — FrankenSQLite Deep-Mined Expansion Execution Track (9J)\n\nWhy This Exists:\nFrankenSQLite deep-mined control integrity track: evidence ledgers, proofs, epochs, remote effects, markers/MMR, and deterministic fault labs.\n\nTask Objective:\nImplement monotonic hardening mode state machine with one-way escalation semantics.\n\nAcceptance Criteria:\n- Hardening transitions are monotonic unless explicit governance rollback artifact is present; state transitions are durable and replayable; illegal regressions are rejected.\n\nExpected Artifacts:\n- `src/policy/hardening_state_machine.rs`, `tests/security/monotonic_hardening.rs`, `artifacts/10.14/hardening_state_history.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_14/bd-3rya/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_14/bd-3rya/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.14] Implement monotonic hardening mode state machine with one-way escalation semantics.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.14] Implement monotonic hardening mode state machine with one-way escalation semantics.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.14] Implement monotonic hardening mode state machine with one-way escalation semantics.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.14] Implement monotonic hardening mode state machine with one-way escalation semantics.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.14] Implement monotonic hardening mode state machine with one-way escalation semantics.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"Hardening transitions are monotonic unless explicit governance rollback artifact is present; state transitions are durable and replayable; illegal regressions are rejected.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:36:56.036380415Z","created_by":"ubuntu","updated_at":"2026-02-20T16:23:10.294846441Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-14","self-contained","test-obligations"]}
{"id":"bd-3se1","title":"[11] Contract field: change summary","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 11 — Evidence And Decision Contracts (Mandatory)\nContract Field: 1 of 8 — Change Summary\n\nWhy This Exists:\nSection 11 establishes mandatory evidence contracts for every major subsystem proposal. The 8 required fields are: (1) change summary, (2) compatibility and threat evidence, (3) EV score and tier, (4) expected-loss model, (5) fallback trigger, (6) rollout wedge, (7) rollback command, (8) benchmark and correctness artifacts. The enforcement rule is: NO CONTRACT, NO MERGE. This bead covers field #1: the change summary requirement.\n\nTask Objective:\nImplement the change summary contract requirement: every major subsystem proposal must include a concise, structured change summary with scope, affected contracts, and operational impact.\n\nDetailed Acceptance Criteria:\n1. Change summary template defined with required fields: scope (affected modules/APIs), affected contracts (which beads/specifications this changes), operational impact (what operators need to know), risk delta (how this changes the risk profile).\n2. Template is machine-parseable (structured YAML/TOML/JSON in PR description or companion file).\n3. CI gate enforces presence and completeness of change summary on PRs touching subsystem code.\n4. Change summary links to relevant section beads and contract documents for traceability.\n5. Summary includes backward-compatibility assessment: does this change any existing contracts?\n6. Summary includes forward-compatibility note: does this change enable or block future planned work?\n\nKey Dependencies:\n- Depends on all execution tracks (10.0-10.21) for enforcement — contracts must be applied to real changes.\n- This bead is a prerequisite for all other Section 11 contract fields.\n- The no-contract-no-merge gate (bd-2ut3) depends on all 8 contract field beads.\n\nExpected Artifacts:\n- docs/templates/change_summary_template.md — structured template.\n- CI gate configuration for contract enforcement.\n- Example change summary from a representative subsystem change.\n- artifacts/section_11/bd-3se1/verification_evidence.json\n\nTesting and Logging Requirements:\n- Unit tests: template schema validation (valid/invalid change summaries).\n- Integration tests: CI gate blocks PRs with missing/incomplete change summaries.\n- E2E tests: full PR workflow with change summary -> CI gate -> merge.\n- Structured logs: CONTRACT_CHANGE_SUMMARY_VALIDATED, CONTRACT_MISSING, CONTRACT_INCOMPLETE with PR metadata and trace IDs.","acceptance_criteria":"1. Every proposal/PR includes a structured change-summary field in the evidence contract header.\n2. The change summary must contain: (a) one-line intent statement, (b) list of affected subsystems/modules, (c) surface area delta (new APIs, removed APIs, changed signatures), (d) dependency changes if any.\n3. A CI lint rejects any proposal missing the change-summary field or any of its sub-fields.\n4. Unit test: a mock proposal with all fields present passes validation; one missing any sub-field fails.\n5. The change summary is machine-parseable (JSON or YAML front-matter) so downstream tooling can consume it.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:39:32.476069866Z","created_by":"ubuntu","updated_at":"2026-02-20T16:07:09.152633341Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-11","self-contained","test-obligations"]}
{"id":"bd-3t08","title":"[10.17] Section-wide verification gate: comprehensive unit+e2e+logging","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.17\n\nTask Objective:\nCreate a hard section completion gate that verifies all implemented behavior in this section with comprehensive unit tests, integration/e2e scripts, and detailed structured logging evidence.\n\nAcceptance Criteria:\n- Section test matrix covers happy-path, edge-case, and adversarial/error-path scenarios for all section deliverables.\n- E2E scripts execute representative end-user workflows and policy/control-plane transitions for this section.\n- Structured logs include stable event/error codes, trace correlation IDs, and enough context for deterministic replay triage.\n- Verification report is deterministic and machine-readable for CI/release gating.\n\nExpected Artifacts:\n- Section-level test matrix and coverage summary artifact.\n- E2E script suite with pass/fail outputs and reproducible fixtures/seeds.\n- Structured log validation report and traceability bundle.\n- Gate verdict artifact consumable by release automation.\n\n- Machine-readable verification artifact at `artifacts/section_10_17/bd-3t08/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_17/bd-3t08/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests must validate contracts, invariants, and failure semantics.\n- E2E tests must validate cross-component behavior from user/operator entrypoints to persisted effects.\n- Logging must support root-cause analysis without hidden context requirements.\n\nTask-Specific Clarification:\n- For \"[10.17] Section-wide verification gate: comprehensive unit+e2e+logging\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.17] Section-wide verification gate: comprehensive unit+e2e+logging\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.17] Section-wide verification gate: comprehensive unit+e2e+logging\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.17] Section-wide verification gate: comprehensive unit+e2e+logging\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.17] Section-wide verification gate: comprehensive unit+e2e+logging\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:48:16.943916897Z","created_by":"ubuntu","updated_at":"2026-02-20T08:43:51.425912183Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-17","test-obligations","verification-gate"],"dependencies":[{"issue_id":"bd-3t08","depends_on_id":"bd-1dpd","type":"blocks","created_at":"2026-02-20T08:24:26.149551545Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3t08","depends_on_id":"bd-1nl1","type":"blocks","created_at":"2026-02-20T07:48:17.740999805Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3t08","depends_on_id":"bd-1xbc","type":"blocks","created_at":"2026-02-20T07:48:17.644349328Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3t08","depends_on_id":"bd-21fo","type":"blocks","created_at":"2026-02-20T07:48:17.334312960Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3t08","depends_on_id":"bd-26mk","type":"blocks","created_at":"2026-02-20T07:48:17.388792190Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3t08","depends_on_id":"bd-274s","type":"blocks","created_at":"2026-02-20T07:48:17.693220864Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3t08","depends_on_id":"bd-2iyk","type":"blocks","created_at":"2026-02-20T07:48:17.235791188Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3t08","depends_on_id":"bd-2kd9","type":"blocks","created_at":"2026-02-20T07:48:17.041981697Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3t08","depends_on_id":"bd-2o8b","type":"blocks","created_at":"2026-02-20T07:48:17.139438656Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3t08","depends_on_id":"bd-2twu","type":"blocks","created_at":"2026-02-20T08:20:51.947807078Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3t08","depends_on_id":"bd-383z","type":"blocks","created_at":"2026-02-20T07:48:17.089793810Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3t08","depends_on_id":"bd-3ku8","type":"blocks","created_at":"2026-02-20T07:48:17.595255839Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3t08","depends_on_id":"bd-3l2p","type":"blocks","created_at":"2026-02-20T07:48:17.286109428Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3t08","depends_on_id":"bd-al8i","type":"blocks","created_at":"2026-02-20T07:48:17.439502851Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3t08","depends_on_id":"bd-gad3","type":"blocks","created_at":"2026-02-20T07:48:17.546454985Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3t08","depends_on_id":"bd-kcg9","type":"blocks","created_at":"2026-02-20T07:48:17.494071919Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3t08","depends_on_id":"bd-nbwo","type":"blocks","created_at":"2026-02-20T07:48:17.187204082Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-3tpg","title":"[10.15] Enforce canonical all-point cancellation injection gate (from `10.14`) for critical control workflows.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.15 — Asupersync-First Integration Execution Track (8.4-8.6)\n\nWhy This Exists:\nAsupersync-first control-plane migration track enforcing Cx-first, region ownership, cancellation correctness, and protocol-grade verification gates.\n\nTask Objective:\nEnforce canonical all-point cancellation injection gate (from `10.14`) for critical control workflows.\n\nAcceptance Criteria:\n- Canonical cancellation injection runs on every critical protocol flow; no obligation leaks, no half-commit outcomes, no quiescence violations.\n\nExpected Artifacts:\n- `tests/lab/control_cancellation_injection.rs`, `artifacts/10.15/control_cancel_injection_report.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_15/bd-3tpg/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_15/bd-3tpg/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.15] Enforce canonical all-point cancellation injection gate (from `10.14`) for critical control workflows.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.15] Enforce canonical all-point cancellation injection gate (from `10.14`) for critical control workflows.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.15] Enforce canonical all-point cancellation injection gate (from `10.14`) for critical control workflows.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.15] Enforce canonical all-point cancellation injection gate (from `10.14`) for critical control workflows.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.15] Enforce canonical all-point cancellation injection gate (from `10.14`) for critical control workflows.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"- Canonical cancellation injection runs on every critical protocol flow; no obligation leaks, no half-commit outcomes, no quiescence violations.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:37:00.790939581Z","created_by":"ubuntu","updated_at":"2026-02-20T17:00:44.248267306Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-15","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-3tpg","depends_on_id":"bd-876n","type":"blocks","created_at":"2026-02-20T14:59:37.895649831Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-3tzl","title":"[10.13] Add bounded parser/resource-accounting guardrails on control-channel frame decode.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.13 — FCP Deep-Mined Expansion Execution Track (9I)\n\nWhy This Exists:\nDeep connector/provider contract track: lifecycle FSMs, conformance harnesses, fencing, sandboxing, revocation freshness, and protocol hardening.\n\nTask Objective:\nAdd bounded parser/resource-accounting guardrails on control-channel frame decode.\n\nAcceptance Criteria:\n- Decode path enforces byte/CPU/allocation ceilings; oversized/malformed frames fail fast; parse budgets are reflected in telemetry.\n\nExpected Artifacts:\n- `docs/specs/control_channel_parser_limits.md`, `tests/security/parser_budget_guardrails.rs`, `artifacts/10.13/parser_guardrail_metrics.csv`.\n\n- Machine-readable verification artifact at `artifacts/section_10_13/bd-3tzl/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_13/bd-3tzl/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.13] Add bounded parser/resource-accounting guardrails on control-channel frame decode.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.13] Add bounded parser/resource-accounting guardrails on control-channel frame decode.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.13] Add bounded parser/resource-accounting guardrails on control-channel frame decode.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.13] Add bounded parser/resource-accounting guardrails on control-channel frame decode.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.13] Add bounded parser/resource-accounting guardrails on control-channel frame decode.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","status":"closed","priority":1,"issue_type":"task","assignee":"CrimsonCrane","created_at":"2026-02-20T07:36:54.496150518Z","created_by":"ubuntu","updated_at":"2026-02-20T13:12:05.795600879Z","closed_at":"2026-02-20T13:12:05.795572466Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-13","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-3tzl","depends_on_id":"bd-v97o","type":"blocks","created_at":"2026-02-20T07:43:13.853864652Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-3u2o","title":"[10.16] Add substrate conformance gate in CI to block non-compliant feature merges.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.16 — Adjacent Substrate Integration Execution Track (8.7)\n\nWhy This Exists:\nAdjacent substrate integration track to enforce deep composition with frankentui, frankensqlite, sqlmodel_rust, and fastapi_rust.\n\nTask Objective:\nAdd substrate conformance gate in CI to block non-compliant feature merges.\n\nAcceptance Criteria:\n- CI detects relevant-feature noncompliance with substrate policy; failures include remediation hints and waiver path.\n\nExpected Artifacts:\n- `.github/workflows/adjacent-substrate-gate.yml`, `tests/conformance/adjacent_substrate_gate.rs`, `artifacts/10.16/adjacent_substrate_gate_report.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_16/bd-3u2o/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_16/bd-3u2o/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.16] Add substrate conformance gate in CI to block non-compliant feature merges.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.16] Add substrate conformance gate in CI to block non-compliant feature merges.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.16] Add substrate conformance gate in CI to block non-compliant feature merges.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.16] Add substrate conformance gate in CI to block non-compliant feature merges.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.16] Add substrate conformance gate in CI to block non-compliant feature merges.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"- CI detects relevant-feature noncompliance with substrate policy; failures include remediation hints and waiver path.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:37:02.600606433Z","created_by":"ubuntu","updated_at":"2026-02-20T17:05:34.135023422Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-16","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-3u2o","depends_on_id":"bd-2owx","type":"blocks","created_at":"2026-02-20T17:05:34.134974090Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-3u4","title":"[10.11] Implement BOCPD regime detector for workload/incident stream shifts.","description":"[10.11] Implement BOCPD regime detector for workload/incident stream shifts.\n\n## Why This Exists\n\nSection 9G.5 requires automated detection of regime shifts in operational streams. Workload patterns, incident rates, trust-signal distributions, and error frequencies can shift abruptly due to deployments, attacks, infrastructure changes, or organic growth. Human operators often detect these shifts too late — after SLO violations or security incidents have already occurred. Bayesian Online Changepoint Detection (BOCPD) provides a principled, low-latency mechanism for detecting these shifts as they happen, enabling proactive policy recalibration and early alerting.\n\n## What It Must Do\n\n1. **BOCPD core algorithm.** Implement the Adams & MacKay (2007) Bayesian Online Changepoint Detection algorithm. The implementation must support configurable hazard functions (constant hazard rate is the default, with geometric and custom hazard as options). The posterior over run lengths must be maintained efficiently using the standard message-passing scheme.\n\n2. **Observation models.** Support at least three conjugate observation models:\n   - **Gaussian** (normal-inverse-gamma prior): for latency, throughput, and continuous metrics.\n   - **Poisson** (gamma prior): for incident counts, error counts, and event rates.\n   - **Categorical** (Dirichlet prior): for trust-signal class distributions and error code distributions.\n   Each model must support online parameter updates without storing the full history.\n\n3. **Changepoint signaling.** When the posterior probability of a changepoint exceeds a configurable threshold (default: 0.7), emit a `regime_shift_detected` event containing: stream name, timestamp, confidence, old regime summary statistics, new regime preliminary statistics, and recommended actions.\n\n4. **Multi-stream correlation.** Support monitoring multiple streams simultaneously. When changepoints are detected in multiple correlated streams within a configurable time window (default: 60 seconds), emit a `correlated_regime_shift` event that links the individual detections, suggesting a common root cause.\n\n5. **Policy recalibration hooks.** When a regime shift is detected, trigger configurable policy recalibration actions: adjust admission budgets, modify monitoring intervals, escalate alerting thresholds, or trigger diagnostic deep-scans (VOI-budgeted per bd-2nt).\n\n6. **False positive control.** Implement a minimum run-length filter: changepoints are only signaled if the new regime persists for at least `min_run_length` observations (default: 10). This prevents spurious alerts from transient spikes.\n\n7. **Regime history log.** Maintain a bounded history of detected regimes with their start/end timestamps, summary statistics, and confidence scores. This history is queryable for post-incident analysis.\n\n## Acceptance Criteria\n\n1. BOCPD algorithm implemented in `crates/franken-node/src/connector/bocpd.rs` with constant, geometric, and custom hazard functions.\n2. All three observation models (Gaussian, Poisson, Categorical) implemented and tested.\n3. Changepoint detection correctly identifies regime shifts in synthetic test streams with < 5% false positive rate and < 10% false negative rate on reference benchmarks.\n4. Multi-stream correlation links concurrent changepoints within the configured window.\n5. Policy recalibration hooks are invoked on regime shift detection.\n6. Minimum run-length filter suppresses transient false positives.\n7. Regime history log is queryable and bounded (configurable max entries, default: 1000).\n8. Verification script `scripts/check_bocpd.py` with `--json` flag confirms all criteria.\n9. Evidence artifacts written to `artifacts/section_10_11/bd-3u4/`.\n\n## Key Dependencies\n\n- bd-2nt (VOI-budgeted monitoring) — regime shifts can trigger diagnostic deep-scans.\n- 10.13 telemetry namespace — regime shift events must conform to telemetry schema.\n- 10.13 stable error namespace — detection errors use registered codes.\n- Statistical libraries: `statrs` crate for probability distributions (or equivalent pure-Rust impl).\n\n## Testing & Logging Requirements\n\n- Unit tests covering each observation model's parameter update and predictive distribution.\n- Integration tests with synthetic regime-shift streams (step change, gradual drift, periodic shift).\n- Golden vector tests with known changepoint locations and expected detection times.\n- Self-test mode that generates a synthetic stream with a known changepoint and confirms detection.\n- Structured logging: `bocpd.observation`, `bocpd.changepoint_candidate`, `bocpd.regime_shift_detected`, `bocpd.correlated_shift`, `bocpd.false_positive_suppressed` events.\n\n## Expected Artifacts\n\n- `docs/specs/section_10_11/bd-3u4_contract.md` — specification document.\n- `crates/franken-node/src/connector/bocpd.rs` — Rust implementation.\n- `scripts/check_bocpd.py` — verification script.\n- `tests/test_check_bocpd.py` — unit tests.\n- `vectors/bocpd_regime_shifts.json` — golden test vectors.\n- `artifacts/section_10_11/bd-3u4/verification_evidence.json` — evidence.\n- `artifacts/section_10_11/bd-3u4/verification_summary.md` — summary.","acceptance_criteria":"AC for bd-3u4:\n1. Implement a BOCPD (Bayesian Online Changepoint Detection) regime detector that processes streaming workload metrics and incident event streams to detect distributional shifts (regime changes) in real time.\n2. The detector maintains a run-length distribution P(r_t | x_{1:t}) updated incrementally with each new observation; the underlying predictive model uses a conjugate-exponential hazard function with configurable prior parameters (hazard_lambda, prior_mean, prior_variance).\n3. A regime change is signaled when the posterior probability of run-length r=0 (new regime) exceeds a configurable threshold (default: 0.5); the signal includes the estimated changepoint timestamp, confidence score, and pre/post regime summary statistics.\n4. The detector supports multiple concurrent streams (e.g., request_rate, error_rate, latency_p99) with independent run-length distributions; a meta-detector can fuse per-stream signals into a joint regime-change verdict.\n5. Detection latency is bounded: the detector must signal a regime change within K observations of the true changepoint (K configurable, default: 10) on synthetic step-function and ramp-function test signals.\n6. Memory is bounded: the run-length distribution is truncated at a configurable max_run_length (default: 500) to prevent unbounded growth; truncation error is logged as BOCPD_TRUNCATION_WARNING when the tail probability exceeds 1e-4.\n7. Unit tests verify: (a) step-function changepoint detected within K observations, (b) ramp-function changepoint detected within 2K observations, (c) stationary stream produces no false positives over 10,000 observations, (d) truncation at max_run_length does not cause missed detections on standard test signals, (e) multi-stream fusion correctly requires majority agreement.\n8. Deterministic lab runtime (bd-2ko) scenario fixtures exercise: sudden load spike, gradual degradation, and oscillating workload patterns.\n9. Structured log events: BOCPD_REGIME_CHANGE / BOCPD_OBSERVATION / BOCPD_TRUNCATION_WARNING with stream_id, run_length, posterior_probability, and trace correlation ID.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:36:50.306945145Z","created_by":"ubuntu","updated_at":"2026-02-20T17:01:40.504892300Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-11","self-contained","test-obligations"]}
{"id":"bd-3u6o","title":"[10.15] Enforce canonical virtual transport fault harness (from `10.14`) for distributed control protocols.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.15 — Asupersync-First Integration Execution Track (8.4-8.6)\n\nWhy This Exists:\nAsupersync-first control-plane migration track enforcing Cx-first, region ownership, cancellation correctness, and protocol-grade verification gates.\n\nTask Objective:\nEnforce canonical virtual transport fault harness (from `10.14`) for distributed control protocols.\n\nAcceptance Criteria:\n- Canonical harness scenarios are deterministic by seed, adopted by control-plane gates, and reproduce distributed protocol decisions and failures.\n\nExpected Artifacts:\n- `tests/harness/control_virtual_transport_faults.rs`, `docs/testing/control_virtual_transport_faults.md`, `artifacts/10.15/control_fault_harness_summary.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_15/bd-3u6o/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_15/bd-3u6o/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.15] Enforce canonical virtual transport fault harness (from `10.14`) for distributed control protocols.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.15] Enforce canonical virtual transport fault harness (from `10.14`) for distributed control protocols.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.15] Enforce canonical virtual transport fault harness (from `10.14`) for distributed control protocols.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.15] Enforce canonical virtual transport fault harness (from `10.14`) for distributed control protocols.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.15] Enforce canonical virtual transport fault harness (from `10.14`) for distributed control protocols.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"- Canonical harness scenarios are deterministic by seed, adopted by control-plane gates, and reproduce distributed protocol decisions and failures.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:37:00.871954887Z","created_by":"ubuntu","updated_at":"2026-02-20T17:00:44.387727027Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-15","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-3u6o","depends_on_id":"bd-2qqu","type":"blocks","created_at":"2026-02-20T14:59:45.738976001Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-3ua7","title":"[10.13] Implement sandbox profile system (`strict`, `strict_plus`, `moderate`, `permissive`) with policy compiler.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.13 — FCP Deep-Mined Expansion Execution Track (9I)\n\nWhy This Exists:\nDeep connector/provider contract track: lifecycle FSMs, conformance harnesses, fencing, sandboxing, revocation freshness, and protocol hardening.\n\nTask Objective:\nImplement sandbox profile system (`strict`, `strict_plus`, `moderate`, `permissive`) with policy compiler.\n\nAcceptance Criteria:\n- Profile compiler emits enforceable low-level policy for each tier; profile downgrade attempts are blocked by policy; profile selection is auditable.\n\nExpected Artifacts:\n- `src/security/sandbox_policy_compiler.rs`, `docs/specs/sandbox_profiles.md`, `artifacts/10.13/sandbox_profile_compiler_output.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_13/bd-3ua7/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_13/bd-3ua7/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.13] Implement sandbox profile system (`strict`, `strict_plus`, `moderate`, `permissive`) with policy compiler.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.13] Implement sandbox profile system (`strict`, `strict_plus`, `moderate`, `permissive`) with policy compiler.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.13] Implement sandbox profile system (`strict`, `strict_plus`, `moderate`, `permissive`) with policy compiler.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.13] Implement sandbox profile system (`strict`, `strict_plus`, `moderate`, `permissive`) with policy compiler.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.13] Implement sandbox profile system (`strict`, `strict_plus`, `moderate`, `permissive`) with policy compiler.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","status":"closed","priority":1,"issue_type":"task","assignee":"CrimsonCrane","created_at":"2026-02-20T07:36:52.120604764Z","created_by":"ubuntu","updated_at":"2026-02-20T11:12:22.101895921Z","closed_at":"2026-02-20T11:12:22.101868630Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-13","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-3ua7","depends_on_id":"bd-b44","type":"blocks","created_at":"2026-02-20T07:43:12.610690751Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-3uoo","title":"[10.13] Section-wide verification gate: comprehensive unit+e2e+logging","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.13\n\nTask Objective:\nCreate a hard section completion gate that verifies all implemented behavior in this section with comprehensive unit tests, integration/e2e scripts, and detailed structured logging evidence.\n\nAcceptance Criteria:\n- Section test matrix covers happy-path, edge-case, and adversarial/error-path scenarios for all section deliverables.\n- E2E scripts execute representative end-user workflows and policy/control-plane transitions for this section.\n- Structured logs include stable event/error codes, trace correlation IDs, and enough context for deterministic replay triage.\n- Verification report is deterministic and machine-readable for CI/release gating.\n\nExpected Artifacts:\n- Section-level test matrix and coverage summary artifact.\n- E2E script suite with pass/fail outputs and reproducible fixtures/seeds.\n- Structured log validation report and traceability bundle.\n- Gate verdict artifact consumable by release automation.\n\n- Machine-readable verification artifact at `artifacts/section_10_13/bd-3uoo/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_13/bd-3uoo/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests must validate contracts, invariants, and failure semantics.\n- E2E tests must validate cross-component behavior from user/operator entrypoints to persisted effects.\n- Logging must support root-cause analysis without hidden context requirements.\n\nTask-Specific Clarification:\n- For \"[10.13] Section-wide verification gate: comprehensive unit+e2e+logging\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.13] Section-wide verification gate: comprehensive unit+e2e+logging\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.13] Section-wide verification gate: comprehensive unit+e2e+logging\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.13] Section-wide verification gate: comprehensive unit+e2e+logging\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.13] Section-wide verification gate: comprehensive unit+e2e+logging\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","status":"closed","priority":1,"issue_type":"task","assignee":"CrimsonCrane","created_at":"2026-02-20T07:48:09.057548812Z","created_by":"ubuntu","updated_at":"2026-02-20T14:58:09.613392242Z","closed_at":"2026-02-20T14:58:09.613366023Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-13","test-obligations","verification-gate"],"dependencies":[{"issue_id":"bd-3uoo","depends_on_id":"bd-12h8","type":"blocks","created_at":"2026-02-20T07:48:09.579670911Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3uoo","depends_on_id":"bd-17mb","type":"blocks","created_at":"2026-02-20T07:48:10.671119018Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3uoo","depends_on_id":"bd-18o","type":"blocks","created_at":"2026-02-20T07:48:11.122058666Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3uoo","depends_on_id":"bd-19u","type":"blocks","created_at":"2026-02-20T07:48:11.001470785Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3uoo","depends_on_id":"bd-1cm","type":"blocks","created_at":"2026-02-20T07:48:11.070824520Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3uoo","depends_on_id":"bd-1d7n","type":"blocks","created_at":"2026-02-20T07:48:10.436300577Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3uoo","depends_on_id":"bd-1dpd","type":"blocks","created_at":"2026-02-20T08:24:26.684476571Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3uoo","depends_on_id":"bd-1gnb","type":"blocks","created_at":"2026-02-20T07:48:09.348117250Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3uoo","depends_on_id":"bd-1h6","type":"blocks","created_at":"2026-02-20T07:48:11.215188585Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3uoo","depends_on_id":"bd-1m8r","type":"blocks","created_at":"2026-02-20T07:48:10.281551108Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3uoo","depends_on_id":"bd-1nk5","type":"blocks","created_at":"2026-02-20T07:48:10.717897355Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3uoo","depends_on_id":"bd-1p2b","type":"blocks","created_at":"2026-02-20T07:48:09.625556565Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3uoo","depends_on_id":"bd-1rk","type":"blocks","created_at":"2026-02-20T07:48:11.262083399Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3uoo","depends_on_id":"bd-1ugy","type":"blocks","created_at":"2026-02-20T07:48:09.439289924Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3uoo","depends_on_id":"bd-1vvs","type":"blocks","created_at":"2026-02-20T07:48:10.812923075Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3uoo","depends_on_id":"bd-1z9s","type":"blocks","created_at":"2026-02-20T07:48:10.529322585Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3uoo","depends_on_id":"bd-24s","type":"blocks","created_at":"2026-02-20T07:48:10.950663333Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3uoo","depends_on_id":"bd-29ct","type":"blocks","created_at":"2026-02-20T07:48:09.203505744Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3uoo","depends_on_id":"bd-29w6","type":"blocks","created_at":"2026-02-20T07:48:09.913344775Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3uoo","depends_on_id":"bd-2eun","type":"blocks","created_at":"2026-02-20T07:48:09.718720057Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3uoo","depends_on_id":"bd-2gh","type":"blocks","created_at":"2026-02-20T07:48:11.309242986Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3uoo","depends_on_id":"bd-2k74","type":"blocks","created_at":"2026-02-20T07:48:09.813875579Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3uoo","depends_on_id":"bd-2m2b","type":"blocks","created_at":"2026-02-20T07:48:10.766822721Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3uoo","depends_on_id":"bd-2t5u","type":"blocks","created_at":"2026-02-20T07:48:09.959896239Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3uoo","depends_on_id":"bd-2twu","type":"blocks","created_at":"2026-02-20T08:20:52.585437456Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3uoo","depends_on_id":"bd-2vs4","type":"blocks","created_at":"2026-02-20T07:48:10.144899037Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3uoo","depends_on_id":"bd-2yc4","type":"blocks","created_at":"2026-02-20T07:48:10.388793993Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3uoo","depends_on_id":"bd-35by","type":"blocks","created_at":"2026-02-20T07:48:09.251232357Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3uoo","depends_on_id":"bd-35q1","type":"blocks","created_at":"2026-02-20T07:48:10.574614183Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3uoo","depends_on_id":"bd-3b8m","type":"blocks","created_at":"2026-02-20T07:48:09.767184805Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3uoo","depends_on_id":"bd-3cm3","type":"blocks","created_at":"2026-02-20T07:48:09.671808401Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3uoo","depends_on_id":"bd-3en","type":"blocks","created_at":"2026-02-20T07:48:11.167972443Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3uoo","depends_on_id":"bd-3i9o","type":"blocks","created_at":"2026-02-20T07:48:10.483292972Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3uoo","depends_on_id":"bd-3n2u","type":"blocks","created_at":"2026-02-20T07:48:09.157882057Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3uoo","depends_on_id":"bd-3n58","type":"blocks","created_at":"2026-02-20T07:48:10.622826671Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3uoo","depends_on_id":"bd-3tzl","type":"blocks","created_at":"2026-02-20T07:48:09.487619811Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3uoo","depends_on_id":"bd-3ua7","type":"blocks","created_at":"2026-02-20T07:48:10.858573291Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3uoo","depends_on_id":"bd-8uvb","type":"blocks","created_at":"2026-02-20T07:48:10.099431451Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3uoo","depends_on_id":"bd-8vby","type":"blocks","created_at":"2026-02-20T07:48:10.053066884Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3uoo","depends_on_id":"bd-91gg","type":"blocks","created_at":"2026-02-20T07:48:09.860890276Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3uoo","depends_on_id":"bd-b44","type":"blocks","created_at":"2026-02-20T07:48:10.904491536Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3uoo","depends_on_id":"bd-bq6y","type":"blocks","created_at":"2026-02-20T07:48:10.190145049Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3uoo","depends_on_id":"bd-ck2h","type":"blocks","created_at":"2026-02-20T07:48:09.299046223Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3uoo","depends_on_id":"bd-jxgt","type":"blocks","created_at":"2026-02-20T07:48:10.005938485Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3uoo","depends_on_id":"bd-novi","type":"blocks","created_at":"2026-02-20T07:48:09.393705110Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3uoo","depends_on_id":"bd-v97o","type":"blocks","created_at":"2026-02-20T07:48:09.534477596Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3uoo","depends_on_id":"bd-w0jq","type":"blocks","created_at":"2026-02-20T07:48:10.235406481Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3uoo","depends_on_id":"bd-y7lu","type":"blocks","created_at":"2026-02-20T07:48:10.328102442Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-3v8f","title":"[11] Contract field: fallback trigger","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 11\n\nTask Objective:\nRequire deterministic fallback trigger contract for each major subsystem change.\n\nExecution Requirements:\n- Make this requirement machine-verifiable and release-gated where applicable.\n- Keep behavior self-documenting so future contributors do not need to re-open the master plan.\n\nTesting & Logging Requirements:\n- Unit tests for rule/contract logic and error conditions.\n- Integration/E2E tests for workflow-level enforcement.\n- Structured logs with stable codes and trace IDs.\n- Reproducible artifacts that prove contract satisfaction.\n\nAcceptance Criteria:\n- Success and failure invariants for [11] Contract field: fallback trigger are explicitly quantified and machine-verifiable.\n- Determinism requirements for [11] Contract field: fallback trigger are validated across repeated runs and adversarial perturbations.\n\n\nExpected Artifacts:\n- Machine-readable verification artifact with explicit canonical path under artifacts/section_11/bd-3v8f/verification_evidence.json, suitable for CI/release gating.\n- Human-readable verification summary with explicit canonical path under artifacts/section_11/bd-3v8f/verification_summary.md, linking implementation intent to measured validation outcomes.\n\nTask-Specific Clarification:\n- The implementation must deliver the exact capability named in the title, with no scope dilution and no silent feature omission.\n- Validation artifacts must include capability-specific thresholds, pass/fail criteria, and reproducible evidence bundles tied to this bead ID.\n- Program/section verification gates must be able to consume this bead's outputs without manual interpretation.\n\nWhy This Improves User Outcomes:\n- \"[11] Contract field: fallback trigger\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[11] Contract field: fallback trigger\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"1. Every evidence contract specifies a fallback trigger: a concrete, measurable condition that activates the rollback path.\n2. Trigger must be expressed as a threshold on an observable metric (e.g., error rate > 5%, p99 latency > 200ms, compatibility pass rate < 90%).\n3. The trigger must specify: (a) metric name and source, (b) threshold value, (c) evaluation window (e.g., 5-minute rolling), (d) minimum sample size before trigger is armed.\n4. CI rejects contracts where fallback trigger is missing, uses vague language ('if things go wrong'), or lacks numeric threshold.\n5. Unit test: trigger with all four sub-fields passes; trigger missing threshold or evaluation window fails.\n6. Integration test: a simulated metric breach activates the fallback trigger and logs the activation event.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:39:32.819252135Z","created_by":"ubuntu","updated_at":"2026-02-20T15:16:28.198320212Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-11","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-3v8f","depends_on_id":"bd-2fpj","type":"blocks","created_at":"2026-02-20T07:43:24.465521890Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-3v8g","title":"[14] Version benchmark standards with migration guidance","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 14\n\nTask Objective:\nVersion standards and provide explicit migration guidance between standard revisions.\n\nExecution Requirements:\n- Make this requirement machine-verifiable and release-gated where applicable.\n- Keep behavior self-documenting so future contributors do not need to re-open the master plan.\n\nTesting & Logging Requirements:\n- Unit tests for rule/contract logic and error conditions.\n- Integration/E2E tests for workflow-level enforcement.\n- Structured logs with stable codes and trace IDs.\n- Reproducible artifacts that prove contract satisfaction.\n\nAcceptance Criteria:\n- Success and failure invariants for [14] Version benchmark standards with migration guidance are explicitly quantified and machine-verifiable.\n- Determinism requirements for [14] Version benchmark standards with migration guidance are validated across repeated runs and adversarial perturbations.\n\n\nExpected Artifacts:\n- Machine-readable verification artifact with explicit canonical path under artifacts/section_14/bd-3v8g/verification_evidence.json, suitable for CI/release gating.\n- Human-readable verification summary with explicit canonical path under artifacts/section_14/bd-3v8g/verification_summary.md, linking implementation intent to measured validation outcomes.\n\nTask-Specific Clarification:\n- The implementation must deliver the exact capability named in the title, with no scope dilution and no silent feature omission.\n- Validation artifacts must include capability-specific thresholds, pass/fail criteria, and reproducible evidence bundles tied to this bead ID.\n- Program/section verification gates must be able to consume this bead's outputs without manual interpretation.\n\nWhy This Improves User Outcomes:\n- \"[14] Version benchmark standards with migration guidance\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[14] Version benchmark standards with migration guidance\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"1. Benchmark standards follow semantic versioning (major.minor.patch) with clear compatibility guarantees.\n2. Each major version includes migration guidance: what changed, how to update benchmark configurations, and how to compare results across versions.\n3. Version changelog documents: added/removed/changed metrics, formula modifications, dataset updates, and scoring changes.\n4. Backward compatibility: minor versions can compare results with previous minor versions of the same major version.\n5. Deprecation policy: metrics or scoring formulas are deprecated with >= 1 major version warning before removal.\n6. Migration scripts exist to convert benchmark results from version N to version N+1 format.\n7. Version compatibility matrix published: which benchmark versions work with which franken_node versions.\n8. Evidence: benchmark_version_registry.json with version history, migration guide links, and compatibility matrix.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:39:35.557107936Z","created_by":"ubuntu","updated_at":"2026-02-20T15:24:15.578544310Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-14","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-3v8g","depends_on_id":"bd-yz3t","type":"blocks","created_at":"2026-02-20T07:43:25.893331096Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-3v9l","title":"[10.21] Add performance budgets and release claim gates for predictive pre-compromise trajectory assertions.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.21 — Behavioral Phenotype Evolution Tracker Execution Track (9O)\n\nWhy This Exists:\nBPET longitudinal phenotype intelligence track for pre-compromise trajectory detection and integration with DGIS/ATC/migration/economic policy.\n\nTask Objective:\nAdd performance budgets and release claim gates for predictive pre-compromise trajectory assertions.\n\nAcceptance Criteria:\n- BPET scoring latency and storage overhead meet p95/p99 budgets; release claims about predictive detection are blocked without signed calibration/provenance artifacts.\n\nExpected Artifacts:\n- `tests/perf/bpet_budget_gate.rs`, `.github/workflows/bpet-claim-gate.yml`, `artifacts/10.21/bpet_release_gate_report.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_21/bd-3v9l/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_21/bd-3v9l/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.21] Add performance budgets and release claim gates for predictive pre-compromise trajectory assertions.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.21] Add performance budgets and release claim gates for predictive pre-compromise trajectory assertions.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.21] Add performance budgets and release claim gates for predictive pre-compromise trajectory assertions.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.21] Add performance budgets and release claim gates for predictive pre-compromise trajectory assertions.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.21] Add performance budgets and release claim gates for predictive pre-compromise trajectory assertions.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"- BPET scoring latency and storage overhead meet p95/p99 budgets; release claims about predictive detection are blocked without signed calibration/provenance artifacts.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:37:08.961628016Z","created_by":"ubuntu","updated_at":"2026-02-20T17:01:05.937211084Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-21","self-contained","test-obligations"]}
{"id":"bd-3vk","title":"Implement CLI scaffold for franken-node runtime commands","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md (Bootstrap bridge for Sections 10.0–10.5 command surfaces)\nSection: BOOTSTRAP (CLI command-family scaffold bridge)\n\nBootstrap Context:\nThis bead expands the minimal CLI from `bd-2lb` into a complete runtime command-family skeleton while preserving non-duplicative ownership and deterministic behavior.\n\nTask Objective:\nImplement full command-family scaffold (`init`, `run`, `migrate`, `verify`, `trust`, `incident`, `doctor`) with explicit handlers/stubs and stable output contracts.\n\nIn Scope:\n- Full subcommand registration and deterministic argument validation.\n- Handler routing for each command family, with deterministic stub responses where business logic is not yet implemented.\n- Consistent command-level output/error envelope format for future policy/evidence hooks.\n\nOut of Scope:\n- Re-implementing `run`/`doctor` internals already delivered in `bd-2lb`.\n- Deep feature logic for each command family (tracked in downstream beads).\n\nAcceptance Criteria:\n- Every target command family appears in CLI help and routes to a deterministic handler path.\n- Stubbed commands emit stable, machine-readable placeholder output with explicit TODO ownership references.\n- No behavioral regression in `run`/`doctor` paths introduced by surface expansion.\n\nExpected Artifacts:\n- Command-surface matrix mapping commands to handlers and owning downstream beads.\n- Golden CLI help and routing fixtures.\n- Compatibility note describing migration from bootstrap-only to full command scaffold.\n\n- Machine-readable verification artifact at `artifacts/section_bootstrap/bd-3vk/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_bootstrap/bd-3vk/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for subcommand registration and parse coverage across all command families.\n- Integration tests for handler routing and deterministic placeholder responses.\n- E2E script coverage of representative end-user command sequences across all families.\n- Structured per-command logs with stable command IDs, phase markers, and trace correlation fields.\n\nTask-Specific Clarification:\n- For \"Implement CLI scaffold for franken-node runtime commands\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"Implement CLI scaffold for franken-node runtime commands\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"Implement CLI scaffold for franken-node runtime commands\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"Implement CLI scaffold for franken-node runtime commands\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"Implement CLI scaffold for franken-node runtime commands\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","notes":"Legacy bridge task mapped to plan compatibility/CLI stream; sequenced behind bd-2lb to avoid duplicate implementation overlap.","status":"closed","priority":1,"issue_type":"task","assignee":"OrangeAnchor","created_at":"2026-02-20T07:26:07.680355016Z","created_by":"ubuntu","updated_at":"2026-02-20T13:08:57.452775409Z","closed_at":"2026-02-20T13:08:57.452731157Z","close_reason":"done","closed_by_session":"CoralReef","source_repo":".","compaction_level":0,"original_size":0,"labels":["cli","runtime"],"dependencies":[{"issue_id":"bd-3vk","depends_on_id":"bd-2lb","type":"blocks","created_at":"2026-02-20T07:44:42.107689663Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-3vm","title":"[10.11] Add ambient-authority audit gate for product security-critical modules.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md — Section 10.11 (FrankenSQLite-Inspired Runtime Systems), cross-ref Architecture Invariant #10 (8.5)\n\n## Why This Exists\n\nThe three-kernel architecture (8.5) establishes invariant #10: no ambient authority. This means no product code may perform security-sensitive operations (signing trust artifacts, issuing capability grants, initiating epoch transitions, dispatching remote effects, modifying quarantine state) by relying on implicit global state, environment variables, thread-local singletons, or any mechanism other than explicitly passed capability tokens. Violations of this invariant silently undermine the entire capability-based security model: a single module that reads a signing key from a global static bypasses all the careful Cx-first control, epoch scoping, and obligation tracking that the rest of the system enforces.\n\nThis bead implements an ambient-authority audit gate: a combination of static analysis rules, runtime checks, and CI gate enforcement that detects and rejects ambient authority usage in security-critical product modules. This is the enforcement mechanism that ensures invariant #10 is not merely a guideline but a hard-gated property of the codebase.\n\n## What This Must Do\n\n1. Define the \"security-critical module\" inventory: enumerate all Rust modules and Python scripts that perform trust artifact signing, capability grant issuance, epoch transition initiation, remote effect dispatch, quarantine state modification, or checkpoint writes. This inventory is maintained as a versioned configuration file.\n2. Implement a static analysis pass (clippy custom lint, or a dedicated AST-walking script) that scans security-critical modules for ambient authority patterns: (a) global static mutable access, (b) direct environment variable reads (std::env), (c) thread-local storage access, (d) filesystem access without a path capability, (e) network access without a RemoteCap, (f) clock/time access without an explicit time source parameter.\n3. Implement a runtime `AuthorityAuditGuard` that wraps security-critical entry points and asserts that all required capability tokens are present in the `CapabilityContext` before the operation proceeds — missing tokens produce a structured `AmbientAuthorityViolation` error and hard-fail the operation.\n4. Implement a CI gate (`scripts/check_ambient_authority.py`) that runs the static analysis on every PR touching security-critical modules and fails the build if any ambient authority pattern is detected.\n5. Produce an audit report as a machine-readable artifact listing every security-critical module, its required capabilities, and the pass/fail status of both static and runtime checks.\n6. Integrate with the epoch system (bd-2gr): the audit gate must verify that epoch-sensitive operations receive their epoch context from an explicit parameter, not from a global epoch singleton.\n\n## Context from Enhancement Maps\n\n- Architecture invariant #10 (8.5): \"No ambient authority\" — this bead is the enforcement mechanism for this invariant.\n- Architecture invariant #1 (8.5): \"Cx-first control\" — the audit gate verifies that Cx-first is actually practiced, not just declared.\n- 9G.1: \"Capability-context-first product runtime APIs\" — ambient authority audit ensures the Cx-first APIs are the only path to security operations.\n- 9J.10: \"Remote effects must be capability-gated, named, idempotent, and saga-safe\" — the audit gate verifies the capability-gating requirement.\n- 9J.19: \"Cancellation-complete protocol discipline\" — the audit gate verifies that cancellation tokens are explicitly passed, not ambient.\n\n## Dependencies\n\n- Upstream: bd-2g6r (10.15 Cx-first signature policy — defines the CapabilityContext patterns the audit gate checks for), bd-1nfu (10.14 RemoteCap requirement — defines the remote capability pattern), bd-721z (10.15 ambient-authority audit gate for control-plane — provides the canonical audit pattern that this bead extends to product services)\n- Downstream: All 10.11 security-sensitive beads are gated by this audit (bd-2gr epoch operations, bd-3hw remote sagas, bd-24k bounded masking Cx requirement), bd-1jpo (10.11 section-wide verification gate)\n\n## Acceptance Criteria\n\n1. Security-critical module inventory is a versioned TOML/JSON file listing every module path and its required capabilities; adding a new security-critical module without updating the inventory fails CI.\n2. Static analysis detects at least the following ambient patterns: global mutable static, std::env::var, thread_local!, direct filesystem open without path capability, direct TcpStream without RemoteCap, SystemTime::now() without explicit time source.\n3. Static analysis produces zero false positives on the current codebase (verified by running against known-clean modules).\n4. Runtime AuthorityAuditGuard blocks an operation missing required capabilities with a structured `AmbientAuthorityViolation` error within 1ms (no hanging).\n5. CI gate fails the build on any new ambient authority pattern introduced in a PR touching a security-critical module.\n6. Audit report artifact includes: module_path, required_capabilities, static_analysis_pass (bool), runtime_check_pass (bool), violations (list), and timestamp.\n7. A deliberately introduced ambient authority violation (e.g., adding `std::env::var(\"SECRET\")` to a security-critical module) is caught by both the static analysis and the CI gate.\n8. Verification evidence JSON includes modules_audited, static_violations_found, runtime_violations_found, audit_pass_rate, and ci_gate_executions fields.\n\n## Testing & Logging Requirements\n\n- Unit tests: (a) Static analyzer detects global mutable static in a test module; (b) Static analyzer detects std::env::var in a test module; (c) Static analyzer passes a clean module with only explicit capability usage; (d) AuthorityAuditGuard passes with correct CapabilityContext; (e) AuthorityAuditGuard fails with missing capability.\n- Integration tests: (a) Full CI gate run on a PR with a clean change — verify pass; (b) Full CI gate run on a PR with an introduced ambient violation — verify fail with diagnostic; (c) Audit report generation for all security-critical modules — verify completeness; (d) AuthorityAuditGuard on a real product endpoint (e.g., trust artifact signing) with and without capabilities.\n- Adversarial tests: (a) Ambient authority hidden behind a macro — verify static analyzer expands macros or flags them; (b) Ambient authority in a dependency crate — verify the analysis scope includes transitive dependencies of security-critical modules; (c) Runtime capability spoofing (forged CapabilityContext) — verify cryptographic verification in AuthorityAuditGuard; (d) Module removed from inventory but still performing security operations — verify runtime guard still catches it.\n- Structured logs: Events use stable codes (FN-AA-001 through FN-AA-008), include `module_path`, `trace_id`, `violation_type`, `required_capability`, `audit_result`. JSON-formatted.\n\n## Expected Artifacts\n\n- docs/specs/section_10_11/bd-3vm_contract.md\n- crates/franken-node/src/runtime/authority_audit.rs (or equivalent module path)\n- config/security_critical_modules.toml (module inventory)\n- scripts/check_ambient_authority.py (with --json flag and self_test())\n- tests/test_check_ambient_authority.py\n- artifacts/section_10_11/bd-3vm/verification_evidence.json\n- artifacts/section_10_11/bd-3vm/verification_summary.md","acceptance_criteria":"AC for bd-3vm:\n1. A static-analysis or init-time audit gate scans all product security-critical modules and flags any use of ambient authority (global mutable state, unscoped file/network access, raw syscalls not routed through a CxHandle).\n2. The gate produces a machine-readable AuditReport JSON listing every ambient-authority site found, with module path, line number, and violation category.\n3. The gate returns exit code 0 (pass) only when zero ambient-authority violations exist; any violation returns exit code 1 with structured error output.\n4. CI integrates the gate as a mandatory pre-merge check; PRs introducing new ambient authority are blocked until a CxHandle refactor or an explicit exception annotation is added.\n5. Exception annotations (e.g., #[allow_ambient(reason = \"...\")]) are tracked in the audit report and require review sign-off; the total exception count is emitted as a metric.\n6. Unit tests verify: (a) a clean module passes the gate, (b) a module with raw std::fs::read (no CxHandle) fails, (c) an annotated exception is reported but does not fail the gate, (d) removing a CxHandle wrapper from a previously-clean call causes regression failure.\n7. Structured log events use stable codes AMBIENT_AUDIT_PASS / AMBIENT_AUDIT_VIOLATION / AMBIENT_AUDIT_EXCEPTION with module path context.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:36:49.744255694Z","created_by":"ubuntu","updated_at":"2026-02-20T17:01:41.506603293Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-11","self-contained","test-obligations"]}
{"id":"bd-4jh9","title":"[10.18] Implement degraded-mode policy for proof lag/outage (`restricted`, `quarantine`, `halt`) with explicit SLOs.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.18 — Verifiable Execution Fabric Execution Track (9L)\n\nWhy This Exists:\nVEF track for proof-carrying runtime compliance: receipt chains, commitment checkpoints, proof generation/verification, and claim gating.\n\nTask Objective:\nImplement degraded-mode policy for proof lag/outage (`restricted`, `quarantine`, `halt`) with explicit SLOs.\n\nAcceptance Criteria:\n- Proof pipeline lag/outage triggers deterministic degraded mode by policy tier; mode transitions emit mandatory audit events and recovery receipts.\n\nExpected Artifacts:\n- `docs/specs/vef_degraded_mode_policy.md`, `tests/security/vef_degraded_mode_transitions.rs`, `artifacts/10.18/vef_degraded_mode_events.jsonl`.\n\n- Machine-readable verification artifact at `artifacts/section_10_18/bd-4jh9/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_18/bd-4jh9/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.18] Implement degraded-mode policy for proof lag/outage (`restricted`, `quarantine`, `halt`) with explicit SLOs.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.18] Implement degraded-mode policy for proof lag/outage (`restricted`, `quarantine`, `halt`) with explicit SLOs.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.18] Implement degraded-mode policy for proof lag/outage (`restricted`, `quarantine`, `halt`) with explicit SLOs.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.18] Implement degraded-mode policy for proof lag/outage (`restricted`, `quarantine`, `halt`) with explicit SLOs.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.18] Implement degraded-mode policy for proof lag/outage (`restricted`, `quarantine`, `halt`) with explicit SLOs.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"- Proof pipeline lag/outage triggers deterministic degraded mode by policy tier; mode transitions emit mandatory audit events and recovery receipts.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:37:04.790616914Z","created_by":"ubuntu","updated_at":"2026-02-20T17:00:47.368012668Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-18","self-contained","test-obligations"]}
{"id":"bd-4ou","title":"[PLAN 11] Evidence And Decision Contracts","description":"Section 11 codification epic. Every major subsystem must ship change summary, threat/compat evidence, EV tiering, expected-loss model, fallback trigger, rollout wedge, rollback command, and benchmark/correctness artifacts. No contract, no merge.\n\n## Success Criteria\n- All child beads for this section are completed with acceptance artifacts attached and no unresolved blockers.\n- Section-level verification gate for comprehensive unit tests, integration/e2e workflows, and detailed structured logging evidence is green.\n- Scope-to-plan traceability is explicit, with no feature loss versus the canonical plan section.\n\n## Optimization Notes\n- User-Outcome Lens: \"[PLAN 11] Evidence And Decision Contracts\" must improve real operator and end-user reliability, explainability, and time-to-safe-outcome under both normal and degraded conditions.\n- Cross-Section Coordination: this epic's child beads must explicitly encode integration expectations with neighboring sections to prevent local optimizations that break global behavior.\n- Verification Non-Negotiable: completion requires deterministic unit coverage, integration/E2E replay evidence, and structured logs sufficient for independent third-party reproduction and triage.\n- Scope Protection: any proposed simplification must prove feature parity against plan intent and document tradeoffs explicitly; silent scope reduction is forbidden.","status":"open","priority":1,"issue_type":"epic","created_at":"2026-02-20T07:36:42.021201087Z","created_by":"ubuntu","updated_at":"2026-02-20T08:12:48.085724528Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-11"],"dependencies":[{"issue_id":"bd-4ou","depends_on_id":"bd-1hf","type":"blocks","created_at":"2026-02-20T07:38:34.250357758Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-4ou","depends_on_id":"bd-1jmq","type":"blocks","created_at":"2026-02-20T07:39:32.689946661Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-4ou","depends_on_id":"bd-1ow","type":"blocks","created_at":"2026-02-20T07:38:33.862941846Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-4ou","depends_on_id":"bd-1ta","type":"blocks","created_at":"2026-02-20T07:38:34.378934506Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-4ou","depends_on_id":"bd-1u9","type":"blocks","created_at":"2026-02-20T07:38:34.073268017Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-4ou","depends_on_id":"bd-1wz","type":"blocks","created_at":"2026-02-20T07:38:34.555391699Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-4ou","depends_on_id":"bd-1xg","type":"blocks","created_at":"2026-02-20T07:38:33.992324625Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-4ou","depends_on_id":"bd-20a","type":"blocks","created_at":"2026-02-20T07:38:34.033061445Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-4ou","depends_on_id":"bd-20z","type":"blocks","created_at":"2026-02-20T07:38:34.293242839Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-4ou","depends_on_id":"bd-229","type":"blocks","created_at":"2026-02-20T07:38:33.951183813Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-4ou","depends_on_id":"bd-26k","type":"blocks","created_at":"2026-02-20T07:38:34.204005308Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-4ou","depends_on_id":"bd-2fpj","type":"blocks","created_at":"2026-02-20T07:39:32.774414577Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-4ou","depends_on_id":"bd-2ut3","type":"blocks","created_at":"2026-02-20T07:39:33.195957557Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-4ou","depends_on_id":"bd-2ymp","type":"blocks","created_at":"2026-02-20T07:39:32.943331225Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-4ou","depends_on_id":"bd-32p","type":"blocks","created_at":"2026-02-20T07:38:34.598840290Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-4ou","depends_on_id":"bd-36wa","type":"blocks","created_at":"2026-02-20T07:39:32.604423719Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-4ou","depends_on_id":"bd-37i","type":"blocks","created_at":"2026-02-20T07:38:34.726688450Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-4ou","depends_on_id":"bd-39a","type":"blocks","created_at":"2026-02-20T07:38:34.641587755Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-4ou","depends_on_id":"bd-3fo","type":"blocks","created_at":"2026-02-20T07:38:33.820883124Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-4ou","depends_on_id":"bd-3il","type":"blocks","created_at":"2026-02-20T07:38:33.906443706Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-4ou","depends_on_id":"bd-3l8d","type":"blocks","created_at":"2026-02-20T07:39:33.111697057Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-4ou","depends_on_id":"bd-3qo","type":"blocks","created_at":"2026-02-20T07:38:34.465887772Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-4ou","depends_on_id":"bd-3rc","type":"blocks","created_at":"2026-02-20T07:38:34.117390473Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-4ou","depends_on_id":"bd-3se1","type":"blocks","created_at":"2026-02-20T07:39:32.519718871Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-4ou","depends_on_id":"bd-3v8f","type":"blocks","created_at":"2026-02-20T07:39:32.858939821Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-4ou","depends_on_id":"bd-5rh","type":"blocks","created_at":"2026-02-20T07:38:34.423399830Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-4ou","depends_on_id":"bd-c4f","type":"blocks","created_at":"2026-02-20T07:38:34.161121339Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-4ou","depends_on_id":"bd-c781","type":"blocks","created_at":"2026-02-20T07:48:27.939791388Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-4ou","depends_on_id":"bd-go4","type":"blocks","created_at":"2026-02-20T07:38:34.336288179Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-4ou","depends_on_id":"bd-n71","type":"blocks","created_at":"2026-02-20T07:38:34.511864762Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-4ou","depends_on_id":"bd-nglx","type":"blocks","created_at":"2026-02-20T07:39:33.028007430Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-4ou","depends_on_id":"bd-ybe","type":"blocks","created_at":"2026-02-20T07:38:34.684929376Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-4skb","title":"Epic: Behavioral Phenotype Evolution Tracker (BPET) [10.21]","description":"- type: epic","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-20T07:47:58.072163029Z","updated_at":"2026-02-20T07:49:21.322442785Z","closed_at":"2026-02-20T07:49:21.322424401Z","close_reason":"Superseded by canonical detailed master-plan graph (bd-33v) with full 10.N-10.21 and 11-16 coverage, explicit dependencies, and per-section verification gates.","source_repo":".","compaction_level":0,"original_size":0}
{"id":"bd-4yv","title":"[10.1] Add reproducibility contract templates (`env.json`, `manifest.json`, `repro.lock`).","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.1 — Charter + Split Governance\n\nWhy This Exists:\nProgram charter and governance baseline: split contracts, reproducibility, and anti-regression rules for strategic integrity.\n\nTask Objective:\nAdd reproducibility contract templates (`env.json`, `manifest.json`, `repro.lock`).\n\nAcceptance Criteria:\n- Implement with explicit success/failure invariants and deterministic behavior under normal, edge, and fault scenarios.\n- Ensure outcomes are verifiable via automated tests and reproducible artifact outputs.\n\nExpected Artifacts:\n- Section-owned design/spec artifact at `docs/specs/section_10_1/bd-4yv_contract.md`, capturing decision rationale, invariants, and interface boundaries.\n- Machine-readable verification artifact at `artifacts/section_10_1/bd-4yv/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_1/bd-4yv/verification_summary.md` linking implementation intent to measured outcomes.\n\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.1] Add reproducibility contract templates (`env.json`, `manifest.json`, `repro.lock`).\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.1] Add reproducibility contract templates (`env.json`, `manifest.json`, `repro.lock`).\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.1] Add reproducibility contract templates (`env.json`, `manifest.json`, `repro.lock`).\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.1] Add reproducibility contract templates (`env.json`, `manifest.json`, `repro.lock`).\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.1] Add reproducibility contract templates (`env.json`, `manifest.json`, `repro.lock`).\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","status":"closed","priority":1,"issue_type":"task","assignee":"CrimsonCrane","created_at":"2026-02-20T07:36:43.525939936Z","created_by":"ubuntu","updated_at":"2026-02-20T09:16:10.283235333Z","closed_at":"2026-02-20T09:16:10.283207021Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-1","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-4yv","depends_on_id":"bd-2zz","type":"blocks","created_at":"2026-02-20T07:43:10.659397831Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-5rh","title":"[PLAN 10.14] FrankenSQLite Deep-Mined Expansion Execution Track (9J)","description":"Section: 10.14 — FrankenSQLite Deep-Mined Expansion Execution Track (9J)\n\nStrategic Context:\nFrankenSQLite deep-mined control integrity track: evidence ledgers, proofs, epochs, remote effects, markers/MMR, and deterministic fault labs.\n\nExecution Requirements:\n- Preserve all scoped capabilities from the canonical plan.\n- Keep contracts self-contained so future contributors can execute without reopening the master plan.\n- Require explicit testing strategy (unit + integration/e2e) and structured logging/telemetry for every child bead.\n- Require artifact-backed validation for performance, security, and correctness claims.\n\nDependency Semantics:\n- This epic is blocked by its child implementation beads.\n- Cross-epic dependencies encode strategic sequencing and canonical ownership boundaries.\n\n## Success Criteria\n- All child beads for this section are completed with acceptance artifacts attached and no unresolved blockers.\n- Section-level verification gate for comprehensive unit tests, integration/e2e workflows, and detailed structured logging evidence is green.\n- Scope-to-plan traceability is explicit, with no feature loss versus the canonical plan section.\n\n## Optimization Notes\n- User-Outcome Lens: \"[PLAN 10.14] FrankenSQLite Deep-Mined Expansion Execution Track (9J)\" must improve operator confidence, safety posture, and deterministic recovery behavior under both normal and adversarial conditions.\n- Cross-Section Coordination: child beads must encode integration assumptions explicitly to avoid local optimizations that degrade system-wide correctness.\n- Verification Non-Negotiable: completion requires reproducible unit/integration/E2E evidence and structured logs suitable for independent replay.\n- Scope Protection: any simplification must preserve canonical-plan feature intent and be justified with explicit tradeoff documentation.","status":"open","priority":2,"issue_type":"epic","created_at":"2026-02-20T07:36:41.367896930Z","created_by":"ubuntu","updated_at":"2026-02-20T16:05:56.282378187Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-14"],"dependencies":[{"issue_id":"bd-5rh","depends_on_id":"bd-126h","type":"blocks","created_at":"2026-02-20T07:36:58.584272274Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-5rh","depends_on_id":"bd-129f","type":"blocks","created_at":"2026-02-20T07:36:58.664252462Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-5rh","depends_on_id":"bd-12n3","type":"blocks","created_at":"2026-02-20T07:36:57.849114510Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-5rh","depends_on_id":"bd-15u3","type":"blocks","created_at":"2026-02-20T07:36:55.911410566Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-5rh","depends_on_id":"bd-18ud","type":"blocks","created_at":"2026-02-20T07:36:57.443831206Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-5rh","depends_on_id":"bd-1ayu","type":"blocks","created_at":"2026-02-20T07:36:56.256022438Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-5rh","depends_on_id":"bd-1dar","type":"blocks","created_at":"2026-02-20T07:36:58.830555140Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-5rh","depends_on_id":"bd-1daz","type":"blocks","created_at":"2026-02-20T07:36:56.336425843Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-5rh","depends_on_id":"bd-1fck","type":"blocks","created_at":"2026-02-20T07:36:57.524529972Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-5rh","depends_on_id":"bd-1fp4","type":"blocks","created_at":"2026-02-20T07:36:56.418318804Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-5rh","depends_on_id":"bd-1iyx","type":"blocks","created_at":"2026-02-20T07:36:57.007491124Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-5rh","depends_on_id":"bd-1l62","type":"blocks","created_at":"2026-02-20T07:36:56.581499457Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-5rh","depends_on_id":"bd-1nfu","type":"blocks","created_at":"2026-02-20T07:36:57.687319648Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-5rh","depends_on_id":"bd-1oof","type":"blocks","created_at":"2026-02-20T07:36:55.424301086Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-5rh","depends_on_id":"bd-1ru2","type":"blocks","created_at":"2026-02-20T07:36:57.606828969Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-5rh","depends_on_id":"bd-1ta","type":"blocks","created_at":"2026-02-20T07:37:11.049606534Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-5rh","depends_on_id":"bd-1vsr","type":"blocks","created_at":"2026-02-20T07:36:58.502941391Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-5rh","depends_on_id":"bd-1zym","type":"blocks","created_at":"2026-02-20T07:36:56.174814453Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-5rh","depends_on_id":"bd-206h","type":"blocks","created_at":"2026-02-20T07:36:57.929921317Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-5rh","depends_on_id":"bd-20uo","type":"blocks","created_at":"2026-02-20T07:36:56.662081255Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-5rh","depends_on_id":"bd-22yy","type":"blocks","created_at":"2026-02-20T07:36:59.348957639Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-5rh","depends_on_id":"bd-2573","type":"blocks","created_at":"2026-02-20T07:36:57.095910962Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-5rh","depends_on_id":"bd-25nl","type":"blocks","created_at":"2026-02-20T07:36:58.992656193Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-5rh","depends_on_id":"bd-27o2","type":"blocks","created_at":"2026-02-20T07:36:57.259914508Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-5rh","depends_on_id":"bd-2808","type":"blocks","created_at":"2026-02-20T07:36:59.098862682Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-5rh","depends_on_id":"bd-29r6","type":"blocks","created_at":"2026-02-20T07:36:56.926619145Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-5rh","depends_on_id":"bd-29yx","type":"blocks","created_at":"2026-02-20T07:36:56.745473859Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-5rh","depends_on_id":"bd-2e73","type":"blocks","created_at":"2026-02-20T07:36:55.259894609Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-5rh","depends_on_id":"bd-2igi","type":"blocks","created_at":"2026-02-20T07:36:55.830081325Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-5rh","depends_on_id":"bd-2ona","type":"blocks","created_at":"2026-02-20T07:36:55.506408025Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-5rh","depends_on_id":"bd-2qqu","type":"blocks","created_at":"2026-02-20T07:36:59.183045588Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-5rh","depends_on_id":"bd-2wsm","type":"blocks","created_at":"2026-02-20T07:36:58.421016370Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-5rh","depends_on_id":"bd-2xv8","type":"blocks","created_at":"2026-02-20T07:36:58.258507198Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-5rh","depends_on_id":"bd-3a3q","type":"blocks","created_at":"2026-02-20T07:36:55.748889100Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-5rh","depends_on_id":"bd-3cs3","type":"blocks","created_at":"2026-02-20T07:36:58.339724430Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-5rh","depends_on_id":"bd-3epz","type":"blocks","created_at":"2026-02-20T07:48:14.216064370Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-5rh","depends_on_id":"bd-3hdv","type":"blocks","created_at":"2026-02-20T07:36:58.178437884Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-5rh","depends_on_id":"bd-3i6c","type":"blocks","created_at":"2026-02-20T07:36:59.429744529Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-5rh","depends_on_id":"bd-3ort","type":"blocks","created_at":"2026-02-20T07:36:56.846304565Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-5rh","depends_on_id":"bd-3rya","type":"blocks","created_at":"2026-02-20T07:36:56.087182583Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-5rh","depends_on_id":"bd-876n","type":"blocks","created_at":"2026-02-20T07:36:59.268237132Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-5rh","depends_on_id":"bd-8tvs","type":"blocks","created_at":"2026-02-20T07:36:57.177752907Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-5rh","depends_on_id":"bd-ac83","type":"blocks","created_at":"2026-02-20T07:36:57.768811311Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-5rh","depends_on_id":"bd-b9b6","type":"blocks","created_at":"2026-02-20T07:36:56.500384807Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-5rh","depends_on_id":"bd-bq4p","type":"blocks","created_at":"2026-02-20T07:36:55.668220139Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-5rh","depends_on_id":"bd-cda","type":"blocks","created_at":"2026-02-20T07:37:09.585164496Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-5rh","depends_on_id":"bd-mwvn","type":"blocks","created_at":"2026-02-20T07:36:55.992250224Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-5rh","depends_on_id":"bd-nupr","type":"blocks","created_at":"2026-02-20T07:36:55.177813869Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-5rh","depends_on_id":"bd-nwhn","type":"blocks","created_at":"2026-02-20T07:36:58.911575656Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-5rh","depends_on_id":"bd-okqy","type":"blocks","created_at":"2026-02-20T07:36:57.359502148Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-5rh","depends_on_id":"bd-oolt","type":"blocks","created_at":"2026-02-20T07:36:55.342190951Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-5rh","depends_on_id":"bd-qlc6","type":"blocks","created_at":"2026-02-20T07:36:58.098545019Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-5rh","depends_on_id":"bd-sddz","type":"blocks","created_at":"2026-02-20T07:36:55.586773089Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-5rh","depends_on_id":"bd-v4l0","type":"blocks","created_at":"2026-02-20T07:36:58.017343276Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-5rh","depends_on_id":"bd-xwk5","type":"blocks","created_at":"2026-02-20T07:36:58.748174161Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-5si","title":"[10.12] Implement trust fabric convergence protocol and degraded-mode semantics.","description":"[10.12] Implement trust fabric convergence protocol and degraded-mode semantics.\n\n## Why This Exists\n\nSection 9H.2 defines the trust fabric as the distributed system of signed trust artifacts, revocation-first execution policies, and rapid global containment mechanisms that together ensure every node in the fleet operates with a consistent, up-to-date trust state. Without a formal convergence protocol, trust state updates propagate inconsistently — some nodes see revocations before others, creating windows where revoked extensions or compromised trust cards are still honored. This bead implements the convergence protocol that guarantees bounded convergence time and defines exactly how nodes behave when convergence is delayed or the network is partitioned.\n\n## What It Must Do\n\n1. **Trust state vector.** Define a trust state vector that captures the full trust posture of a node: set of active trust cards, revocation list version, extension authorization set, policy checkpoint epoch, and trust anchor fingerprints. The vector has a monotonically increasing version number and a cryptographic digest for fast comparison.\n\n2. **Convergence protocol.** Implement a gossip-based convergence protocol where nodes periodically exchange trust state vector digests. When a digest mismatch is detected, the node with the older state pulls updates from the node with the newer state. The protocol must converge within a configurable bounded time (default: 30 seconds for a 1000-node fleet) under normal network conditions.\n\n3. **Revocation-first execution.** Revocations are always propagated before authorizations. The protocol uses a priority channel for revocation messages. A node that receives a revocation immediately applies it, even if it hasn't received the full trust state update yet. This ensures that compromised artifacts are neutralized globally as fast as possible.\n\n4. **Convergence monitoring.** Each node tracks its convergence lag: the time since it last confirmed its trust state matches the global latest. Convergence lag is reported as a health metric. If convergence lag exceeds a configurable threshold (default: 60 seconds), the node enters degraded mode.\n\n5. **Degraded-mode semantics.** When a node cannot converge (network partition, upstream failure, or excessive lag), it enters degraded mode with these semantics:\n   - All trust decisions are made conservatively: deny by default, allow only previously cached positive decisions that are within their TTL.\n   - New trust artifacts (extensions, trust cards) are not accepted until convergence is restored.\n   - Revocations continue to be processed from any available source (local cache, peer gossip, or direct push).\n   - The node advertises its degraded status to peers and operators.\n   - A configurable maximum degraded duration (default: 5 minutes) triggers escalation to the supervision tree (bd-3he).\n\n6. **Partition healing.** When a network partition heals, nodes must rapidly re-converge. The protocol supports delta synchronization: only trust state changes since the partition are exchanged, not the full state. Partition healing is logged with duration, number of missed updates, and time to re-convergence.\n\n7. **Anti-entropy mechanism.** Periodic full-state comparison (anti-entropy sweep) runs on a configurable interval (default: every 5 minutes) to catch any updates missed by the gossip protocol. Anti-entropy is complementary to gossip, not a replacement.\n\n## Acceptance Criteria\n\n1. Trust state vector type implemented in `crates/franken-node/src/connector/trust_fabric.rs` with version, digest, and component sets.\n2. Gossip-based convergence protocol converges a simulated 100-node fleet within 30 seconds.\n3. Revocation-first priority channel ensures revocations propagate in < 5 seconds to 95% of nodes in simulation.\n4. Convergence lag metric is computed and reported per-node.\n5. Degraded-mode semantics are implemented with conservative deny-by-default behavior.\n6. Partition healing uses delta synchronization and logs healing metrics.\n7. Anti-entropy sweep detects and repairs missed updates in simulation.\n8. Verification script `scripts/check_trust_fabric.py` with `--json` flag confirms all criteria.\n9. Evidence artifacts written to `artifacts/section_10_12/bd-5si/`.\n\n## Key Dependencies\n\n- bd-1l5 (trust object IDs) — trust artifacts carry canonical IDs.\n- bd-3he (supervision tree) — degraded-mode timeout escalates to supervisor.\n- bd-cvt (capability profiles) — trust fabric has `cap:trust:read`, `cap:trust:write` capabilities.\n- 10.13 telemetry namespace — convergence metrics conform to telemetry schema.\n- 10.13 stable error namespace — convergence errors use registered codes.\n\n## Testing & Logging Requirements\n\n- Unit tests for trust state vector operations: merge, compare, digest computation.\n- Simulation test with 100 nodes, injected partitions, and convergence time measurement.\n- Degraded-mode test: partition a node, confirm conservative deny-by-default, confirm revocations still apply.\n- Partition healing test: split fleet, apply updates to each partition, heal, confirm delta sync.\n- Property-based test: arbitrary message reordering still converges to correct state.\n- Structured logging: `trust_fabric.state_updated`, `trust_fabric.digest_mismatch`, `trust_fabric.revocation_applied`, `trust_fabric.convergence_lag`, `trust_fabric.degraded_mode_entered`, `trust_fabric.degraded_mode_exited`, `trust_fabric.partition_healed`, `trust_fabric.anti_entropy_sweep` events.\n\n## Expected Artifacts\n\n- `docs/specs/section_10_12/bd-5si_contract.md` — specification document.\n- `crates/franken-node/src/connector/trust_fabric.rs` — Rust implementation.\n- `scripts/check_trust_fabric.py` — verification script.\n- `tests/test_check_trust_fabric.py` — unit tests.\n- `artifacts/section_10_12/bd-5si/verification_evidence.json` — evidence.\n- `artifacts/section_10_12/bd-5si/verification_summary.md` — summary.\n\n### Additional E2E Requirement\n\n- E2E test scripts must execute full end-to-end workflows from clean fixtures, emit deterministic machine-readable pass/fail evidence, and capture stepwise structured logs for root-cause triage.","acceptance_criteria":"1. Define a TrustFabricState enum: CONVERGED (all nodes agree on trust state), DEGRADED (some nodes have stale or missing trust artifacts), PARTITIONED (one or more nodes are unreachable and their trust state is unknown), DIVERGED (conflicting trust states detected, requires manual resolution).\n2. Implement a TrustFabricConvergenceProtocol with: (a) announce(node_id, trust_state_vector) to broadcast a node's current trust state, (b) merge(local_state, remote_state) -> MergeResult that produces a merged trust state or identifies conflicts, (c) evaluate_fabric_health() -> TrustFabricState based on all known node states.\n3. Define convergence criteria: the fabric is CONVERGED when all active nodes report the same trust_state_hash within a configurable convergence_window (default 30s). Nodes that have not announced within the window are classified as STALE.\n4. Implement degraded-mode semantics: when the fabric is DEGRADED, (a) read operations proceed with a staleness warning attached to responses, (b) write operations (token issuance, key binding, policy changes) are blocked unless an override flag force_degraded=true is provided, (c) DANGEROUS actions (from bd-2sx risk tiers) are unconditionally blocked in DEGRADED mode.\n5. Implement partition detection: if a node has not announced for > 3x convergence_window, classify it as PARTITIONED. Emit a CRITICAL structured log event with the partitioned node list.\n6. Implement divergence resolution: when merge() detects conflicting trust states, produce a DivergenceReport containing (a) conflicting node IDs, (b) their respective trust_state_hashes, (c) the specific trust objects that differ. Do NOT auto-resolve; require operator intervention.\n7. Implement health metrics: expose fabric_convergence_ratio (converged_nodes / total_nodes), fabric_state, time_since_last_full_convergence, and stale_node_count.\n8. Unit tests: (a) two nodes converge, (b) one node stale => DEGRADED, (c) one node partitioned => PARTITIONED, (d) conflicting states => DIVERGED, (e) degraded-mode write blocking, (f) degraded-mode read with warning, (g) DANGEROUS action blocked in DEGRADED.\n9. Integration test: simulate a 5-node fabric, partition one node, verify state transitions and metric updates.\n10. Verification: scripts/check_trust_fabric.py --json, artifacts at artifacts/section_10_12/bd-5si/.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:36:50.995452214Z","created_by":"ubuntu","updated_at":"2026-02-20T17:08:34.574143137Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-12","self-contained","test-obligations"]}
{"id":"bd-5y9q","title":"Epic: Workspace + Build Infrastructure","description":"- type: task","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-20T07:47:58.072163029Z","updated_at":"2026-02-20T07:49:21.071277203Z","closed_at":"2026-02-20T07:49:21.071253589Z","close_reason":"Superseded by canonical detailed master-plan graph (bd-33v) with full 10.N-10.21 and 11-16 coverage, explicit dependencies, and per-section verification gates.","source_repo":".","compaction_level":0,"original_size":0}
{"id":"bd-721z","title":"[10.15] Add ambient-authority audit gate for control-plane modules.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.15 — Asupersync-First Integration Execution Track (8.4-8.6)\n\nWhy This Exists:\nAsupersync-first control-plane migration track enforcing Cx-first, region ownership, cancellation correctness, and protocol-grade verification gates.\n\nTask Objective:\nAdd ambient-authority audit gate for control-plane modules.\n\nAcceptance Criteria:\n- Ambient network/spawn/time effects in restricted modules fail CI; allowlist is explicit and signed.\n\nExpected Artifacts:\n- `tools/lints/ambient_authority_gate.rs`, `docs/specs/ambient_authority_policy.md`, `artifacts/10.15/ambient_authority_findings.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_15/bd-721z/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_15/bd-721z/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.15] Add ambient-authority audit gate for control-plane modules.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.15] Add ambient-authority audit gate for control-plane modules.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.15] Add ambient-authority audit gate for control-plane modules.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.15] Add ambient-authority audit gate for control-plane modules.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.15] Add ambient-authority audit gate for control-plane modules.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"- Ambient network/spawn/time effects in restricted modules fail CI; allowlist is explicit and signed.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:36:59.726628519Z","created_by":"ubuntu","updated_at":"2026-02-20T17:00:44.538766420Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-15","self-contained","test-obligations"]}
{"id":"bd-7mt","title":"[10.2] Add CI gate: compatibility implementations must cite spec section + fixture IDs; missing references fail review gates.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.2 — Compatibility Core\n\nWhy This Exists:\nCompatibility core as strategic wedge: policy-visible behavior, divergence receipts, L1/L2 lockstep, and spec-first fixture governance.\n\nTask Objective:\nAdd CI gate: compatibility implementations must cite spec section + fixture IDs; missing references fail review gates.\n\nAcceptance Criteria:\n- Implement with explicit success/failure invariants and deterministic behavior under normal, edge, and fault scenarios.\n- Ensure outcomes are verifiable via automated tests and reproducible artifact outputs.\n\nExpected Artifacts:\n- Section-owned design/spec artifact at `docs/specs/section_10_2/bd-7mt_contract.md`, capturing decision rationale, invariants, and interface boundaries.\n- Machine-readable verification artifact at `artifacts/section_10_2/bd-7mt/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_2/bd-7mt/verification_summary.md` linking implementation intent to measured outcomes.\n\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.2] Add CI gate: compatibility implementations must cite spec section + fixture IDs; missing references fail review gates.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.2] Add CI gate: compatibility implementations must cite spec section + fixture IDs; missing references fail review gates.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.2] Add CI gate: compatibility implementations must cite spec section + fixture IDs; missing references fail review gates.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.2] Add CI gate: compatibility implementations must cite spec section + fixture IDs; missing references fail review gates.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.2] Add CI gate: compatibility implementations must cite spec section + fixture IDs; missing references fail review gates.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","status":"closed","priority":1,"issue_type":"task","assignee":"CrimsonCrane","created_at":"2026-02-20T07:36:44.719663372Z","created_by":"ubuntu","updated_at":"2026-02-20T10:03:25.554310012Z","closed_at":"2026-02-20T10:03:25.554283553Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-2","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-7mt","depends_on_id":"bd-80g","type":"blocks","created_at":"2026-02-20T07:43:20.518881424Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-7om","title":"[10.11] Adopt canonical cancel -> drain -> finalize protocol contracts (from `10.15`) for product services.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md — Section 10.11 (FrankenSQLite-Inspired Runtime Systems), cross-ref Section 9G.2, 9J.19\n\n## Why This Exists\n\nEvery long-running product operation in franken_node — migration orchestration, anti-entropy sweeps, trust rotation, bulk replication, rollout state transitions — must be cancellable in a disciplined manner. The naive approach (drop the future / kill the task) leaves resources leaked, obligations unfulfilled, and partial state scattered across services. Enhancement Map 9G.2 mandates cancellation as a strict protocol for all long-running orchestration tasks, implemented as a three-phase sequence: cancel (signal intent to stop), drain (complete in-flight sub-operations, flush buffers, release held resources), and finalize (produce a terminal status record and obligation closure proof). 9J.19 extends this with cancellation-complete protocol discipline, requiring that every cancellation produces a proof that all obligations spawned by the cancelled operation have been accounted for.\n\nThis bead adopts the canonical cancel-drain-finalize protocol from 10.15 (bd-1cs7) into franken_node's product service layer, providing a `CancellableTask` trait and runtime support that ensures every product service's long-running operations implement the full three-phase cancellation protocol.\n\n## What This Must Do\n\n1. Define a `CancellableTask` trait with three required methods: `on_cancel(&mut self)` (enter drain phase), `on_drain_complete(&mut self) -> DrainResult` (signal drain completion with status), and `on_finalize(&mut self) -> FinalizeRecord` (produce terminal status and obligation closure proof).\n2. Implement a `CancellationRuntime` that manages the lifecycle of cancellable tasks: accepts cancel signals, enforces drain timeout, and calls finalize regardless of drain outcome.\n3. Enforce drain timeout: if `on_drain_complete` is not signalled within a configurable deadline after `on_cancel`, the runtime force-transitions to finalize with a `DrainTimeout` status, ensuring the system never hangs waiting for a stalled drain.\n4. Produce obligation closure proofs on finalize: the `FinalizeRecord` must enumerate every obligation spawned by the task and its terminal state (fulfilled, compensated, or force-closed).\n5. Integrate with the scheduler lane system (bd-lus): cancellation of a task immediately releases its lane slot and bulkhead permit during the cancel phase (not deferred to finalize).\n6. Ensure `CancellableTask` implementations are verified at registration time: a task that does not implement all three methods is rejected with a clear diagnostic.\n\n## Context from Enhancement Maps\n\n- 9G.2: \"Cancellation as a strict protocol for all long-running orchestration tasks\"\n- 9J.19: \"Cancellation-complete protocol discipline with obligation closure proofs\"\n- Architecture invariant #3 (8.5): Cancellation protocol semantics — cancellation is a first-class protocol, not an afterthought.\n- Architecture invariant #4 (8.5): Two-phase effects — drain phase must complete two-phase effects before finalize.\n- Architecture invariant #8 (8.5): Evidence-by-default — finalize records are evidence and must be persisted.\n\n## Dependencies\n\n- Upstream: bd-1cs7 (10.15 cancel-drain-finalize protocol implementation), bd-1n5p (10.15 obligation-tracked channels — for obligation closure proof generation)\n- Downstream: bd-lus (scheduler lanes release slots on cancel), bd-2ah (obligation channels integrate with cancellation for closure proofs), bd-24k (bounded masking defers cancel signals), bd-390 (anti-entropy reconciliation is cancellable), bd-3hw (saga orchestrator cancellation triggers compensation), bd-93k (checkpoint placement is cancel-aware), bd-1jpo (10.11 section-wide verification gate)\n\n## Acceptance Criteria\n\n1. Every long-running product operation (migration, anti-entropy, trust rotation, rollout, replication) implements `CancellableTask` — verified by startup-time registration check.\n2. Cancel signal transitions a running task to drain phase within 1 event-loop tick; the task's lane slot is released immediately.\n3. Drain timeout enforcement: a task whose drain exceeds the configured deadline (default 5 seconds) is force-transitioned to finalize with `DrainTimeout` status.\n4. `FinalizeRecord` includes: task_id, cancel_reason, drain_status (completed/timed_out), obligation_closure_proof (list of obligation IDs and terminal states), and wall-clock timestamps for each phase transition.\n5. Obligation closure proof is complete: every obligation spawned by the task appears in the proof with a terminal state. A missing obligation causes a `ClosureProofIncomplete` alert.\n6. A task that is not cancelled completes normally and produces a `FinalizeRecord` with `cancel_reason: None` (the protocol applies to all task completions, not just cancellations).\n7. Nested cancellation: a parent task cancellation propagates to child tasks, and the parent's finalize waits for all children's finalize records.\n8. Verification evidence JSON includes tasks_cancelled, tasks_drain_completed, tasks_drain_timed_out, closure_proofs_generated, closure_proofs_complete, and avg_drain_duration_ms fields.\n\n## Testing & Logging Requirements\n\n- Unit tests: (a) CancellableTask lifecycle: create -> run -> cancel -> drain -> finalize; (b) Drain timeout enforcement; (c) FinalizeRecord completeness; (d) Lane slot release timing on cancel; (e) Normal completion (no cancel) still produces FinalizeRecord.\n- Integration tests: (a) Migration orchestration cancel mid-flight with obligation closure; (b) Anti-entropy sweep cancel with partial progress safely abandoned; (c) Nested task cancellation propagation (parent -> 3 children); (d) Cancel during two-phase flow prepare — verify rollback and closure proof.\n- Adversarial tests: (a) Drain handler that panics — verify finalize still executes with error status; (b) Cancel signal sent to already-finalizing task — verify idempotent handling; (c) 100 concurrent cancellations — verify no resource leaks; (d) Cancel with no obligations — verify empty but valid closure proof.\n- Structured logs: Events use stable codes (FN-CX-001 through FN-CX-010), include `task_id`, `trace_id`, `phase` (cancel/drain/finalize), `drain_status`, `obligations_count`. JSON-formatted.\n\n## Expected Artifacts\n\n- docs/specs/section_10_11/bd-7om_contract.md\n- crates/franken-node/src/runtime/cancellation.rs (or equivalent module path)\n- crates/franken-node/src/runtime/cancellable_task.rs (trait definition + registration)\n- scripts/check_cancellation_protocol.py (with --json flag and self_test())\n- tests/test_check_cancellation_protocol.py\n- artifacts/section_10_11/bd-7om/verification_evidence.json\n- artifacts/section_10_11/bd-7om/verification_summary.md","acceptance_criteria":"AC for bd-7om:\n1. All product services adopt the canonical three-phase cancellation protocol from 10.15: Cancel (signal intent) -> Drain (complete in-flight work, reject new work) -> Finalize (release resources, emit completion evidence).\n2. A CancellationProtocol trait is defined with methods: cancel(), drain(timeout: Duration) -> DrainOutcome, finalize() -> FinalizeReport.\n3. State transitions are strictly ordered: Idle -> Cancelled -> Draining -> Finalized. Any out-of-order transition attempt returns Err with stable code CANCEL_PROTOCOL_VIOLATION.\n4. Drain has a configurable timeout; if in-flight work does not complete within the timeout, drain returns DrainOutcome::TimedOut with a list of still-active work items, and finalize force-drops them with a CANCEL_FORCE_DROP log event.\n5. Double-cancel is idempotent (no error, no state change beyond first cancel).\n6. Every service implementing CancellationProtocol emits structured telemetry: CANCEL_SIGNAL_RECEIVED, DRAIN_STARTED, DRAIN_COMPLETED / DRAIN_TIMED_OUT, FINALIZE_COMPLETED with trace correlation IDs and elapsed-time measurements.\n7. Unit tests verify: (a) happy-path cancel -> drain -> finalize completes cleanly, (b) drain timeout triggers forced finalization, (c) out-of-order transition is rejected, (d) double-cancel is idempotent, (e) new work submitted during drain phase is rejected with WORK_REJECTED_DRAINING.\n8. Integration test: a multi-service orchestration cancels all services concurrently and verifies total drain time is bounded by the maximum individual drain timeout (parallel drain, not serial).","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:36:49.901866673Z","created_by":"ubuntu","updated_at":"2026-02-20T17:01:41.229687937Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-11","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-7om","depends_on_id":"bd-1cs7","type":"blocks","created_at":"2026-02-20T15:00:17.548435204Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-7rt","title":"Generate transplant hash lockfile for tamper detection","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md (Bootstrap bridge for integrity/provenance controls)\n\nTask Objective:\nGenerate a deterministic transplant hash lockfile (`sha256`/equivalent) for restored snapshot assets and define verification semantics used by CI and drift tooling.\n\nIn Scope:\n- Deterministic hash computation and canonical ordering rules.\n- Lockfile format contract including algorithm, path normalization, and metadata fields.\n- Verification command/flow that compares current snapshot state against lockfile.\n\nAcceptance Criteria:\n- Equivalent snapshot inputs always produce byte-identical lockfile outputs.\n- Verification flow reliably detects additions, deletions, and content mutations.\n- Lockfile output is consumable by downstream drift workflow (`bd-29q`) and CI checks.\n\nExpected Artifacts:\n- Lockfile format specification and generation procedure.\n- Golden lockfile fixtures for known snapshot states.\n- Verification report format for CI/release evidence.\n\nTesting & Logging Requirements:\n- Unit tests for hash generation, ordering, and path-normalization edge cases.\n- Integration tests covering positive/negative verification scenarios.\n- E2E tests for restore -> lockfile -> verification workflow.\n- Structured logs with hashed-file counts, mismatch categories, and trace IDs.\n\nTask-Specific Clarification:\n- For \"Generate transplant hash lockfile for tamper detection\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"Generate transplant hash lockfile for tamper detection\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"Generate transplant hash lockfile for tamper detection\" must support deterministic replay and root-cause triage without hidden context.","notes":"Legacy transplant lockfile task retained; now explicitly sequenced after bd-1qz to avoid orphan lockfile generation.","status":"closed","priority":1,"issue_type":"task","assignee":"ubuntu","created_at":"2026-02-20T07:26:02.917369250Z","created_by":"ubuntu","updated_at":"2026-02-20T08:11:40.703967519Z","closed_at":"2026-02-20T08:11:40.703863445Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["integrity","transplant"],"dependencies":[{"issue_id":"bd-7rt","depends_on_id":"bd-1qz","type":"blocks","created_at":"2026-02-20T07:44:42.162279998Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-80g","title":"[10.2] Build prioritized Node/Bun reference capture programs and fixture corpora per API band (CLI/process/fs/network/module/tooling).","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.2 — Compatibility Core\n\nWhy This Exists:\nCompatibility core as strategic wedge: policy-visible behavior, divergence receipts, L1/L2 lockstep, and spec-first fixture governance.\n\nTask Objective:\nBuild prioritized Node/Bun reference capture programs and fixture corpora per API band (CLI/process/fs/network/module/tooling).\n\nAcceptance Criteria:\n- Implement with explicit success/failure invariants and deterministic behavior under normal, edge, and fault scenarios.\n- Ensure outcomes are verifiable via automated tests and reproducible artifact outputs.\n\nExpected Artifacts:\n- Section-owned design/spec artifact at `docs/specs/section_10_2/bd-80g_contract.md`, capturing decision rationale, invariants, and interface boundaries.\n- Machine-readable verification artifact at `artifacts/section_10_2/bd-80g/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_2/bd-80g/verification_summary.md` linking implementation intent to measured outcomes.\n\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.2] Build prioritized Node/Bun reference capture programs and fixture corpora per API band (CLI/process/fs/network/module/tooling).\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.2] Build prioritized Node/Bun reference capture programs and fixture corpora per API band (CLI/process/fs/network/module/tooling).\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.2] Build prioritized Node/Bun reference capture programs and fixture corpora per API band (CLI/process/fs/network/module/tooling).\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.2] Build prioritized Node/Bun reference capture programs and fixture corpora per API band (CLI/process/fs/network/module/tooling).\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.2] Build prioritized Node/Bun reference capture programs and fixture corpora per API band (CLI/process/fs/network/module/tooling).\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","status":"closed","priority":1,"issue_type":"task","assignee":"CrimsonCrane","created_at":"2026-02-20T07:36:44.641054328Z","created_by":"ubuntu","updated_at":"2026-02-20T10:00:58.376678667Z","closed_at":"2026-02-20T10:00:58.376653841Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-2","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-80g","depends_on_id":"bd-2hs","type":"blocks","created_at":"2026-02-20T07:43:20.475881267Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-876n","title":"[10.14] Add cancellation injection at all await points for critical control workflows in lab tests.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.14 — FrankenSQLite Deep-Mined Expansion Execution Track (9J)\n\nWhy This Exists:\nFrankenSQLite deep-mined control integrity track: evidence ledgers, proofs, epochs, remote effects, markers/MMR, and deterministic fault labs.\n\nTask Objective:\nAdd cancellation injection at all await points for critical control workflows in lab tests.\n\nAcceptance Criteria:\n- Critical workflows are instrumented for all-point cancellation injection; leak-free and half-commit-free invariants hold under injected cancellations.\n\nExpected Artifacts:\n- `tests/lab/cancellation_injection_control_workflows.rs`, `docs/testing/cancel_injection_matrix.md`, `artifacts/10.14/cancel_injection_report.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_14/bd-876n/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_14/bd-876n/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.14] Add cancellation injection at all await points for critical control workflows in lab tests.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.14] Add cancellation injection at all await points for critical control workflows in lab tests.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.14] Add cancellation injection at all await points for critical control workflows in lab tests.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.14] Add cancellation injection at all await points for critical control workflows in lab tests.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.14] Add cancellation injection at all await points for critical control workflows in lab tests.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"Critical workflows are instrumented for all-point cancellation injection; leak-free and half-commit-free invariants hold under injected cancellations.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:36:59.227759656Z","created_by":"ubuntu","updated_at":"2026-02-20T16:24:30.128108810Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-14","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-876n","depends_on_id":"bd-1ru2","type":"blocks","created_at":"2026-02-20T16:24:30.128058837Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-876n","depends_on_id":"bd-2wsm","type":"blocks","created_at":"2026-02-20T16:24:29.933181884Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-8h8m","title":"Epic: Lab + Verification Infrastructure [10.14h]","description":"- type: epic","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-20T07:47:58.072163029Z","updated_at":"2026-02-20T07:49:21.250401734Z","closed_at":"2026-02-20T07:49:21.250384001Z","close_reason":"Superseded by canonical detailed master-plan graph (bd-33v) with full 10.N-10.21 and 11-16 coverage, explicit dependencies, and per-section verification gates.","source_repo":".","compaction_level":0,"original_size":0}
{"id":"bd-8l9k","title":"[10.16] Add cross-substrate contract tests validating end-to-end behavior (`frankentui` -> service -> persistence).","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.16 — Adjacent Substrate Integration Execution Track (8.7)\n\nWhy This Exists:\nAdjacent substrate integration track to enforce deep composition with frankentui, frankensqlite, sqlmodel_rust, and fastapi_rust.\n\nTask Objective:\nAdd cross-substrate contract tests validating end-to-end behavior (`frankentui` -> service -> persistence).\n\nAcceptance Criteria:\n- End-to-end tests cover representative operator flows and replay determinism; failure includes cross-layer trace.\n\nExpected Artifacts:\n- `tests/e2e/adjacent_substrate_flow.rs`, `artifacts/10.16/adjacent_substrate_e2e_report.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_16/bd-8l9k/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_16/bd-8l9k/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.16] Add cross-substrate contract tests validating end-to-end behavior (`frankentui` -> service -> persistence).\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.16] Add cross-substrate contract tests validating end-to-end behavior (`frankentui` -> service -> persistence).\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.16] Add cross-substrate contract tests validating end-to-end behavior (`frankentui` -> service -> persistence).\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.16] Add cross-substrate contract tests validating end-to-end behavior (`frankentui` -> service -> persistence).\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.16] Add cross-substrate contract tests validating end-to-end behavior (`frankentui` -> service -> persistence).\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"- End-to-end tests cover representative operator flows and replay determinism; failure includes cross-layer trace.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:37:02.512563237Z","created_by":"ubuntu","updated_at":"2026-02-20T17:05:30.781435397Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-16","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-8l9k","depends_on_id":"bd-2owx","type":"blocks","created_at":"2026-02-20T17:05:30.781385504Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-8qlj","title":"[10.18] Integrate VEF verification state into high-risk control transitions and action authorization.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.18 — Verifiable Execution Fabric Execution Track (9L)\n\nWhy This Exists:\nVEF track for proof-carrying runtime compliance: receipt chains, commitment checkpoints, proof generation/verification, and claim gating.\n\nTask Objective:\nIntegrate VEF verification state into high-risk control transitions and action authorization.\n\nAcceptance Criteria:\n- High-risk actions require configured VEF verification state; policy can enforce strict/graded modes; gate decisions are auditable and replayable.\n\nExpected Artifacts:\n- `docs/integration/vef_control_plane_integration.md`, `tests/integration/vef_high_risk_action_gating.rs`, `artifacts/10.18/vef_control_gate_decisions.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_18/bd-8qlj/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_18/bd-8qlj/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.18] Integrate VEF verification state into high-risk control transitions and action authorization.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.18] Integrate VEF verification state into high-risk control transitions and action authorization.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.18] Integrate VEF verification state into high-risk control transitions and action authorization.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.18] Integrate VEF verification state into high-risk control transitions and action authorization.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.18] Integrate VEF verification state into high-risk control transitions and action authorization.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"- High-risk actions require configured VEF verification state; policy can enforce strict/graded modes; gate decisions are auditable and replayable.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:37:04.708319971Z","created_by":"ubuntu","updated_at":"2026-02-20T17:05:54.673725902Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-18","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-8qlj","depends_on_id":"bd-1o4v","type":"blocks","created_at":"2026-02-20T17:05:54.673675678Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-8tvs","title":"[10.14] Implement per-class symbol-size/overhead/fetch policy with benchmark-derived defaults.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.14 — FrankenSQLite Deep-Mined Expansion Execution Track (9J)\n\nWhy This Exists:\nFrankenSQLite deep-mined control integrity track: evidence ledgers, proofs, epochs, remote effects, markers/MMR, and deterministic fault labs.\n\nTask Objective:\nImplement per-class symbol-size/overhead/fetch policy with benchmark-derived defaults.\n\nAcceptance Criteria:\n- Policy engine applies class-specific defaults at runtime; defaults are justified by benchmark data; policy override path is audited.\n\nExpected Artifacts:\n- `src/policy/object_class_tuning.rs`, `benchmarks/object_class_tuning/*`, `artifacts/10.14/object_class_policy_report.csv`.\n\n- Machine-readable verification artifact at `artifacts/section_10_14/bd-8tvs/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_14/bd-8tvs/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.14] Implement per-class symbol-size/overhead/fetch policy with benchmark-derived defaults.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.14] Implement per-class symbol-size/overhead/fetch policy with benchmark-derived defaults.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.14] Implement per-class symbol-size/overhead/fetch policy with benchmark-derived defaults.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.14] Implement per-class symbol-size/overhead/fetch policy with benchmark-derived defaults.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.14] Implement per-class symbol-size/overhead/fetch policy with benchmark-derived defaults.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"Policy engine applies class-specific defaults at runtime; defaults are justified by benchmark data; policy override path is audited.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:36:57.139699366Z","created_by":"ubuntu","updated_at":"2026-02-20T15:44:07.556386758Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-14","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-8tvs","depends_on_id":"bd-2573","type":"blocks","created_at":"2026-02-20T07:43:15.232335331Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-8uvb","title":"[10.13] Implement overlapping-lease conflict policy and deterministic fork handling logs.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.13 — FCP Deep-Mined Expansion Execution Track (9I)\n\nWhy This Exists:\nDeep connector/provider contract track: lifecycle FSMs, conformance harnesses, fencing, sandboxing, revocation freshness, and protocol hardening.\n\nTask Objective:\nImplement overlapping-lease conflict policy and deterministic fork handling logs.\n\nAcceptance Criteria:\n- Overlapping lease conflicts resolve via documented deterministic rule; dangerous conflicts halt and alert; fork logs contain reproducible evidence.\n\nExpected Artifacts:\n- `docs/specs/lease_conflict_policy.md`, `tests/integration/overlapping_lease_conflicts.rs`, `artifacts/10.13/lease_fork_log_samples.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_13/bd-8uvb/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_13/bd-8uvb/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.13] Implement overlapping-lease conflict policy and deterministic fork handling logs.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.13] Implement overlapping-lease conflict policy and deterministic fork handling logs.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.13] Implement overlapping-lease conflict policy and deterministic fork handling logs.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.13] Implement overlapping-lease conflict policy and deterministic fork handling logs.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.13] Implement overlapping-lease conflict policy and deterministic fork handling logs.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","status":"closed","priority":1,"issue_type":"task","assignee":"CrimsonCrane","created_at":"2026-02-20T07:36:53.435407011Z","created_by":"ubuntu","updated_at":"2026-02-20T12:22:09.970492099Z","closed_at":"2026-02-20T12:22:09.970467493Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-13","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-8uvb","depends_on_id":"bd-2vs4","type":"blocks","created_at":"2026-02-20T07:43:13.309101118Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-8vby","title":"[10.13] Implement device profile registry and placement policy schema for execution targeting.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.13 — FCP Deep-Mined Expansion Execution Track (9I)\n\nWhy This Exists:\nDeep connector/provider contract track: lifecycle FSMs, conformance harnesses, fencing, sandboxing, revocation freshness, and protocol hardening.\n\nTask Objective:\nImplement device profile registry and placement policy schema for execution targeting.\n\nAcceptance Criteria:\n- Device profiles have validated schema and freshness checks; placement policies reject invalid constraints; policy evaluation is deterministic.\n\nExpected Artifacts:\n- `docs/specs/device_profile_schema.md`, `tests/conformance/placement_policy_schema.rs`, `artifacts/10.13/device_profile_examples.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_13/bd-8vby/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_13/bd-8vby/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.13] Implement device profile registry and placement policy schema for execution targeting.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.13] Implement device profile registry and placement policy schema for execution targeting.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.13] Implement device profile registry and placement policy schema for execution targeting.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.13] Implement device profile registry and placement policy schema for execution targeting.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.13] Implement device profile registry and placement policy schema for execution targeting.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","status":"closed","priority":1,"issue_type":"task","assignee":"CrimsonCrane","created_at":"2026-02-20T07:36:53.517398144Z","created_by":"ubuntu","updated_at":"2026-02-20T12:25:55.036302890Z","closed_at":"2026-02-20T12:25:55.036274818Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-13","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-8vby","depends_on_id":"bd-8uvb","type":"blocks","created_at":"2026-02-20T07:43:13.350198230Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-91gg","title":"[10.13] Implement background repair controller with bounded work-per-cycle and fairness controls.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.13 — FCP Deep-Mined Expansion Execution Track (9I)\n\nWhy This Exists:\nDeep connector/provider contract track: lifecycle FSMs, conformance harnesses, fencing, sandboxing, revocation freshness, and protocol hardening.\n\nTask Objective:\nImplement background repair controller with bounded work-per-cycle and fairness controls.\n\nAcceptance Criteria:\n- Repair loop respects per-cycle work caps and fairness constraints; no tenant starvation under synthetic load; controller decisions are auditable.\n\nExpected Artifacts:\n- `src/repair/background_repair_controller.rs`, `tests/perf/repair_fairness.rs`, `artifacts/10.13/repair_cycle_telemetry.csv`.\n\n- Machine-readable verification artifact at `artifacts/section_10_13/bd-91gg/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_13/bd-91gg/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.13] Implement background repair controller with bounded work-per-cycle and fairness controls.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.13] Implement background repair controller with bounded work-per-cycle and fairness controls.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.13] Implement background repair controller with bounded work-per-cycle and fairness controls.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.13] Implement background repair controller with bounded work-per-cycle and fairness controls.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.13] Implement background repair controller with bounded work-per-cycle and fairness controls.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","status":"closed","priority":1,"issue_type":"task","assignee":"CrimsonCrane","created_at":"2026-02-20T07:36:53.843418486Z","created_by":"ubuntu","updated_at":"2026-02-20T12:41:27.513000782Z","closed_at":"2026-02-20T12:41:27.512973661Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-13","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-91gg","depends_on_id":"bd-29w6","type":"blocks","created_at":"2026-02-20T07:43:13.516444147Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-93k","title":"[10.11] Add checkpoint-placement contract in all long orchestration loops.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md — Section 10.11 (FrankenSQLite-Inspired Runtime Systems), cross-ref Section 9G.2, 9G.9\n\n## Why This Exists\n\nLong orchestration loops in franken_node — iterating over hundreds of trust artifacts for migration, scanning thousands of quarantine entries for promotion, replicating large batches of state across regions — must be resumable after crashes or cancellations without restarting from the beginning. Enhancement Map 9G.2 (cancellation protocol) implies that cancelled loops need to record their progress, and 9G.9 (three-tier integrity strategy) requires that progress records are tamper-evident and part of the append-only decision stream. The checkpoint-placement contract formalizes this: every long orchestration loop must place checkpoints at defined intervals, each checkpoint records the loop's progress state, and the loop can resume from the last valid checkpoint after restart.\n\nThis bead adds a checkpoint-placement contract that mandates and enforces checkpoint discipline in all long orchestration loops, providing both the checkpoint infrastructure and a compile-time/runtime gate that rejects loops exceeding a configurable iteration count without a checkpoint.\n\n## What This Must Do\n\n1. Implement a `CheckpointWriter` that accepts a serializable progress state and atomically writes it to the checkpoint store using the bounded masking helper (bd-24k) to prevent cancellation during the atomic write.\n2. Implement a `CheckpointReader` that loads the latest valid checkpoint for a given orchestration ID, enabling resumption from the last checkpointed state.\n3. Define the checkpoint-placement contract: every orchestration loop must call `checkpoint()` at least once per `max_iterations_between_checkpoints` (configurable, default 100 iterations) or once per `max_duration_between_checkpoints` (configurable, default 5 seconds), whichever comes first.\n4. Implement a `CheckpointGuard` that wraps orchestration loops and monitors compliance with the placement contract — if the loop exceeds the configured limits without checkpointing, the guard emits a structured warning and (in strict mode) aborts the loop.\n5. Record each checkpoint as an event in the append-only decision stream (per 9G.9), including: orchestration_id, iteration_count, progress_state_hash, wall_clock_time, and epoch.\n6. Ensure checkpoint data is integrity-protected: each checkpoint includes a hash chain linking it to the previous checkpoint for the same orchestration, enabling tamper detection.\n\n## Context from Enhancement Maps\n\n- 9G.2: \"Cancellation as a strict protocol for all long-running orchestration tasks\" — checkpoints enable safe resumption after cancellation.\n- 9G.9: \"Three-tier integrity strategy + append-only tamper-evident decision stream\" — checkpoints are part of the integrity stream.\n- 9J.19: \"Cancellation-complete protocol discipline with obligation closure proofs\" — checkpoint state is included in obligation closure proofs for cancelled orchestrations.\n- Architecture invariant #3 (8.5): Cancellation protocol semantics — checkpoints are the mechanism for making cancellation non-destructive.\n- Architecture invariant #8 (8.5): Evidence-by-default — checkpoint history is auditable evidence of orchestration progress.\n\n## Dependencies\n\n- Upstream: bd-24k (bounded masking for atomic checkpoint writes), bd-7om (cancel-drain-finalize — checkpoint integrates with drain phase), bd-126h (10.14 append-only marker stream for checkpoint recording), bd-2ah (obligation channels — checkpoint state referenced in obligation closure proofs)\n- Downstream: bd-390 (anti-entropy reconciliation loop uses checkpoints for resumable reconciliation), bd-1jpo (10.11 section-wide verification gate)\n\n## Acceptance Criteria\n\n1. `CheckpointWriter` atomically writes checkpoint state; a crash during write does not corrupt the checkpoint store (verified by kill-during-write test).\n2. `CheckpointReader` correctly loads the latest valid checkpoint; corrupted checkpoints are detected via hash chain and skipped with a structured alert.\n3. `CheckpointGuard` in strict mode aborts a loop that exceeds 2x `max_iterations_between_checkpoints` without checkpointing, with a clear `CheckpointContractViolation` error.\n4. `CheckpointGuard` in warn mode logs a structured warning but allows the loop to continue.\n5. Checkpoint events in the decision stream include orchestration_id, iteration_count, progress_state_hash, previous_checkpoint_hash, and epoch.\n6. Resumption from checkpoint: an orchestration loop that is killed and restarted resumes from the last checkpoint, processing only the remaining iterations (verified by counting total work done).\n7. Hash chain integrity: tampering with a checkpoint's progress_state produces a chain verification failure on the next checkpoint read.\n8. Verification evidence JSON includes checkpoints_written, checkpoints_resumed_from, checkpoint_contract_violations, hash_chain_verifications_passed, and avg_iterations_between_checkpoints fields.\n\n## Testing & Logging Requirements\n\n- Unit tests: (a) CheckpointWriter serialization round-trip for various progress state types; (b) CheckpointReader loads latest checkpoint correctly; (c) Hash chain verification detects tampered checkpoint; (d) CheckpointGuard triggers on iteration limit; (e) CheckpointGuard triggers on duration limit.\n- Integration tests: (a) Full orchestration loop with periodic checkpoints and successful completion; (b) Orchestration loop killed at iteration 150 (checkpoint at 100), restarted, resumes from iteration 100; (c) Checkpoint placement within cancel-drain-finalize lifecycle — checkpoint written during drain phase; (d) Concurrent orchestration loops with independent checkpoint streams.\n- Adversarial tests: (a) Power-kill during checkpoint write — verify atomic write prevents corruption; (b) Checkpoint store full — verify graceful degradation with structured error; (c) Extremely fast loop (1M iterations/sec) — verify checkpoint overhead is bounded; (d) Forged checkpoint with valid hash but wrong epoch — verify epoch validation catches it.\n- Structured logs: Events use stable codes (FN-CK-001 through FN-CK-008), include `orchestration_id`, `trace_id`, `iteration_count`, `checkpoint_hash`, `contract_status`. JSON-formatted.\n\n## Expected Artifacts\n\n- docs/specs/section_10_11/bd-93k_contract.md\n- crates/franken-node/src/runtime/checkpoint.rs (or equivalent module path)\n- crates/franken-node/src/runtime/checkpoint_guard.rs\n- scripts/check_checkpoint_placement.py (with --json flag and self_test())\n- tests/test_check_checkpoint_placement.py\n- artifacts/section_10_11/bd-93k/verification_evidence.json\n- artifacts/section_10_11/bd-93k/verification_summary.md","acceptance_criteria":"AC for bd-93k:\n1. Every long orchestration loop (defined as any loop that may execute more than N iterations or exceed T wall-clock seconds, where N and T are configurable) must contain at least one checkpoint call that persists recoverable state.\n2. A CheckpointContract trait is defined with methods: save_checkpoint(state) -> CheckpointId, restore_checkpoint(id) -> State, and list_checkpoints() -> Vec<CheckpointMeta>.\n3. A compile-time or lint-time contract checker verifies that all orchestration loops annotated with #[long_orchestration] contain at least one checkpoint call; missing checkpoints cause a build warning (or error in strict mode).\n4. Checkpoints are idempotent: calling save_checkpoint with identical state produces the same CheckpointId (content-addressed).\n5. On crash recovery, the orchestration loop resumes from the latest valid checkpoint rather than restarting from scratch; a test demonstrates this by injecting a panic mid-loop and verifying resumed iteration count.\n6. Checkpoint storage is pluggable (in-memory for tests, persistent for production) via a CheckpointBackend trait.\n7. Unit tests verify: (a) checkpoint save/restore round-trip, (b) idempotent checkpoint ID stability, (c) loop resumption after simulated crash, (d) missing-checkpoint lint fires on unannotated loop.\n8. Structured log events use stable codes CHECKPOINT_SAVE / CHECKPOINT_RESTORE / CHECKPOINT_MISSING with loop identifier and iteration count context.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:36:49.822962089Z","created_by":"ubuntu","updated_at":"2026-02-20T17:01:41.368309470Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-11","self-contained","test-obligations"]}
{"id":"bd-9is","title":"[10.9] Build autonomous adversarial campaign runner with continuous updates.","description":"## [10.9] Build autonomous adversarial campaign runner with continuous updates\n\n### Why This Exists\n\nSection 6.3 (threat model coverage) requires that franken_node's security posture be continuously tested against evolving adversarial techniques, not just point-in-time penetration tests. Static security tests become stale as threat landscapes shift. An autonomous adversarial campaign runner continuously generates, evolves, and executes attack campaigns — ensuring that the system's defenses are tested against both known attack patterns and novel combinations. Results feed back into the adversary graph (10.17) and trust card updates (10.4), creating a closed loop between attack simulation and defense improvement.\n\n### What It Must Do\n\nBuild an automated system that continuously generates and executes adversarial campaigns against franken_node:\n\n- **Campaign corpus**: A versioned collection of adversarial campaign definitions, each specifying: attack vector, target component, expected defense response, success/failure criteria. Initial corpus must include:\n  1. **Malicious extension injection**: Extensions that attempt to load native code, access restricted APIs, or escalate privileges beyond declared permissions.\n  2. **Credential exfiltration**: Attempts to read environment variables, file system credentials, network credential stores, or memory regions containing secrets.\n  3. **Policy evasion**: Techniques that attempt to bypass trust policies via timing attacks, TOCTOU exploits, or policy interpretation ambiguities.\n  4. **Delayed payload activation**: Extensions that behave normally during initial validation but activate malicious behavior after a delay or trigger condition.\n  5. **Supply-chain compromise simulation**: Dependency substitution, typosquatting, and version pinning attacks in the extension/package ecosystem.\n- **Campaign evolution**: The corpus evolves based on: (a) new threat intelligence feeds (CVEs, security advisories), (b) mutation of existing campaigns (parameter variation, technique combination), (c) coverage gaps identified by the adversary graph. Evolution is automated but auditable — every campaign mutation is logged with its provenance.\n- **Execution engine**: Campaigns run in isolated sandboxed environments (containers or VMs) with no access to production infrastructure. The engine executes campaigns against the current franken_node build, records all behavior (system calls, network traffic, file access, trust decisions), and evaluates success/failure against campaign criteria.\n- **Result integration**: Campaign results are structured JSON that feeds into: (a) the adversary graph (10.17) as attack surface coverage data, (b) trust card updates (10.4) as security posture evidence, (c) benchmark resilience scores (bd-f5d). Failed defenses are automatically flagged as high-priority issues.\n- **Continuous operation**: The runner operates continuously (not just in CI), with configurable campaign frequency and prioritization. High-priority campaigns (new threat intelligence) run immediately; coverage-gap campaigns run on a schedule.\n\n### Acceptance Criteria\n\n1. Initial campaign corpus contains at least five campaigns covering each of the specified attack categories, with documented attack vectors and expected defense responses.\n2. Campaign evolution produces new campaign variants from existing campaigns via mutation; at least 3 mutation strategies are implemented (parameter variation, technique combination, timing variation).\n3. All campaigns execute in isolated sandboxed environments with verified containment (no access to host network, filesystem, or production infrastructure).\n4. Campaign results are structured JSON with full execution traces, defense decision recordings, and pass/fail evaluation.\n5. Results integrate with adversary graph (10.17) and trust card (10.4) data formats; integration is verified by round-trip tests.\n6. Failed defense detections are automatically flagged with severity classification and linked to the specific campaign that triggered the failure.\n7. The runner supports both continuous operation and on-demand execution of specific campaigns.\n8. Campaign corpus and evolution history are versioned and auditable; every campaign mutation has logged provenance.\n\n### Key Dependencies\n\n- Adversary graph infrastructure (10.17) for result integration\n- Trust card system (10.4) for security posture updates\n- Benchmark infrastructure (bd-f5d) for resilience score integration\n- Sandbox/container infrastructure for isolated campaign execution\n- Extension loading and trust policy infrastructure for attack surface\n\n### Testing & Logging Requirements\n\n- Unit tests for campaign parsing, mutation strategies, and result evaluation.\n- Integration tests that execute sample campaigns and verify defense response detection.\n- Verification script (`scripts/check_adversarial_runner.py`) with `--json` and `self_test()`.\n- Campaign execution logged at INFO; defense failures logged at ERROR; mutations logged at DEBUG.\n\n### Expected Artifacts\n\n- `docs/specs/section_10_9/bd-9is_contract.md` — adversarial runner specification\n- `scripts/check_adversarial_runner.py` — verification script\n- `tests/test_check_adversarial_runner.py` — unit tests\n- `fixtures/campaigns/` — initial campaign corpus\n- `artifacts/section_10_9/bd-9is/verification_evidence.json`\n- `artifacts/section_10_9/bd-9is/verification_summary.md`","acceptance_criteria":"1. Adversarial campaign runner continuously generates and executes attack scenarios against franken_node: malformed inputs, privilege escalation attempts, resource exhaustion, timing attacks, and supply-chain injection simulations.\n2. Runner updates its attack corpus automatically by ingesting new CVEs, fuzzer findings, and adversarial patterns from configured feeds.\n3. Campaign runs on a schedule (at least daily in CI) and produces a structured JSON report with: scenarios executed, passes, failures, new findings, and severity classifications.\n4. Any new finding is automatically filed as a bead with severity tag and linked to the campaign run that discovered it.\n5. Runner supports pluggable attack generators: new attack categories can be added via a documented interface without modifying the runner core.\n6. Per Section 9F moonshot bets: runner tracks attacker-ROI metrics — estimated cost-to-attack vs. cost-to-defend for each scenario.\n7. Historical trend data is persisted and a regression detector alerts when previously-passing scenarios begin failing.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:36:48.365808717Z","created_by":"ubuntu","updated_at":"2026-02-20T16:59:05.328829536Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-9","self-contained","test-obligations"]}
{"id":"bd-ac83","title":"[10.14] Implement named remote computation registry and reject unknown computation identifiers.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.14 — FrankenSQLite Deep-Mined Expansion Execution Track (9J)\n\nWhy This Exists:\nFrankenSQLite deep-mined control integrity track: evidence ledgers, proofs, epochs, remote effects, markers/MMR, and deterministic fault labs.\n\nTask Objective:\nImplement named remote computation registry and reject unknown computation identifiers.\n\nAcceptance Criteria:\n- Remote execution accepts only registered computation names; unknown or malformed names are rejected with stable codes; registry is versioned.\n\nExpected Artifacts:\n- `src/remote/computation_registry.rs`, `tests/conformance/remote_name_registry.rs`, `artifacts/10.14/remote_registry_catalog.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_14/bd-ac83/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_14/bd-ac83/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.14] Implement named remote computation registry and reject unknown computation identifiers.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.14] Implement named remote computation registry and reject unknown computation identifiers.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.14] Implement named remote computation registry and reject unknown computation identifiers.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.14] Implement named remote computation registry and reject unknown computation identifiers.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.14] Implement named remote computation registry and reject unknown computation identifiers.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"Remote execution accepts only registered computation names; unknown or malformed names are rejected with stable codes; registry is versioned.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:36:57.731532743Z","created_by":"ubuntu","updated_at":"2026-02-20T15:44:06.037402580Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-14","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-ac83","depends_on_id":"bd-1nfu","type":"blocks","created_at":"2026-02-20T07:43:15.527702593Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-al8i","title":"[10.17] Implement L2 engine-boundary N-version semantic oracle across franken_engine and reference runtimes.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.17 — Radical Expansion Execution Track (9K)\n\nWhy This Exists:\nRadical expansion track for proof-carrying speculation, Bayesian adversary control, time-travel replay, ZK attestations, and claim compiler systems.\n\nTask Objective:\nImplement L2 engine-boundary N-version semantic oracle across franken_engine and reference runtimes.\n\nAcceptance Criteria:\n- Differential harness classifies boundary divergences by risk tier and blocks release on high-risk unresolved deltas; low-risk deltas require explicit policy receipts and link back to L1 product-oracle results.\n\nExpected Artifacts:\n- `tests/oracle/n_version_semantic_oracle.rs`, `docs/testing/semantic_oracle_policy.md`, `artifacts/10.17/semantic_oracle_divergence_matrix.csv`.\n\n- Machine-readable verification artifact at `artifacts/section_10_17/bd-al8i/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_17/bd-al8i/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.17] Implement L2 engine-boundary N-version semantic oracle across franken_engine and reference runtimes.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.17] Implement L2 engine-boundary N-version semantic oracle across franken_engine and reference runtimes.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.17] Implement L2 engine-boundary N-version semantic oracle across franken_engine and reference runtimes.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.17] Implement L2 engine-boundary N-version semantic oracle across franken_engine and reference runtimes.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.17] Implement L2 engine-boundary N-version semantic oracle across franken_engine and reference runtimes.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"- Differential harness classifies boundary divergences by risk tier and blocks release on high-risk unresolved deltas; low-risk deltas require explicit policy receipts and link back to L1 product-oracle results.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:37:03.433918277Z","created_by":"ubuntu","updated_at":"2026-02-20T15:45:59.739854822Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-17","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-al8i","depends_on_id":"bd-kcg9","type":"blocks","created_at":"2026-02-20T07:43:18.519351208Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-aoq6","title":"[10.21] Integrate evolution stability scoring into migration autopilot dependency admission and rollback gates.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.21 — Behavioral Phenotype Evolution Tracker Execution Track (9O)\n\nWhy This Exists:\nBPET longitudinal phenotype intelligence track for pre-compromise trajectory detection and integration with DGIS/ATC/migration/economic policy.\n\nTask Objective:\nIntegrate evolution stability scoring into migration autopilot dependency admission and rollback gates.\n\nAcceptance Criteria:\n- Migration planner includes trajectory-stability constraints; upgrades crossing risk thresholds require additional evidence or staged rollout with automated fallback plans.\n\nExpected Artifacts:\n- `src/migration/bpet_migration_gate.rs`, `tests/integration/bpet_migration_stability_gate.rs`, `artifacts/10.21/bpet_migration_gate_results.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_21/bd-aoq6/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_21/bd-aoq6/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.21] Integrate evolution stability scoring into migration autopilot dependency admission and rollback gates.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.21] Integrate evolution stability scoring into migration autopilot dependency admission and rollback gates.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.21] Integrate evolution stability scoring into migration autopilot dependency admission and rollback gates.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.21] Integrate evolution stability scoring into migration autopilot dependency admission and rollback gates.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.21] Integrate evolution stability scoring into migration autopilot dependency admission and rollback gates.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"- Migration planner includes trajectory-stability constraints; upgrades crossing risk thresholds require additional evidence or staged rollout with automated fallback plans.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:37:08.627241090Z","created_by":"ubuntu","updated_at":"2026-02-20T17:01:06.763732188Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-21","self-contained","test-obligations"]}
{"id":"bd-b44","title":"[10.13] Add state schema version contracts and deterministic migration hint execution checks.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.13 — FCP Deep-Mined Expansion Execution Track (9I)\n\nWhy This Exists:\nDeep connector/provider contract track: lifecycle FSMs, conformance harnesses, fencing, sandboxing, revocation freshness, and protocol hardening.\n\nTask Objective:\nAdd state schema version contracts and deterministic migration hint execution checks.\n\nAcceptance Criteria:\n- Version transitions require declared migration path; migrations are idempotent and replay-stable; failed migrations rollback cleanly.\n\nExpected Artifacts:\n- `docs/specs/state_schema_migrations.md`, `tests/integration/state_migration_contract.rs`, `artifacts/10.13/state_migration_receipts.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_13/bd-b44/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_13/bd-b44/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.13] Add state schema version contracts and deterministic migration hint execution checks.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.13] Add state schema version contracts and deterministic migration hint execution checks.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.13] Add state schema version contracts and deterministic migration hint execution checks.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.13] Add state schema version contracts and deterministic migration hint execution checks.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.13] Add state schema version contracts and deterministic migration hint execution checks.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","status":"closed","priority":1,"issue_type":"task","assignee":"CrimsonCrane","created_at":"2026-02-20T07:36:52.041101645Z","created_by":"ubuntu","updated_at":"2026-02-20T11:07:38.066236989Z","closed_at":"2026-02-20T11:07:38.066211973Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-13","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-b44","depends_on_id":"bd-24s","type":"blocks","created_at":"2026-02-20T07:43:12.569358060Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-b541","title":"[10.20] Define canonical dependency/topology graph schema covering packages, extensions, publishers, maintainers, and transitive edge semantics.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.20 — Dependency Graph Immune System Execution Track (9N)\n\nWhy This Exists:\nDGIS topological immune system track for dependency contagion modeling, choke-point immunization, and graph-aware containment economics.\n\nTask Objective:\nDefine canonical dependency/topology graph schema covering packages, extensions, publishers, maintainers, and transitive edge semantics.\n\nAcceptance Criteria:\n- Schema captures runtime/build/provenance edge types, trust metadata, update cadence, and policy annotations; identical inputs yield hash-stable graph serialization and signed snapshots.\n\nExpected Artifacts:\n- `docs/specs/dgis_graph_schema.md`, `spec/dgis_graph_schema_v1.json`, `artifacts/10.20/dgis_graph_schema_vectors.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_20/bd-b541/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_20/bd-b541/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.20] Define canonical dependency/topology graph schema covering packages, extensions, publishers, maintainers, and transitive edge semantics.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.20] Define canonical dependency/topology graph schema covering packages, extensions, publishers, maintainers, and transitive edge semantics.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.20] Define canonical dependency/topology graph schema covering packages, extensions, publishers, maintainers, and transitive edge semantics.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.20] Define canonical dependency/topology graph schema covering packages, extensions, publishers, maintainers, and transitive edge semantics.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.20] Define canonical dependency/topology graph schema covering packages, extensions, publishers, maintainers, and transitive edge semantics.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"- Schema captures runtime/build/provenance edge types, trust metadata, update cadence, and policy annotations; identical inputs yield hash-stable graph serialization and signed snapshots.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:37:06.418562507Z","created_by":"ubuntu","updated_at":"2026-02-20T15:45:26.154166219Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-20","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-b541","depends_on_id":"bd-1wz","type":"blocks","created_at":"2026-02-20T07:46:35.183993121Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-b541","depends_on_id":"bd-39a","type":"blocks","created_at":"2026-02-20T07:46:35.230039473Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-b541","depends_on_id":"bd-cda","type":"blocks","created_at":"2026-02-20T07:46:35.274404895Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-b9b6","title":"[10.14] Emit \"durability contract violated\" diagnostic bundles when hardening cannot restore verifiability.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.14 — FrankenSQLite Deep-Mined Expansion Execution Track (9J)\n\nWhy This Exists:\nFrankenSQLite deep-mined control integrity track: evidence ledgers, proofs, epochs, remote effects, markers/MMR, and deterministic fault labs.\n\nTask Objective:\nEmit \"durability contract violated\" diagnostic bundles when hardening cannot restore verifiability.\n\nAcceptance Criteria:\n- Violation bundles include causal event sequence, failed artifacts, and proof context; bundle generation is deterministic; gating operations are halted per policy.\n\nExpected Artifacts:\n- `docs/runbooks/durability_contract_violated.md`, `tests/integration/durability_violation_bundle.rs`, `artifacts/10.14/durability_violation_bundle_example.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_14/bd-b9b6/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_14/bd-b9b6/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.14] Emit \"durability contract violated\" diagnostic bundles when hardening cannot restore verifiability.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.14] Emit \"durability contract violated\" diagnostic bundles when hardening cannot restore verifiability.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.14] Emit \"durability contract violated\" diagnostic bundles when hardening cannot restore verifiability.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.14] Emit \"durability contract violated\" diagnostic bundles when hardening cannot restore verifiability.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.14] Emit \"durability contract violated\" diagnostic bundles when hardening cannot restore verifiability.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"Violation bundles include causal event sequence, failed artifacts, and proof context; bundle generation is deterministic; gating operations are halted per policy.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:36:56.463373737Z","created_by":"ubuntu","updated_at":"2026-02-20T16:24:04.015945523Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-14","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-b9b6","depends_on_id":"bd-nupr","type":"blocks","created_at":"2026-02-20T16:24:04.015887003Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-bq4p","title":"[10.14] Implement controller boundary checks rejecting any attempted correctness-semantic mutation.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.14 — FrankenSQLite Deep-Mined Expansion Execution Track (9J)\n\nWhy This Exists:\nFrankenSQLite deep-mined control integrity track: evidence ledgers, proofs, epochs, remote effects, markers/MMR, and deterministic fault labs.\n\nTask Objective:\nImplement controller boundary checks rejecting any attempted correctness-semantic mutation.\n\nAcceptance Criteria:\n- Boundary checks run pre-apply for every policy proposal; violation attempts return stable error class; audit trail records rejected mutation intent.\n\nExpected Artifacts:\n- `src/policy/controller_boundary_checks.rs`, `tests/security/controller_mutation_rejection.rs`, `artifacts/10.14/controller_boundary_rejections.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_14/bd-bq4p/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_14/bd-bq4p/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.14] Implement controller boundary checks rejecting any attempted correctness-semantic mutation.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.14] Implement controller boundary checks rejecting any attempted correctness-semantic mutation.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.14] Implement controller boundary checks rejecting any attempted correctness-semantic mutation.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.14] Implement controller boundary checks rejecting any attempted correctness-semantic mutation.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.14] Implement controller boundary checks rejecting any attempted correctness-semantic mutation.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"Boundary checks run pre-apply for every policy proposal; violation attempts return stable error class; audit trail records rejected mutation intent.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:36:55.629743289Z","created_by":"ubuntu","updated_at":"2026-02-20T15:44:11.455822653Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-14","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-bq4p","depends_on_id":"bd-sddz","type":"blocks","created_at":"2026-02-20T07:43:14.470559086Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-bq6y","title":"[10.13] Implement generic lease service for operation execution, state writes, and migration handoff.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.13 — FCP Deep-Mined Expansion Execution Track (9I)\n\nWhy This Exists:\nDeep connector/provider contract track: lifecycle FSMs, conformance harnesses, fencing, sandboxing, revocation freshness, and protocol hardening.\n\nTask Objective:\nImplement generic lease service for operation execution, state writes, and migration handoff.\n\nAcceptance Criteria:\n- Lease API supports all required purposes with shared semantics; lease expiry and renewal behavior is deterministic; stale lease usage is rejected.\n\nExpected Artifacts:\n- `src/control_plane/lease_service.rs`, `docs/specs/generic_leases.md`, `artifacts/10.13/lease_service_contract.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_13/bd-bq6y/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_13/bd-bq6y/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.13] Implement generic lease service for operation execution, state writes, and migration handoff.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.13] Implement generic lease service for operation execution, state writes, and migration handoff.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.13] Implement generic lease service for operation execution, state writes, and migration handoff.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.13] Implement generic lease service for operation execution, state writes, and migration handoff.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.13] Implement generic lease service for operation execution, state writes, and migration handoff.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","status":"closed","priority":1,"issue_type":"task","assignee":"CrimsonCrane","created_at":"2026-02-20T07:36:53.275390171Z","created_by":"ubuntu","updated_at":"2026-02-20T12:11:47.261333863Z","closed_at":"2026-02-20T12:11:47.261308806Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-13","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-bq6y","depends_on_id":"bd-w0jq","type":"blocks","created_at":"2026-02-20T07:43:13.225997189Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-bt82","title":"[10.16] Define `sqlmodel_rust` usage policy and typed model boundaries.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.16 — Adjacent Substrate Integration Execution Track (8.7)\n\nWhy This Exists:\nAdjacent substrate integration track to enforce deep composition with frankentui, frankensqlite, sqlmodel_rust, and fastapi_rust.\n\nTask Objective:\nDefine `sqlmodel_rust` usage policy and typed model boundaries.\n\nAcceptance Criteria:\n- Policy defines where typed models are mandatory vs optional; model ownership and codegen/versioning expectations are explicit.\n\nExpected Artifacts:\n- `docs/specs/sqlmodel_rust_usage_policy.md`, `artifacts/10.16/sqlmodel_policy_matrix.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_16/bd-bt82/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_16/bd-bt82/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.16] Define `sqlmodel_rust` usage policy and typed model boundaries.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.16] Define `sqlmodel_rust` usage policy and typed model boundaries.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.16] Define `sqlmodel_rust` usage policy and typed model boundaries.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.16] Define `sqlmodel_rust` usage policy and typed model boundaries.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.16] Define `sqlmodel_rust` usage policy and typed model boundaries.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"- Policy defines where typed models are mandatory vs optional; model ownership and codegen/versioning expectations are explicit.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:37:02.185287076Z","created_by":"ubuntu","updated_at":"2026-02-20T17:00:57.706540610Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-16","self-contained","test-obligations"]}
{"id":"bd-c1ri","title":"Epic: Control Channel + Telemetry [10.13g]","description":"- type: epic","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-20T07:47:58.072163029Z","updated_at":"2026-02-20T07:49:21.192901846Z","closed_at":"2026-02-20T07:49:21.192884895Z","close_reason":"Superseded by canonical detailed master-plan graph (bd-33v) with full 10.N-10.21 and 11-16 coverage, explicit dependencies, and per-section verification gates.","source_repo":".","compaction_level":0,"original_size":0}
{"id":"bd-c4f","title":"[PLAN 10.8] Operational Readiness","description":"Section: 10.8 — Operational Readiness\n\nStrategic Context:\nOperational readiness and fleet safety posture: control APIs, observability contracts, safe-mode operations, and disaster drills.\n\nExecution Requirements:\n- Preserve all scoped capabilities from the canonical plan.\n- Keep contracts self-contained so future contributors can execute without reopening the master plan.\n- Require explicit testing strategy (unit + integration/e2e) and structured logging/telemetry for every child bead.\n- Require artifact-backed validation for performance, security, and correctness claims.\n\nDependency Semantics:\n- This epic is blocked by its child implementation beads.\n- Cross-epic dependencies encode strategic sequencing and canonical ownership boundaries.\n\n## Success Criteria\n- All child beads for this section are completed with acceptance artifacts attached and no unresolved blockers.\n- Section-level verification gate for comprehensive unit tests, integration/e2e workflows, and detailed structured logging evidence is green.\n- Scope-to-plan traceability is explicit, with no feature loss versus the canonical plan section.\n\n## Optimization Notes\n- User-Outcome Lens: \"[PLAN 10.8] Operational Readiness\" must improve operator confidence, safety posture, and deterministic recovery behavior under both normal and adversarial conditions.\n- Cross-Section Coordination: child beads must encode integration assumptions explicitly to avoid local optimizations that degrade system-wide correctness.\n- Verification Non-Negotiable: completion requires reproducible unit/integration/E2E evidence and structured logs suitable for independent replay.\n- Scope Protection: any simplification must preserve canonical-plan feature intent and be justified with explicit tradeoff documentation.","status":"open","priority":2,"issue_type":"epic","created_at":"2026-02-20T07:36:40.869070676Z","created_by":"ubuntu","updated_at":"2026-02-20T08:16:48.401705139Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-8"],"dependencies":[{"issue_id":"bd-c4f","depends_on_id":"bd-1fi2","type":"blocks","created_at":"2026-02-20T07:48:26.408839126Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-c4f","depends_on_id":"bd-1hf","type":"blocks","created_at":"2026-02-20T07:37:10.473264242Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-c4f","depends_on_id":"bd-1ta","type":"blocks","created_at":"2026-02-20T07:37:10.512039328Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-c4f","depends_on_id":"bd-20a","type":"blocks","created_at":"2026-02-20T07:37:10.433812366Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-c4f","depends_on_id":"bd-3m6","type":"blocks","created_at":"2026-02-20T07:36:48.244705350Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-c4f","depends_on_id":"bd-3o6","type":"blocks","created_at":"2026-02-20T07:36:47.903942117Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-c4f","depends_on_id":"bd-cda","type":"blocks","created_at":"2026-02-20T07:37:09.347462613Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-c4f","depends_on_id":"bd-f2y","type":"blocks","created_at":"2026-02-20T07:36:48.062724949Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-c4f","depends_on_id":"bd-k6o","type":"blocks","created_at":"2026-02-20T07:36:47.984291001Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-c4f","depends_on_id":"bd-nr4","type":"blocks","created_at":"2026-02-20T07:36:48.163231450Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-c4f","depends_on_id":"bd-tg2","type":"blocks","created_at":"2026-02-20T07:36:47.787141765Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-c781","title":"[11] Section-wide verification gate: comprehensive unit+e2e+logging","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 11\n\nTask Objective:\nCreate a hard section completion gate that verifies all implemented behavior in this section with comprehensive unit tests, integration/e2e scripts, and detailed structured logging evidence.\n\nAcceptance Criteria:\n- Section test matrix covers happy-path, edge-case, and adversarial/error-path scenarios for all section deliverables.\n- E2E scripts execute representative end-user workflows and policy/control-plane transitions for this section.\n- Structured logs include stable event/error codes, trace correlation IDs, and enough context for deterministic replay triage.\n- Verification report is deterministic and machine-readable for CI/release gating.\n\nExpected Artifacts:\n- Section-level test matrix and coverage summary artifact.\n- E2E script suite with pass/fail outputs and reproducible fixtures/seeds.\n- Structured log validation report and traceability bundle.\n- Gate verdict artifact consumable by release automation.\n\n- Machine-readable verification artifact at `artifacts/section_11/bd-c781/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_11/bd-c781/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests must validate contracts, invariants, and failure semantics.\n- E2E tests must validate cross-component behavior from user/operator entrypoints to persisted effects.\n- Logging must support root-cause analysis without hidden context requirements.\n\nTask-Specific Clarification:\n- For \"[11] Section-wide verification gate: comprehensive unit+e2e+logging\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[11] Section-wide verification gate: comprehensive unit+e2e+logging\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[11] Section-wide verification gate: comprehensive unit+e2e+logging\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[11] Section-wide verification gate: comprehensive unit+e2e+logging\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[11] Section-wide verification gate: comprehensive unit+e2e+logging\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"1. Section 11 verification gate runs all section-11 check scripts and confirms 100% pass rate.\n2. Gate validates: (a) every contract field check script exists and passes self-test, (b) every contract field has unit tests that pass, (c) integration test demonstrates end-to-end contract validation on a sample PR.\n3. Evidence artifacts for all section-11 beads are present under artifacts/section_11/.\n4. Logging covers: gate start/end timestamps, per-check pass/fail, overall verdict.\n5. Gate produces a section_11_verification_summary.md with pass/fail matrix for all sub-beads.\n6. The gate itself has a unit test verifying it correctly aggregates sub-check results.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:48:27.383018037Z","created_by":"ubuntu","updated_at":"2026-02-20T15:17:26.990000734Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-11","test-obligations","verification-gate"],"dependencies":[{"issue_id":"bd-c781","depends_on_id":"bd-1dpd","type":"blocks","created_at":"2026-02-20T08:24:24.173598144Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-c781","depends_on_id":"bd-1jmq","type":"blocks","created_at":"2026-02-20T07:48:27.780026176Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-c781","depends_on_id":"bd-2fpj","type":"blocks","created_at":"2026-02-20T07:48:27.731249838Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-c781","depends_on_id":"bd-2twu","type":"blocks","created_at":"2026-02-20T08:20:49.636723874Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-c781","depends_on_id":"bd-2ut3","type":"blocks","created_at":"2026-02-20T07:48:27.481438841Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-c781","depends_on_id":"bd-2ymp","type":"blocks","created_at":"2026-02-20T07:48:27.631507492Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-c781","depends_on_id":"bd-36wa","type":"blocks","created_at":"2026-02-20T07:48:27.831121033Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-c781","depends_on_id":"bd-3l8d","type":"blocks","created_at":"2026-02-20T07:48:27.530747802Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-c781","depends_on_id":"bd-3se1","type":"blocks","created_at":"2026-02-20T07:48:27.882729136Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-c781","depends_on_id":"bd-3v8f","type":"blocks","created_at":"2026-02-20T07:48:27.682492504Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-c781","depends_on_id":"bd-nglx","type":"blocks","created_at":"2026-02-20T07:48:27.582602815Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-c97l","title":"[10.20] Integrate DGIS topological context into trust cards, adversary graph posterior updates, and extension risk UI.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.20 — Dependency Graph Immune System Execution Track (9N)\n\nWhy This Exists:\nDGIS topological immune system track for dependency contagion modeling, choke-point immunization, and graph-aware containment economics.\n\nTask Objective:\nIntegrate DGIS topological context into trust cards, adversary graph posterior updates, and extension risk UI.\n\nAcceptance Criteria:\n- Risk surfaces show node-level topological blast-radius context and delta impact from planned updates; posterior scoring incorporates topology features with explicit attribution.\n\nExpected Artifacts:\n- `src/security/dgis/risk_surface_integration.rs`, `tests/integration/dgis_trust_card_integration.rs`, `artifacts/10.20/dgis_risk_ui_snapshot.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_20/bd-c97l/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_20/bd-c97l/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.20] Integrate DGIS topological context into trust cards, adversary graph posterior updates, and extension risk UI.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.20] Integrate DGIS topological context into trust cards, adversary graph posterior updates, and extension risk UI.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.20] Integrate DGIS topological context into trust cards, adversary graph posterior updates, and extension risk UI.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.20] Integrate DGIS topological context into trust cards, adversary graph posterior updates, and extension risk UI.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.20] Integrate DGIS topological context into trust cards, adversary graph posterior updates, and extension risk UI.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"- Risk surfaces show node-level topological blast-radius context and delta impact from planned updates; posterior scoring incorporates topology features with explicit attribution.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:37:06.997270565Z","created_by":"ubuntu","updated_at":"2026-02-20T17:05:04.514586581Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-20","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-c97l","depends_on_id":"bd-t89w","type":"blocks","created_at":"2026-02-20T17:05:04.514535866Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-cclm","title":"[10.20] Add adversarial validation suite (graph poisoning, edge-obfuscation, fake-low-risk pivots, delayed activation) with fail-closed semantics.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.20 — Dependency Graph Immune System Execution Track (9N)\n\nWhy This Exists:\nDGIS topological immune system track for dependency contagion modeling, choke-point immunization, and graph-aware containment economics.\n\nTask Objective:\nAdd adversarial validation suite (graph poisoning, edge-obfuscation, fake-low-risk pivots, delayed activation) with fail-closed semantics.\n\nAcceptance Criteria:\n- Adversarial campaigns are encoded as deterministic fixtures; DGIS detects or bounds damage within defined limits; bypass attempts emit stable error classes and remediation hints.\n\nExpected Artifacts:\n- `tests/security/dgis_adversarial_suite.rs`, `docs/security/dgis_attack_playbook.md`, `artifacts/10.20/dgis_adversarial_results.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_20/bd-cclm/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_20/bd-cclm/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.20] Add adversarial validation suite (graph poisoning, edge-obfuscation, fake-low-risk pivots, delayed activation) with fail-closed semantics.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.20] Add adversarial validation suite (graph poisoning, edge-obfuscation, fake-low-risk pivots, delayed activation) with fail-closed semantics.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.20] Add adversarial validation suite (graph poisoning, edge-obfuscation, fake-low-risk pivots, delayed activation) with fail-closed semantics.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.20] Add adversarial validation suite (graph poisoning, edge-obfuscation, fake-low-risk pivots, delayed activation) with fail-closed semantics.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.20] Add adversarial validation suite (graph poisoning, edge-obfuscation, fake-low-risk pivots, delayed activation) with fail-closed semantics.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"- Adversarial campaigns are encoded as deterministic fixtures; DGIS detects or bounds damage within defined limits; bypass attempts emit stable error classes and remediation hints.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:37:07.527097073Z","created_by":"ubuntu","updated_at":"2026-02-20T17:05:13.841978035Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-20","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-cclm","depends_on_id":"bd-2bj4","type":"blocks","created_at":"2026-02-20T17:05:13.841930056Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-cda","title":"[PLAN 10.N] Execution Normalization Contract (No Duplicate Implementations)","description":"Section: 10.N — Execution Normalization Contract (No Duplicate Implementations)\n\nStrategic Context:\nExecution normalization to prevent duplicate implementations, enforce canonical ownership, and keep cross-track integration coherent.\n\nExecution Requirements:\n- Preserve all scoped capabilities from the canonical plan.\n- Keep contracts self-contained so future contributors can execute without reopening the master plan.\n- Require explicit testing strategy (unit + integration/e2e) and structured logging/telemetry for every child bead.\n- Require artifact-backed validation for performance, security, and correctness claims.\n\nDependency Semantics:\n- This epic is blocked by its child implementation beads.\n- Cross-epic dependencies encode strategic sequencing and canonical ownership boundaries.\n\n## Success Criteria\n- All child beads for this section are completed with acceptance artifacts attached and no unresolved blockers.\n- Section-level verification gate for comprehensive unit tests, integration/e2e workflows, and detailed structured logging evidence is green.\n- Scope-to-plan traceability is explicit, with no feature loss versus the canonical plan section.\n\n## Optimization Notes\n- User-Outcome Lens: \"[PLAN 10.N] Execution Normalization Contract (No Duplicate Implementations)\" must improve operator confidence, safety posture, and deterministic recovery behavior under both normal and adversarial conditions.\n- Cross-Section Coordination: child beads must encode integration assumptions explicitly to avoid local optimizations that degrade system-wide correctness.\n- Verification Non-Negotiable: completion requires reproducible unit/integration/E2E evidence and structured logs suitable for independent replay.\n- Scope Protection: any simplification must preserve canonical-plan feature intent and be justified with explicit tradeoff documentation.","status":"closed","priority":1,"issue_type":"epic","created_at":"2026-02-20T07:36:40.132488047Z","created_by":"ubuntu","updated_at":"2026-02-20T08:41:50.891275800Z","closed_at":"2026-02-20T08:41:50.891190381Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-N"],"dependencies":[{"issue_id":"bd-cda","depends_on_id":"bd-1neb","type":"blocks","created_at":"2026-02-20T07:48:27.220550663Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-cda","depends_on_id":"bd-1oyt","type":"blocks","created_at":"2026-02-20T07:50:04.664351830Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-cda","depends_on_id":"bd-1v2c","type":"blocks","created_at":"2026-02-20T07:50:04.758779969Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-cda","depends_on_id":"bd-2yhs","type":"blocks","created_at":"2026-02-20T07:50:04.569309366Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-cda","depends_on_id":"bd-zxk8","type":"blocks","created_at":"2026-02-20T07:50:04.448172801Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-ck2h","title":"[10.13] Define MVP vs Full conformance profile matrix and publication claim rules.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.13 — FCP Deep-Mined Expansion Execution Track (9I)\n\nWhy This Exists:\nDeep connector/provider contract track: lifecycle FSMs, conformance harnesses, fencing, sandboxing, revocation freshness, and protocol hardening.\n\nTask Objective:\nDefine MVP vs Full conformance profile matrix and publication claim rules.\n\nAcceptance Criteria:\n- Profile matrix maps required capabilities to claim language; publication metadata is generated from measured profile results; unsupported claims are blocked.\n\nExpected Artifacts:\n- `docs/conformance/profile_matrix.md`, `tests/conformance/profile_claim_gate.rs`, `artifacts/10.13/profile_claim_report.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_13/bd-ck2h/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_13/bd-ck2h/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.13] Define MVP vs Full conformance profile matrix and publication claim rules.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.13] Define MVP vs Full conformance profile matrix and publication claim rules.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.13] Define MVP vs Full conformance profile matrix and publication claim rules.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.13] Define MVP vs Full conformance profile matrix and publication claim rules.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.13] Define MVP vs Full conformance profile matrix and publication claim rules.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","status":"closed","priority":1,"issue_type":"task","assignee":"CrimsonCrane","created_at":"2026-02-20T07:36:54.818421576Z","created_by":"ubuntu","updated_at":"2026-02-20T13:29:24.402952603Z","closed_at":"2026-02-20T13:29:24.402923880Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-13","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-ck2h","depends_on_id":"bd-1gnb","type":"blocks","created_at":"2026-02-20T07:43:14.020689699Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-cuut","title":"[10.15] Define lane mapping policy for control-plane workloads (Cancel/Timed/Ready).","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.15 — Asupersync-First Integration Execution Track (8.4-8.6)\n\nWhy This Exists:\nAsupersync-first control-plane migration track enforcing Cx-first, region ownership, cancellation correctness, and protocol-grade verification gates.\n\nTask Objective:\nDefine lane mapping policy for control-plane workloads (Cancel/Timed/Ready).\n\nAcceptance Criteria:\n- Every control task class has lane assignment and budget policy; starvation checks are automated.\n\nExpected Artifacts:\n- `docs/specs/control_lane_mapping.md`, `tests/conformance/control_lane_policy.rs`, `artifacts/10.15/lane_starvation_metrics.csv`.\n\n- Machine-readable verification artifact at `artifacts/section_10_15/bd-cuut/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_15/bd-cuut/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.15] Define lane mapping policy for control-plane workloads (Cancel/Timed/Ready).\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.15] Define lane mapping policy for control-plane workloads (Cancel/Timed/Ready).\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.15] Define lane mapping policy for control-plane workloads (Cancel/Timed/Ready).\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.15] Define lane mapping policy for control-plane workloads (Cancel/Timed/Ready).\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.15] Define lane mapping policy for control-plane workloads (Cancel/Timed/Ready).\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"- Every control task class has lane assignment and budget policy; starvation checks are automated.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:37:00.053329409Z","created_by":"ubuntu","updated_at":"2026-02-20T17:00:44.686083018Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-15","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-cuut","depends_on_id":"bd-qlc6","type":"blocks","created_at":"2026-02-20T14:59:48.249813942Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-cv49","title":"[15] Adoption target: published security/ops improvement case studies","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 15\n\nTask Objective:\nDeliver deterministic migration validation and publish measurable security/ops case studies.\n\nExecution Requirements:\n- Make this requirement machine-verifiable and release-gated where applicable.\n- Keep behavior self-documenting so future contributors do not need to re-open the master plan.\n\nTesting & Logging Requirements:\n- Unit tests for rule/contract logic and error conditions.\n- Integration/E2E tests for workflow-level enforcement.\n- Structured logs with stable codes and trace IDs.\n- Reproducible artifacts that prove contract satisfaction.\n\nAcceptance Criteria:\n- Success and failure invariants for [15] Adoption target: published security/ops improvement case studies are explicitly quantified and machine-verifiable.\n- Determinism requirements for [15] Adoption target: published security/ops improvement case studies are validated across repeated runs and adversarial perturbations.\n\n\nExpected Artifacts:\n- Machine-readable verification artifact with explicit canonical path under artifacts/section_15/bd-cv49/verification_evidence.json, suitable for CI/release gating.\n- Human-readable verification summary with explicit canonical path under artifacts/section_15/bd-cv49/verification_summary.md, linking implementation intent to measured validation outcomes.\n\nTask-Specific Clarification:\n- The implementation must deliver the exact capability named in the title, with no scope dilution and no silent feature omission.\n- Validation artifacts must include capability-specific thresholds, pass/fail criteria, and reproducible evidence bundles tied to this bead ID.\n- Program/section verification gates must be able to consume this bead's outputs without manual interpretation.\n\nWhy This Improves User Outcomes:\n- \"[15] Adoption target: published security/ops improvement case studies\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[15] Adoption target: published security/ops improvement case studies\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"1. At least 3 case studies published documenting real-world security and operational improvements from adopting franken_node.\n2. Each case study includes: (a) organization context (size, industry, anonymized if needed), (b) pre-adoption security posture metrics, (c) post-adoption security posture metrics, (d) operational improvements (incident response time, deployment frequency, etc.), (e) migration effort and timeline, (f) lessons learned and recommendations.\n3. Quantitative improvements documented: at least 2 case studies show measurable security improvement (e.g., X% reduction in vulnerabilities, Y% faster incident containment).\n4. Case studies are reviewed by the featured organization before publication.\n5. Case studies are published on project website and submitted to at least 1 industry publication or conference.\n6. Case study template exists for partners to self-author with editorial support.\n7. Evidence: case_study_registry.json with per-study: title, organization type, key metrics, publication status, and URL.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:39:36.696204017Z","created_by":"ubuntu","updated_at":"2026-02-20T15:27:30.435957516Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-15","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-cv49","depends_on_id":"bd-elog","type":"blocks","created_at":"2026-02-20T07:43:26.472641055Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-cvt","title":"[10.11] Define capability profiles for product subsystems and enforce narrowing.","description":"[10.11] Define capability profiles for product subsystems and enforce narrowing.\n\n## Why This Exists\n\nSection 9G.1 requires least-privilege capability narrowing for every product subsystem. Today, subsystems implicitly have access to all host capabilities (network, filesystem, process spawning, crypto operations, trust-state mutation). This violates the Impossible-by-Default security posture: a compromised or buggy subsystem can escalate beyond its intended scope. This bead establishes explicit capability profiles — each subsystem declares what it needs, and the runtime enforces that declaration. Undeclared capability usage is rejected at compile time (via static analysis) and at runtime (via capability guards).\n\n## What It Must Do\n\n1. **Capability taxonomy.** Define the canonical set of capabilities: `cap:network:listen`, `cap:network:connect`, `cap:fs:read`, `cap:fs:write`, `cap:fs:temp`, `cap:process:spawn`, `cap:crypto:sign`, `cap:crypto:verify`, `cap:crypto:derive`, `cap:trust:read`, `cap:trust:write`, `cap:trust:revoke`. Each capability has a description, risk level (low/medium/high/critical), and audit requirements.\n\n2. **Capability profile declaration.** Each subsystem declares its capability profile in a TOML file (`capabilities/<subsystem>.toml`). The profile lists required capabilities with justification strings. Example: `[capabilities] network_connect = { required = true, justification = \"Connects to upstream trust anchors\" }`.\n\n3. **Narrowing enforcement at CI.** A CI gate script (`scripts/check_capability_profiles.py`) statically analyzes the codebase to detect capability usage (e.g., `std::net::TcpStream` implies `cap:network:connect`, `std::fs::write` implies `cap:fs:write`). If a subsystem uses a capability not declared in its profile, the gate fails with a specific diagnostic.\n\n4. **Runtime capability guards.** Implement a `CapabilityGuard` middleware that wraps capability-gated operations. At runtime, the guard checks the calling subsystem's profile before allowing the operation. Violations are logged as security events and the operation is denied.\n\n5. **Capability audit trail.** Every capability exercise is logged with subsystem identity, capability name, timestamp, and outcome (granted/denied). Audit logs feed into the telemetry namespace (10.13) for monitoring.\n\n6. **Profile review workflow.** When a subsystem's capability profile changes (new capability added), the change requires explicit review. The CI gate detects profile diffs and flags them for security review.\n\n## Acceptance Criteria\n\n1. Capability taxonomy documented in `docs/specs/section_10_11/bd-cvt_contract.md` with all capabilities enumerated.\n2. At least 5 subsystem capability profiles exist in `capabilities/` directory.\n3. `scripts/check_capability_profiles.py` with `--json` flag detects undeclared capability usage with zero false negatives on test fixtures.\n4. `CapabilityGuard` implemented in `crates/franken-node/src/connector/capability_guard.rs` with grant/deny logging.\n5. Audit trail events are emitted for every capability exercise in structured log format.\n6. CI workflow includes capability profile gate as a required check.\n7. Verification evidence written to `artifacts/section_10_11/bd-cvt/`.\n\n## Key Dependencies\n\n- 10.13 telemetry namespace for audit event schema.\n- 10.13 stable error namespace for denied-capability error codes.\n- Rust static analysis tooling (clippy custom lints or `cargo-audit` extensions).\n- CI workflow infrastructure from 10.1.\n\n## Testing & Logging Requirements\n\n- Unit tests in `tests/test_check_capability_profiles.py` covering: profile parsing, static analysis detection, guard grant/deny logic.\n- Integration test with a mock subsystem that attempts undeclared capability usage and confirms denial.\n- Self-test mode validates the script correctly identifies a deliberately undeclared capability.\n- Structured logging: `capability.granted`, `capability.denied`, `capability.profile_changed`, `capability.audit_gap` events.\n\n## Expected Artifacts\n\n- `docs/specs/section_10_11/bd-cvt_contract.md` — specification document.\n- `capabilities/` directory with subsystem profiles.\n- `crates/franken-node/src/connector/capability_guard.rs` — runtime guard.\n- `scripts/check_capability_profiles.py` — verification script.\n- `tests/test_check_capability_profiles.py` — unit tests.\n- `artifacts/section_10_11/bd-cvt/verification_evidence.json` — evidence.\n- `artifacts/section_10_11/bd-cvt/verification_summary.md` — summary.","acceptance_criteria":"AC for bd-cvt:\n1. Every product subsystem declares a CapabilityProfile enum listing exactly the capabilities it requires (e.g., FileRead, NetConnect, CryptoSign); profiles are defined in a central registry module.\n2. A compile-time or init-time narrowing gate enforces that a subsystem cannot acquire capabilities outside its declared profile; attempts to exceed the profile panic or return Err with a stable CAPABILITY_VIOLATION error code.\n3. Profile narrowing is monotonic: once a subsystem drops a capability, it cannot re-acquire it within the same process lifetime.\n4. A deny-by-default policy applies: subsystems start with an empty capability set and explicitly request each capability via a typed CxHandle (capability-context-first API pattern from Section 9G).\n5. The capability registry exposes a machine-readable JSON manifest listing every subsystem and its granted capability set, suitable for offline audit.\n6. Unit tests verify: (a) subsystem within profile succeeds, (b) subsystem exceeding profile is rejected, (c) monotonic narrowing prevents re-acquisition, (d) empty-profile subsystem cannot perform any privileged operation.\n7. Integration test demonstrates two subsystems with disjoint profiles running concurrently without cross-contamination.\n8. Structured log events use stable code CAPABILITY_GRANT / CAPABILITY_DENY / CAPABILITY_NARROW with trace correlation IDs.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:36:49.664148019Z","created_by":"ubuntu","updated_at":"2026-02-20T16:24:13.610807878Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-11","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-cvt","depends_on_id":"bd-3qo","type":"blocks","created_at":"2026-02-20T07:46:31.606158050Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-cvt","depends_on_id":"bd-5rh","type":"blocks","created_at":"2026-02-20T07:46:31.751296865Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-cvt","depends_on_id":"bd-cda","type":"blocks","created_at":"2026-02-20T07:46:31.811317014Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-e5cz","title":"[16] Output contract: externally replicated high-impact claims","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 16\n\nTask Objective:\nDeliver externally replicated high-impact claims.\n\nExecution Requirements:\n- Make this requirement machine-verifiable and release-gated where applicable.\n- Keep behavior self-documenting so future contributors do not need to re-open the master plan.\n\nTesting & Logging Requirements:\n- Unit tests for rule/contract logic and error conditions.\n- Integration/E2E tests for workflow-level enforcement.\n- Structured logs with stable codes and trace IDs.\n- Reproducible artifacts that prove contract satisfaction.\n\nAcceptance Criteria:\n- Success and failure invariants for [16] Output contract: externally replicated high-impact claims are explicitly quantified and machine-verifiable.\n- Determinism requirements for [16] Output contract: externally replicated high-impact claims are validated across repeated runs and adversarial perturbations.\n\n\nExpected Artifacts:\n- Machine-readable verification artifact with explicit canonical path under artifacts/section_16/bd-e5cz/verification_evidence.json, suitable for CI/release gating.\n- Human-readable verification summary with explicit canonical path under artifacts/section_16/bd-e5cz/verification_summary.md, linking implementation intent to measured validation outcomes.\n\nTask-Specific Clarification:\n- The implementation must deliver the exact capability named in the title, with no scope dilution and no silent feature omission.\n- Validation artifacts must include capability-specific thresholds, pass/fail criteria, and reproducible evidence bundles tied to this bead ID.\n- Program/section verification gates must be able to consume this bead's outputs without manual interpretation.\n\nWhy This Improves User Outcomes:\n- \"[16] Output contract: externally replicated high-impact claims\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[16] Output contract: externally replicated high-impact claims\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"1. At least 3 high-impact claims are identified for external replication: (a) >= 95% compatibility corpus pass rate, (b) >= 10x host-compromise reduction, (c) >= 3x migration velocity improvement.\n2. Each claim has a replication kit: (a) precise claim statement with measurement methodology, (b) all data/tools needed to replicate, (c) step-by-step replication instructions, (d) expected results with acceptable variance bounds (within 10%).\n3. At least 2 claims have been replicated by independent external parties (different from the project team).\n4. Replication results are published alongside original claims with: replicator identity, methodology notes, results, and delta from original.\n5. Discrepancies > 10% between original and replicated results trigger investigation and published explanation.\n6. Replication kits are tested in CI to ensure they remain functional as the project evolves.\n7. Evidence: replication_results.json with per-claim: claim statement, original result, replicator, replicated result, delta, and investigation notes if applicable.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:39:37.303012227Z","created_by":"ubuntu","updated_at":"2026-02-20T15:29:10.247328185Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-16","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-e5cz","depends_on_id":"bd-1sgr","type":"blocks","created_at":"2026-02-20T07:43:26.813544107Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-elog","title":"[15] Adoption target: automation-first safe-extension onboarding","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 15\n\nTask Objective:\nDeliver friction-minimized onboarding from install to first safe extension with deterministic validation.\n\nExecution Requirements:\n- Make this requirement machine-verifiable and release-gated where applicable.\n- Keep behavior self-documenting so future contributors do not need to re-open the master plan.\n\nTesting & Logging Requirements:\n- Unit tests for rule/contract logic and error conditions.\n- Integration/E2E tests for workflow-level enforcement.\n- Structured logs with stable codes and trace IDs.\n- Reproducible artifacts that prove contract satisfaction.\n\nAcceptance Criteria:\n- Success and failure invariants for [15] Adoption target: automation-first safe-extension onboarding are explicitly quantified and machine-verifiable.\n- Determinism requirements for [15] Adoption target: automation-first safe-extension onboarding are validated across repeated runs and adversarial perturbations.\n\n\nExpected Artifacts:\n- Machine-readable verification artifact with explicit canonical path under artifacts/section_15/bd-elog/verification_evidence.json, suitable for CI/release gating.\n- Human-readable verification summary with explicit canonical path under artifacts/section_15/bd-elog/verification_summary.md, linking implementation intent to measured validation outcomes.\n\nTask-Specific Clarification:\n- The implementation must deliver the exact capability named in the title, with no scope dilution and no silent feature omission.\n- Validation artifacts must include capability-specific thresholds, pass/fail criteria, and reproducible evidence bundles tied to this bead ID.\n- Program/section verification gates must be able to consume this bead's outputs without manual interpretation.\n\nWhy This Improves User Outcomes:\n- \"[15] Adoption target: automation-first safe-extension onboarding\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[15] Adoption target: automation-first safe-extension onboarding\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"1. First safe extension onboarding pathway exists: from 'I have an idea' to 'signed, published, running in production' with minimal friction.\n2. Onboarding is automation-first: (a) project scaffold generator ('npx create-franken-extension'), (b) automatic signing setup integrated into publish workflow, (c) automated compatibility and security checks pre-publish, (d) one-command publish to signed registry.\n3. Total time from scaffold to published signed extension <= 30 minutes for a simple extension.\n4. Onboarding produces a 'safety report' for the extension: compatibility score, security scan results, trust requirements.\n5. Documentation: step-by-step tutorial with estimated time per step.\n6. CI test: scaffold, build, sign, and publish a test extension end-to-end; verify it appears in registry and is installable.\n7. Onboarding error messages are actionable: each failure includes 'what went wrong' and 'how to fix it'.\n8. Evidence: extension_onboarding_timing.json with per-step timings for the CI test run.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:39:36.611525969Z","created_by":"ubuntu","updated_at":"2026-02-20T15:27:05.117955447Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-15","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-elog","depends_on_id":"bd-31tg","type":"blocks","created_at":"2026-02-20T07:43:26.428914866Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-evb9","title":"Epic: Performance + Packaging [10.6]","description":"- type: epic","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-20T07:47:58.072163029Z","updated_at":"2026-02-20T07:49:21.118764591Z","closed_at":"2026-02-20T07:49:21.118746537Z","close_reason":"Superseded by canonical detailed master-plan graph (bd-33v) with full 10.N-10.21 and 11-16 coverage, explicit dependencies, and per-section verification gates.","source_repo":".","compaction_level":0,"original_size":0}
{"id":"bd-f2y","title":"[10.8] Implement incident bundle retention and export policy.","description":"## [10.8] Implement incident bundle retention and export policy\n\n### Why This Exists\n\nSection 9I.16 requires classification of control-plane artifacts into retention classes with enforceable policies. Without a formal retention system, incident bundles accumulate without bound (causing storage pressure), or are prematurely deleted (destroying compliance evidence). The operational readiness track (10.8) demands that retention be automated, auditable, and policy-driven — not left to ad-hoc operator cleanup. This bead ensures that critical artifacts survive for compliance and forensics while ephemeral artifacts are cleaned up predictably.\n\n### What It Must Do\n\nImplement a retention policy engine that classifies, stores, and expires control-plane artifacts according to configurable policies:\n\n- **Artifact classification**: Every control-plane artifact is assigned a retention class at creation time. Two base classes: `required` (incident bundles, decision receipts, revocation events, audit logs, epoch transition records, evidence ledger snapshots) and `ephemeral` (health pings, status polls, routine heartbeats, transient cache entries). Classification is determined by artifact type metadata, not by manual tagging.\n- **Retention periods**: Required artifacts have configurable minimum retention periods (default: 90 days for incident bundles, 365 days for revocation events, 7 years for audit logs). Ephemeral artifacts have configurable maximum lifetimes (default: 24 hours for health pings, 7 days for status polls). Periods are configurable per deployment via config.\n- **Storage enforcement**: Required artifacts must be stored durably (not in-memory or temp directories). The system must verify durability at write time (fsync or equivalent) and raise an alert if durable write fails. Ephemeral artifacts may use volatile storage.\n- **Automated expiry**: A retention sweeper runs on a configurable schedule (default: hourly) and removes expired ephemeral artifacts. Required artifacts are never automatically deleted — only moved to a `pending_archive` state after retention period, requiring explicit operator action to delete.\n- **Capacity alerts**: When storage utilization crosses configurable thresholds (default: 70% warn, 85% critical), structured alerts are emitted with the largest artifact categories and recommended actions.\n- **Export policy**: Incident bundles must be exportable as self-contained archives (tar.gz) containing all related artifacts, metadata, and a manifest. The export format must be versioned and include a cryptographic integrity hash. Exports can target local filesystem, S3-compatible storage, or stdout for piping.\n- **Compliance evidence**: Every retention policy decision (keep, expire, archive, export) is logged as an auditable event with timestamp, artifact ID, policy rule applied, and outcome.\n\n### Acceptance Criteria\n\n1. All control-plane artifacts are automatically classified as `required` or `ephemeral` at creation time based on artifact type; no unclassified artifacts exist after one sweep cycle.\n2. Retention periods are configurable per artifact class and per deployment; defaults match the specification above.\n3. Required artifacts are stored durably with write-time verification; a test simulates durable write failure and confirms alert emission.\n4. The retention sweeper removes expired ephemeral artifacts on schedule and never deletes required artifacts automatically.\n5. Capacity alerts fire at configurable thresholds with structured output identifying the largest artifact categories.\n6. Incident bundle export produces a self-contained versioned archive with integrity hash; re-import of exported bundle passes integrity verification.\n7. Every retention decision is logged as an auditable event; the audit trail can be queried by artifact ID or time range.\n8. A verification script validates that all current artifacts have valid classifications and no required artifacts are missing or corrupted.\n\n### Key Dependencies\n\n- Artifact persistence layer (from 10.13 chain, retention_policy.rs)\n- Evidence ledger infrastructure for storing compliance evidence\n- Config system (config.rs) for retention period configuration\n- Health gate (health_gate.rs) for capacity alert integration\n\n### Testing & Logging Requirements\n\n- Unit tests for classification logic, expiry calculation, and export format.\n- Integration test simulating full lifecycle: create -> classify -> retain -> expire -> export.\n- Verification script (`scripts/check_retention_policy.py`) with `--json` and `self_test()`.\n- All retention decisions logged at INFO level; capacity alerts at WARN/ERROR.\n\n### Expected Artifacts\n\n- `docs/specs/section_10_8/bd-f2y_contract.md` — retention policy specification\n- `scripts/check_retention_policy.py` — verification script\n- `tests/test_check_retention_policy.py` — unit tests\n- `artifacts/section_10_8/bd-f2y/verification_evidence.json`\n- `artifacts/section_10_8/bd-f2y/verification_summary.md`","acceptance_criteria":"1. Incident bundles are self-contained archives containing: logs, metrics snapshot, configuration state, error context, and timeline of events for a specific incident window.\n2. Retention policy is configurable: minimum retention period, maximum storage budget, and automatic expiry/rotation.\n3. Export policy supports at least two formats: compressed archive (tar.gz) for offline analysis and structured JSON stream for ingestion by external SIEM/observability tools.\n4. Bundles are integrity-protected: each bundle includes a SHA-256 checksum and an optional signature for chain-of-custody verification.\n5. Retention policy enforcement is automated: expired bundles are cleaned up without operator intervention, with a log entry for each deletion.\n6. A CLI command (e.g., franken-node incident export <incident-id>) produces the export artifact.\n7. Integration test creates an incident bundle, applies retention policy to expire it, and verifies cleanup occurred correctly.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:36:48.026309178Z","created_by":"ubuntu","updated_at":"2026-02-20T16:59:04.338510447Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-8","self-contained","test-obligations"]}
{"id":"bd-f5d","title":"[10.9] Build public Node/Bun/franken_node benchmark campaign infrastructure.","description":"## [10.9] Build public Node/Bun/franken_node benchmark campaign infrastructure\n\n### Why This Exists\n\nSection 14 (public benchmark specs) and Section 3.2 #10 (public verifier toolkit) require that franken_node's claims of superior compatibility, performance, and security be backed by reproducible, externally verifiable benchmarks. Without a public benchmark campaign, claims are marketing assertions rather than engineering evidence. The benchmark infrastructure must be rigorous enough that skeptical external parties can reproduce results independently, and comprehensive enough to cover the full competitive surface: correctness, performance, security, and migration velocity.\n\n### What It Must Do\n\nBuild end-to-end benchmark campaign infrastructure that enables reproducible, multi-dimensional comparison of Node.js, Bun, and franken_node:\n\n- **Benchmark harness**: A containerized, hermetic benchmark runner that executes benchmark suites against Node.js, Bun, and franken_node under identical conditions. The harness must control: runtime version, OS/kernel version, hardware profile (or normalized cloud instance type), warmup cycles, iteration count, and statistical significance thresholds.\n- **Benchmark dimensions** (all required):\n  1. **Compatibility correctness**: Pass rate by API family (fs, net, crypto, streams, etc.) and risk band (safe, conditional, unsafe). Measured against the official Node.js test suite and franken_node's extended compatibility matrix.\n  2. **Performance**: p50/p95/p99 latency, throughput (req/s), cold start time, memory overhead, and overhead under hardening (trust verification enabled vs disabled). Both micro-benchmarks and macro-benchmarks (realistic workloads).\n  3. **Containment/revocation latency**: Time from policy violation detection to containment action, and from revocation issuance to fleet-wide propagation.\n  4. **Replay determinism**: Percentage of operations that produce bit-identical results under replay. Measured across runtime versions.\n  5. **Adversarial resilience**: Pass rate against adversarial test suite (malicious extensions, credential exfiltration, policy evasion).\n  6. **Migration speed/failure-rate**: Time and success rate for migrating representative Node.js projects to franken_node.\n- **Scoring formulas**: Each dimension has a published scoring formula that converts raw measurements into a normalized score (0-100). Formulas are versioned and published alongside results so that score changes are attributable to measurement changes, not formula changes.\n- **Dataset catalog**: Benchmark workloads and datasets are versioned, published, and downloadable. No benchmark depends on private data.\n- **Result publishing**: Results are published as structured JSON with full provenance (harness version, runtime versions, hardware profile, raw measurements, computed scores). Results include cryptographic hashes for integrity verification.\n- **External reproducibility**: A \"reproduce this benchmark\" script that external parties can run on their own infrastructure. The script downloads the harness, datasets, and runtime versions, executes the suite, and produces a comparison report.\n\n### Acceptance Criteria\n\n1. Benchmark harness runs in a hermetic container with pinned runtime versions and produces deterministic results (variance < 5% across runs on identical hardware).\n2. All six benchmark dimensions are implemented with at least one benchmark suite each.\n3. Scoring formulas are documented, versioned, and published alongside results.\n4. Benchmark datasets and workloads are publicly downloadable with integrity hashes.\n5. Results are published as structured JSON with full provenance metadata and integrity verification.\n6. An external reproducibility script enables independent parties to reproduce benchmarks and verify published results.\n7. A CI integration runs a subset of benchmarks (smoke suite) on every release and flags performance regressions exceeding configurable thresholds.\n8. Benchmark infrastructure documentation explains how to add new dimensions, workloads, and scoring formulas.\n\n### Key Dependencies\n\n- Compatibility matrix from 10.2 (compatibility core) for correctness benchmarks\n- Adversarial test suite (bd-9is) for resilience benchmarks\n- Migration pipeline (bd-1e0) for migration speed benchmarks\n- Trust verification infrastructure for containment/revocation latency benchmarks\n\n### Testing & Logging Requirements\n\n- Unit tests for scoring formula calculations and result serialization.\n- Integration tests that run the full harness against a mock runtime and verify output format.\n- Verification script (`scripts/check_benchmark_infra.py`) with `--json` and `self_test()`.\n- Benchmark execution logged at INFO with per-suite timing; anomalies logged at WARN.\n\n### Expected Artifacts\n\n- `docs/specs/section_10_9/bd-f5d_contract.md` — benchmark specification and scoring formulas\n- `scripts/check_benchmark_infra.py` — verification script\n- `tests/test_check_benchmark_infra.py` — unit tests\n- `fixtures/benchmarks/` — benchmark suite definitions and datasets\n- `artifacts/section_10_9/bd-f5d/verification_evidence.json`\n- `artifacts/section_10_9/bd-f5d/verification_summary.md`","acceptance_criteria":"1. Benchmark campaign compares Node.js, Bun, and franken_node across at least 10 real-world workloads: HTTP server throughput, module loading, cold start, JSON processing, file I/O, child process spawning, stream throughput, crypto operations, URL parsing, and compatibility-shim overhead.\n2. Campaign infrastructure produces reproducible results: all benchmarks run in containerized environments with pinned runtime versions and identical hardware profiles.\n3. Results are published as a structured JSON dataset with per-benchmark, per-runtime mean/median/p95/p99 latency and throughput numbers.\n4. Campaign includes a comparative visualization generator (HTML or Markdown report) with charts and tables suitable for public consumption.\n5. Per Section 3 category-defining targets: report highlights where franken_node achieves >= 95% Node.js compatibility, >= 3x migration velocity, or >= 10x compromise reduction.\n6. Campaign is re-runnable on new releases with a single command (scripts/run_benchmark_campaign.sh) and automatically diffs against previous results.\n7. Methodology document explains statistical rigor: number of iterations, warm-up policy, outlier handling, and confidence intervals.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:36:48.287999303Z","created_by":"ubuntu","updated_at":"2026-02-20T16:26:22.000820639Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-9","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-f5d","depends_on_id":"bd-1xg","type":"blocks","created_at":"2026-02-20T07:46:37.840224751Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-f5d","depends_on_id":"bd-20a","type":"blocks","created_at":"2026-02-20T07:46:37.895375020Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-f5d","depends_on_id":"bd-229","type":"blocks","created_at":"2026-02-20T07:46:37.943889570Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-f5d","depends_on_id":"bd-3il","type":"blocks","created_at":"2026-02-20T07:46:37.988491682Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-f5d","depends_on_id":"bd-cda","type":"blocks","created_at":"2026-02-20T07:46:38.032583014Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-f7im","title":"Epic: Conformance + Verification [10.7]","description":"- type: epic","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-20T07:47:58.072163029Z","updated_at":"2026-02-20T07:49:21.124358690Z","closed_at":"2026-02-20T07:49:21.124341068Z","close_reason":"Superseded by canonical detailed master-plan graph (bd-33v) with full 10.N-10.21 and 11-16 coverage, explicit dependencies, and per-section verification gates.","source_repo":".","compaction_level":0,"original_size":0}
{"id":"bd-f955","title":"[16] Contribution: open trust/compatibility specs","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 16\n\nTask Objective:\nPublish open product-layer trust and compatibility specifications.\n\nExecution Requirements:\n- Make this requirement machine-verifiable and release-gated where applicable.\n- Keep behavior self-documenting so future contributors do not need to re-open the master plan.\n\nTesting & Logging Requirements:\n- Unit tests for rule/contract logic and error conditions.\n- Integration/E2E tests for workflow-level enforcement.\n- Structured logs with stable codes and trace IDs.\n- Reproducible artifacts that prove contract satisfaction.\n\nAcceptance Criteria:\n- Success and failure invariants for [16] Contribution: open trust/compatibility specs are explicitly quantified and machine-verifiable.\n- Determinism requirements for [16] Contribution: open trust/compatibility specs are validated across repeated runs and adversarial perturbations.\n\n\nExpected Artifacts:\n- Machine-readable verification artifact with explicit canonical path under artifacts/section_16/bd-f955/verification_evidence.json, suitable for CI/release gating.\n- Human-readable verification summary with explicit canonical path under artifacts/section_16/bd-f955/verification_summary.md, linking implementation intent to measured validation outcomes.\n\nTask-Specific Clarification:\n- The implementation must deliver the exact capability named in the title, with no scope dilution and no silent feature omission.\n- Validation artifacts must include capability-specific thresholds, pass/fail criteria, and reproducible evidence bundles tied to this bead ID.\n- Program/section verification gates must be able to consume this bead's outputs without manual interpretation.\n\nWhy This Improves User Outcomes:\n- \"[16] Contribution: open trust/compatibility specs\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[16] Contribution: open trust/compatibility specs\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"1. Open specifications published for trust primitives: (a) trust signal format and semantics, (b) trust aggregation algorithm (with formal properties: convergence, Byzantine tolerance threshold), (c) trust decision protocol (inputs, outputs, determinism guarantees).\n2. Open specifications published for compatibility primitives: (a) compatibility test case format, (b) lockstep comparison protocol, (c) divergence receipt format and integrity guarantees.\n3. Specifications follow a recognized format (RFC-style or W3C-style) with: abstract, terminology, normative requirements (MUST/SHOULD/MAY), security considerations, and IANA-style registry for extensible fields.\n4. Each specification is versioned with semantic versioning and includes change history.\n5. Specifications are reviewed by >= 2 external domain experts before publication.\n6. Specifications are hosted in a public, version-controlled repository with contribution guidelines.\n7. Reference implementations exist for each specification (in the franken_node codebase) with test suites validating conformance.\n8. Evidence: open_specs_registry.json with per-spec: title, version, review status, reference implementation path, and conformance test pass rate.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:39:36.783733016Z","created_by":"ubuntu","updated_at":"2026-02-20T16:09:35.923547893Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-16","self-contained","test-obligations"]}
{"id":"bd-gad3","title":"[10.17] Ship adaptive multi-rail isolation mesh with hot-elevation policy.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.17 — Radical Expansion Execution Track (9K)\n\nWhy This Exists:\nRadical expansion track for proof-carrying speculation, Bayesian adversary control, time-travel replay, ZK attestations, and claim compiler systems.\n\nTask Objective:\nShip adaptive multi-rail isolation mesh with hot-elevation policy.\n\nAcceptance Criteria:\n- Workloads can be promoted to stricter rails at runtime without losing policy continuity; latency-sensitive trusted workloads remain on high-performance rails within budget.\n\nExpected Artifacts:\n- `docs/architecture/isolation_mesh.md`, `src/security/isolation_rail_router.rs`, `tests/integration/isolation_hot_elevation.rs`, `artifacts/10.17/isolation_mesh_profile_report.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_17/bd-gad3/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_17/bd-gad3/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.17] Ship adaptive multi-rail isolation mesh with hot-elevation policy.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.17] Ship adaptive multi-rail isolation mesh with hot-elevation policy.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.17] Ship adaptive multi-rail isolation mesh with hot-elevation policy.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.17] Ship adaptive multi-rail isolation mesh with hot-elevation policy.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.17] Ship adaptive multi-rail isolation mesh with hot-elevation policy.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"- Workloads can be promoted to stricter rails at runtime without losing policy continuity; latency-sensitive trusted workloads remain on high-performance rails within budget.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:37:03.266990043Z","created_by":"ubuntu","updated_at":"2026-02-20T15:46:00.166252804Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-17","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-gad3","depends_on_id":"bd-3ku8","type":"blocks","created_at":"2026-02-20T07:43:18.432602651Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-gldk","title":"Epic: Tiered Trust Storage [10.14e]","description":"- type: epic","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-20T07:47:58.072163029Z","updated_at":"2026-02-20T07:49:21.230815012Z","closed_at":"2026-02-20T07:49:21.230796528Z","close_reason":"Superseded by canonical detailed master-plan graph (bd-33v) with full 10.N-10.21 and 11-16 coverage, explicit dependencies, and per-section verification gates.","source_repo":".","compaction_level":0,"original_size":0}
{"id":"bd-go4","title":"[PLAN 10.12] Frontier Programs Execution Track (9H)","description":"Section: 10.12 — Frontier Programs Execution Track (9H)\n\nStrategic Context:\nFrontier program execution turning migration singularity, trust fabric, verifier economy, and operator intelligence into production flow.\n\nExecution Requirements:\n- Preserve all scoped capabilities from the canonical plan.\n- Keep contracts self-contained so future contributors can execute without reopening the master plan.\n- Require explicit testing strategy (unit + integration/e2e) and structured logging/telemetry for every child bead.\n- Require artifact-backed validation for performance, security, and correctness claims.\n\nDependency Semantics:\n- This epic is blocked by its child implementation beads.\n- Cross-epic dependencies encode strategic sequencing and canonical ownership boundaries.\n\n## Success Criteria\n- All child beads for this section are completed with acceptance artifacts attached and no unresolved blockers.\n- Section-level verification gate for comprehensive unit tests, integration/e2e workflows, and detailed structured logging evidence is green.\n- Scope-to-plan traceability is explicit, with no feature loss versus the canonical plan section.\n\n## Optimization Notes\n- User-Outcome Lens: \"[PLAN 10.12] Frontier Programs Execution Track (9H)\" must improve operator confidence, safety posture, and deterministic recovery behavior under both normal and adversarial conditions.\n- Cross-Section Coordination: child beads must encode integration assumptions explicitly to avoid local optimizations that degrade system-wide correctness.\n- Verification Non-Negotiable: completion requires reproducible unit/integration/E2E evidence and structured logs suitable for independent replay.\n- Scope Protection: any simplification must preserve canonical-plan feature intent and be justified with explicit tradeoff documentation.","status":"open","priority":2,"issue_type":"epic","created_at":"2026-02-20T07:36:41.196986528Z","created_by":"ubuntu","updated_at":"2026-02-20T08:16:46.688501747Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-12"],"dependencies":[{"issue_id":"bd-go4","depends_on_id":"bd-1d6x","type":"blocks","created_at":"2026-02-20T07:48:08.894422511Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-go4","depends_on_id":"bd-1wz","type":"blocks","created_at":"2026-02-20T07:37:10.973643649Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-go4","depends_on_id":"bd-1xg","type":"blocks","created_at":"2026-02-20T07:37:10.897091667Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-go4","depends_on_id":"bd-20a","type":"blocks","created_at":"2026-02-20T07:37:10.935900927Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-go4","depends_on_id":"bd-229","type":"blocks","created_at":"2026-02-20T07:37:10.858130625Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-go4","depends_on_id":"bd-2aj","type":"blocks","created_at":"2026-02-20T07:36:51.271224289Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-go4","depends_on_id":"bd-3c2","type":"blocks","created_at":"2026-02-20T07:36:51.111605120Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-go4","depends_on_id":"bd-3hm","type":"blocks","created_at":"2026-02-20T07:36:50.871882924Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-go4","depends_on_id":"bd-3j4","type":"blocks","created_at":"2026-02-20T07:36:50.952975423Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-go4","depends_on_id":"bd-5si","type":"blocks","created_at":"2026-02-20T07:36:51.032232654Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-go4","depends_on_id":"bd-cda","type":"blocks","created_at":"2026-02-20T07:37:09.508382185Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-go4","depends_on_id":"bd-n1w","type":"blocks","created_at":"2026-02-20T07:36:51.350685200Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-go4","depends_on_id":"bd-y0v","type":"blocks","created_at":"2026-02-20T07:36:51.191796741Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-h93z","title":"[10.15] Add release gate requiring asupersync-backed conformance on high-impact features.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.15 — Asupersync-First Integration Execution Track (8.4-8.6)\n\nWhy This Exists:\nAsupersync-first control-plane migration track enforcing Cx-first, region ownership, cancellation correctness, and protocol-grade verification gates.\n\nTask Objective:\nAdd release gate requiring asupersync-backed conformance on high-impact features.\n\nAcceptance Criteria:\n- Release pipeline blocks claims/features lacking required conformance artifacts; gate output is machine-readable and signed.\n\nExpected Artifacts:\n- `.github/workflows/asupersync-integration-gate.yml`, `docs/conformance/asupersync_release_gate.md`, `artifacts/10.15/release_gate_report.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_15/bd-h93z/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_15/bd-h93z/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.15] Add release gate requiring asupersync-backed conformance on high-impact features.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.15] Add release gate requiring asupersync-backed conformance on high-impact features.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.15] Add release gate requiring asupersync-backed conformance on high-impact features.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.15] Add release gate requiring asupersync-backed conformance on high-impact features.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.15] Add release gate requiring asupersync-backed conformance on high-impact features.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"- Release pipeline blocks claims/features lacking required conformance artifacts; gate output is machine-readable and signed.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:37:01.035150388Z","created_by":"ubuntu","updated_at":"2026-02-20T17:04:40.937781083Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-15","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-h93z","depends_on_id":"bd-2177","type":"blocks","created_at":"2026-02-20T17:04:40.937729848Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-hg1","title":"[10.3] Build one-command migration report export for enterprise review.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.3 — Migration System\n\nWhy This Exists:\nMigration autopilot pipeline from audit to rollout, designed to collapse migration friction with deterministic confidence and replayability.\n\nTask Objective:\nBuild one-command migration report export for enterprise review.\n\nAcceptance Criteria:\n- Implement with explicit success/failure invariants and deterministic behavior under normal, edge, and fault scenarios.\n- Ensure outcomes are verifiable via automated tests and reproducible artifact outputs.\n\nExpected Artifacts:\n- Section-owned design/spec artifact at `docs/specs/section_10_3/bd-hg1_contract.md`, capturing decision rationale, invariants, and interface boundaries.\n- Machine-readable verification artifact at `artifacts/section_10_3/bd-hg1/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_3/bd-hg1/verification_summary.md` linking implementation intent to measured outcomes.\n\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.3] Build one-command migration report export for enterprise review.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.3] Build one-command migration report export for enterprise review.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.3] Build one-command migration report export for enterprise review.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.3] Build one-command migration report export for enterprise review.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.3] Build one-command migration report export for enterprise review.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","status":"closed","priority":1,"issue_type":"task","assignee":"CrimsonCrane","created_at":"2026-02-20T07:36:45.267595003Z","created_by":"ubuntu","updated_at":"2026-02-20T10:20:06.070654409Z","closed_at":"2026-02-20T10:20:06.070629784Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-3","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-hg1","depends_on_id":"bd-12f","type":"blocks","created_at":"2026-02-20T07:43:22.277603002Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-i8fh","title":"Epic: Activation Pipeline + Revocation [10.13d]","description":"- type: epic","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-20T07:47:58.072163029Z","updated_at":"2026-02-20T07:49:21.176058124Z","closed_at":"2026-02-20T07:49:21.176039930Z","close_reason":"Superseded by canonical detailed master-plan graph (bd-33v) with full 10.N-10.21 and 11-16 coverage, explicit dependencies, and per-section verification gates.","source_repo":".","compaction_level":0,"original_size":0}
{"id":"bd-jbp1","title":"[14] Metric family: replay determinism and artifact completeness","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 14\n\nTask Objective:\nInstrument replay determinism and artifact completeness metric family.\n\nExecution Requirements:\n- Make this requirement machine-verifiable and release-gated where applicable.\n- Keep behavior self-documenting so future contributors do not need to re-open the master plan.\n\nTesting & Logging Requirements:\n- Unit tests for rule/contract logic and error conditions.\n- Integration/E2E tests for workflow-level enforcement.\n- Structured logs with stable codes and trace IDs.\n- Reproducible artifacts that prove contract satisfaction.\n\nAcceptance Criteria:\n- Success and failure invariants for [14] Metric family: replay determinism and artifact completeness are explicitly quantified and machine-verifiable.\n- Determinism requirements for [14] Metric family: replay determinism and artifact completeness are validated across repeated runs and adversarial perturbations.\n\n\nExpected Artifacts:\n- Machine-readable verification artifact with explicit canonical path under artifacts/section_14/bd-jbp1/verification_evidence.json, suitable for CI/release gating.\n- Human-readable verification summary with explicit canonical path under artifacts/section_14/bd-jbp1/verification_summary.md, linking implementation intent to measured validation outcomes.\n\nTask-Specific Clarification:\n- The implementation must deliver the exact capability named in the title, with no scope dilution and no silent feature omission.\n- Validation artifacts must include capability-specific thresholds, pass/fail criteria, and reproducible evidence bundles tied to this bead ID.\n- Program/section verification gates must be able to consume this bead's outputs without manual interpretation.\n\nWhy This Improves User Outcomes:\n- \"[14] Metric family: replay determinism and artifact completeness\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[14] Metric family: replay determinism and artifact completeness\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"METRIC FAMILY: Replay determinism and artifact completeness.\n1. Metrics measured: (a) determinism rate (% of replays that produce bit-identical output), (b) artifact completeness (% of incident types with full replay artifacts), (c) replay fidelity (% of state transitions correctly reproduced), (d) replay overhead (time to replay vs original execution).\n2. Determinism target: >= 99.9% of replays produce identical output on same inputs.\n3. Artifact completeness target: 100% of high-severity incident types have complete replay artifacts.\n4. Replay fidelity: >= 99% of state transitions reproduced correctly (verified by trace comparison).\n5. Replay overhead: replay takes <= 2x the original execution time.\n6. Measured across: trust decisions, containment actions, migration operations, compatibility checks.\n7. Publication: determinism and completeness metrics in benchmark report with per-category breakdown.\n8. Evidence: replay_determinism_metrics.json with per-category determinism rate, completeness, fidelity, and overhead.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:39:35.900424585Z","created_by":"ubuntu","updated_at":"2026-02-20T15:25:04.600163361Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-14","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-jbp1","depends_on_id":"bd-2a6g","type":"blocks","created_at":"2026-02-20T07:43:26.070265130Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-jjm","title":"[10.10] Enforce product-level adoption of canonical deterministic serialization and signature preimage rules (from `10.13` + `10.14`).","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md — Section 10.10 (FCP-Inspired Hardening), cross-ref Section 9E.2\n\n## Why This Exists\n\nEnhancement Map 9E.2 identifies that without deterministic serialization and strict signature preimage contracts, any trust artifact (operator receipts, policy checkpoints, delegation tokens) can be silently re-encoded in ways that break signature verification across kernel boundaries. This bead enforces product-level adoption of the canonical serialization and preimage rules defined in Sections 10.13 (FCP trust primitives) and 10.14 (append-only marker streams), ensuring that every signed object produced by franken_node serializes to exactly one byte sequence for any given logical value. Without this discipline, the three-kernel architecture (franken_engine + asupersync + franken_node) cannot exchange signed artifacts reliably, and operators lose the ability to independently verify receipts — breaking the category-defining evidence-by-default guarantee.\n\n## What This Must Do\n\n1. Define a `CanonicalSerializer` trait (or equivalent module boundary) that enforces deterministic field ordering, no-float, length-prefixed encoding, and domain-separated tag bytes for every product trust object type (policy checkpoints, delegation tokens, revocation assertions, session tickets, zone boundary claims).\n2. Implement a `SignaturePreimage` builder that constructs the exact byte sequence to be signed/verified, including version prefix, domain-separation tag, and canonical payload — rejecting any non-canonical input at the API boundary.\n3. Integrate with the golden vector corpus from 10.13 (bd-3n2u) so that every serialization path is verified against reference test vectors on every CI run.\n4. Add a compile-time or module-init assertion that no product code path can produce a signed artifact without routing through the canonical serializer — no bypass paths allowed.\n5. Provide a `round_trip_canonical(obj) -> Result<obj>` function that serializes, deserializes, and re-serializes to prove byte-level stability, used as the standard verification primitive across all downstream beads in 10.10.\n6. Emit structured log events (`CANONICAL_SERIALIZE`, `PREIMAGE_CONSTRUCT`, `CANONICAL_REJECT`) with the object type, domain tag, byte length, and truncated content hash for audit trail.\n\n## Context from Enhancement Maps\n\n- 9E.2: \"Deterministic serialization and signature preimage contracts for operator receipts\"\n- 9A.3 (Observability): Stable error codes and structured logging across all trust-critical paths support diagnosis when serialization mismatches occur.\n- 9D.1 (Interop): Golden vector conformance ensures cross-implementation compatibility of signed artifacts.\n\n## Dependencies\n\n- Upstream: bd-3n2u ([10.13] Publish formal schema spec files and golden vectors for serialization, signatures, and control-channel frames) — provides the reference vectors and schema definitions this bead enforces at the product level.\n- Upstream: bd-1l5 ([10.10] Define canonical product trust object IDs with domain separation) — provides the domain-separation tag registry that the signature preimage builder consumes.\n- Downstream: bd-174 ([10.10] Implement policy checkpoint chain for product release channels) — depends on canonical serialization for checkpoint integrity.\n- Downstream: bd-1jjq ([10.10] Section-wide verification gate) — aggregates verification evidence from this bead.\n\n## Acceptance Criteria\n\n1. All product trust object types (minimum 6: policy checkpoint, delegation token, revocation assertion, session ticket, zone boundary claim, operator receipt) have registered canonical serialization schemas with golden vector coverage.\n2. `round_trip_canonical()` passes for every registered object type with zero byte-level divergence across 1000 randomized property-test inputs.\n3. The golden vector test suite from 10.13 (bd-3n2u) passes with 100% vector coverage — no skipped or ignored vectors.\n4. Attempting to sign any object without routing through `CanonicalSerializer` produces a compile-time error or panics at module-init with a clear diagnostic message.\n5. `SignaturePreimage` output is byte-identical across Rust (franken_node), and any future Python/TypeScript verification tooling, proven by cross-language golden vector checks.\n6. Structured log events are emitted for every serialize/preimage/reject operation with stable event codes and trace correlation IDs.\n7. No floating-point values appear anywhere in serialized trust artifacts (enforced by type system or runtime assertion).\n8. Verification evidence JSON artifact passes the section 10.10 gate schema with all fields populated.\n\n## Testing & Logging Requirements\n\n- Unit tests: Property-based round-trip tests for each object type (use `proptest` or equivalent). Edge cases: empty fields, maximum-length fields, unicode boundary values, nested structures. Verify that re-ordering struct fields in source code does not change serialized output.\n- Integration tests: Cross-validate serialized output against golden vectors from `vectors/` directory. Verify that `SignaturePreimage` output matches reference preimages byte-for-byte. Test that a signature produced by one kernel can be verified by another kernel's deserializer.\n- Adversarial tests: Attempt to bypass `CanonicalSerializer` via unsafe code or alternative serialization paths — verify compile-time or runtime rejection. Feed malformed/non-canonical byte sequences and verify clean rejection with correct error codes. Test length-prefix overflow and truncation attacks.\n- Structured logs: `CANONICAL_SERIALIZE` on every successful serialization (object_type, domain_tag, byte_length, content_hash_prefix). `PREIMAGE_CONSTRUCT` on every signature preimage build (preimage_length, domain_tag, version). `CANONICAL_REJECT` on any non-canonical input (reason, object_type, caller_location). All events include `trace_id` and `epoch_id` fields.\n\n## Expected Artifacts\n\n- docs/specs/section_10_10/bd-jjm_contract.md\n- crates/franken-node/src/connector/canonical_serializer.rs (or similar module path)\n- scripts/check_canonical_serialization.py with --json flag and self_test()\n- tests/test_check_canonical_serialization.py\n- artifacts/section_10_10/bd-jjm/verification_evidence.json\n- artifacts/section_10_10/bd-jjm/verification_summary.md","acceptance_criteria":"1. Define a CanonicalSerializer trait with a single method `canonical_bytes(&self) -> Vec<u8>` that produces a deterministic byte representation. The canonical form MUST sort map keys lexicographically, use fixed-width integer encoding (big-endian), and omit optional fields that are None (not encode them as null).\n2. Define a SignaturePreimage struct wrapping: (a) a 4-byte context tag identifying the signing context (e.g., POLICY_SIGN, TOKEN_SIGN, RELEASE_SIGN), (b) canonical_bytes of the payload, (c) a 32-byte domain-separation salt unique per context tag. The preimage is the concatenation of these three fields.\n3. Implement CanonicalSerializer for all trust object types defined in bd-1l5 (TrustObjectId), policy checkpoint structs (bd-174), and token chain structs (bd-1r2).\n4. Add a compile-time or test-time check that ensures no trust object type is added to the crate without a corresponding CanonicalSerializer impl (use an inventory pattern or exhaustive test).\n5. Enforce that re-serializing a deserialized object produces byte-identical output (idempotency test) for all implemented types.\n6. Provide at least 3 golden vector fixtures in vectors/canonical_serialization.json containing known inputs and expected byte outputs. Verify these in CI.\n7. Unit tests: (a) deterministic output across multiple serializations, (b) map-key ordering, (c) big-endian integer encoding, (d) preimage domain separation (same payload + different context tag = different preimage), (e) idempotency round-trip.\n8. Integration test: serialize 1000 random instances of each type, verify all produce deterministic output when serialized twice.\n9. Verification script scripts/check_canonical_serialization.py with --json flag, emitting to artifacts/section_10_10/bd-jjm/verification_evidence.json.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:36:48.838214999Z","created_by":"ubuntu","updated_at":"2026-02-20T17:01:31.099306015Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-10","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-jjm","depends_on_id":"bd-3n2u","type":"blocks","created_at":"2026-02-20T14:59:53.867505683Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-jvzc","title":"[BOOTSTRAP] Define foundation test matrix + deterministic fixtures (CLI/config/transplant)","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md (Bootstrap integration quality overlay for Sections 10.0–10.5)\nSection: BOOTSTRAP (Foundation test matrix and fixtures)\n\nTask Objective:\nDefine a comprehensive bootstrap test matrix and deterministic fixture contract spanning CLI bootstrap, configuration resolution, transplant integrity, and operator diagnostics.\n\nAcceptance Criteria:\n- Matrix covers happy path, edge cases, and adversarial/error paths for each bootstrap capability family.\n- Fixture contract defines deterministic seeds/inputs/expected outputs and replay expectations.\n- Matrix maps each test family to owning implementation beads and verification gate consumption.\n\nExpected Artifacts:\n- Bootstrap test-matrix document with traceability links to implementation beads.\n- Deterministic fixture catalog and replay instructions.\n- Machine-readable mapping artifact for gate tooling.\n\nTesting & Logging Requirements:\n- Unit tests for any matrix/fixture validators.\n- E2E dry-run validation that matrix entries can be executed in CI order without ambiguity.\n- Structured logs for matrix validation, fixture resolution, and mapping integrity checks.\n\nTask-Specific Clarification:\n- For \"[BOOTSTRAP] Define foundation test matrix + deterministic fixtures (CLI/config/transplant)\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[BOOTSTRAP] Define foundation test matrix + deterministic fixtures (CLI/config/transplant)\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[BOOTSTRAP] Define foundation test matrix + deterministic fixtures (CLI/config/transplant)\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[BOOTSTRAP] Define foundation test matrix + deterministic fixtures (CLI/config/transplant)\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[BOOTSTRAP] Define foundation test matrix + deterministic fixtures (CLI/config/transplant)\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","status":"closed","priority":1,"issue_type":"task","assignee":"ubuntu","created_at":"2026-02-20T08:03:10.278221451Z","created_by":"ubuntu","updated_at":"2026-02-20T08:39:39.046279559Z","closed_at":"2026-02-20T08:39:39.046193930Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["bootstrap","test-obligations","verification"]}
{"id":"bd-jxgt","title":"[10.13] Build execution planner scorer (latency/risk/capability-aware) with deterministic tie-breakers.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.13 — FCP Deep-Mined Expansion Execution Track (9I)\n\nWhy This Exists:\nDeep connector/provider contract track: lifecycle FSMs, conformance harnesses, fencing, sandboxing, revocation freshness, and protocol hardening.\n\nTask Objective:\nBuild execution planner scorer (latency/risk/capability-aware) with deterministic tie-breakers.\n\nAcceptance Criteria:\n- Scorer output is stable for identical inputs; tie-breakers are explicit and tested; planner decisions include explainable factor weights.\n\nExpected Artifacts:\n- `src/planner/execution_scorer.rs`, `tests/integration/execution_planner_determinism.rs`, `artifacts/10.13/planner_decision_explanations.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_13/bd-jxgt/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_13/bd-jxgt/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.13] Build execution planner scorer (latency/risk/capability-aware) with deterministic tie-breakers.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.13] Build execution planner scorer (latency/risk/capability-aware) with deterministic tie-breakers.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.13] Build execution planner scorer (latency/risk/capability-aware) with deterministic tie-breakers.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.13] Build execution planner scorer (latency/risk/capability-aware) with deterministic tie-breakers.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.13] Build execution planner scorer (latency/risk/capability-aware) with deterministic tie-breakers.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","status":"closed","priority":1,"issue_type":"task","assignee":"CrimsonCrane","created_at":"2026-02-20T07:36:53.601124550Z","created_by":"ubuntu","updated_at":"2026-02-20T12:29:26.551814531Z","closed_at":"2026-02-20T12:29:26.551787230Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-13","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-jxgt","depends_on_id":"bd-8vby","type":"blocks","created_at":"2026-02-20T07:43:13.391829577Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-k25j","title":"[8] Architecture Blueprint — 3-kernel design, 10 invariants, 5 alignment contracts","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md — Section 8\n\n## Why This Exists\nThis captures the full architecture blueprint including repository topology, product planes, control planes, three-kernel architecture, 10 hard runtime invariants, and 5 alignment contracts. All implementation must conform to this architecture.\n\n## Repository and Package Topology (8.1)\n- Engine: /dp/franken_engine (crates/franken-engine, crates/franken-extension-host)\n- Product: /dp/franken_node (crates/franken-node)\n- Adjacent substrates: /dp/frankentui, /dp/frankensqlite, /dp/sqlmodel_rust, /dp/fastapi_rust\n\n## Product Planes (8.2)\n1. Compatibility plane: Node/Bun behavior surfaces and divergence governance\n2. Migration plane: discovery, risk scoring, automated rewrites, rollout guidance\n3. Trust plane: policy controls, trust cards, revocation and quarantine UX\n4. Ecosystem plane: registry, reputation graph, certification channels\n5. Operations plane: fleet control, audit/replay export, benchmark verifier interfaces\n\n## Control Planes (8.3)\n1. Release control plane: staged rollout, rollback, feature-policy gating\n2. Incident control plane: replay, counterfactual simulation, response automation\n3. Economics control plane: expected-loss and attack-cost aware policy guidance\n\n## Three-Kernel Architecture (8.4)\n- Execution kernel: /dp/franken_engine (language/runtime internals)\n- Correctness/control kernel: /dp/asupersync (concurrency, cancellation, remote effects, epochs, evidence)\n- Product kernel: /dp/franken_node (compatibility, migration, trust UX, ecosystem capture)\n\n## 10 Hard Runtime Invariants (8.5, Non-Negotiable)\n1. Cx-first control APIs — all high-impact async ops take &Cx\n2. Region-owned lifecycle execution — region close implies quiescence\n3. Cancellation protocol semantics — request -> drain -> finalize, not task-drop\n4. Two-phase effects for high-impact operations — reserve/commit with obligation guarantees\n5. Scheduler lane discipline — Cancel/Timed/Ready lanes with starvation protection\n6. Remote effects contract — capability-gated, named, idempotent, saga-safe\n7. Epoch and transition barriers — epoch-scoped, barrier-mediated transitions\n8. Evidence-by-default decisions — deterministic evidence ledger with trace witnesses\n9. Deterministic protocol verification gates — lab, cancellation injection, schedule exploration\n10. No ambient authority — any ambient network/spawn/privileged side effect is a defect\n\n## 5 Alignment Contracts (8.8)\n1. Scope boundary: franken_node defines policy/orchestration/verification; engine internals stay in franken_engine\n2. Terminology: \"extension\" is primary; \"connector/provider\" maps to extension integration class\n3. Dual-oracle: L1 product oracle (Node/Bun/franken_node) + L2 engine boundary oracle\n4. Path convention: src/ paths are crate-root relative; docs/ paths are repo-root relative\n5. KPI clarity: primary KPI is migration-friction collapse with safety + verifier-backed trust guarantees\n\n## Implementation Mapping\n- 8.4-8.6 (Asupersync): Tracked in 10.15\n- 8.7 (Adjacent substrates): Tracked in 10.16\n- 8.5 invariants: Enforced across 10.13, 10.14, 10.15\n- 8.8 alignment: Enforced across all sections\n\n## Acceptance Criteria\n- Three-kernel boundaries are enforced by CI\n- All 10 runtime invariants have conformance tests\n- All 5 alignment contracts are machine-enforceable\n- Architecture deviations require signed waiver artifacts\n\n\n## Success Criteria\n- Three-kernel boundaries are encoded in actionable planning constraints and reflected in dependency structure.\n- All 10 hard runtime invariants have explicit ownership and verification hooks in downstream tracks.\n- All 5 alignment contracts are operationally testable and auditable in planning and implementation gates.\n\n## Testing & Logging Requirements\n- Unit tests for architecture-contract validation helpers and invariant-mapping checks.\n- E2E architecture-conformance scripts validating cross-kernel boundary compliance across representative workflows.\n- Structured logs for architecture scans, invariant violations, and alignment-contract drift diagnostics.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T16:15:41.514714884Z","created_by":"ubuntu","updated_at":"2026-02-20T17:08:28.053120732Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["architecture","invariants","plan","section-8"],"dependencies":[{"issue_id":"bd-k25j","depends_on_id":"bd-3hyk","type":"blocks","created_at":"2026-02-20T16:17:13.207859053Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-k4s","title":"[10.6] Build product-level benchmark suite with secure-extension scenarios.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.6 — Performance + Packaging (Item 1 of 7)\nCross-references: 9D global rule, Section 14 benchmark families\n\nWhy This Exists:\nThe product-level benchmark suite validates that franken_node meets performance targets under realistic security-hardened conditions. This is the foundation for all public benchmark claims and the benchmark ownership strategy (10.0 Initiative #10).\n\nTask Objective:\nBuild a comprehensive product-level benchmark suite covering all major workflow categories with secure-extension scenarios (not just vanilla performance).\n\nDetailed Acceptance Criteria:\n1. Benchmark scenarios cover: cold-start latency, p99 tail latency, extension-host overhead with sandbox active, migration scanner throughput, lockstep harness throughput, quarantine propagation latency, trust-card materialization latency.\n2. Secure-extension scenarios: benchmarks run with sandbox enforcement active (not bypassed), measuring realistic overhead.\n3. Deterministic benchmarking: identical inputs produce statistically equivalent results across runs (within confidence intervals).\n4. Benchmark results include confidence intervals and reproducibility metadata per 9C.10.\n5. Results exported in machine-readable format for CI/release gating and public reporting.\n6. Baseline profiling methodology: baseline first, profile top hotspots, one lever per change, validate invariance, re-measure with tail metrics (9D global rule).\n7. All 14 metric families from Section 14 represented: compatibility correctness, performance under hardening, containment latency, replay determinism, migration speed, adversarial resilience.\n\nExpected Artifacts:\n- benchmarks/ directory with harness, scenarios, fixture data, and scoring formulas.\n- CI integration for automated benchmark runs with regression detection.\n- docs/specs/section_10_6/bd-k4s_contract.md\n- artifacts/section_10_6/bd-k4s/verification_evidence.json\n\nTesting and Logging Requirements:\n- Unit tests for benchmark scenario parsing, threshold policy evaluation, and deterministic fixture selection logic.\n- E2E benchmark execution scripts that run representative secure-extension scenarios and emit reproducible result bundles.\n- Benchmark suite self-test: harness produces deterministic results on fixture data.\n- Regression detection: automatic flagging when metrics degrade beyond threshold.\n- Structured logs: BENCHMARK_STARTED, SCENARIO_EXECUTED, METRIC_COMPUTED, REGRESSION_DETECTED with trace IDs and scenario metadata.","acceptance_criteria":"1. Benchmark suite covers at minimum: module-load, require-resolve, extension-hook, and secure-sandbox-spawn scenarios.\n2. Each benchmark produces a structured JSON report with mean, median, p50, p95, p99, and max latency fields.\n3. Suite includes at least 3 secure-extension scenarios (permission-gated import, sandbox cold-start, cross-boundary callback).\n4. Baseline report artifact is persisted under artifacts/ with before/after comparison table per Section 7 performance doctrine.\n5. Profile artifacts (flamegraph or perf-map) are generated for each scenario and stored alongside results.\n6. CI gate fails if any benchmark regresses >5% from the stored baseline without explicit override.\n7. All benchmarks are reproducible on a clean checkout with a single command (e.g., cargo bench or scripts/run_benchmarks.sh).","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:36:46.701147665Z","created_by":"ubuntu","updated_at":"2026-02-20T16:06:02.454062450Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-6","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-k4s","depends_on_id":"bd-20a","type":"blocks","created_at":"2026-02-20T07:46:36.674331390Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-k4s","depends_on_id":"bd-229","type":"blocks","created_at":"2026-02-20T07:46:36.718945745Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-k4s","depends_on_id":"bd-3il","type":"blocks","created_at":"2026-02-20T07:46:36.838506401Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-k4s","depends_on_id":"bd-cda","type":"blocks","created_at":"2026-02-20T07:46:36.883756781Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-k6o","title":"[10.8] Implement deterministic safe-mode startup and operation flags.","description":"## [10.8] Implement deterministic safe-mode startup and operation flags\n\n### Why This Exists\n\nWhen franken_node encounters a compromised trust state, repeated crash loops, or epoch mismatches, it needs a well-defined fallback posture that maximizes safety while preserving basic operability. Without a formal safe-mode specification, failure recovery is ad-hoc: operators manually disable features, restart with undocumented flags, and hope for the best. Section 10.8 (Operational Readiness) requires that safe mode be a first-class operational state with deterministic entry conditions, explicit capability restrictions, and a verified exit path back to normal operation.\n\n### What It Must Do\n\nSafe mode is a reduced-functionality operating state where franken_node runs only essential services with maximum safety guarantees. The implementation must cover:\n\n- **Entry triggers**: Safe mode activates via three paths: (a) explicit flag `--safe-mode` at startup, (b) environment variable `FRANKEN_SAFE_MODE=1`, (c) config field `safe_mode: true`, or (d) automatic activation when the runtime detects trust state corruption, three or more crash loops within a configurable window, or an epoch mismatch between local state and federation peers.\n- **Capability restrictions**: In safe mode, all non-essential extensions are suspended (not loaded), network listeners are restricted to health and admin endpoints only, no new trust delegations are issued, and all write operations to the trust ledger require explicit operator confirmation.\n- **Trust re-verification**: On safe-mode entry, the trust state is verified from scratch against the evidence ledger. Any inconsistencies are logged as incidents and flagged for operator review. The re-verification result is persisted as a safe-mode entry receipt.\n- **Maximum verbosity logging**: All operations in safe mode are logged at TRACE level with full context, including every decision point, every skipped capability, and every trust check result. This ensures post-incident forensics have complete data.\n- **Exit protocol**: Leaving safe mode requires explicit operator action (not automatic). The exit protocol verifies that: trust state is consistent, no unresolved incidents exist, and the operator acknowledges the transition. Exit is logged as an auditable event.\n- **Status reporting**: A dedicated health endpoint and CLI command (`franken_node status --safe-mode`) reports current safe-mode state, entry reason, time in safe mode, and list of suspended capabilities.\n\n### Acceptance Criteria\n\n1. `--safe-mode` flag, `FRANKEN_SAFE_MODE=1` env var, and `safe_mode: true` config all deterministically activate safe mode at startup, verified by integration tests.\n2. Automatic safe-mode activation triggers on trust state corruption, crash loop detection (configurable threshold, default 3 crashes in 60 seconds), and epoch mismatch — each trigger path has a dedicated test.\n3. In safe mode, non-essential extensions are not loaded, and attempting to load one returns a structured error with recovery hint.\n4. Trust state re-verification runs on safe-mode entry and produces a persisted receipt with pass/fail status and details of any inconsistencies found.\n5. All safe-mode operations are logged at TRACE level; log output in safe mode is at least 3x more verbose than normal mode for equivalent operations.\n6. Exiting safe mode requires explicit operator action and passes a pre-exit verification checklist; automatic exit is not possible.\n7. `franken_node status --safe-mode` returns structured JSON with entry reason, duration, and suspended capability list.\n8. A drill test simulates each automatic trigger condition and verifies correct safe-mode entry and behavior.\n\n### Key Dependencies\n\n- Trust state model (state_model.rs, fencing.rs) for corruption detection\n- Health gate infrastructure (health_gate.rs) for safe-mode health endpoint\n- CLI infrastructure (cli.rs) for `--safe-mode` flag and status command\n- Config system (config.rs) for `safe_mode` config field\n- Crash loop detection requires process supervisor integration or self-monitoring\n\n### Testing & Logging Requirements\n\n- Unit tests for each entry trigger path and the exit protocol.\n- Integration tests that simulate crash loops and epoch mismatches.\n- Verification script (`scripts/check_safe_mode.py`) with `--json` output and `self_test()`.\n- Safe-mode log entries must include a `safe_mode: true` field for filtering.\n\n### Expected Artifacts\n\n- `docs/specs/section_10_8/bd-k6o_contract.md` — safe-mode specification\n- `scripts/check_safe_mode.py` — verification script\n- `tests/test_check_safe_mode.py` — unit tests\n- `artifacts/section_10_8/bd-k6o/verification_evidence.json`\n- `artifacts/section_10_8/bd-k6o/verification_summary.md`","acceptance_criteria":"1. Safe-mode startup: when --safe-mode flag is passed (or FRANKEN_SAFE_MODE=1 env var is set), the node starts with a minimal, hardened configuration that disables all non-essential features.\n2. Safe-mode operation is deterministic: given identical inputs, safe-mode produces identical outputs across runs (no randomness, no wall-clock dependencies).\n3. Safe-mode disables: extension loading, network-initiated migrations, auto-update checks, and any feature flagged as experimental.\n4. Safe-mode enables: enhanced logging (debug level), integrity self-checks on startup, and read-only mode for mutable state stores.\n5. A health-check endpoint in safe-mode reports which features are disabled and why.\n6. Transition from safe-mode to normal-mode requires explicit operator action — no automatic escalation.\n7. Integration test verifies safe-mode startup, exercises core workflows, and confirms disabled features are unreachable.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:36:47.947732023Z","created_by":"ubuntu","updated_at":"2026-02-20T16:59:04.481125249Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-8","self-contained","test-obligations"]}
{"id":"bd-ka0n","title":"[14] Metric family: performance under hardening","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 14\n\nTask Objective:\nInstrument p50/p95/p99, cold start, and hardening overhead metric family.\n\nExecution Requirements:\n- Make this requirement machine-verifiable and release-gated where applicable.\n- Keep behavior self-documenting so future contributors do not need to re-open the master plan.\n\nTesting & Logging Requirements:\n- Unit tests for rule/contract logic and error conditions.\n- Integration/E2E tests for workflow-level enforcement.\n- Structured logs with stable codes and trace IDs.\n- Reproducible artifacts that prove contract satisfaction.\n\nAcceptance Criteria:\n- Success and failure invariants for [14] Metric family: performance under hardening are explicitly quantified and machine-verifiable.\n- Determinism requirements for [14] Metric family: performance under hardening are validated across repeated runs and adversarial perturbations.\n\n\nExpected Artifacts:\n- Machine-readable verification artifact with explicit canonical path under artifacts/section_14/bd-ka0n/verification_evidence.json, suitable for CI/release gating.\n- Human-readable verification summary with explicit canonical path under artifacts/section_14/bd-ka0n/verification_summary.md, linking implementation intent to measured validation outcomes.\n\nTask-Specific Clarification:\n- The implementation must deliver the exact capability named in the title, with no scope dilution and no silent feature omission.\n- Validation artifacts must include capability-specific thresholds, pass/fail criteria, and reproducible evidence bundles tied to this bead ID.\n- Program/section verification gates must be able to consume this bead's outputs without manual interpretation.\n\nWhy This Improves User Outcomes:\n- \"[14] Metric family: performance under hardening\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[14] Metric family: performance under hardening\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"METRIC FAMILY: Performance under hardening.\n1. Metrics measured: p50, p95, p99 latency (ms); throughput (requests/sec); cold start time (ms); overhead ratio (hardened/unhardened).\n2. Measured under 3 hardening profiles: permissive (minimal hardening), balanced (recommended), strict (maximum hardening).\n3. Workloads: (a) HTTP request/response (JSON serialization), (b) file I/O (read/write 1MB), (c) crypto operations (AES-256-GCM encrypt/decrypt), (d) stream processing (pipe 10MB through transform), (e) cold start (time to first response).\n4. Overhead gates: balanced profile overhead <= 15% vs unhardened; strict profile overhead documented but not gated.\n5. Cold start gate: balanced profile cold start <= 500ms on reference hardware.\n6. Benchmark is run on defined reference hardware (documented CPU, RAM, OS) for reproducibility.\n7. Publication: all metrics included in benchmark report with statistical confidence intervals (>= 10 runs per workload).\n8. Evidence: performance_under_hardening.json with per-profile, per-workload, per-percentile metrics.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:39:35.727723518Z","created_by":"ubuntu","updated_at":"2026-02-20T15:24:41.205539333Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-14","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-ka0n","depends_on_id":"bd-18ie","type":"blocks","created_at":"2026-02-20T07:43:25.979948238Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-kcg9","title":"[10.17] Add zero-knowledge attestation support for selective compliance verification.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.17 — Radical Expansion Execution Track (9K)\n\nWhy This Exists:\nRadical expansion track for proof-carrying speculation, Bayesian adversary control, time-travel replay, ZK attestations, and claim compiler systems.\n\nTask Objective:\nAdd zero-knowledge attestation support for selective compliance verification.\n\nAcceptance Criteria:\n- Verifiers can validate compliance predicates without privileged disclosure of full private metadata; invalid/forged proofs fail admission.\n\nExpected Artifacts:\n- `docs/specs/zk_attestation_contract.md`, `src/trust/zk_attestation.rs`, `tests/security/zk_attestation_verification.rs`, `artifacts/10.17/zk_attestation_vectors.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_17/bd-kcg9/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_17/bd-kcg9/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.17] Add zero-knowledge attestation support for selective compliance verification.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.17] Add zero-knowledge attestation support for selective compliance verification.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.17] Add zero-knowledge attestation support for selective compliance verification.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.17] Add zero-knowledge attestation support for selective compliance verification.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.17] Add zero-knowledge attestation support for selective compliance verification.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"- Verifiers can validate compliance predicates without privileged disclosure of full private metadata; invalid/forged proofs fail admission.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:37:03.348902039Z","created_by":"ubuntu","updated_at":"2026-02-20T15:45:59.957333473Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-17","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-kcg9","depends_on_id":"bd-gad3","type":"blocks","created_at":"2026-02-20T07:43:18.477551437Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-khy","title":"[10.0] Implement benchmark + standard ownership stack.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.0 — Top 10 Initiative Tracking (Initiative #10)\nCross-references: 9A.10, 9B.10, 9C.10, 9D.10\n\nWhy This Exists:\nBenchmark and standard ownership is the #10 strategic initiative. It defines and maintains the benchmark and verification standards for secure extension runtime quality, then makes external adoption part of product strategy. Owning the benchmark category means franken_node defines what \"good\" looks like for the industry.\n\nTask Objective:\nBuild the benchmark and standardization ownership stack: define benchmark suites, publish verification standards, create verifier toolkits for independent validation, and establish the category-defining measurement infrastructure.\n\nDetailed Acceptance Criteria:\n1. Public benchmark suite covering: compatibility correctness by API/risk band, performance under hardening, containment/revocation latency, replay determinism, migration speed, adversarial resilience (14 metric families).\n2. Benchmark specs/harness/datasets/scoring formulas published openly with reproducibility guarantees (14).\n3. Verifier toolkit for independent validation of all benchmark claims (14).\n4. Conformance vectors + external verifier contracts to force reproducible claim standards (9B.10).\n5. Statistical rigor: confidence intervals, reproducibility guarantees, verifier receipts for headline claims (9C.10).\n6. Benchmark runner determinism and throughput optimized without weakening rigor (9D.10).\n7. Version benchmark standards with migration guidance for standard evolution (14).\n8. Security and trust co-metrics included alongside performance metrics (14).\n\nKey Dependencies:\n- Depends on all other initiatives for benchmark subject matter.\n- Consumed by 10.9 (Moonshot) for public benchmark campaign infrastructure.\n- Consumed by 10.12 (Frontier) for demo gates with external reproducibility requirements.\n- Consumed by 16 (Scientific Contributions) for reproducible technical reports.\n\nExpected Artifacts:\n- Benchmark suite with harness, datasets, scoring formulas in benchmarks/ directory.\n- Verifier toolkit SDK and CLI.\n- Published benchmark specifications.\n- docs/specs/section_10_0/bd-khy_contract.md\n- artifacts/section_10_0/bd-khy/verification_evidence.json\n\nTesting and Logging Requirements:\n- Unit tests: scoring formula correctness, metric computation, determinism validation.\n- Integration tests: full benchmark run on fixture workloads producing reproducible scores.\n- E2E tests: franken-node bench run CLI producing benchmark report with all metric families.\n- Reproducibility tests: same benchmark inputs produce identical scores across runs.\n- Structured logs: BENCHMARK_STARTED, METRIC_COMPUTED, SCORE_FINALIZED, VERIFIER_RECEIPT_GENERATED with trace IDs and metric breakdown.","acceptance_criteria":"1. Benchmark suite covers all 4 category targets from Section 3: >= 95% compatibility, >= 3x migration velocity, >= 10x compromise reduction, 100% replay coverage.\n2. Each benchmark has: defined methodology, input corpus, measurement procedure, baseline value, target value, and pass/fail threshold.\n3. Benchmark results are reproducible: same input corpus and environment produces results within <= 2% variance across 5 consecutive runs.\n4. Verification standards define: what constitutes passing evidence, required artifact format, minimum test coverage, and review process for each category.\n5. Standard ownership map (10.N cross-reference): each benchmark and standard has a designated canonical owner track; ownership is machine-readable and queryable.\n6. External adoption readiness: benchmark definitions published in standalone format (Markdown + JSON schema) suitable for third-party consumption without internal tooling dependencies.\n7. Regression detection: CI runs benchmark suite on each release candidate; regression beyond threshold triggers release-gate block and owner notification.\n8. Historical tracking: benchmark results stored in append-only time-series; trend visualization available via JSON API (data points: date, version, score, pass/fail).\n9. Cross-initiative validation: benchmark suite validates deliverables from all other 9 initiatives (bd-1qp through bd-2g0); each initiative has >= 1 dedicated benchmark.\n10. Verification evidence includes: benchmark suite execution report, reproducibility variance measurement, standard ownership map snapshot, regression detection test with intentional regression injection.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:36:43.207029086Z","created_by":"ubuntu","updated_at":"2026-02-20T15:36:43.932338003Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-0","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-khy","depends_on_id":"bd-2g0","type":"blocks","created_at":"2026-02-20T07:43:10.478302436Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-khy","depends_on_id":"bd-2ja","type":"blocks","created_at":"2026-02-20T15:01:24.719582173Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-khy","depends_on_id":"bd-3ex","type":"blocks","created_at":"2026-02-20T15:01:24.901907126Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-kiqr","title":"[12] Risk control: trust-system complexity","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 12\n\nTask Objective:\nImplement deterministic replay and degraded-mode contract enforcement for trust-system complexity risk.\n\nExecution Requirements:\n- Make this requirement machine-verifiable and release-gated where applicable.\n- Keep behavior self-documenting so future contributors do not need to re-open the master plan.\n\nTesting & Logging Requirements:\n- Unit tests for rule/contract logic and error conditions.\n- Integration/E2E tests for workflow-level enforcement.\n- Structured logs with stable codes and trace IDs.\n- Reproducible artifacts that prove contract satisfaction.\n\nAcceptance Criteria:\n- Success and failure invariants for [12] Risk control: trust-system complexity are explicitly quantified and machine-verifiable.\n- Determinism requirements for [12] Risk control: trust-system complexity are validated across repeated runs and adversarial perturbations.\n\n\nExpected Artifacts:\n- Machine-readable verification artifact with explicit canonical path under artifacts/section_12/bd-kiqr/verification_evidence.json, suitable for CI/release gating.\n- Human-readable verification summary with explicit canonical path under artifacts/section_12/bd-kiqr/verification_summary.md, linking implementation intent to measured validation outcomes.\n\nTask-Specific Clarification:\n- The implementation must deliver the exact capability named in the title, with no scope dilution and no silent feature omission.\n- Validation artifacts must include capability-specific thresholds, pass/fail criteria, and reproducible evidence bundles tied to this bead ID.\n- Program/section verification gates must be able to consume this bead's outputs without manual interpretation.\n\nWhy This Improves User Outcomes:\n- \"[12] Risk control: trust-system complexity\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[12] Risk control: trust-system complexity\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"RISK: Trust-system complexity — the trust subsystem becomes so complex that reasoning about its behavior is impossible, leading to subtle security gaps.\nIMPACT: Undetectable trust violations, inability to audit trust decisions, security incidents from logic errors in trust evaluation.\nCOUNTERMEASURES:\n  (a) Deterministic replay: every trust decision is replayable from a recorded input state, producing identical output.\n  (b) Degraded-mode contracts: explicit specification of trust-system behavior when components fail (e.g., default-deny, cached decisions with TTL).\n  (c) Trust decision logging: every trust evaluation logs input, decision, reasoning chain, and timestamp.\nVERIFICATION:\n  1. Deterministic replay test: record N trust decisions, replay them, verify bit-identical outputs for >= 99.9% of cases.\n  2. Degraded-mode test: simulate trust-component failure; verify system falls back to specified degraded behavior (not undefined).\n  3. Trust decision audit log is complete — no trust evaluation occurs without a log entry.\n  4. Trust logic has < 500 lines of core decision code (complexity budget).\nTEST SCENARIOS:\n  - Scenario A: Replay 1000 recorded trust decisions; verify 100% deterministic reproduction.\n  - Scenario B: Kill the trust-evaluation service; verify degraded-mode contract activates (default-deny within 1s).\n  - Scenario C: Inject a novel trust input not in training data; verify it is logged and handled by fallback policy.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:39:33.416363092Z","created_by":"ubuntu","updated_at":"2026-02-20T15:18:14.716977030Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-12","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-kiqr","depends_on_id":"bd-38ri","type":"blocks","created_at":"2026-02-20T07:43:24.820420308Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-kwwg","title":"[10.21] Integrate BPET with DGIS for topology-amplified early warning prioritization.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.21 — Behavioral Phenotype Evolution Tracker Execution Track (9O)\n\nWhy This Exists:\nBPET longitudinal phenotype intelligence track for pre-compromise trajectory detection and integration with DGIS/ATC/migration/economic policy.\n\nTask Objective:\nIntegrate BPET with DGIS for topology-amplified early warning prioritization.\n\nAcceptance Criteria:\n- Trajectory anomalies at high-centrality nodes are escalated by policy with explicit expected-loss context; prioritization logic is deterministic and replayable.\n\nExpected Artifacts:\n- `src/security/bpet/dgis_fusion.rs`, `tests/integration/bpet_dgis_priority_escalation.rs`, `artifacts/10.21/bpet_dgis_escalation_report.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_21/bd-kwwg/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_21/bd-kwwg/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.21] Integrate BPET with DGIS for topology-amplified early warning prioritization.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.21] Integrate BPET with DGIS for topology-amplified early warning prioritization.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.21] Integrate BPET with DGIS for topology-amplified early warning prioritization.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.21] Integrate BPET with DGIS for topology-amplified early warning prioritization.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.21] Integrate BPET with DGIS for topology-amplified early warning prioritization.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"- Trajectory anomalies at high-centrality nodes are escalated by policy with explicit expected-loss context; prioritization logic is deterministic and replayable.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:37:08.460878340Z","created_by":"ubuntu","updated_at":"2026-02-20T17:05:51.405395195Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-21","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-kwwg","depends_on_id":"bd-1jpc","type":"blocks","created_at":"2026-02-20T17:05:51.405345743Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-kwwg","depends_on_id":"bd-t89w","type":"blocks","created_at":"2026-02-20T15:01:15.323735085Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-kxdz","title":"Epic: Security + Policy Product Surfaces [10.5]","description":"- type: epic","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-20T07:47:58.072163029Z","updated_at":"2026-02-20T07:49:21.113054365Z","closed_at":"2026-02-20T07:49:21.113036612Z","close_reason":"Superseded by canonical detailed master-plan graph (bd-33v) with full 10.N-10.21 and 11-16 coverage, explicit dependencies, and per-section verification gates.","source_repo":".","compaction_level":0,"original_size":0}
{"id":"bd-lim8","title":"Epic: Control-Plane Execution Migration [10.15b]","description":"- type: epic","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-20T07:47:58.072163029Z","updated_at":"2026-02-20T07:49:21.261623906Z","closed_at":"2026-02-20T07:49:21.261603528Z","close_reason":"Superseded by canonical detailed master-plan graph (bd-33v) with full 10.N-10.21 and 11-16 coverage, explicit dependencies, and per-section verification gates.","source_repo":".","compaction_level":0,"original_size":0}
{"id":"bd-lus","title":"[10.11] Integrate canonical scheduler lane and global bulkhead policies (from `10.14` + `10.15`) for product operations.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md — Section 10.11 (FrankenSQLite-Inspired Runtime Systems), cross-ref Section 9G.8, 9J.11, 9J.20\n\n## Why This Exists\n\nProduct operations in franken_node span a wide range of latency and criticality profiles: real-time cancellation signals, timed migration checkpoints, background anti-entropy sweeps, and bulk trust-state replication. Without explicit scheduler lane discipline and global bulkhead enforcement, a burst of low-priority background work can starve time-critical cancel signals, or a retry storm against a failing remote endpoint can exhaust all available concurrency and cause cascading p99 blowups. Enhancement Map 9G.8 mandates formal scheduler lanes with priority preemption and bulkheads for p99 stability. 9J.11 specifically requires a global remote bulkhead to prevent retry-storm self-DoS, and 9J.20 elevates lane-aware scheduling to a formal product SLA mechanism.\n\nThis bead integrates the canonical lane definitions and bulkhead primitives from 10.14 (bd-qlc6 lane mapping, bd-v4l0 global remote bulkhead) and the lane mapping policy from 10.15 (bd-cuut) into the franken_node product service layer, ensuring every async entrypoint is assigned to a named lane and every remote call passes through the global bulkhead.\n\n## What This Must Do\n\n1. Define the product-layer lane taxonomy: `Cancel` (highest priority, preemptive), `Timed` (deadline-aware, e.g., migration checkpoints), `Realtime` (interactive control-plane RPCs), `Background` (anti-entropy, replication, telemetry flush) — adapting the canonical definitions from bd-cuut/bd-qlc6.\n2. Implement a `LaneRouter` that assigns every incoming product operation to a lane based on its `CapabilityContext` metadata (Cx-first, per invariant #1 from 8.5), rejecting operations that lack a lane annotation.\n3. Integrate the global remote bulkhead (from bd-v4l0) with a configurable `remote_max_in_flight` limit, shared across all lanes, that rejects new remote calls with a structured backpressure signal when capacity is exhausted.\n4. Implement per-lane concurrency limits and queue depth bounds, with overflow behavior configurable per lane (reject, enqueue-with-timeout, or shed-oldest for Background lane only).\n5. Emit lane utilization metrics: per-lane active count, queue depth, rejection count, and p99 latency — exposed via the telemetry namespace and logged as structured events.\n6. Ensure all lane and bulkhead decisions respect cancellation protocol (bd-7om): a cancelled operation immediately releases its lane slot and bulkhead permit.\n\n## Context from Enhancement Maps\n\n- 9G.8: \"Scheduler lanes + bulkheads for p99 stability\"\n- 9J.11: \"Global remote bulkhead to prevent retry-storm self-DoS\"\n- 9J.20: \"Lane-aware scheduling as a formal product SLA mechanism\"\n- Architecture invariant #1 (8.5): Cx-first control — lane assignment must be derived from the capability context, not ambient state.\n- Architecture invariant #5 (8.5): Scheduler lane discipline — every async entrypoint must declare its lane.\n- Architecture invariant #3 (8.5): Cancellation protocol semantics — cancelled work must release lane/bulkhead resources immediately.\n\n## Dependencies\n\n- Upstream: bd-qlc6 (10.14 lane-aware scheduler classes), bd-v4l0 (10.14 global remote bulkhead), bd-cuut (10.15 lane mapping policy), bd-7om (cancel-drain-finalize protocol)\n- Downstream: bd-390 (anti-entropy reconciliation uses Background lane), bd-3hw (remote saga workflows use Realtime lane + bulkhead), bd-1jpo (10.11 section-wide verification gate)\n\n## Acceptance Criteria\n\n1. Every product async entrypoint is annotated with a lane; compile-time or startup-time check rejects unannotated entrypoints.\n2. Cancel-lane operations preempt Background-lane operations under contention (demonstrated by test with saturated Background queue).\n3. Global remote bulkhead at capacity returns a structured `BulkheadExhausted` error with retry-after hint, not a hang or timeout.\n4. Per-lane concurrency limits are enforced: exceeding the limit for Timed lane results in enqueue-with-timeout behavior; exceeding Background limit results in shed-oldest.\n5. Lane utilization metrics are emitted at 1-second granularity with fields: `lane_name`, `active_count`, `queue_depth`, `rejected_count`, `p99_latency_ms`.\n6. Cancellation of an in-flight operation releases its lane slot within one event-loop tick (verified by test measuring slot release latency).\n7. Under a simulated retry storm (1000 concurrent remote calls), the bulkhead caps in-flight to `remote_max_in_flight` and the Cancel lane remains responsive (p99 < 10ms).\n8. Verification evidence JSON includes per-lane throughput, rejection rates, bulkhead saturation events, and p99 latency measurements.\n\n## Testing & Logging Requirements\n\n- Unit tests: (a) Lane assignment from CapabilityContext metadata; (b) Rejection of unannotated operations; (c) Per-lane concurrency limit enforcement; (d) Bulkhead permit acquire/release lifecycle; (e) Shed-oldest behavior for Background lane overflow.\n- Integration tests: (a) End-to-end request flow through LaneRouter to execution with correct lane accounting; (b) Mixed-lane workload with Cancel-lane preemption verified; (c) Bulkhead integration with actual remote calls (mocked endpoint).\n- Adversarial tests: (a) Retry storm: 10x normal remote call rate, verify bulkhead holds and Cancel lane stays responsive; (b) All lanes saturated simultaneously, verify graceful degradation; (c) Rapid cancel/re-submit cycles to test slot release correctness; (d) Bulkhead limit set to 1, verify serialization of remote calls.\n- Structured logs: Events use stable codes (FN-LN-001 through FN-LN-010), include `lane_name`, `trace_id`, `operation_type`, `bulkhead_permits_available`. JSON-formatted for machine parsing.\n\n## Expected Artifacts\n\n- docs/specs/section_10_11/bd-lus_contract.md\n- crates/franken-node/src/runtime/lane_router.rs (or equivalent module path)\n- crates/franken-node/src/runtime/bulkhead.rs (product-layer bulkhead integration)\n- scripts/check_scheduler_lanes.py (with --json flag and self_test())\n- tests/test_check_scheduler_lanes.py\n- artifacts/section_10_11/bd-lus/verification_evidence.json\n- artifacts/section_10_11/bd-lus/verification_summary.md","acceptance_criteria":"AC for bd-lus:\n1. Integrate the canonical scheduler lane model from 10.14 + 10.15: all product operations are assigned to one of the defined lanes (e.g., Cancel, Timed, Ready, Background) based on a LanePolicy mapping; operations without an explicit lane assignment default to Background.\n2. Each lane has configurable concurrency limits (max_concurrent), priority weight, and preemption rules; the scheduler respects lane isolation such that a saturated Background lane cannot starve Cancel or Timed lanes.\n3. A global bulkhead enforces a system-wide max_in_flight limit across all lanes; when the global limit is reached, new operations are rejected with BULKHEAD_OVERLOAD error and backpressure signal rather than silently queuing.\n4. Lane configuration is loaded from the same config system (config.rs) and can be updated at runtime via config reload; lane limit changes take effect for newly submitted operations (in-flight operations retain their lane assignment).\n5. Priority ordering within a lane follows: Cancel > Timed > Ready > Background. Operations in the Cancel lane are always scheduled first, ensuring cancellation signals are never starved by application work.\n6. Metrics are exposed per-lane: in_flight (gauge), queued (gauge), completed (counter), rejected (counter), and p99_queue_wait_ms (histogram). Global metrics: total_in_flight (gauge), bulkhead_rejections (counter).\n7. Unit tests verify: (a) operations are assigned to correct lanes per policy, (b) lane concurrency limit is respected, (c) global bulkhead rejects when limit reached, (d) Cancel lane is never starved when Background lane is saturated, (e) runtime config reload updates lane limits for new operations, (f) unknown lane in policy defaults to Background with a warning.\n8. Integration test: simulate 100 concurrent operations across 4 lanes with a global bulkhead of 50 and verify no lane starvation and correct rejection count.\n9. Structured log events: LANE_ASSIGNED / LANE_SATURATED / BULKHEAD_OVERLOAD / LANE_CONFIG_RELOAD with operation_id, lane_name, and current_in_flight counts.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:36:50.637018927Z","created_by":"ubuntu","updated_at":"2026-02-20T17:01:39.950530741Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-11","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-lus","depends_on_id":"bd-cuut","type":"blocks","created_at":"2026-02-20T15:00:19.396010444Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-lus","depends_on_id":"bd-qlc6","type":"blocks","created_at":"2026-02-20T15:00:19.016005147Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-lus","depends_on_id":"bd-v4l0","type":"blocks","created_at":"2026-02-20T15:00:19.207380051Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-m8p","title":"[10.9] Build verifier economy portal and external attestation publishing flow.","description":"## [10.9] Build verifier economy portal and external attestation publishing flow\n\n### Why This Exists\n\nSection 9H.3 (Verifier Economy Program) envisions a marketplace where external parties can verify franken_node's claims and publish their attestations, creating a trust network that extends beyond the core team. The verifier SDK (10.17) provides the technical tools, but without a portal and publishing flow, external verification remains an isolated activity with no network effect. This bead builds the market-facing surface: a web portal where verifiers register, access toolkits, submit results, and where the public can view a trust scoreboard backed by cryptographic evidence.\n\n### What It Must Do\n\nBuild a verifier economy portal and attestation publishing flow:\n\n- **Verifier registration**: External verifiers register via the portal with: identity (organization or individual), verification capabilities (which benchmark dimensions they can verify), and public key for attestation signing. Registration is lightweight (no approval gate for basic verification; advanced verification tiers may require vetting).\n- **Verification toolkit access**: Registered verifiers can download: the verification SDK (tooling for running verification suites), replay capsules (deterministic execution recordings that can be independently verified), benchmark datasets, and reference results for comparison.\n- **Result submission**: Verifiers submit verification results through a structured API. Submissions include: verifier identity, verification suite executed, raw measurements, computed scores, execution environment description, and a cryptographic signature over the entire result payload.\n- **Attestation publishing flow**: Verified claims (results that pass integrity checks and consistency validation) are published as attestations. Each attestation includes: the claim (what was verified), the evidence (measurements and execution trace), the verifier identity, the cryptographic signature, and a timestamp. Attestations are immutable once published.\n- **Public trust scoreboard**: A public-facing view showing: aggregate trust scores across all verifiers, individual attestation details, historical score trends, and verifier reputation (based on consistency and coverage of their attestations). The scoreboard is filterable by verification dimension, time range, and verifier.\n- **Replay capsule access**: The portal provides access to replay capsules — deterministic execution recordings that allow anyone to independently verify a specific claim. Capsules include: input state, execution trace, output state, and expected result. Capsule integrity is cryptographically verifiable.\n- **Anti-gaming measures**: The system must resist: sybil attacks (fake verifiers inflating scores), selective reporting (verifiers only submitting favorable results), and result fabrication (submitting results without actually running verification). Measures include: result consistency cross-checks, execution environment attestation, and statistical anomaly detection.\n\n### Acceptance Criteria\n\n1. Verifier registration flow works end-to-end: register, receive credentials, download toolkit, submit results, see attestation published.\n2. Verification toolkit download includes SDK, replay capsules, benchmark datasets, and reference results with integrity hashes.\n3. Result submission API validates payload structure, cryptographic signature, and basic consistency before accepting submissions.\n4. Published attestations are immutable, cryptographically signed, and include full provenance metadata.\n5. Public trust scoreboard displays aggregate scores, individual attestations, and historical trends; data is queryable via API.\n6. Replay capsules can be independently executed and produce results matching the published attestation.\n7. At least two anti-gaming measures are implemented and tested (sybil resistance and selective reporting detection).\n8. Portal API has structured JSON responses throughout; a verification script validates all API endpoints.\n\n### Key Dependencies\n\n- Verifier SDK (10.17) for verification tooling\n- Benchmark infrastructure (bd-f5d) for benchmark datasets and scoring\n- Replay determinism infrastructure for capsule generation\n- Cryptographic signing infrastructure for attestation integrity\n\n### Testing & Logging Requirements\n\n- Unit tests for registration, submission validation, attestation publishing, and anti-gaming checks.\n- Integration test covering full verifier lifecycle from registration to published attestation.\n- Verification script (`scripts/check_verifier_portal.py`) with `--json` and `self_test()`.\n- All portal operations logged at INFO; anti-gaming detections logged at WARN; signature failures logged at ERROR.\n\n### Expected Artifacts\n\n- `docs/specs/section_10_9/bd-m8p_contract.md` — verifier portal specification\n- `scripts/check_verifier_portal.py` — verification script\n- `tests/test_check_verifier_portal.py` — unit tests\n- `fixtures/verifier-portal/` — sample attestations and replay capsules\n- `artifacts/section_10_9/bd-m8p/verification_evidence.json`\n- `artifacts/section_10_9/bd-m8p/verification_summary.md`","acceptance_criteria":"1. Verifier economy portal provides a web-accessible interface where external parties can submit artifacts for independent verification and retrieve attestation results.\n2. Attestation publishing flow: verified results are signed, timestamped, and published in a machine-readable format (JSON + optional human-readable summary).\n3. Per Section 3.2 capability #10 (public verifier toolkit): portal is usable without any internal credentials — external verifiers authenticate via a public registration flow.\n4. Each attestation includes: artifact hash, verification method used, result (pass/fail/partial), verifier identity, and a reproducibility pointer (git commit + command to re-run).\n5. Portal tracks attestation history per artifact with a tamper-evident log (append-only, hash-chained entries).\n6. API endpoints support: submit-for-verification, check-status, retrieve-attestation, and list-attestations-for-artifact.\n7. Per Section 9H frontier programs: portal design supports federation — third-party verifier nodes can publish attestations through a documented protocol.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:36:48.524728794Z","created_by":"ubuntu","updated_at":"2026-02-20T16:59:05.048225078Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-9","self-contained","test-obligations"]}
{"id":"bd-mglg","title":"Epic: Compatibility Core [10.2]","description":"- type: epic","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-20T07:47:58.072163029Z","updated_at":"2026-02-20T07:49:21.095746608Z","closed_at":"2026-02-20T07:49:21.095729126Z","close_reason":"Superseded by canonical detailed master-plan graph (bd-33v) with full 10.N-10.21 and 11-16 coverage, explicit dependencies, and per-section verification gates.","source_repo":".","compaction_level":0,"original_size":0}
{"id":"bd-ml1","title":"[10.4] Implement publisher reputation model with explainable transitions.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.4 — Extension Ecosystem + Registry\n\nWhy This Exists:\nExtension ecosystem and registry trust foundation: signed artifacts, provenance, trust cards, revocation, and quarantine flows.\n\nTask Objective:\nImplement publisher reputation model with explainable transitions.\n\nAcceptance Criteria:\n(See structured acceptance_criteria field for domain-specific criteria.)\n\nExpected Artifacts:\n- Section-owned design/spec artifact at `docs/specs/section_10_4/bd-ml1_contract.md`, capturing decision rationale, invariants, and interface boundaries.\n- Machine-readable verification artifact at `artifacts/section_10_4/bd-ml1/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_4/bd-ml1/verification_summary.md` linking implementation intent to measured outcomes.\n\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.4] Implement publisher reputation model with explainable transitions.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.4] Implement publisher reputation model with explainable transitions.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.4] Implement publisher reputation model with explainable transitions.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.4] Implement publisher reputation model with explainable transitions.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.4] Implement publisher reputation model with explainable transitions.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"1. Publisher reputation is a composite score in range [0.0, 1.0] computed from weighted signals: extension install count and retention (20%), security incident history (30%), attestation chain completeness (15%), behavioral telemetry anomaly rate (15%), community reports (10%), certification level maintenance (10%). 2. Reputation state machine has five levels: Unverified (new publisher, score=0), Provisional (score 0.2-0.5, <90 days history), Established (score 0.5-0.8), Trusted (score >0.8, sustained for >=180 days), Suspended (any level, triggered by security event). 3. Every state transition emits a structured explanation containing: previous_level, new_level, trigger_event, contributing_signals[] (each with signal_name, signal_value, weight, delta_contribution), and counterfactual_actions[] (actions the publisher could take to improve score, per 9C posterior decomposition). 4. Publisher staking integration per 9B: publishers at Trusted level may stake tokens; slashing occurs on verified security incidents proportional to incident severity and affected install base. Staking/slashing events update reputation score immediately. 5. Reputation decay: inactive publishers (no updates in 180 days) decay toward Provisional at rate of 0.01/week; decay is paused during active maintenance periods. 6. Anti-gaming protections: install count signal uses unique-node deduplication; anomaly detection flags sudden install spikes; self-referential dependency chains are excluded from scoring. 7. Reputation is queryable via trust card API (bd-2yh) and CLI: 'franken-node publisher reputation show <publisher-id>' with full signal breakdown. 8. Historical reputation trajectory is stored and queryable for at least 365 days, enabling trend analysis and audit. 9. Reputation changes are propagated to all trust cards referencing the publisher's extensions within one freshness cycle. 10. Structured log events: REPUTATION_TRANSITION with publisher_id, old_level, new_level, explanation_hash, and all contributing signal values.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:36:45.745765563Z","created_by":"ubuntu","updated_at":"2026-02-20T16:59:00.163156252Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-4","self-contained","test-obligations"]}
{"id":"bd-mwf","title":"[10.0] Implement policy-visible compatibility shim system.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.0 — Top 10 Initiative Tracking (Initiative #5)\nCross-references: 9A.5, 9B.5, 9C.5, 9D.5\n\nWhy This Exists:\nPolicy-visible compatibility shims are the #5 strategic initiative. Any behavior shim must be typed, auditable, and policy-gated so operators can choose compatibility level by risk appetite with full traceability. This prevents the common antipattern of hidden compatibility hacks that silently change security or correctness properties.\n\nTask Objective:\nImplement a system where every compatibility shim (code that adapts franken_node behavior to match Node/Bun expectations) is: explicitly typed with metadata, auditable through the divergence ledger, and gated by policy rules that operators control per-profile.\n\nDetailed Acceptance Criteria:\n1. Every shim is registered in the compatibility behavior registry (10.2) with typed metadata: affected API, behavior change description, risk level, and policy gate.\n2. Shim activation is cryptographically constrained and auditable — policy-as-data signatures with attenuation semantics (9B.5).\n3. Non-interference and monotonicity checks encoded as machine-verifiable policy compiler outputs (9C.5).\n4. Policy evaluation path optimized while preserving deterministic rule order (9D.5).\n5. Operators can enable/disable shims per compatibility mode (strict/balanced/legacy-risky) with full audit trail.\n6. Shim activation emits structured audit events consumable by observability stack.\n7. Shim registry supports versioned evolution — shims can be deprecated, superseded, or mandatory.\n\nKey Dependencies:\n- Depends on 10.2 (Compatibility Core) for behavior registry and mode selection.\n- Depends on 10.1 (Charter) for governance rules on what constitutes acceptable shimming.\n- Consumed by 10.5 (Security) for policy-visible compatibility gate APIs.\n- Consumed by 10.7 (Conformance) for verification of shim behavior.\n\nExpected Artifacts:\n- src/conformance/shim_registry.rs — typed shim registration and query.\n- src/conformance/shim_policy.rs — policy evaluation and gate logic.\n- docs/specs/section_10_0/bd-mwf_contract.md\n- artifacts/section_10_0/bd-mwf/verification_evidence.json\n\nTesting and Logging Requirements:\n- Unit tests: shim registration, policy gate evaluation per mode, attenuation semantics, monotonicity checks.\n- Integration tests: shim activation workflow from API call through policy gate to audit event emission.\n- E2E tests: operator configuring compatibility mode and observing different shim activation sets.\n- Structured logs: SHIM_REGISTERED, SHIM_ACTIVATED, SHIM_BLOCKED_BY_POLICY, POLICY_EVALUATED with trace IDs and shim metadata.","acceptance_criteria":"1. Every shim is typed: Rust struct with source API signature, target API signature, behavioral contract, and version applicability range.\n2. Shims are auditable: each shim has a unique ID, creation timestamp, author, review status, and change history in append-only log.\n3. Policy-gated activation: shims only activate when referenced policy rule evaluates to true; default is shim-disabled (deny-by-default).\n4. Shim registry queryable via CLI (`franken-node shim list --format json`) with filtering by status (active/inactive/deprecated), API category, policy gate.\n5. Shim behavioral contracts include: input/output type mapping, side-effect declaration, performance budget (max latency overhead <= 5%), and error propagation semantics.\n6. Compatibility contribution: shim system raises overall compatibility score toward >= 95% category target (Section 3) by bridging identified divergences from the divergence ledger (bd-1qp).\n7. Each shim has unit tests verifying behavioral contract; integration tests verifying policy-gate activation/deactivation.\n8. Shim overhead measurable: runtime telemetry reports per-shim invocation count, latency overhead, and error rate.\n9. Shim deprecation workflow: deprecated shims emit warnings for 2 release cycles before removal; policy can force-disable deprecated shims immediately.\n10. Verification evidence includes: shim registry snapshot, policy-gate test results, performance overhead measurements, compatibility score delta with/without shims.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:36:42.804939462Z","created_by":"ubuntu","updated_at":"2026-02-20T15:35:55.668033836Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-0","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-mwf","depends_on_id":"bd-137","type":"blocks","created_at":"2026-02-20T15:01:23.773763722Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-mwf","depends_on_id":"bd-uo4","type":"blocks","created_at":"2026-02-20T07:43:10.265371610Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-mwvn","title":"[10.14] Add policy action explainer that distinguishes diagnostic confidence from guarantee confidence.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.14 — FrankenSQLite Deep-Mined Expansion Execution Track (9J)\n\nWhy This Exists:\nFrankenSQLite deep-mined control integrity track: evidence ledgers, proofs, epochs, remote effects, markers/MMR, and deterministic fault labs.\n\nTask Objective:\nAdd policy action explainer that distinguishes diagnostic confidence from guarantee confidence.\n\nAcceptance Criteria:\n- Explainer output contains separate sections for heuristic confidence and guarantee confidence; UI/API contracts expose both values; ambiguity-free wording validated.\n\nExpected Artifacts:\n- `docs/specs/policy_explainer_contract.md`, `tests/integration/policy_explainer_output.rs`, `artifacts/10.14/policy_explainer_examples.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_14/bd-mwvn/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_14/bd-mwvn/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.14] Add policy action explainer that distinguishes diagnostic confidence from guarantee confidence.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.14] Add policy action explainer that distinguishes diagnostic confidence from guarantee confidence.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.14] Add policy action explainer that distinguishes diagnostic confidence from guarantee confidence.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.14] Add policy action explainer that distinguishes diagnostic confidence from guarantee confidence.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.14] Add policy action explainer that distinguishes diagnostic confidence from guarantee confidence.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"Explainer output contains separate sections for heuristic confidence and guarantee confidence; UI/API contracts expose both values; ambiguity-free wording validated.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:36:55.955648978Z","created_by":"ubuntu","updated_at":"2026-02-20T15:44:10.585753761Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-14","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-mwvn","depends_on_id":"bd-15u3","type":"blocks","created_at":"2026-02-20T07:43:14.638476928Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-n1w","title":"[10.12] Add frontier demo gates with external reproducibility requirements.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md — Section 10.12 (Frontier Programs), cross-ref Section 9H (all programs), 9F.10, 9F.15\n\n## Why This Exists\nEvery frontier program in franken_node (Migration Singularity, Trust Fabric, Verifier Economy, Operator Intelligence, Ecosystem Network Effects) must produce results that an independent external party can reproduce end-to-end without access to internal state or tooling. This bead implements the shared demo-gate infrastructure that enforces external reproducibility as a hard requirement across all five frontier programs. It directly advances the category-defining disruptive floor target of \"100% deterministic replay artifact availability\" and underpins the \">= 3 impossible-by-default capabilities broadly adopted\" target by ensuring every capability claim is backed by externally verifiable evidence.\n\n## What This Must Do\n1. Define a `FrontierDemoGate` trait/interface that each frontier program must implement, specifying: (a) input corpus definition, (b) execution harness invocation, (c) output artifact schema, and (d) reproducibility attestation format.\n2. Implement a demo-gate runner (`scripts/check_frontier_demo_gates.py`) that discovers all registered frontier programs, executes their demo gates in isolated environments (no network, no shared mutable state), and collects pass/fail evidence with timing and resource metrics.\n3. Produce a machine-readable reproducibility manifest (`frontier_demo_manifest.json`) containing: git commit hash, input corpus fingerprints (SHA-256), output artifact fingerprints, execution environment metadata (OS, toolchain versions), and wall-clock timing per gate.\n4. Implement an external-verifier bootstrap script that downloads the manifest, fetches pinned inputs from the artifact store, re-executes each gate, and diffs outputs byte-for-byte against attested results.\n5. Add a CI gate (`scripts/check_section_10_12_demo_gate.py`) that blocks release if any frontier program lacks a passing demo gate or if the reproducibility manifest is stale (> 1 commit behind HEAD on relevant paths).\n6. Integrate with the section 10.12 verification gate (bd-1d6x) so demo-gate failures propagate as hard blockers to the section rollup.\n\n## Context from Enhancement Maps\n- 9H (all programs): \"Every frontier program must demonstrate results with external reproducibility\" — this is the cross-cutting enforcement layer for that mandate.\n- 9F.10: \"Public adversarial olympiad benchmark infrastructure\" — demo gates are the foundation for any public benchmark; external reproducibility is prerequisite to adversarial testing by third parties.\n- 9F.15: \"Universal verifier SDK used by customers, auditors, and researchers\" — the external-verifier bootstrap script is a precursor to the universal verifier SDK, establishing the contract that external parties consume.\n- Category-defining targets (Section 3.2): \"100% deterministic replay artifact availability\" requires every frontier demo to produce replay-capable artifacts. \">= 3 impossible-by-default capabilities broadly adopted\" requires external proof that capabilities actually work.\n\n## Dependencies\n- Upstream: bd-2aj (ecosystem network-effect APIs — provides registry/reputation primitives that demo gates publish results into), bd-3c2 (verifier-economy SDK — provides the independent validation workflow that demo gates invoke), bd-3j4 (migration singularity pipeline — one of the frontier programs that must register a demo gate), bd-y0v (operator intelligence engine — another frontier program that must register a demo gate), bd-5si (trust fabric convergence — another frontier program registering a demo gate).\n- Downstream: bd-1d6x (section-wide verification gate — consumes demo-gate pass/fail as hard input), bd-go4 (section 10.12 plan rollup — requires all beads including demo gates).\n\n## Acceptance Criteria\n1. All five frontier programs (Migration Singularity, Trust Fabric, Verifier Economy, Operator Intelligence, Ecosystem Network Effects) have registered demo gates that conform to the `FrontierDemoGate` interface.\n2. The demo-gate runner executes all gates in isolated mode and produces a `frontier_demo_manifest.json` with all required fields (commit hash, input fingerprints, output fingerprints, environment metadata, timing).\n3. The external-verifier bootstrap script can re-execute every demo gate on a clean machine (no prior state) and confirm byte-identical outputs, achieving 100% deterministic replay artifact availability.\n4. CI gate blocks release if any demo gate is missing, failing, or if the manifest is stale relative to HEAD.\n5. Demo-gate execution completes within 5 minutes total for all five programs (performance budget).\n6. Each demo-gate output artifact is signed with the build key and includes a chain-of-custody log entry.\n7. The verification evidence JSON (`artifacts/section_10_12/bd-n1w/verification_evidence.json`) records pass/fail per frontier program, manifest fingerprint, and external-verifier dry-run result.\n8. At least one demo gate exercises a capability classified as \"impossible-by-default\" in the category-defining targets, with the external verifier confirming the result.\n\n## Testing & Logging Requirements\n- Unit tests: Test `FrontierDemoGate` trait conformance for mock programs; test manifest generation with known inputs/outputs; test fingerprint computation determinism; test stale-manifest detection logic.\n- Integration tests: Run the demo-gate runner against at least two real frontier programs end-to-end; verify manifest schema compliance; verify CI gate correctly blocks on missing/failing gates.\n- E2E tests: Execute the full external-verifier bootstrap flow in a clean container, confirming reproducibility of all five demo gates from manifest to output diff.\n- External reproducibility tests: Provide a `Dockerfile` or script that an independent party can run to verify all demo gates without any franken_node-internal tooling beyond the published manifest and pinned inputs.\n- Structured logs: Emit `DEMO_GATE_START`, `DEMO_GATE_PASS`, `DEMO_GATE_FAIL`, `MANIFEST_GENERATED`, `EXTERNAL_VERIFY_START`, `EXTERNAL_VERIFY_MATCH`, `EXTERNAL_VERIFY_MISMATCH` events with trace correlation IDs, program identifiers, and wall-clock durations.\n\n## Expected Artifacts\n- docs/specs/section_10_12/bd-n1w_contract.md\n- artifacts/section_10_12/bd-n1w/verification_evidence.json\n- artifacts/section_10_12/bd-n1w/verification_summary.md","acceptance_criteria":"1. Define a FrontierDemoGate struct containing: (a) gate_id, (b) demo_name (one of: MIGRATION_SINGULARITY, TRUST_FABRIC, VERIFIER_ECONOMY, OPERATOR_INTELLIGENCE, ECOSYSTEM_NETWORK), (c) required_artifacts (list of artifact IDs that must exist and pass verification), (d) external_reproducibility_requirements (list of {requirement_name, verification_method, pass/fail}).\n2. Define external reproducibility requirements for each demo: (a) MIGRATION_SINGULARITY: an independent verifier (from bd-3c2 SDK) must successfully validate the migration artifact from a clean environment with no franken_node runtime dependencies. (b) TRUST_FABRIC: a simulated 3-node fabric must converge within 60s from cold start, reproducible on any Linux x86_64 host. (c) VERIFIER_ECONOMY: the verifier CLI must produce identical results when run by three different operators on the same artifact. (d) OPERATOR_INTELLIGENCE: recommendation engine must produce the same top-1 recommendation for a fixed input scenario across 10 runs (determinism). (e) ECOSYSTEM_NETWORK: registry search must return consistent results for the same query within a 5s window.\n3. Implement gate evaluation: evaluate_demo_gate(gate) -> DemoGateResult that: (a) checks all required artifacts exist, (b) runs the verification script for each artifact, (c) runs external reproducibility checks, (d) produces a DemoGateResult with per-check pass/fail and an overall verdict.\n4. Implement a reproducibility harness: a script or binary that sets up a clean environment (using temp directories, no state leakage), runs the demo scenario, and captures output hashes for comparison. The harness MUST be runnable without root privileges.\n5. Implement a demo manifest: a machine-readable JSON document listing all demos, their gates, their status (NOT_STARTED, PASSED, FAILED), and the last evaluation timestamp. Store at artifacts/section_10_12/demo_manifest.json.\n6. Enforce that no frontier program (10.12) can be marked as complete unless its corresponding demo gate has PASSED.\n7. Unit tests: (a) gate evaluation with all artifacts passing, (b) gate evaluation with one artifact missing, (c) gate evaluation with reproducibility failure, (d) demo manifest update after evaluation.\n8. Integration test: run the VERIFIER_ECONOMY demo gate end-to-end using the SDK from bd-3c2 and sample artifacts from fixtures/verifier_sdk/.\n9. Verification: scripts/check_demo_gates.py --json, artifacts at artifacts/section_10_12/bd-n1w/.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:36:51.314668632Z","created_by":"ubuntu","updated_at":"2026-02-20T17:01:50.032698237Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-12","self-contained","test-obligations"]}
{"id":"bd-n71","title":"[PLAN 10.16] Adjacent Substrate Integration Execution Track (8.7)","description":"Section: 10.16 — Adjacent Substrate Integration Execution Track (8.7)\n\nStrategic Context:\nAdjacent substrate integration track to enforce deep composition with frankentui, frankensqlite, sqlmodel_rust, and fastapi_rust.\n\nExecution Requirements:\n- Preserve all scoped capabilities from the canonical plan.\n- Keep contracts self-contained so future contributors can execute without reopening the master plan.\n- Require explicit testing strategy (unit + integration/e2e) and structured logging/telemetry for every child bead.\n- Require artifact-backed validation for performance, security, and correctness claims.\n\nDependency Semantics:\n- This epic is blocked by its child implementation beads.\n- Cross-epic dependencies encode strategic sequencing and canonical ownership boundaries.\n\n## Success Criteria\n- All child beads for this section are completed with acceptance artifacts attached and no unresolved blockers.\n- Section-level verification gate for comprehensive unit tests, integration/e2e workflows, and detailed structured logging evidence is green.\n- Scope-to-plan traceability is explicit, with no feature loss versus the canonical plan section.\n\n## Optimization Notes\n- User-Outcome Lens: \"[PLAN 10.16] Adjacent Substrate Integration Execution Track (8.7)\" must improve operator confidence, safety posture, and deterministic recovery behavior under both normal and adversarial conditions.\n- Cross-Section Coordination: child beads must encode integration assumptions explicitly to avoid local optimizations that degrade system-wide correctness.\n- Verification Non-Negotiable: completion requires reproducible unit/integration/E2E evidence and structured logs suitable for independent replay.\n- Scope Protection: any simplification must preserve canonical-plan feature intent and be justified with explicit tradeoff documentation.","status":"open","priority":1,"issue_type":"epic","created_at":"2026-02-20T07:36:41.539920124Z","created_by":"ubuntu","updated_at":"2026-02-20T08:16:39.699401457Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-16"],"dependencies":[{"issue_id":"bd-n71","depends_on_id":"bd-10g0","type":"blocks","created_at":"2026-02-20T07:48:16.765575787Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-n71","depends_on_id":"bd-159q","type":"blocks","created_at":"2026-02-20T07:37:02.719637680Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-n71","depends_on_id":"bd-1719","type":"blocks","created_at":"2026-02-20T07:37:01.891414144Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-n71","depends_on_id":"bd-1a1j","type":"blocks","created_at":"2026-02-20T07:37:01.973617753Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-n71","depends_on_id":"bd-1ow","type":"blocks","created_at":"2026-02-20T07:37:11.163855654Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-n71","depends_on_id":"bd-1v65","type":"blocks","created_at":"2026-02-20T07:37:02.304645182Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-n71","depends_on_id":"bd-1xtf","type":"blocks","created_at":"2026-02-20T07:37:01.808199702Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-n71","depends_on_id":"bd-26ux","type":"blocks","created_at":"2026-02-20T07:37:02.140030738Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-n71","depends_on_id":"bd-28ld","type":"blocks","created_at":"2026-02-20T07:37:01.644950751Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-n71","depends_on_id":"bd-2f5l","type":"blocks","created_at":"2026-02-20T07:37:02.469218239Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-n71","depends_on_id":"bd-2ji2","type":"blocks","created_at":"2026-02-20T07:37:02.882954758Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-n71","depends_on_id":"bd-2owx","type":"blocks","created_at":"2026-02-20T07:37:01.563730614Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-n71","depends_on_id":"bd-2tua","type":"blocks","created_at":"2026-02-20T07:37:02.056631491Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-n71","depends_on_id":"bd-34ll","type":"blocks","created_at":"2026-02-20T07:37:01.726594778Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-n71","depends_on_id":"bd-35l5","type":"blocks","created_at":"2026-02-20T07:37:02.801132770Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-n71","depends_on_id":"bd-3ndj","type":"blocks","created_at":"2026-02-20T07:37:02.386653778Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-n71","depends_on_id":"bd-3qo","type":"blocks","created_at":"2026-02-20T07:37:11.205242795Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-n71","depends_on_id":"bd-3u2o","type":"blocks","created_at":"2026-02-20T07:37:02.638135137Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-n71","depends_on_id":"bd-8l9k","type":"blocks","created_at":"2026-02-20T07:37:02.552609009Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-n71","depends_on_id":"bd-bt82","type":"blocks","created_at":"2026-02-20T07:37:02.222768632Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-n71","depends_on_id":"bd-cda","type":"blocks","created_at":"2026-02-20T07:37:09.661776379Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-n9r","title":"Implement configuration system (franken_node.toml) with profile support","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md (Bootstrap bridge for Sections 10.0, 10.2, 10.3)\nSection: BOOTSTRAP (Configuration baseline bridge)\n\nTask Objective:\nImplement the foundational `franken_node.toml` configuration system with profile support and deterministic precedence resolution for CLI, env, and file inputs.\n\nIn Scope:\n- Typed config schema with profile blocks and validation rules.\n- Deterministic merge/precedence order (CLI overrides > env > profile > defaults).\n- Deterministic diagnostics for missing, invalid, and conflicting configuration.\n\nUser-Value Rationale:\nReliable configuration is required for reproducible migrations, predictable operator behavior, and low-friction adoption.\n\nAcceptance Criteria:\n- Profile selection and merge precedence produce deterministic resolved config snapshots.\n- Invalid configs fail with stable, actionable diagnostics and non-zero exit semantics.\n- Config resolution can be consumed cleanly by `init`/`doctor` command flows without duplicated parsing logic.\n\nExpected Artifacts:\n- Config contract/spec documenting schema, precedence, and failure semantics.\n- Example config fixtures for default/dev/prod or equivalent profile classes.\n- Machine-readable resolved-config artifact for CI validation.\n\n- Machine-readable verification artifact at `artifacts/section_bootstrap/bd-n9r/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_bootstrap/bd-n9r/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for parser/validator behavior, precedence rules, and edge/failure cases.\n- Integration tests covering config loading from file/env/CLI combinations.\n- E2E tests validating real command execution under multiple profile scenarios.\n- Structured logs capturing config source provenance, merge decisions, and validation outcomes.\n\nTask-Specific Clarification:\n- For \"Implement configuration system (franken_node.toml) with profile support\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"Implement configuration system (franken_node.toml) with profile support\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"Implement configuration system (franken_node.toml) with profile support\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"Implement configuration system (franken_node.toml) with profile support\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"Implement configuration system (franken_node.toml) with profile support\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","status":"in_progress","priority":1,"issue_type":"task","assignee":"CoralReef","created_at":"2026-02-20T07:29:19.182078063Z","created_by":"ubuntu","updated_at":"2026-02-20T08:46:43.830684199Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["config","foundation"],"dependencies":[{"issue_id":"bd-n9r","depends_on_id":"bd-3rp","type":"blocks","created_at":"2026-02-20T07:29:33.007345052Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-nbh7","title":"[16] Contribution: benchmark/verifier methodology publications","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 16\n\nTask Objective:\nPublish methodology for benchmark and verifier design.\n\nExecution Requirements:\n- Make this requirement machine-verifiable and release-gated where applicable.\n- Keep behavior self-documenting so future contributors do not need to re-open the master plan.\n\nTesting & Logging Requirements:\n- Unit tests for rule/contract logic and error conditions.\n- Integration/E2E tests for workflow-level enforcement.\n- Structured logs with stable codes and trace IDs.\n- Reproducible artifacts that prove contract satisfaction.\n\nAcceptance Criteria:\n- Success and failure invariants for [16] Contribution: benchmark/verifier methodology publications are explicitly quantified and machine-verifiable.\n- Determinism requirements for [16] Contribution: benchmark/verifier methodology publications are validated across repeated runs and adversarial perturbations.\n\n\nExpected Artifacts:\n- Machine-readable verification artifact with explicit canonical path under artifacts/section_16/bd-nbh7/verification_evidence.json, suitable for CI/release gating.\n- Human-readable verification summary with explicit canonical path under artifacts/section_16/bd-nbh7/verification_summary.md, linking implementation intent to measured validation outcomes.\n\nTask-Specific Clarification:\n- The implementation must deliver the exact capability named in the title, with no scope dilution and no silent feature omission.\n- Validation artifacts must include capability-specific thresholds, pass/fail criteria, and reproducible evidence bundles tied to this bead ID.\n- Program/section verification gates must be able to consume this bead's outputs without manual interpretation.\n\nWhy This Improves User Outcomes:\n- \"[16] Contribution: benchmark/verifier methodology publications\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[16] Contribution: benchmark/verifier methodology publications\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"1. Published methodology document for benchmark design: how metrics were selected, how workloads were designed, how scoring formulas were derived, and what validity threats were considered.\n2. Published methodology document for verifier design: what claims the verifier validates, how validation is performed, what evidence is required, and what limitations exist.\n3. Methodology documents follow academic standards: (a) related work section comparing to existing approaches, (b) threat-to-validity analysis, (c) reproducibility instructions.\n4. Methodologies are peer-reviewed: >= 2 external reviewers provide written feedback; feedback and responses are published.\n5. Methodology enables independent replication: a reader can build a comparable benchmark/verifier from the methodology document alone (validated by >= 1 external replication attempt).\n6. Methodologies are versioned and updated when benchmark/verifier design changes materially.\n7. Evidence: methodology_publication_registry.json with per-document: title, version, reviewer names, review status, and replication attempts.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:39:36.959551099Z","created_by":"ubuntu","updated_at":"2026-02-20T15:28:20.458041922Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-16","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-nbh7","depends_on_id":"bd-2ad0","type":"blocks","created_at":"2026-02-20T07:43:26.636344418Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-nbwo","title":"[10.17] Publish universal verifier SDK and replay capsule format.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.17 — Radical Expansion Execution Track (9K)\n\nWhy This Exists:\nRadical expansion track for proof-carrying speculation, Bayesian adversary control, time-travel replay, ZK attestations, and claim compiler systems.\n\nTask Objective:\nPublish universal verifier SDK and replay capsule format.\n\nAcceptance Criteria:\n- External verifiers can replay signed capsules and reproduce claim verdicts without privileged internal access; capsule schema and verification APIs are stable and versioned.\n\nExpected Artifacts:\n- `sdk/verifier/*`, `docs/specs/replay_capsule_format.md`, `tests/conformance/verifier_sdk_capsule_replay.rs`, `artifacts/10.17/verifier_sdk_certification_report.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_17/bd-nbwo/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_17/bd-nbwo/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.17] Publish universal verifier SDK and replay capsule format.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.17] Publish universal verifier SDK and replay capsule format.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.17] Publish universal verifier SDK and replay capsule format.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.17] Publish universal verifier SDK and replay capsule format.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.17] Publish universal verifier SDK and replay capsule format.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"- External verifiers can replay signed capsules and reproduce claim verdicts without privileged internal access; capsule schema and verification APIs are stable and versioned.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:37:03.842488483Z","created_by":"ubuntu","updated_at":"2026-02-20T15:45:58.631738243Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-17","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-nbwo","depends_on_id":"bd-2iyk","type":"blocks","created_at":"2026-02-20T07:43:18.733727838Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-nglx","title":"[11] Contract field: rollback command","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 11\n\nTask Objective:\nRequire concrete rollback command/procedure for every major subsystem change.\n\nExecution Requirements:\n- Make this requirement machine-verifiable and release-gated where applicable.\n- Keep behavior self-documenting so future contributors do not need to re-open the master plan.\n\nTesting & Logging Requirements:\n- Unit tests for rule/contract logic and error conditions.\n- Integration/E2E tests for workflow-level enforcement.\n- Structured logs with stable codes and trace IDs.\n- Reproducible artifacts that prove contract satisfaction.\n\nAcceptance Criteria:\n- Success and failure invariants for [11] Contract field: rollback command are explicitly quantified and machine-verifiable.\n- Determinism requirements for [11] Contract field: rollback command are validated across repeated runs and adversarial perturbations.\n\n\nExpected Artifacts:\n- Machine-readable verification artifact with explicit canonical path under artifacts/section_11/bd-nglx/verification_evidence.json, suitable for CI/release gating.\n- Human-readable verification summary with explicit canonical path under artifacts/section_11/bd-nglx/verification_summary.md, linking implementation intent to measured validation outcomes.\n\nTask-Specific Clarification:\n- The implementation must deliver the exact capability named in the title, with no scope dilution and no silent feature omission.\n- Validation artifacts must include capability-specific thresholds, pass/fail criteria, and reproducible evidence bundles tied to this bead ID.\n- Program/section verification gates must be able to consume this bead's outputs without manual interpretation.\n\nWhy This Improves User Outcomes:\n- \"[11] Contract field: rollback command\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[11] Contract field: rollback command\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"1. Every evidence contract includes a rollback command: a single executable command or script that reverts the change completely.\n2. The rollback command must be: (a) copy-pasteable (no placeholders requiring manual substitution), (b) idempotent (safe to run multiple times), (c) tested in CI (the command is executed in a test environment and verified to restore prior state).\n3. The contract must specify rollback scope: what is reverted (config, binary, data) and what is NOT reverted (e.g., already-processed events).\n4. CI rejects contracts where rollback command is missing, contains unresolved placeholders, or is not tested.\n5. Unit test: a contract with tested, idempotent rollback command passes; one with placeholder variables or untested command fails.\n6. Rollback command execution time must be documented (estimated seconds/minutes).","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:39:32.988255014Z","created_by":"ubuntu","updated_at":"2026-02-20T15:16:48.817420566Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-11","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-nglx","depends_on_id":"bd-2ymp","type":"blocks","created_at":"2026-02-20T07:43:24.557214665Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-novi","title":"[10.13] Define stable error code namespace and machine-readable `retryable/retry_after/recovery_hint` contract.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.13 — FCP Deep-Mined Expansion Execution Track (9I)\n\nWhy This Exists:\nDeep connector/provider contract track: lifecycle FSMs, conformance harnesses, fencing, sandboxing, revocation freshness, and protocol hardening.\n\nTask Objective:\nDefine stable error code namespace and machine-readable `retryable/retry_after/recovery_hint` contract.\n\nAcceptance Criteria:\n- Error codes are unique and namespaced; machine-readable recovery fields are present for all non-fatal errors; compatibility tests catch breaking changes.\n\nExpected Artifacts:\n- `docs/specs/error_code_contract.md`, `tests/conformance/error_contract_stability.rs`, `artifacts/10.13/error_code_registry.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_13/bd-novi/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_13/bd-novi/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.13] Define stable error code namespace and machine-readable `retryable/retry_after/recovery_hint` contract.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.13] Define stable error code namespace and machine-readable `retryable/retry_after/recovery_hint` contract.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.13] Define stable error code namespace and machine-readable `retryable/retry_after/recovery_hint` contract.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.13] Define stable error code namespace and machine-readable `retryable/retry_after/recovery_hint` contract.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.13] Define stable error code namespace and machine-readable `retryable/retry_after/recovery_hint` contract.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","status":"closed","priority":1,"issue_type":"task","assignee":"CrimsonCrane","created_at":"2026-02-20T07:36:54.657859380Z","created_by":"ubuntu","updated_at":"2026-02-20T13:22:47.359794706Z","closed_at":"2026-02-20T13:22:47.359758960Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-13","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-novi","depends_on_id":"bd-1ugy","type":"blocks","created_at":"2026-02-20T07:43:13.937275412Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-nr4","title":"[10.8] Implement operator runbooks for high-severity trust incidents.","description":"## [10.8] Implement operator runbooks for high-severity trust incidents\n\n### Why This Exists\n\nSection 10.15 (invariant-breach runbooks) and the broader operational readiness track (10.8) require that every high-severity trust incident has a pre-written, tested, machine-readable runbook. When a trust state corruption event or mass revocation hits production at 3 AM, operators cannot be expected to improvise a recovery procedure. Runbooks encode institutional knowledge into executable steps that reduce mean-time-to-recovery (MTTR) and eliminate decision paralysis. Machine-readable runbooks also enable autonomous operator agents to initiate containment automatically while paging humans for judgment calls.\n\n### What It Must Do\n\nDeliver a comprehensive set of operator runbooks covering the highest-severity trust incident categories, in both human-readable (Markdown) and machine-readable (JSON) formats:\n\n- **Trust state corruption**: Detection via integrity check failure on trust artifacts. Containment: activate safe mode (bd-k6o), freeze trust delegation. Investigation: diff current trust state against last known-good evidence ledger snapshot. Repair: restore from verified snapshot, re-derive trust chain. Verify: full trust re-verification pass. Rollback: revert to previous epoch if repair fails.\n- **Mass revocation event**: Detection via revocation rate exceeding threshold. Containment: rate-limit revocation propagation to prevent cascade. Investigation: identify revocation source and scope. Repair: process revocations in batches with verification. Verify: confirm all revoked entities are excluded and no false revocations occurred. Rollback: re-instate falsely revoked entities via recovery receipts.\n- **Fleet quarantine activation**: Detection via quarantine threshold breach. Containment: isolate quarantined nodes from healthy fleet. Investigation: determine quarantine trigger root cause. Repair: remediate root cause, promote nodes from quarantine (per quarantine_store.rs). Verify: promoted nodes pass full health gate. Rollback: re-quarantine if promotion verification fails.\n- **Epoch transition failure**: Detection via epoch barrier timeout or mismatch. Containment: hold current epoch, prevent partial transitions. Investigation: identify which peers failed to transition and why. Repair: retry transition with extended timeout or manual override. Verify: all peers on same epoch with consistent state. Rollback: revert to previous epoch.\n- **Evidence ledger divergence**: Detection via cross-peer ledger hash mismatch. Containment: suspend ledger writes, activate read-only mode. Investigation: binary search for divergence point. Repair: reconcile from authoritative source. Verify: all peers report consistent ledger hash. Rollback: restore from last consistent snapshot.\n- **Proof pipeline outage**: Detection via proof generation timeout or failure rate spike. Containment: queue pending proofs, extend grace periods. Investigation: identify pipeline component failure. Repair: restart failed components, replay queued proofs. Verify: proof generation rate returns to baseline. Rollback: fall back to cached proofs within validity window.\n\nEach runbook must include: detection signature (metric/log patterns), severity classification, estimated time-to-recover, required operator permissions, immediate containment steps, investigation procedure, repair procedure, post-incident verification, rollback path, and drill scenario for testing.\n\n### Acceptance Criteria\n\n1. All six incident categories have runbooks in both Markdown (`docs/runbooks/`) and machine-readable JSON (`fixtures/runbooks/`) formats.\n2. Each runbook contains all required sections: detection signature, containment, investigation, repair, verification, and rollback.\n3. Machine-readable runbooks conform to a documented JSON schema; a validation script checks all runbooks against the schema.\n4. Each runbook has an associated drill scenario that can be executed to verify the runbook's procedures are current and functional.\n5. Drill scenarios are executable via a test harness (`scripts/run_drill.py`) that simulates the incident condition and walks through runbook steps.\n6. Runbook cross-references to safe mode (bd-k6o), quarantine store, and epoch management are verified to reference current code paths.\n7. A verification script confirms all runbooks are present, schema-valid, and have been drilled within the configured freshness window (default: 30 days).\n\n### Key Dependencies\n\n- Safe-mode implementation (bd-k6o) for containment steps\n- Quarantine store (quarantine_store.rs) for fleet quarantine runbook\n- Epoch/fencing infrastructure (fencing.rs) for epoch transition runbook\n- Evidence ledger for divergence detection\n- Proof pipeline for outage runbook\n\n### Testing & Logging Requirements\n\n- Unit tests for runbook schema validation and drill scenario parsing.\n- Integration tests that execute each drill scenario in a test environment.\n- Verification script (`scripts/check_operator_runbooks.py`) with `--json` and `self_test()`.\n- Drill execution logged at INFO with step-by-step outcome recording.\n\n### Expected Artifacts\n\n- `docs/runbooks/` — six Markdown runbooks\n- `fixtures/runbooks/` — six JSON machine-readable runbooks + schema\n- `scripts/check_operator_runbooks.py` — verification script\n- `tests/test_check_operator_runbooks.py` — unit tests\n- `artifacts/section_10_8/bd-nr4/verification_evidence.json`\n- `artifacts/section_10_8/bd-nr4/verification_summary.md`","acceptance_criteria":"1. Runbooks cover at minimum: trust-anchor compromise, fleet-wide quarantine escalation, control-plane split-brain, key rotation emergency, and malicious-extension detection.\n2. Each runbook follows a standard template: trigger conditions, severity classification, step-by-step response procedure, verification checks, and rollback instructions.\n3. Runbooks are stored as Markdown under docs/runbooks/ and are indexed in a table-of-contents file.\n4. Each runbook includes estimated time-to-resolution and required operator privilege level.\n5. At least one runbook is exercised end-to-end in a test environment as part of the DR drill program (cross-reference bd-3m6).\n6. Runbooks reference specific CLI commands and API calls — no vague instructions like 'contact support'.\n7. Runbook review cadence is documented: each runbook has a last-reviewed date and must be re-validated at least once per release cycle.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:36:48.105511076Z","created_by":"ubuntu","updated_at":"2026-02-20T16:59:04.199315024Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-8","self-contained","test-obligations"]}
{"id":"bd-nupr","title":"[10.14] Define `EvidenceEntry` schema for product control decisions with deterministic field and candidate ordering.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.14 — FrankenSQLite Deep-Mined Expansion Execution Track (9J)\n\nWhy This Exists:\nFrankenSQLite deep-mined control integrity track: evidence ledgers, proofs, epochs, remote effects, markers/MMR, and deterministic fault labs.\n\nTask Objective:\nDefine `EvidenceEntry` schema for product control decisions with deterministic field and candidate ordering.\n\nAcceptance Criteria:\n- Schema covers decision kind, candidates, constraints, chosen action, and witness references; field ordering is canonical; schema validation is enforced in CI.\n\nExpected Artifacts:\n- `docs/specs/evidence_entry_schema.md`, `spec/evidence_entry_v1.json`, `artifacts/10.14/evidence_schema_validation_report.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_14/bd-nupr/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_14/bd-nupr/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.14] Define `EvidenceEntry` schema for product control decisions with deterministic field and candidate ordering.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.14] Define `EvidenceEntry` schema for product control decisions with deterministic field and candidate ordering.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.14] Define `EvidenceEntry` schema for product control decisions with deterministic field and candidate ordering.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.14] Define `EvidenceEntry` schema for product control decisions with deterministic field and candidate ordering.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.14] Define `EvidenceEntry` schema for product control decisions with deterministic field and candidate ordering.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"Schema covers decision kind, candidates, constraints, chosen action, and witness references; field ordering is canonical; schema validation is enforced in CI.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:36:55.140064103Z","created_by":"ubuntu","updated_at":"2026-02-20T15:44:12.748893732Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-14","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-nupr","depends_on_id":"bd-1ta","type":"blocks","created_at":"2026-02-20T07:46:32.911199206Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-nupr","depends_on_id":"bd-cda","type":"blocks","created_at":"2026-02-20T07:46:32.971880877Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-nwhn","title":"[10.14] Implement root pointer atomic publication protocol (`write temp -> fsync temp -> rename -> fsync dir`).","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.14 — FrankenSQLite Deep-Mined Expansion Execution Track (9J)\n\nWhy This Exists:\nFrankenSQLite deep-mined control integrity track: evidence ledgers, proofs, epochs, remote effects, markers/MMR, and deterministic fault labs.\n\nTask Objective:\nImplement root pointer atomic publication protocol (`write temp -> fsync temp -> rename -> fsync dir`).\n\nAcceptance Criteria:\n- Publication protocol survives crash-injection tests without ambiguous root; missing fsync steps are detected by tests; root switch is atomic.\n\nExpected Artifacts:\n- `docs/specs/root_publication_protocol.md`, `tests/integration/root_pointer_crash_safety.rs`, `artifacts/10.14/root_publication_crash_matrix.csv`.\n\n- Machine-readable verification artifact at `artifacts/section_10_14/bd-nwhn/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_14/bd-nwhn/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.14] Implement root pointer atomic publication protocol (`write temp -> fsync temp -> rename -> fsync dir`).\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.14] Implement root pointer atomic publication protocol (`write temp -> fsync temp -> rename -> fsync dir`).\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.14] Implement root pointer atomic publication protocol (`write temp -> fsync temp -> rename -> fsync dir`).\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.14] Implement root pointer atomic publication protocol (`write temp -> fsync temp -> rename -> fsync dir`).\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.14] Implement root pointer atomic publication protocol (`write temp -> fsync temp -> rename -> fsync dir`).\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"Publication protocol survives crash-injection tests without ambiguous root; missing fsync steps are detected by tests; root switch is atomic.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:36:58.874236996Z","created_by":"ubuntu","updated_at":"2026-02-20T16:24:19.202738625Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-14","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-nwhn","depends_on_id":"bd-126h","type":"blocks","created_at":"2026-02-20T16:24:19.202657413Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-okqy","title":"[10.14] Implement L1/L2/L3 trust artifact storage abstraction with explicit source-of-truth designation.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.14 — FrankenSQLite Deep-Mined Expansion Execution Track (9J)\n\nWhy This Exists:\nFrankenSQLite deep-mined control integrity track: evidence ledgers, proofs, epochs, remote effects, markers/MMR, and deterministic fault labs.\n\nTask Objective:\nImplement L1/L2/L3 trust artifact storage abstraction with explicit source-of-truth designation.\n\nAcceptance Criteria:\n- Tier abstraction exposes clear authority boundaries; source-of-truth is explicit and immutable by class; recovery path reconstructs derived tiers.\n\nExpected Artifacts:\n- `docs/specs/tiered_trust_storage.md`, `tests/integration/tiered_storage_recovery.rs`, `artifacts/10.14/tiered_storage_authority_map.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_14/bd-okqy/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_14/bd-okqy/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.14] Implement L1/L2/L3 trust artifact storage abstraction with explicit source-of-truth designation.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.14] Implement L1/L2/L3 trust artifact storage abstraction with explicit source-of-truth designation.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.14] Implement L1/L2/L3 trust artifact storage abstraction with explicit source-of-truth designation.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.14] Implement L1/L2/L3 trust artifact storage abstraction with explicit source-of-truth designation.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.14] Implement L1/L2/L3 trust artifact storage abstraction with explicit source-of-truth designation.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"Tier abstraction exposes clear authority boundaries; source-of-truth is explicit and immutable by class; recovery path reconstructs derived tiers.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:36:57.303257262Z","created_by":"ubuntu","updated_at":"2026-02-20T16:24:04.452147719Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-14","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-okqy","depends_on_id":"bd-2573","type":"blocks","created_at":"2026-02-20T16:24:04.452065085Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-oolt","title":"[10.14] Require evidence emission for policy-driven commit/abort/quarantine/release actions.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.14 — FrankenSQLite Deep-Mined Expansion Execution Track (9J)\n\nWhy This Exists:\nFrankenSQLite deep-mined control integrity track: evidence ledgers, proofs, epochs, remote effects, markers/MMR, and deterministic fault labs.\n\nTask Objective:\nRequire evidence emission for policy-driven commit/abort/quarantine/release actions.\n\nAcceptance Criteria:\n- All policy-driven control decisions emit mandatory evidence entries; missing entry causes conformance failure; evidence links to action IDs.\n\nExpected Artifacts:\n- `tests/conformance/policy_decision_evidence.rs`, `docs/specs/policy_evidence_requirements.md`, `artifacts/10.14/policy_decision_evidence_matrix.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_14/bd-oolt/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_14/bd-oolt/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.14] Require evidence emission for policy-driven commit/abort/quarantine/release actions.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.14] Require evidence emission for policy-driven commit/abort/quarantine/release actions.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.14] Require evidence emission for policy-driven commit/abort/quarantine/release actions.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.14] Require evidence emission for policy-driven commit/abort/quarantine/release actions.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.14] Require evidence emission for policy-driven commit/abort/quarantine/release actions.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"All policy-driven control decisions emit mandatory evidence entries; missing entry causes conformance failure; evidence links to action IDs.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:36:55.303257010Z","created_by":"ubuntu","updated_at":"2026-02-20T15:44:12.323964618Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-14","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-oolt","depends_on_id":"bd-2e73","type":"blocks","created_at":"2026-02-20T07:43:14.303523007Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-oty","title":"[10.10] Integrate canonical session-authenticated control channel + monotonic anti-replay framing (from `10.13`) across product control APIs.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md — Section 10.10 (FCP-Inspired Hardening), cross-ref Section 9E.6\n\n## Why This Exists\n\nEnhancement Map 9E.6 mandates a session-authenticated data plane for high-throughput control traffic. The canonical control channel and anti-replay framing primitives from Section 10.13 provide the cryptographic foundation, but product control APIs need a complete integration layer that establishes authenticated sessions, enforces monotonic sequence numbers per direction, and rejects replayed or out-of-order frames across all control endpoints. Without this, control-plane APIs are vulnerable to replay attacks (re-executing old commands), message reordering (executing operations out of sequence), and session hijacking (injecting commands into an unauthenticated channel). This bead bridges the 10.13 primitives into every product control API surface in franken_node, ensuring that all control traffic — between kernels, between operator tooling and the node, and between nodes in a cluster — is session-authenticated with anti-replay guarantees.\n\n## What This Must Do\n\n1. Integrate the authenticated control channel from 10.13 (bd-v97o) as the mandatory transport for all product control APIs, wrapping existing API endpoints in session-authenticated framing.\n2. Implement per-session, per-direction monotonic sequence counters: each message in a session carries a sequence number that must strictly increase in its direction (client-to-server, server-to-client), and any non-monotonic message is rejected.\n3. Implement a configurable replay window (default: 0, meaning strict monotonicity with no gap tolerance) that allows operators to tune for network reordering while maintaining bounded replay resistance.\n4. Implement session lifecycle management: `establish_session()` (mutual authentication using role-separated keys from bd-364), `send_authenticated()`, `receive_authenticated()`, and `terminate_session()` with explicit session teardown and sequence counter reset.\n5. Ensure that session establishment uses the Encryption role key from bd-364 for key exchange and the Signing role key for message authentication — no single-key shortcuts.\n6. Add mandatory session-binding to all control actions: the action dispatcher verifies that the incoming request was received over an authenticated session before executing.\n\n## Context from Enhancement Maps\n\n- 9E.6: \"Session-authenticated data plane for high-throughput control traffic\"\n- 9E.5 (cross-ref): Key-role separation (bd-364) provides distinct keys for session encryption vs. message signing.\n- 9E.7 (cross-ref): Revocation freshness checks (bd-2sx) operate within authenticated sessions to prevent an attacker from injecting stale revocation state.\n- 9D.3 (Interop): Session framing format must be compatible across kernel implementations for cross-kernel control traffic.\n\n## Dependencies\n\n- Upstream: bd-v97o ([10.13] Implement authenticated control channel with per-direction sequence monotonicity and replay-window checks) — provides the core control channel primitives and framing format.\n- Upstream: bd-364 ([10.10] Implement key-role separation for control-plane signing/encryption/issuance) — provides role-separated keys for session establishment.\n- Downstream: bd-2sx ([10.10] Integrate canonical revocation freshness semantics before risky product actions) — revocation checks operate within authenticated sessions.\n- Downstream: bd-1jjq ([10.10] Section-wide verification gate) — aggregates verification evidence.\n\n## Acceptance Criteria\n\n1. Every product control API endpoint is wrapped in session-authenticated framing — no unauthenticated control endpoint exists (verified by exhaustive endpoint enumeration test).\n2. Per-direction sequence monotonicity is enforced: a message with sequence N followed by sequence N or N-1 is rejected with `SEQUENCE_MONOTONICITY_VIOLATION`.\n3. Replay window is configurable: with window=0, any out-of-order message is rejected; with window=K, messages within K of the latest sequence are accepted but duplicates are still rejected.\n4. Session establishment uses mutual authentication: both parties prove identity before any control message is accepted.\n5. Session establishment uses Encryption role key for key exchange and Signing role key for message authentication — cross-role usage is rejected.\n6. Session teardown explicitly invalidates all session state: attempting to send on a terminated session produces `SESSION_TERMINATED`.\n7. Throughput benchmark: authenticated control channel sustains at least 10,000 messages/second with sub-millisecond per-message overhead for authentication and sequence checking.\n8. Verification evidence JSON includes endpoint coverage count, sequence violation rejection counts, and throughput benchmark results.\n\n## Testing & Logging Requirements\n\n- Unit tests: Establish a session, send 1000 messages with monotonic sequences, verify all accepted. Send a message with a duplicate sequence number — verify rejection. Send a message with a decremented sequence — verify rejection. Test replay window at sizes 0, 1, 10, 100. Test session teardown followed by attempted send. Test mutual authentication failure (invalid key, wrong role key).\n- Integration tests: Two-node control traffic: establish session, exchange control messages bidirectionally, verify both directions maintain independent sequence counters. Verify that a control action received over an unauthenticated channel is rejected before dispatch. Test session re-establishment after network partition.\n- Adversarial tests: Capture and replay an authenticated control message on the same session (must be rejected by sequence check). Attempt session hijacking by injecting a message with a guessed sequence number. Attempt to establish a session using the wrong role key (Issuance instead of Encryption). Attempt to strip session framing and send a raw control message directly to the API endpoint.\n- Structured logs: `SESSION_ESTABLISHED` (session_id, peer_identity, auth_method, key_roles_used). `SESSION_MESSAGE_SENT` / `SESSION_MESSAGE_RECEIVED` (session_id, direction, sequence_number, payload_type). `SESSION_SEQUENCE_VIOLATION` (session_id, direction, expected_min, received, gap). `SESSION_TERMINATED` (session_id, reason, messages_exchanged). All events include `trace_id` and `epoch_id`.\n\n## Expected Artifacts\n\n- docs/specs/section_10_10/bd-oty_contract.md\n- crates/franken-node/src/connector/session_auth.rs (or similar module path)\n- scripts/check_session_auth.py with --json flag and self_test()\n- tests/test_check_session_auth.py\n- artifacts/section_10_10/bd-oty/verification_evidence.json\n- artifacts/section_10_10/bd-oty/verification_summary.md","acceptance_criteria":"1. Integrate the canonical session-authenticated control channel from 10.13 (control_channel.rs) into all product control API endpoints. Every control API call MUST be dispatched through an authenticated session; unauthenticated control traffic is rejected with SESSION_REQUIRED error.\n2. Enforce monotonic anti-replay framing from 10.13 (frame_parser.rs): every control message carries a session-scoped monotonic sequence number. The receiver MUST reject any message whose sequence number is <= the highest previously accepted sequence number for that session. Return REPLAY_DETECTED error with the offending and expected sequence numbers.\n3. Implement a session lifecycle for control APIs: (a) session_open(client_id, auth_token) -> session_id, (b) session_send(session_id, message, seq) -> Result, (c) session_close(session_id). Sessions have a configurable max idle timeout (default 300s) and max lifetime (default 3600s).\n4. Implement session state tracking: maintain per-session high-water-mark for sequence numbers, last activity timestamp, and total message count.\n5. On session timeout (idle or lifetime), automatically close the session and reject further messages with SESSION_EXPIRED error.\n6. Integrate with the trace_context module from 10.13: every control message MUST propagate a trace context header. Log session open/close/replay-reject events with trace correlation IDs.\n7. Implement a control API inventory: maintain a list of all control endpoints and verify each is wrapped with session authentication. Provide an audit function list_unprotected_endpoints() that returns an empty list when fully integrated.\n8. Unit tests: (a) session open/send/close lifecycle, (b) replay rejection, (c) idle timeout expiry, (d) lifetime timeout expiry, (e) unauthenticated rejection, (f) trace context propagation.\n9. Integration test: send 100 messages with correct monotonic sequences, then replay message #50 and verify rejection.\n10. Verification: scripts/check_session_auth_channel.py --json, artifacts at artifacts/section_10_10/bd-oty/.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:36:49.247751565Z","created_by":"ubuntu","updated_at":"2026-02-20T17:01:30.402450862Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-10","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-oty","depends_on_id":"bd-v97o","type":"blocks","created_at":"2026-02-20T14:59:51.922380476Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-p6v1","title":"Epic: Adjacent Substrate Integration [10.16]","description":"- type: epic","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-20T07:47:58.072163029Z","updated_at":"2026-02-20T07:49:21.278189660Z","closed_at":"2026-02-20T07:49:21.278169974Z","close_reason":"Superseded by canonical detailed master-plan graph (bd-33v) with full 10.N-10.21 and 11-16 coverage, explicit dependencies, and per-section verification gates.","source_repo":".","compaction_level":0,"original_size":0}
{"id":"bd-p73r","title":"[10.18] Define canonical `ExecutionReceipt` schema and deterministic serialization rules.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.18 — Verifiable Execution Fabric Execution Track (9L)\n\nWhy This Exists:\nVEF track for proof-carrying runtime compliance: receipt chains, commitment checkpoints, proof generation/verification, and claim gating.\n\nTask Objective:\nDefine canonical `ExecutionReceipt` schema and deterministic serialization rules.\n\nAcceptance Criteria:\n- Receipt schema includes action type, capability context, actor/artifact identity, policy snapshot hash, timestamp/sequence, and witness references; serialization is canonical and hash-stable.\n\nExpected Artifacts:\n- `docs/specs/vef_execution_receipt.md`, `spec/vef_execution_receipt_v1.json`, `artifacts/10.18/vef_receipt_schema_vectors.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_18/bd-p73r/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_18/bd-p73r/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.18] Define canonical `ExecutionReceipt` schema and deterministic serialization rules.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.18] Define canonical `ExecutionReceipt` schema and deterministic serialization rules.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.18] Define canonical `ExecutionReceipt` schema and deterministic serialization rules.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.18] Define canonical `ExecutionReceipt` schema and deterministic serialization rules.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.18] Define canonical `ExecutionReceipt` schema and deterministic serialization rules.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"- Receipt schema includes action type, capability context, actor/artifact identity, policy snapshot hash, timestamp/sequence, and witness references; serialization is canonical and hash-stable.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:37:04.289355524Z","created_by":"ubuntu","updated_at":"2026-02-20T17:05:37.616070227Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-18","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-p73r","depends_on_id":"bd-16fq","type":"blocks","created_at":"2026-02-20T17:05:37.616018200Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-paui","title":"[12] Risk control: topological choke-point false positives","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 12\n\nTask Objective:\nUse counterfactual simulation, expected-loss calibration, and staged barrier rollout with rollback receipts.\n\nExecution Requirements:\n- Make this requirement machine-verifiable and release-gated where applicable.\n- Keep behavior self-documenting so future contributors do not need to re-open the master plan.\n\nTesting & Logging Requirements:\n- Unit tests for rule/contract logic and error conditions.\n- Integration/E2E tests for workflow-level enforcement.\n- Structured logs with stable codes and trace IDs.\n- Reproducible artifacts that prove contract satisfaction.\n\nAcceptance Criteria:\n- Success and failure invariants for [12] Risk control: topological choke-point false positives are explicitly quantified and machine-verifiable.\n- Determinism requirements for [12] Risk control: topological choke-point false positives are validated across repeated runs and adversarial perturbations.\n\n\nExpected Artifacts:\n- Machine-readable verification artifact with explicit canonical path under artifacts/section_12/bd-paui/verification_evidence.json, suitable for CI/release gating.\n- Human-readable verification summary with explicit canonical path under artifacts/section_12/bd-paui/verification_summary.md, linking implementation intent to measured validation outcomes.\n\nTask-Specific Clarification:\n- The implementation must deliver the exact capability named in the title, with no scope dilution and no silent feature omission.\n- Validation artifacts must include capability-specific thresholds, pass/fail criteria, and reproducible evidence bundles tied to this bead ID.\n- Program/section verification gates must be able to consume this bead's outputs without manual interpretation.\n\nWhy This Improves User Outcomes:\n- \"[12] Risk control: topological choke-point false positives\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[12] Risk control: topological choke-point false positives\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"RISK: Over-hardening false positives — security hardening rules are too aggressive, blocking legitimate operations and creating alert fatigue.\nIMPACT: Developers disable hardening, legitimate extensions are rejected, operational overhead from investigating false positives.\nCOUNTERMEASURES:\n  (a) Counterfactual simulation: before deploying a new hardening rule, simulate it against historical traffic to measure false-positive rate.\n  (b) Expected-loss calibration: each hardening rule has an expected-loss threshold; rules whose false-positive cost exceeds blocked-threat cost are rejected or tuned.\n  (c) Staged rollout: new hardening rules deploy in audit-only mode first, then warn mode, then enforce mode.\nVERIFICATION:\n  1. Counterfactual simulation framework exists and can replay >= 1000 historical operations against a proposed rule.\n  2. False-positive rate for any enforced rule is <= 1% (measured on historical data).\n  3. Expected-loss calibration: every enforced rule has documented false-positive cost vs blocked-threat cost, with net positive expected value.\n  4. Staged rollout: every new rule goes through audit -> warn -> enforce stages with minimum 24h per stage.\nTEST SCENARIOS:\n  - Scenario A: Propose a rule that would block 5% of legitimate operations; verify counterfactual simulation catches this and rejects enforcement.\n  - Scenario B: Deploy a rule in audit mode; verify it logs violations without blocking.\n  - Scenario C: Promote a rule from warn to enforce; verify only rules with FP rate <= 1% are promotable.\n  - Scenario D: Verify expected-loss calculation: a rule blocking $100/day of legitimate ops to prevent $10/day of threats is flagged as net-negative.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:39:33.940100297Z","created_by":"ubuntu","updated_at":"2026-02-20T15:19:53.232640682Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-12","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-paui","depends_on_id":"bd-1n1t","type":"blocks","created_at":"2026-02-20T07:43:25.084668493Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-pga7","title":"[13] Success criterion: deterministic incident containment/explanation","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 13\n\nTask Objective:\nEnsure operator workflows can contain and explain high-severity incidents deterministically.\n\nExecution Requirements:\n- Make this requirement machine-verifiable and release-gated where applicable.\n- Keep behavior self-documenting so future contributors do not need to re-open the master plan.\n\nTesting & Logging Requirements:\n- Unit tests for rule/contract logic and error conditions.\n- Integration/E2E tests for workflow-level enforcement.\n- Structured logs with stable codes and trace IDs.\n- Reproducible artifacts that prove contract satisfaction.\n\nAcceptance Criteria:\n- Success and failure invariants for [13] Success criterion: deterministic incident containment/explanation are explicitly quantified and machine-verifiable.\n- Determinism requirements for [13] Success criterion: deterministic incident containment/explanation are validated across repeated runs and adversarial perturbations.\n\n\nExpected Artifacts:\n- Machine-readable verification artifact with explicit canonical path under artifacts/section_13/bd-pga7/verification_evidence.json, suitable for CI/release gating.\n- Human-readable verification summary with explicit canonical path under artifacts/section_13/bd-pga7/verification_summary.md, linking implementation intent to measured validation outcomes.\n\nTask-Specific Clarification:\n- The implementation must deliver the exact capability named in the title, with no scope dilution and no silent feature omission.\n- Validation artifacts must include capability-specific thresholds, pass/fail criteria, and reproducible evidence bundles tied to this bead ID.\n- Program/section verification gates must be able to consume this bead's outputs without manual interpretation.\n\nWhy This Improves User Outcomes:\n- \"[13] Success criterion: deterministic incident containment/explanation\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[13] Success criterion: deterministic incident containment/explanation\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"1. Every production incident of severity >= high produces a deterministic replay artifact within 24 hours of detection.\n2. Replay artifact enables full causal explanation: root cause, propagation path, blast radius, and containment timeline.\n3. Containment is deterministic: given the same incident inputs, the containment system produces identical isolation decisions every time.\n4. Incident explanation report is generated automatically from replay artifacts (no manual investigation required for initial triage).\n5. Containment latency target: from detection to isolation <= 30 seconds for automated containment, <= 5 minutes for human-in-the-loop.\n6. Post-incident verification: replay the incident with the fix applied and confirm the incident does not recur.\n7. Evidence: incident_containment_log.json with per-incident: detection time, containment time, replay artifact path, explanation completeness score.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:39:34.584488977Z","created_by":"ubuntu","updated_at":"2026-02-20T15:22:54.121598689Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-13","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-pga7","depends_on_id":"bd-2a4l","type":"blocks","created_at":"2026-02-20T07:43:25.400230188Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-phf","title":"[10.4] Implement ecosystem telemetry for trust and adoption metrics.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.4 — Extension Ecosystem + Registry\n\nWhy This Exists:\nExtension ecosystem and registry trust foundation: signed artifacts, provenance, trust cards, revocation, and quarantine flows.\n\nTask Objective:\nImplement ecosystem telemetry for trust and adoption metrics.\n\nAcceptance Criteria:\n(See structured acceptance_criteria field for domain-specific criteria.)\n\nExpected Artifacts:\n- Section-owned design/spec artifact at `docs/specs/section_10_4/bd-phf_contract.md`, capturing decision rationale, invariants, and interface boundaries.\n- Machine-readable verification artifact at `artifacts/section_10_4/bd-phf/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_4/bd-phf/verification_summary.md` linking implementation intent to measured outcomes.\n\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.4] Implement ecosystem telemetry for trust and adoption metrics.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.4] Implement ecosystem telemetry for trust and adoption metrics.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.4] Implement ecosystem telemetry for trust and adoption metrics.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.4] Implement ecosystem telemetry for trust and adoption metrics.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.4] Implement ecosystem telemetry for trust and adoption metrics.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"1. Define ecosystem telemetry schema with metric categories: trust metrics (certification level distribution, reputation score histogram, revocation rate, mean-time-to-quarantine), adoption metrics (install/uninstall counts, active extension count per node, capability usage frequency, extension retention rate at 7d/30d/90d), and health metrics (provenance verification failure rate, attestation chain gap rate, policy violation rate). 2. All telemetry metrics use a stable metric naming convention: 'franken.ecosystem.<category>.<metric_name>' with consistent label dimensions (ext_id, publisher_id, certification_level, safety_tier). 3. Metrics are emitted via OpenTelemetry-compatible interface: counters for event counts, gauges for current-state values, histograms for latency/distribution metrics. Export targets include Prometheus exposition format and OTLP gRPC. 4. Trust score aggregation: compute fleet-wide trust posture score as weighted average of per-extension trust card scores, with anomaly detection flagging sudden trust posture drops (>10% decline in rolling 1-hour window). 5. Adoption funnel tracking: measure conversion from discovery -> install -> active_use -> retention for each extension, with cohort analysis by certification level and publisher reputation tier. 6. Privacy controls: telemetry is aggregated at the fleet level before export; no per-user or per-session data leaves the node boundary. Node-level telemetry requires explicit operator opt-in. Per-extension telemetry is anonymized using k-anonymity (k>=5 nodes) before fleet aggregation. 7. Dashboard-ready data: telemetry API endpoint GET /api/v1/ecosystem/telemetry/summary returns a pre-computed summary suitable for rendering trust posture dashboards, updated at configurable interval (default 60s). 8. Alerting thresholds: configurable alert rules for: revocation_rate > threshold, mean_trust_score < threshold, quarantine_count > threshold within time window. Alert rules are defined in policy configuration alongside certification requirements. 9. Historical telemetry retention: raw metrics retained for 30 days, hourly aggregates for 365 days, daily aggregates indefinitely. Retention is configurable per deployment. 10. Structured log events: TELEMETRY_EXPORT_CYCLE with metrics_count, export_target, export_latency_ms; TRUST_POSTURE_ANOMALY with posture_score, delta, contributing_extensions[]; TELEMETRY_PRIVACY_REDACTION with redacted_fields_count, k_anonymity_threshold.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:36:45.982792949Z","created_by":"ubuntu","updated_at":"2026-02-20T16:58:59.613096456Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-4","self-contained","test-obligations"]}
{"id":"bd-q5y7","title":"Epic: Testing Infrastructure Framework","description":"- type: task","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-20T07:47:58.072163029Z","updated_at":"2026-02-20T07:49:21.077495114Z","closed_at":"2026-02-20T07:49:21.077477812Z","close_reason":"Superseded by canonical detailed master-plan graph (bd-33v) with full 10.N-10.21 and 11-16 coverage, explicit dependencies, and per-section verification gates.","source_repo":".","compaction_level":0,"original_size":0}
{"id":"bd-qlc6","title":"[10.14] Map remote/control tasks to lane-aware scheduler classes with priority policies.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.14 — FrankenSQLite Deep-Mined Expansion Execution Track (9J)\n\nWhy This Exists:\nFrankenSQLite deep-mined control integrity track: evidence ledgers, proofs, epochs, remote effects, markers/MMR, and deterministic fault labs.\n\nTask Objective:\nMap remote/control tasks to lane-aware scheduler classes with priority policies.\n\nAcceptance Criteria:\n- Task classes are mapped to lanes by policy; lane starvation and misclassification checks are enforced; lane telemetry is exposed.\n\nExpected Artifacts:\n- `docs/specs/lane_mapping_policy.md`, `tests/conformance/lane_mapping_enforcement.rs`, `artifacts/10.14/lane_mapping_metrics.csv`.\n\n- Machine-readable verification artifact at `artifacts/section_10_14/bd-qlc6/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_14/bd-qlc6/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.14] Map remote/control tasks to lane-aware scheduler classes with priority policies.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.14] Map remote/control tasks to lane-aware scheduler classes with priority policies.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.14] Map remote/control tasks to lane-aware scheduler classes with priority policies.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.14] Map remote/control tasks to lane-aware scheduler classes with priority policies.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.14] Map remote/control tasks to lane-aware scheduler classes with priority policies.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"Task classes are mapped to lanes by policy; lane starvation and misclassification checks are enforced; lane telemetry is exposed.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:36:58.061700429Z","created_by":"ubuntu","updated_at":"2026-02-20T15:44:05.186931120Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-14","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-qlc6","depends_on_id":"bd-v4l0","type":"blocks","created_at":"2026-02-20T07:43:15.698361701Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-r6i","title":"[PLAN 16] Scientific Contribution Output","description":"Section 16 research-output epic. Produce open specs, reproducible datasets, publishable methodology, external evaluations, and transparent failure/correction reports.\n\n## Success Criteria\n- All child beads for this section are completed with acceptance artifacts attached and no unresolved blockers.\n- Section-level verification gate for comprehensive unit tests, integration/e2e workflows, and detailed structured logging evidence is green.\n- Scope-to-plan traceability is explicit, with no feature loss versus the canonical plan section.\n\n## Optimization Notes\n- User-Outcome Lens: \"[PLAN 16] Scientific Contribution Output\" must improve real operator and end-user reliability, explainability, and time-to-safe-outcome under both normal and degraded conditions.\n- Cross-Section Coordination: this epic's child beads must explicitly encode integration expectations with neighboring sections to prevent local optimizations that break global behavior.\n- Verification Non-Negotiable: completion requires deterministic unit coverage, integration/E2E replay evidence, and structured logs sufficient for independent third-party reproduction and triage.\n- Scope Protection: any proposed simplification must prove feature parity against plan intent and document tradeoffs explicitly; silent scope reduction is forbidden.","status":"open","priority":2,"issue_type":"epic","created_at":"2026-02-20T07:36:42.403660417Z","created_by":"ubuntu","updated_at":"2026-02-20T08:12:48.507532518Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-16"],"dependencies":[{"issue_id":"bd-r6i","depends_on_id":"bd-10ee","type":"blocks","created_at":"2026-02-20T07:39:37.171008727Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-r6i","depends_on_id":"bd-1sgr","type":"blocks","created_at":"2026-02-20T07:39:37.257015480Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-r6i","depends_on_id":"bd-1wz","type":"blocks","created_at":"2026-02-20T07:38:36.701224981Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-r6i","depends_on_id":"bd-2ad0","type":"blocks","created_at":"2026-02-20T07:39:36.913292975Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-r6i","depends_on_id":"bd-2ke","type":"blocks","created_at":"2026-02-20T07:38:36.615754206Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-r6i","depends_on_id":"bd-32p","type":"blocks","created_at":"2026-02-20T07:38:36.745702849Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-r6i","depends_on_id":"bd-33u2","type":"blocks","created_at":"2026-02-20T07:39:37.428935502Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-r6i","depends_on_id":"bd-37i","type":"blocks","created_at":"2026-02-20T07:38:36.878486012Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-r6i","depends_on_id":"bd-39a","type":"blocks","created_at":"2026-02-20T07:38:36.788618076Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-r6i","depends_on_id":"bd-3id1","type":"blocks","created_at":"2026-02-20T07:39:37.085703220Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-r6i","depends_on_id":"bd-4ou","type":"blocks","created_at":"2026-02-20T07:38:36.574375200Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-r6i","depends_on_id":"bd-e5cz","type":"blocks","created_at":"2026-02-20T07:39:37.343195305Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-r6i","depends_on_id":"bd-f955","type":"blocks","created_at":"2026-02-20T07:39:36.824189704Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-r6i","depends_on_id":"bd-nbh7","type":"blocks","created_at":"2026-02-20T07:39:36.999379977Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-r6i","depends_on_id":"bd-t8m","type":"blocks","created_at":"2026-02-20T07:38:36.659507525Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-r6i","depends_on_id":"bd-unkm","type":"blocks","created_at":"2026-02-20T07:48:31.769111976Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-r6i","depends_on_id":"bd-ybe","type":"blocks","created_at":"2026-02-20T07:38:36.831764184Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-s4cu","title":"[12] Risk control: compatibility illusion","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 12 — Risk Register\nRisk: Compatibility Illusion (Risk #1 of 12)\n\nWhy This Exists:\nThe compatibility illusion risk is that franken_node appears compatible in testing but diverges in subtle, hard-to-detect ways under real-world conditions. This is the #1 risk because it directly undermines the core value proposition.\n\nTask Objective:\nImplement countermeasures for the compatibility illusion risk: lockstep oracle + divergence receipts. Ensure the risk is continuously monitored and the countermeasures are active.\n\nDetailed Acceptance Criteria:\n1. Lockstep oracle (from 10.0 Initiative #4) actively running on all compatibility bands with continuous divergence detection.\n2. Divergence receipts (from 10.0 Initiative #1) automatically generated for any detected behavioral difference.\n3. Compatibility corpus targets >= 95% pass rate (Success Criteria from Section 13).\n4. Monitoring dashboard shows real-time compatibility health per API family (from 10.2 regression dashboard).\n5. Alert system triggers when compatibility drops below threshold or new divergences appear.\n6. Risk mitigation documented with explicit evidence artifacts proving countermeasure effectiveness.\n\nKey Dependencies:\n- Depends on 10.2 (Compatibility Core) for lockstep runners and band definitions.\n- Depends on 10.0 (compatibility envelope, lockstep oracle) for countermeasure implementation.\n- Feeds into 13 (Success Criteria) for >= 95% compatibility corpus pass target.\n\nExpected Artifacts:\n- Risk mitigation report with evidence of countermeasure effectiveness.\n- Monitoring configuration for compatibility health.\n- artifacts/section_12/bd-s4cu/verification_evidence.json\n\nTesting and Logging Requirements:\n- Unit tests: risk threshold detection, alert triggering logic.\n- Integration tests: lockstep oracle -> divergence detection -> receipt generation -> alert pipeline.\n- E2E tests: simulated compatibility degradation triggering alert and risk escalation.\n- Structured logs: RISK_COMPATIBILITY_CHECKED, DIVERGENCE_DETECTED, RISK_THRESHOLD_BREACHED, COUNTERMEASURE_ACTIVE with risk level and trace IDs.","acceptance_criteria":"RISK: Compatibility illusion — passing compatibility tests while actual runtime behavior diverges, creating false confidence.\nIMPACT: Production failures in migrated workloads that passed all pre-migration checks; erosion of trust in compatibility claims.\nCOUNTERMEASURES:\n  (a) Lockstep oracle: run franken_node and reference Node.js side-by-side on identical inputs, compare outputs byte-for-byte.\n  (b) Divergence receipts: every oracle comparison produces a signed receipt recording inputs, outputs, and match/mismatch verdict.\n  (c) Receipt persistence: all divergence receipts are stored in artifacts/ with retention policy.\nVERIFICATION:\n  1. Lockstep oracle exists and runs on >= 500 representative API call sequences from the compatibility corpus.\n  2. Divergence receipts are generated for every comparison (both match and mismatch).\n  3. Any mismatch receipt triggers a blocking CI failure — no silent divergences.\n  4. Receipt format is machine-parseable (JSON) with fields: input_hash, expected_output_hash, actual_output_hash, verdict, timestamp.\nTEST SCENARIOS:\n  - Scenario A: Inject a subtle behavioral divergence (e.g., different error message format); verify oracle detects it and blocks merge.\n  - Scenario B: Run 100 matching comparisons; verify all produce match receipts and CI passes.\n  - Scenario C: Tamper with a receipt; verify integrity check catches the tampering.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:39:33.242758563Z","created_by":"ubuntu","updated_at":"2026-02-20T16:08:06.512489468Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-12","self-contained","test-obligations"]}
{"id":"bd-s6y","title":"[10.7] Adopt canonical trust protocol vectors from `10.13` + `10.14` and enforce release/publication gates on those vectors.","description":"## [10.7] Adopt Canonical Trust Protocol Vectors and Enforce Release Gates\n\n### Why This Exists\n\nSections 9E.10 and 9I.20 require \"CDDL-like schema + golden vectors for cross-impl parity.\" The 10.13 and 10.14 tracks have produced golden test vectors — `fnode_trust_vectors_v1.json`, interop fixtures, idempotency vectors, epoch key vectors, seed derivation vectors, and MMR proof vectors — that define the canonical behavior of franken_node's trust protocol. These vectors must be promoted from development-time test assets to mandatory release gates. No release may ship if any canonical vector fails. This ensures cross-implementation parity and prevents protocol drift between releases.\n\n### What It Must Do\n\n**Vector adoption**: Collect all golden vectors from 10.13 (`vectors/fnode_trust_vectors_v1.json`, `fixtures/interop/`) and 10.14 (idempotency vectors, epoch key vectors, seed derivation vectors, MMR proof vectors) into a canonical vector registry. The registry is a manifest file (`vectors/canonical_manifest.toml`) that lists every vector suite, its source, its schema version, and its required-pass status.\n\n**Release gate enforcement**: A CI gate runs all canonical vector suites before any release. The gate loads the manifest, executes each suite, and blocks the release if any required suite fails. The gate produces structured JSON output listing each suite's name, vector count, pass count, fail count, and overall status.\n\n**Vector change control**: Modifications to any canonical vector file require an explicit changelog entry in `vectors/CHANGELOG.md` documenting what changed, why, and which implementations are affected. A pre-commit hook or CI check enforces that vector file changes are accompanied by changelog entries.\n\n**Schema validation**: Each vector file must conform to its declared schema (JSON Schema or CDDL-derived). The gate validates schema conformance before executing vectors, so malformed vectors are caught early with clear error messages rather than producing confusing runtime failures.\n\n**Cross-implementation parity check**: Where multiple implementations exist (Node shim, Bun shim, native franken_node), the gate runs vectors against all available implementations and reports any divergence. Divergence on a required vector is a release blocker.\n\n### Acceptance Criteria\n\n1. A canonical vector manifest (`vectors/canonical_manifest.toml`) lists all vector suites with source path, schema version, and required-pass status.\n2. CI release gate executes all suites listed in the manifest and blocks release on any required-suite failure.\n3. Gate output is structured JSON: suite name, vector count, pass/fail counts, overall status, execution time.\n4. Modifications to canonical vector files require a corresponding entry in `vectors/CHANGELOG.md`; CI enforces this.\n5. Schema validation runs before vector execution; malformed vectors produce clear error messages identifying the schema violation.\n6. Cross-implementation parity is checked where multiple implementations are available; divergence on required vectors blocks release.\n7. Verification script `scripts/check_canonical_vectors.py` with `--json` flag validates the full vector pipeline.\n8. Unit tests in `tests/test_check_canonical_vectors.py` cover manifest parsing, schema validation, changelog enforcement, parity checking, and gate pass/fail logic.\n\n### Key Dependencies\n\n- Golden vectors from 10.13 (fnode_trust_vectors_v1.json, interop fixtures).\n- Vectors from 10.14 (idempotency, epoch key, seed derivation, MMR proof).\n- Lockstep harness from 10.2 (for cross-implementation execution).\n- JSON Schema or CDDL schema definitions for each vector format.\n\n### Testing & Logging Requirements\n\n- Gate pass test: run with all vectors passing, assert gate passes and structured output is correct.\n- Gate fail test: inject a failing vector, assert gate blocks with clear identification of the failure.\n- Changelog enforcement test: modify a vector file without changelog, assert CI rejects.\n- Schema validation test: inject a malformed vector, assert schema error is reported before execution.\n- Structured JSON logs for each suite execution: suite name, vectors tested, pass/fail, duration.\n\n### Expected Artifacts\n\n- `vectors/canonical_manifest.toml` — vector registry.\n- `vectors/CHANGELOG.md` — vector change log.\n- `scripts/check_canonical_vectors.py` — verification script.\n- `tests/test_check_canonical_vectors.py` — unit tests.\n- `artifacts/section_10_7/bd-s6y/verification_evidence.json` — gate results.\n- `artifacts/section_10_7/bd-s6y/verification_summary.md` — human-readable summary.\n\n### Additional E2E Requirement\n\n- E2E test scripts must execute full end-to-end workflows from clean fixtures, emit deterministic machine-readable pass/fail evidence, and capture stepwise structured logs for root-cause triage.","acceptance_criteria":"1. All canonical trust protocol vectors from 10.13 (golden vectors, interop suites, fuzz corpus) and 10.14 are imported into the release verification pipeline.\n2. Release gate: CI blocks release if any canonical vector fails verification — no override without explicit exception.\n3. Publication gate: documentation or artifact publication is blocked if associated trust vectors have not passed in the current build.\n4. Vector adoption is traceable: each imported vector references its source bead ID and version.\n5. Gate produces a structured JSON report listing each vector set, its pass/fail status, and the timestamp of last verification.\n6. New vectors added in 10.13/10.14 are automatically picked up by the gate without manual integration (convention-based discovery).\n7. Per Section 3.2 capability #7 (compatibility lockstep oracle): vectors include cross-runtime comparison results where applicable.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:36:47.344215304Z","created_by":"ubuntu","updated_at":"2026-02-20T17:08:35.643429895Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-7","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-s6y","depends_on_id":"bd-3i6c","type":"blocks","created_at":"2026-02-20T15:00:23.289283159Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-s6y","depends_on_id":"bd-3n2u","type":"blocks","created_at":"2026-02-20T15:00:23.111978942Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-sddz","title":"[10.14] Define immutable correctness envelope that policy controllers are forbidden to modify.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.14 — FrankenSQLite Deep-Mined Expansion Execution Track (9J)\n\nWhy This Exists:\nFrankenSQLite deep-mined control integrity track: evidence ledgers, proofs, epochs, remote effects, markers/MMR, and deterministic fault labs.\n\nTask Objective:\nDefine immutable correctness envelope that policy controllers are forbidden to modify.\n\nAcceptance Criteria:\n- Envelope enumerates non-tunable invariants; controller API rejects writes outside allowed policy set; governance doc maps invariant ownership.\n\nExpected Artifacts:\n- `docs/specs/correctness_envelope.md`, `tests/security/controller_envelope_enforcement.rs`, `artifacts/10.14/correctness_envelope_manifest.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_14/bd-sddz/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_14/bd-sddz/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.14] Define immutable correctness envelope that policy controllers are forbidden to modify.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.14] Define immutable correctness envelope that policy controllers are forbidden to modify.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.14] Define immutable correctness envelope that policy controllers are forbidden to modify.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.14] Define immutable correctness envelope that policy controllers are forbidden to modify.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.14] Define immutable correctness envelope that policy controllers are forbidden to modify.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"Envelope enumerates non-tunable invariants; controller API rejects writes outside allowed policy set; governance doc maps invariant ownership.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:36:55.549959217Z","created_by":"ubuntu","updated_at":"2026-02-20T16:23:28.029217606Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-14","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-sddz","depends_on_id":"bd-nupr","type":"blocks","created_at":"2026-02-20T16:23:28.029162534Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-sh3","title":"[10.5] Implement policy change approval workflows with cryptographic audit trail.","description":"# [10.5] Policy Change Approval Workflows with Cryptographic Audit Trail\n\n## Why This Exists\n\nPolicy changes in franken_node are among the highest-impact mutations an operator can make. A misconfigured policy can silently widen the attack surface, disable safety invariants, or violate the correctness envelope defined in Section 10.14. The plan (Section 6 — Security Doctrine) mandates that every policy mutation must be cryptographically signed, multi-party approved, and recorded in a tamper-evident append-only ledger. Section 10.5 (Security + Policy Product Surfaces) designates this bead as the governance chokepoint: no policy change can take effect without traversing this workflow.\n\nThis bead also enforces the key-role separation principle from Section 10.10 (Governance + Role Separation), ensuring that the entity proposing a change is never the sole approver. Changes that would alter the correctness envelope (10.14) require an elevated governance waiver path with additional cryptographic attestations. Cross-references: Section 3.2 capabilities #4 and #8 (deterministic replay and operator copilot both depend on stable, auditable policy state).\n\n## What It Must Do\n\n1. **Multi-Party Approval Protocol**: Implement a proposal-review-approve pipeline where policy changes require M-of-N cryptographic signatures from distinct role holders. The minimum quorum, eligible roles, and escalation paths must be configurable per policy domain.\n\n2. **Cryptographic Audit Trail**: Every policy change event (proposal, review comment, approval, rejection, activation, rollback) must be recorded in an append-only, hash-chained ledger. Each entry includes: (a) the change payload, (b) the proposer identity and signature, (c) each approver identity and signature, (d) a SHA-256 hash linking to the previous entry, (e) a monotonic sequence number, and (f) a wall-clock timestamp plus a logical clock value.\n\n3. **Correctness Envelope Protection**: Changes that modify parameters guarded by the correctness envelope (10.14) must be flagged automatically and routed to a governance waiver path requiring elevated approval thresholds and explicit attestation that the change has been impact-assessed.\n\n4. **Rollback Mechanism**: Every activated policy change must carry a deterministic rollback command that can be executed atomically. Rollback itself is a policy change event and must traverse the same audit trail (though it may use a fast-path quorum for emergency scenarios).\n\n5. **Key-Role Separation Enforcement**: The system must reject any approval workflow where the proposer is the sole approver. Role identities are bound to cryptographic keys managed by the key-role separation module (10.10).\n\n6. **Change Evidence Package**: Upon activation, the system must emit a structured evidence package containing the full change diff, all signatures, the approval chain, and a merkle proof anchoring the change to the audit ledger root.\n\n## Acceptance Criteria\n\n1. A policy change proposed by role A cannot be activated unless M-of-N distinct role holders (where M >= 2 and A alone is insufficient) have cryptographically signed approval.\n2. The audit ledger is append-only: no entry can be modified or deleted after creation. Tampering with any entry invalidates the hash chain, and the system detects this on startup and on every read.\n3. Hash chain integrity is verified on every ledger append and on system startup; any break halts policy processing and emits a critical alert.\n4. Changes touching correctness-envelope parameters are automatically classified as elevated and require the governance waiver path (higher quorum, explicit attestation fields).\n5. Every activated policy change has a corresponding deterministic rollback command stored in the ledger, and executing that rollback produces a new audited ledger entry.\n6. The system rejects any approval flow where the proposer is the only approver, returning a structured error with the violated constraint.\n7. The change evidence package is machine-readable JSON, contains all required fields (change diff, signatures, approval chain, merkle proof), and is written to the artifact path before activation completes.\n8. Latency for the approval verification step (signature check + quorum validation) is under 50ms for up to 10 approvers.\n9. All structured log events use stable event codes (POLICY_CHANGE_PROPOSED, POLICY_CHANGE_APPROVED, POLICY_CHANGE_ACTIVATED, POLICY_CHANGE_ROLLED_BACK, POLICY_CHANGE_REJECTED, AUDIT_CHAIN_BROKEN) with trace correlation IDs.\n10. The workflow is fully exercisable in a hermetic test environment with no external key management dependencies (test keys are injected via fixture).\n\n## Key Dependencies\n\n- **Depends on bd-3nr** (degraded-mode policy behavior): approval workflows must handle the case where the system is in degraded mode and some approvers are unreachable.\n- **Depends on Section 10.10** (key-role separation): cryptographic identities and role bindings come from 10.10.\n- **Depends on Section 10.14** (correctness envelope): the classification of which parameters are envelope-guarded comes from 10.14 metadata.\n- **Depended on by bd-1koz** (section-wide verification gate): this bead must pass before the 10.5 gate closes.\n- **Depended on by bd-20a** (section rollup): included in the 10.5 section completion.\n\n## Testing and Logging Requirements\n\n- **Unit tests**: Signature verification, hash chain append and integrity check, quorum logic (M-of-N with various M/N), role-separation rejection, correctness-envelope classification.\n- **Integration tests**: Full proposal-approve-activate-rollback cycle with multiple simulated role holders. Verify ledger state after each step. Verify evidence package completeness.\n- **E2E tests**: Simulate a policy change that touches the correctness envelope, verify elevated path is enforced, then simulate emergency rollback with fast-path quorum.\n- **Adversarial tests**: Attempt to tamper with a ledger entry and verify detection. Attempt to approve as the sole proposer. Attempt to activate without quorum. Attempt to bypass correctness-envelope classification.\n- **Logging**: All events must include trace_id, span_id, event_code, actor_identity, and policy_domain fields. Log volume must be bounded (no per-byte logging of change payloads in hot path).\n\n## Expected Artifacts\n\n- `docs/specs/section_10_5/bd-sh3_contract.md` — Design spec with approval protocol state machine, ledger schema, and quorum algebra.\n- `artifacts/section_10_5/bd-sh3/verification_evidence.json` — Machine-readable pass/fail evidence for CI gating.\n- `artifacts/section_10_5/bd-sh3/verification_summary.md` — Human-readable summary linking design intent to test outcomes.\n- Rust module(s) in `crates/franken-node/src/` implementing the approval workflow engine and audit ledger.\n- Python verification script `scripts/check_policy_change_workflow.py` with `--json` flag and `self_test()`.\n- Unit tests in `tests/test_check_policy_change_workflow.py`.","acceptance_criteria":"1. Define a PolicyChangeProposal struct containing: proposal_id (UUID v7), proposed_by (string, operator identity), proposed_at (RFC-3339), policy_diff (structured diff showing old and new values for each changed field), justification (string, minimum 20 characters), risk_assessment (enum: Low | Medium | High | Critical), and required_approvers (Vec<String>, minimum 1).\n2. Implement a multi-step approval workflow: Proposed -> UnderReview -> Approved | Rejected -> Applied | Rolled-back. Each state transition must produce a PolicyChangeAuditEntry containing: transition_from, transition_to, actor, timestamp, and signature (Ed25519 over canonical JSON of the entry).\n3. Approval requires signatures from all identities listed in required_approvers; the system must verify each signature before advancing to Approved state. Partial approval (some but not all signatures) keeps the proposal in UnderReview.\n4. Implement a cryptographic audit trail: an append-only log of PolicyChangeAuditEntry records where each entry includes a prev_hash field (SHA-256 of the preceding entry's canonical JSON), forming a hash chain. The first entry uses a well-known genesis hash (SHA-256 of the empty string).\n5. Provide verify_audit_chain(entries: &[PolicyChangeAuditEntry]) -> Result<bool> that walks the chain and confirms every prev_hash and signature is valid; any break returns Err with the index of the first invalid entry.\n6. Rollback: an applied policy change can be rolled back by creating a new proposal whose policy_diff is the inverse of the original; it must reference the original proposal_id via a rollback_of field and follow the same approval workflow.\n7. Verification: scripts/check_policy_approval.py --json exercises the full lifecycle (propose, review, approve with valid sigs, apply, rollback), verifies the audit chain, and tests rejection of a tampered entry; unit tests in tests/test_check_policy_approval.py cover partial approval, invalid signature rejection, chain verification with N=50 entries, and rollback linkage; evidence in artifacts/section_10_5/bd-sh3/.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:36:46.622383372Z","created_by":"ubuntu","updated_at":"2026-02-20T16:59:00.897860655Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-5","self-contained","test-obligations"]}
{"id":"bd-sxt5","title":"[15] Adoption target: deterministic migration validation on representative Node/Bun project cohorts","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 15 — Ecosystem Capture Strategy\n\nWhy This Exists:\nSection 15 defines 5 execution pillars and 3 adoption targets. This bead covers the adoption target requiring deterministic migration validation on representative Node/Bun project cohorts, ensuring that the migration autopilot and lockstep oracle produce reliable results on real-world projects.\n\nTask Objective:\nBuild and maintain a representative project cohort for deterministic migration validation. The cohort must include diverse Node and Bun project archetypes. Migration audit, rewrite, and lockstep validation must produce deterministic, reproducible results on every cohort member.\n\nAcceptance Criteria:\n- Define cohort of minimum 10 representative Node/Bun projects (covering Express, Fastify, Next.js, Remix, CLI tools, Bun workers, extension hosts, monorepos, etc.).\n- Migration audit produces identical findings on repeated runs for each cohort member.\n- Migration rewrite produces deterministic transforms with rollback artifacts.\n- Lockstep validation passes with documented divergence receipts for known edge cases.\n- Cohort is version-pinned and CI-reproducible.\n- Results are published as machine-readable validation evidence.\n\nExpected Artifacts:\n- tests/e2e/migration_cohort_validation.sh\n- docs/ecosystem/migration_cohort_definition.md\n- artifacts/15/migration_cohort_results.json\n\n- Machine-readable verification artifact at `artifacts/section_15/bd-sxt5/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_15/bd-sxt5/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- E2E test scripts exercising full migration pipeline on each cohort member.\n- Structured logging with stable codes and trace correlation IDs.\n- Deterministic replay fixtures for divergence analysis.\n\nTask-Specific Clarification:\n- For \"[15] Adoption target: deterministic migration validation on representative Node/Bun project cohorts\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[15] Adoption target: deterministic migration validation on representative Node/Bun project cohorts\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[15] Adoption target: deterministic migration validation on representative Node/Bun project cohorts\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[15] Adoption target: deterministic migration validation on representative Node/Bun project cohorts\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[15] Adoption target: deterministic migration validation on representative Node/Bun project cohorts\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"1. Representative project cohort defined: >= 10 real-world Node.js/Bun projects spanning all major archetypes (web server, SSR app, CLI, library, worker, monorepo, native addon project, TypeScript-heavy project, test-heavy project, minimal project).\n2. Each cohort project has: (a) pre-migration baseline (test suite results on original runtime), (b) migration executed using franken_node migration kit, (c) post-migration validation (same test suite on franken_node).\n3. Deterministic validation: post-migration test suite produces identical pass/fail results on repeated runs (flaky rate < 1%).\n4. Migration success criteria per project: >= 95% of original test suite passes on franken_node (or all failures are documented as known incompatibilities).\n5. Cohort-wide success: >= 80% of cohort projects meet the per-project success criteria.\n6. Cohort is refreshed at least once per major release to ensure continued relevance.\n7. Results are published with project descriptions (anonymized if needed) and per-project migration outcomes.\n8. Evidence: migration_cohort_results.json with per-project: archetype, test count, pass rate, known incompatibilities, and overall cohort success rate.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T08:01:45.235566011Z","created_by":"ubuntu","updated_at":"2026-02-20T16:08:49.083357999Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-15","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-sxt5","depends_on_id":"bd-elog","type":"blocks","created_at":"2026-02-20T08:03:58.137496018Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-t89w","title":"[10.20] Implement topological risk metric engine (fan-out, betweenness, articulation points, percolation thresholds, trust bottleneck scores).","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.20 — Dependency Graph Immune System Execution Track (9N)\n\nWhy This Exists:\nDGIS topological immune system track for dependency contagion modeling, choke-point immunization, and graph-aware containment economics.\n\nTask Objective:\nImplement topological risk metric engine (fan-out, betweenness, articulation points, percolation thresholds, trust bottleneck scores).\n\nAcceptance Criteria:\n- Metric computation is deterministic, versioned, and scalable for representative ecosystem graph sizes; output includes explainable feature attribution for each high-risk node.\n\nExpected Artifacts:\n- `src/security/dgis/topology_metrics.rs`, `tests/security/dgis_topology_metrics.rs`, `artifacts/10.20/dgis_topology_risk_snapshot.csv`.\n\n- Machine-readable verification artifact at `artifacts/section_10_20/bd-t89w/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_20/bd-t89w/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.20] Implement topological risk metric engine (fan-out, betweenness, articulation points, percolation thresholds, trust bottleneck scores).\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.20] Implement topological risk metric engine (fan-out, betweenness, articulation points, percolation thresholds, trust bottleneck scores).\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.20] Implement topological risk metric engine (fan-out, betweenness, articulation points, percolation thresholds, trust bottleneck scores).\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.20] Implement topological risk metric engine (fan-out, betweenness, articulation points, percolation thresholds, trust bottleneck scores).\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.20] Implement topological risk metric engine (fan-out, betweenness, articulation points, percolation thresholds, trust bottleneck scores).\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"- Metric computation is deterministic, versioned, and scalable for representative ecosystem graph sizes; output includes explainable feature attribution for each high-risk node.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:37:06.583954630Z","created_by":"ubuntu","updated_at":"2026-02-20T17:04:52.356525812Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-20","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-t89w","depends_on_id":"bd-2bj4","type":"blocks","created_at":"2026-02-20T17:04:52.356468135Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-t8m","title":"[PLAN 15] Ecosystem Capture Execution","description":"Section 15 network-effect epic. Deliver signed registry, migration kits, governance integrations, reputation APIs, and lighthouse partner programs with measurable adoption outcomes.\n\n## Success Criteria\n- All child beads for this section are completed with acceptance artifacts attached and no unresolved blockers.\n- Section-level verification gate for comprehensive unit tests, integration/e2e workflows, and detailed structured logging evidence is green.\n- Scope-to-plan traceability is explicit, with no feature loss versus the canonical plan section.\n\n## Optimization Notes\n- User-Outcome Lens: \"[PLAN 15] Ecosystem Capture Execution\" must improve real operator and end-user reliability, explainability, and time-to-safe-outcome under both normal and degraded conditions.\n- Cross-Section Coordination: this epic's child beads must explicitly encode integration expectations with neighboring sections to prevent local optimizations that break global behavior.\n- Verification Non-Negotiable: completion requires deterministic unit coverage, integration/E2E replay evidence, and structured logs sufficient for independent third-party reproduction and triage.\n- Scope Protection: any proposed simplification must prove feature parity against plan intent and document tradeoffs explicitly; silent scope reduction is forbidden.","status":"open","priority":2,"issue_type":"epic","created_at":"2026-02-20T07:36:42.330234889Z","created_by":"ubuntu","updated_at":"2026-02-20T08:12:48.927337939Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-15"],"dependencies":[{"issue_id":"bd-t8m","depends_on_id":"bd-1961","type":"blocks","created_at":"2026-02-20T07:39:36.482360876Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-t8m","depends_on_id":"bd-1wz","type":"blocks","created_at":"2026-02-20T07:38:36.397618539Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-t8m","depends_on_id":"bd-1xg","type":"blocks","created_at":"2026-02-20T07:38:36.248042346Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-t8m","depends_on_id":"bd-209w","type":"blocks","created_at":"2026-02-20T07:39:36.200965196Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-t8m","depends_on_id":"bd-20a","type":"blocks","created_at":"2026-02-20T07:38:36.292697204Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-t8m","depends_on_id":"bd-26k","type":"blocks","created_at":"2026-02-20T07:38:36.350043742Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-t8m","depends_on_id":"bd-2nre","type":"blocks","created_at":"2026-02-20T07:48:31.095630640Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-t8m","depends_on_id":"bd-31tg","type":"blocks","created_at":"2026-02-20T07:39:36.567295722Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-t8m","depends_on_id":"bd-37i","type":"blocks","created_at":"2026-02-20T07:38:36.530530822Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-t8m","depends_on_id":"bd-39a","type":"blocks","created_at":"2026-02-20T07:38:36.442302170Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-t8m","depends_on_id":"bd-3mj9","type":"blocks","created_at":"2026-02-20T07:39:36.396474277Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-t8m","depends_on_id":"bd-cv49","type":"blocks","created_at":"2026-02-20T07:39:36.738279711Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-t8m","depends_on_id":"bd-elog","type":"blocks","created_at":"2026-02-20T07:39:36.651492344Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-t8m","depends_on_id":"bd-sxt5","type":"blocks","created_at":"2026-02-20T08:02:26.401305399Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-t8m","depends_on_id":"bd-wpck","type":"blocks","created_at":"2026-02-20T07:39:36.286472098Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-t8m","depends_on_id":"bd-ybe","type":"blocks","created_at":"2026-02-20T07:38:36.486762626Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-tg2","title":"[10.8] Implement fleet control API for quarantine/revocation operations.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.8 — Operational Readiness (Item 1 of 6)\n\nWhy This Exists:\nThe fleet control API is the programmatic interface that turns engine-level quarantine/revocation primitives into operator-accessible operations. It is the backbone of the fleet quarantine UX (10.0 Initiative #6) and supports all operational incident response workflows.\n\nTask Objective:\nImplement the fleet control API that enables quarantine and revocation operations with zone/tenant scoping, convergence tracking, and rollback capabilities.\n\nDetailed Acceptance Criteria:\n1. API endpoints: quarantine(extension_id, scope), revoke(extension_id, scope), release(incident_id), status(zone), reconcile().\n2. Scope control: operations scoped to zones/tenants with blast-radius metadata.\n3. Convergence tracking: API reports propagation progress and estimated completion time.\n4. Rollback: release command deterministically rolls back quarantine state with verification.\n5. All operations produce signed decision receipts (10.5).\n6. Canonical structured observability + stable error taxonomy contracts adopted from 10.13.\n7. Deterministic safe-mode startup: fleet control API starts in read-only mode and requires explicit activation.\n8. Integration with incident bundle system (10.5) for post-incident evidence collection.\n\nKey Dependencies:\n- Depends on 10.N (Normalization) for canonical ownership rules.\n- Consumed by fleet quarantine UX (10.0 Initiative #6).\n- Consumed by operator copilot (10.0 Initiative #8).\n- Consumes 10.13 error taxonomy and observability contracts.\n\nExpected Artifacts:\n- src/fleet/ module with fleet_api.rs, quarantine.rs, revocation.rs, reconcile.rs.\n- API documentation and OpenAPI/contract spec.\n- docs/specs/section_10_8/bd-tg2_contract.md\n- artifacts/section_10_8/bd-tg2/verification_evidence.json\n\nTesting and Logging Requirements:\n- Unit tests: quarantine/revocation/release logic, scope validation, convergence computation.\n- Integration tests: multi-zone quarantine propagation and convergence.\n- E2E tests: franken-node fleet status/release/reconcile CLI commands.\n- Fault injection: API behavior during partial fleet connectivity.\n- Structured logs: FLEET_QUARANTINE_INITIATED, FLEET_REVOCATION_ISSUED, CONVERGENCE_PROGRESS, FLEET_RELEASED with zone/scope metadata and trace IDs.","acceptance_criteria":"1. Fleet control API exposes endpoints for: quarantine-node, revoke-node, list-quarantined, promote-from-quarantine, and bulk-quarantine operations.\n2. Quarantine propagation meets bounded convergence guarantee per Section 9A.6: all fleet members acknowledge quarantine within a defined time ceiling (documented in spec).\n3. Anti-entropy reconciliation: nodes that were offline during a quarantine event converge to correct state upon reconnection without operator intervention.\n4. API supports idempotent operations: repeated quarantine/revocation of the same node produces identical state.\n5. Per Section 3.2 capability #5: quarantine propagation is verified with a multi-node integration test simulating network partitions and delayed joins.\n6. API returns structured JSON responses with operation ID, affected nodes, convergence status, and timestamp.\n7. Authorization: quarantine/revocation operations require explicit operator credentials — no unauthenticated fleet mutations are possible.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:36:47.750223438Z","created_by":"ubuntu","updated_at":"2026-02-20T15:18:01.860580048Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-8","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-tg2","depends_on_id":"bd-1hf","type":"blocks","created_at":"2026-02-20T07:46:37.454276130Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-tg2","depends_on_id":"bd-1ta","type":"blocks","created_at":"2026-02-20T07:46:37.498530696Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-tg2","depends_on_id":"bd-20a","type":"blocks","created_at":"2026-02-20T07:46:37.545926983Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-tg2","depends_on_id":"bd-cda","type":"blocks","created_at":"2026-02-20T07:46:37.589692968Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-tyr2","title":"[10.15] Integrate canonical evidence replay validator (from `10.14`) into control-plane decision gates.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.15 — Asupersync-First Integration Execution Track (8.4-8.6)\n\nWhy This Exists:\nAsupersync-first control-plane migration track enforcing Cx-first, region ownership, cancellation correctness, and protocol-grade verification gates.\n\nTask Objective:\nIntegrate canonical evidence replay validator (from `10.14`) into control-plane decision gates.\n\nAcceptance Criteria:\n- Given evidence + inputs, canonical replay validator reproduces chosen decision or emits minimal deterministic diff; control-plane gate consumes verdict.\n\nExpected Artifacts:\n- `tests/conformance/control_evidence_replay.rs`, `docs/integration/control_evidence_replay_adoption.md`, `artifacts/10.15/control_evidence_replay_report.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_15/bd-tyr2/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_15/bd-tyr2/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.15] Integrate canonical evidence replay validator (from `10.14`) into control-plane decision gates.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.15] Integrate canonical evidence replay validator (from `10.14`) into control-plane decision gates.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.15] Integrate canonical evidence replay validator (from `10.14`) into control-plane decision gates.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.15] Integrate canonical evidence replay validator (from `10.14`) into control-plane decision gates.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.15] Integrate canonical evidence replay validator (from `10.14`) into control-plane decision gates.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"- Given evidence + inputs, canonical replay validator reproduces chosen decision or emits minimal deterministic diff; control-plane gate consumes verdict.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:37:00.628849179Z","created_by":"ubuntu","updated_at":"2026-02-20T17:00:44.966661250Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-15","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-tyr2","depends_on_id":"bd-2ona","type":"blocks","created_at":"2026-02-20T14:59:37.389157207Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-ud5h","title":"[6] Security and Trust Product Doctrine — threat model, trust surfaces, safety targets","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md — Section 6\n\n## Why This Exists\nThis captures the security and trust product doctrine that governs ALL security-related implementation in franken_node. It defines the threat model, trust-native product surfaces, and safety guarantee targets that every security bead must satisfy.\n\n## Problem Statement (6.1)\nDevelopers need Node ecosystem speed, but untrusted extension supply chains remain a catastrophic risk surface.\n\n## Product Goal (6.2)\nMake high-trust runtime operation the default workflow without forcing teams to abandon JS/TS ecosystem velocity.\n\n## Threat Model (6.3)\nAdversary classes that all security implementations must address:\n1. Malicious extension updates and maintainer compromises\n2. Credential exfiltration and lateral movement attempts\n3. Policy evasion via compatibility edge cases\n4. Delayed payload activation and long-tail persistence\n5. Operational confusion attacks exploiting non-deterministic incident handling\n\n## Trust-Native Product Surfaces (6.4)\nThese surfaces must be implemented across 10.4, 10.5, 10.8, 10.13, 10.17:\n- Extension trust cards and provenance scoring\n- Policy-visible compatibility mode gates\n- Revocation-first execution prechecks\n- Signed incident receipts and deterministic replay export\n- Autonomous containment recommendations with explicit rationale\n\n## Safety Guarantees Target (6.5)\nQuantitative safety properties that must be measurable and verified:\n- Bounded false-negative rate under adversarial extension corpora\n- Bounded false-positive rate for benign migration workloads\n- Deterministic replay for high-severity security events\n- Auditable degraded-mode semantics when trust state is stale\n\n## Acceptance Criteria\n- All 5 adversary classes have dedicated test scenarios in security test suites\n- All 5 trust-native surfaces are implemented and operational\n- All 4 safety guarantee targets have measurable metrics and CI gates\n- Threat model is reviewed and updated with each major release\n\n## Testing Requirements\n- Adversarial test corpus covering all 5 adversary classes\n- Safety guarantee measurement scripts with deterministic output\n- Structured logging for all trust-plane decisions with trace correlation\n\n\n## Additional Verification Requirements\n- Unit tests for threat-model coverage validators and trust-surface completeness checks.\n- E2E security-drill scripts covering revocation, containment, replay, and degraded-mode behaviors from clean environments.\n- Structured logs with stable event codes for each trust-plane decision and security-drill outcome, including trace correlation IDs.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T16:14:54.800354991Z","created_by":"ubuntu","updated_at":"2026-02-20T17:08:28.458908963Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["doctrine","plan","section-6","security"]}
{"id":"bd-ufk5","title":"[10.18] Add performance budget gates for VEF overhead in p95/p99 control and extension-host hot paths.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.18 — Verifiable Execution Fabric Execution Track (9L)\n\nWhy This Exists:\nVEF track for proof-carrying runtime compliance: receipt chains, commitment checkpoints, proof generation/verification, and claim gating.\n\nTask Objective:\nAdd performance budget gates for VEF overhead in p95/p99 control and extension-host hot paths.\n\nAcceptance Criteria:\n- VEF overhead remains within agreed budgets by mode; regressions fail CI with reproducible profiling evidence.\n\nExpected Artifacts:\n- `tests/perf/vef_overhead_budget_gate.rs`, `benchmarks/vef_overhead/*`, `artifacts/10.18/vef_overhead_report.csv`.\n\n- Machine-readable verification artifact at `artifacts/section_10_18/bd-ufk5/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_18/bd-ufk5/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.18] Add performance budget gates for VEF overhead in p95/p99 control and extension-host hot paths.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.18] Add performance budget gates for VEF overhead in p95/p99 control and extension-host hot paths.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.18] Add performance budget gates for VEF overhead in p95/p99 control and extension-host hot paths.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.18] Add performance budget gates for VEF overhead in p95/p99 control and extension-host hot paths.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.18] Add performance budget gates for VEF overhead in p95/p99 control and extension-host hot paths.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"- VEF overhead remains within agreed budgets by mode; regressions fail CI with reproducible profiling evidence.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:37:05.118324759Z","created_by":"ubuntu","updated_at":"2026-02-20T17:00:46.685707914Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-18","self-contained","test-obligations"]}
{"id":"bd-uh9","title":"Implement franken-node CLI scaffold aligned with README command families","description":"Introduce command surface skeleton (init/migrate/verify/trust/incident/fleet/doctor) to reduce README-implementation mismatch.","status":"closed","priority":1,"issue_type":"feature","created_at":"2026-02-20T07:26:05.511034353Z","created_by":"ubuntu","updated_at":"2026-02-20T07:26:56.719179182Z","closed_at":"2026-02-20T07:26:56.719155639Z","close_reason":"Duplicate scope; superseded by active CLI beads bd-2lb/bd-3vk","source_repo":".","compaction_level":0,"original_size":0}
{"id":"bd-ukh7","title":"[10.19] Implement privacy envelope layer: secure aggregation + differential-privacy budget enforcement.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.19 — Adversarial Trust Commons Execution Track (9M)\n\nWhy This Exists:\nATC federated intelligence track for privacy-preserving cross-deployment threat learning, robust aggregation, and verifier-backed trust metrics.\n\nTask Objective:\nImplement privacy envelope layer: secure aggregation + differential-privacy budget enforcement.\n\nAcceptance Criteria:\n- Participant contributions are aggregated without exposing per-participant raw values; privacy budget accounting is deterministic and policy-gated.\n\nExpected Artifacts:\n- `docs/specs/atc_privacy_envelope.md`, `src/federation/atc_secure_aggregation.rs`, `artifacts/10.19/atc_privacy_budget_report.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_19/bd-ukh7/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_19/bd-ukh7/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.19] Implement privacy envelope layer: secure aggregation + differential-privacy budget enforcement.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.19] Implement privacy envelope layer: secure aggregation + differential-privacy budget enforcement.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.19] Implement privacy envelope layer: secure aggregation + differential-privacy budget enforcement.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.19] Implement privacy envelope layer: secure aggregation + differential-privacy budget enforcement.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.19] Implement privacy envelope layer: secure aggregation + differential-privacy budget enforcement.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"- Participant contributions are aggregated without exposing per-participant raw values; privacy budget accounting is deterministic and policy-gated.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:37:05.585135739Z","created_by":"ubuntu","updated_at":"2026-02-20T17:00:54.935274602Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-19","self-contained","test-obligations"]}
{"id":"bd-unkm","title":"[16] Section-wide verification gate: comprehensive unit+e2e+logging","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 16\n\nTask Objective:\nCreate a hard section completion gate that verifies all implemented behavior in this section with comprehensive unit tests, integration/e2e scripts, and detailed structured logging evidence.\n\nAcceptance Criteria:\n- Section test matrix covers happy-path, edge-case, and adversarial/error-path scenarios for all section deliverables.\n- E2E scripts execute representative end-user workflows and policy/control-plane transitions for this section.\n- Structured logs include stable event/error codes, trace correlation IDs, and enough context for deterministic replay triage.\n- Verification report is deterministic and machine-readable for CI/release gating.\n\nExpected Artifacts:\n- Section-level test matrix and coverage summary artifact.\n- E2E script suite with pass/fail outputs and reproducible fixtures/seeds.\n- Structured log validation report and traceability bundle.\n- Gate verdict artifact consumable by release automation.\n\n- Machine-readable verification artifact at `artifacts/section_16/bd-unkm/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_16/bd-unkm/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests must validate contracts, invariants, and failure semantics.\n- E2E tests must validate cross-component behavior from user/operator entrypoints to persisted effects.\n- Logging must support root-cause analysis without hidden context requirements.\n\nTask-Specific Clarification:\n- For \"[16] Section-wide verification gate: comprehensive unit+e2e+logging\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[16] Section-wide verification gate: comprehensive unit+e2e+logging\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[16] Section-wide verification gate: comprehensive unit+e2e+logging\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[16] Section-wide verification gate: comprehensive unit+e2e+logging\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[16] Section-wide verification gate: comprehensive unit+e2e+logging\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"1. Section 16 verification gate runs all scientific-contribution check scripts and confirms 100% pass rate.\n2. Gate validates: (a) open specs are published and versioned, (b) datasets are published with DOIs, (c) methodology documents are peer-reviewed, (d) external evaluations are completed, (e) transparent reports are published, (f) output contracts (reports, replications, tools) are met.\n3. Publication completeness: >= 3 reproducible reports, >= 2 external replications, >= 2 red-team engagements, >= 1 dataset with DOI.\n4. Gate produces section_16_verification_summary.md with per-contribution status and publication checklist.\n5. Any contribution below target is flagged with gap analysis and remediation timeline.\n6. The gate itself has a unit test verifying correct aggregation of sub-check results.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:48:31.265005180Z","created_by":"ubuntu","updated_at":"2026-02-20T15:29:36.513392656Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-16","test-obligations","verification-gate"],"dependencies":[{"issue_id":"bd-unkm","depends_on_id":"bd-10ee","type":"blocks","created_at":"2026-02-20T07:48:31.511640251Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-unkm","depends_on_id":"bd-1dpd","type":"blocks","created_at":"2026-02-20T08:24:23.472704262Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-unkm","depends_on_id":"bd-1sgr","type":"blocks","created_at":"2026-02-20T07:48:31.462746243Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-unkm","depends_on_id":"bd-2ad0","type":"blocks","created_at":"2026-02-20T07:48:31.660003045Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-unkm","depends_on_id":"bd-2twu","type":"blocks","created_at":"2026-02-20T08:20:48.805444370Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-unkm","depends_on_id":"bd-33u2","type":"blocks","created_at":"2026-02-20T07:48:31.363597834Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-unkm","depends_on_id":"bd-3id1","type":"blocks","created_at":"2026-02-20T07:48:31.560227107Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-unkm","depends_on_id":"bd-e5cz","type":"blocks","created_at":"2026-02-20T07:48:31.413078314Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-unkm","depends_on_id":"bd-f955","type":"blocks","created_at":"2026-02-20T07:48:31.708143710Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-unkm","depends_on_id":"bd-nbh7","type":"blocks","created_at":"2026-02-20T07:48:31.611242326Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-uo4","title":"[10.0] Deliver dual-layer lockstep oracle program (L1 product + L2 engine boundary + release-policy linkage).","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.0 — Top 10 Initiative Tracking (Initiative #4)\nCross-references: 9A.4, 9B.4, 9C.4, 9D.4\n\nWhy This Exists:\nThe dual-layer lockstep oracle is the #4 strategic initiative. It provides externally verifiable compatibility and runtime-integrity guarantees by running Node, Bun, and franken_node in synchronized scenarios at two layers: L1 (product-visible behavior) and L2 (engine-boundary runtime semantics).\n\nTask Objective:\nDeliver the dual-layer lockstep oracle program: L1 runs Node/Bun/franken_node against shared fixture suites for externally visible behavior equivalence; L2 runs franken_engine boundary corpora against reference semantics for runtime-integrity guardrails. Both layers must have release-policy linkage so no release ships without green oracle status.\n\nDetailed Acceptance Criteria:\n1. L1 oracle: runs identical test scenarios across Node, Bun, and franken_node; captures and normalizes outputs; reports divergences with causal trace equivalence reports (9C.4).\n2. L2 oracle: runs franken_engine boundary corpora against reference semantics; validates runtime invariants.\n3. Release-policy linkage: release gates require both L1 and L2 green status.\n4. Oracle delivery close condition: dual-layer oracle is only complete when L1 (10.2) + L2 (10.17) + release policy linkage (10.2) are all green.\n5. Deterministic simulation and delta-debugging reductions to converge quickly on minimal divergence fixtures (9B.4).\n6. Deterministic replay envelopes for each divergence decision (9C.4).\n7. Streaming normalization and parallel fixture evaluation to reduce differential harness cost (9D.4).\n8. Coverage spans all compatibility bands (core/high-value/edge/unsafe) with per-band pass/fail reporting.\n\nKey Dependencies:\n- L1 oracle canonical ownership: 10.2 (Compatibility Core).\n- L2 oracle canonical ownership: 10.17 (Radical Expansion — engine-boundary N-version oracle).\n- Release policy linkage: 10.2 release gates.\n- Depends on 10.1 (Charter) for split-governance contract defining engine vs product boundaries.\n\nExpected Artifacts:\n- src/conformance/lockstep_oracle.rs — L1 oracle harness.\n- Integration with franken_engine for L2 boundary oracle.\n- Release gate configuration linking both layers.\n- docs/specs/section_10_0/bd-uo4_contract.md\n- artifacts/section_10_0/bd-uo4/verification_evidence.json\n\nTesting and Logging Requirements:\n- Unit tests: output normalization, divergence detection, delta-debugging reduction, release gate evaluation.\n- Integration tests: L1 three-runtime comparison on fixture suites, L2 engine boundary corpus validation.\n- E2E tests: franken-node verify lockstep CLI producing divergence reports with per-band breakdown.\n- Performance tests: parallel fixture evaluation throughput and memory profile.\n- Structured logs: LOCKSTEP_L1_RUN, LOCKSTEP_L2_RUN, DIVERGENCE_DETECTED, DELTA_DEBUG_REDUCED, RELEASE_GATE_EVALUATED with trace IDs.","acceptance_criteria":"1. L1 (product boundary) oracle tracks: franken-node release versions, feature flags, API surface changes, deprecation schedule; emits compatibility matrix per release.\n2. L2 (engine boundary) oracle tracks: underlying engine versions (V8/JavaScriptCore/SpiderMonkey), ABI changes, embedding API deltas; emits engine-compatibility matrix.\n3. Release-policy linkage: each release decision references both L1 and L2 oracle outputs; policy gate blocks release if oracle detects unresolved breaking change.\n4. Lockstep verification: oracle detects version skew between L1 product and L2 engine within 1 CI cycle; alerts with specific incompatibility details.\n5. Oracle state is deterministic and reproducible: given same input versions, produces identical compatibility assessment.\n6. Oracle outputs machine-readable compatibility report (JSON): { l1_version, l2_version, compatibility_status, breaking_changes[], policy_disposition, release_recommendation }.\n7. Historical oracle decisions are append-only and auditable; queryable by version range and date range.\n8. Integration with compatibility envelope (bd-1qp): oracle consumes envelope scores as input signal for release readiness.\n9. Dual-layer coverage: oracle covers 100% of catalogued API surface in both L1 and L2 dimensions (Section 3 replay coverage target).\n10. Verification evidence includes: oracle decision log for at least 3 simulated release scenarios, lockstep violation detection test, policy-gate enforcement test.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:36:42.723311996Z","created_by":"ubuntu","updated_at":"2026-02-20T15:35:47.145504542Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-0","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-uo4","depends_on_id":"bd-al8i","type":"blocks","created_at":"2026-02-20T15:01:25.079028856Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-uo4","depends_on_id":"bd-y4g","type":"blocks","created_at":"2026-02-20T07:43:10.224402686Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-v4l0","title":"[10.14] Enforce global remote bulkhead with configurable `remote_max_in_flight` and overload backpressure.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.14 — FrankenSQLite Deep-Mined Expansion Execution Track (9J)\n\nWhy This Exists:\nFrankenSQLite deep-mined control integrity track: evidence ledgers, proofs, epochs, remote effects, markers/MMR, and deterministic fault labs.\n\nTask Objective:\nEnforce global remote bulkhead with configurable `remote_max_in_flight` and overload backpressure.\n\nAcceptance Criteria:\n- In-flight remote operations never exceed cap; overload applies deterministic backpressure policy; p99 foreground latency remains within target under degradation.\n\nExpected Artifacts:\n- `src/remote/remote_bulkhead.rs`, `tests/perf/remote_bulkhead_under_load.rs`, `artifacts/10.14/remote_bulkhead_latency_report.csv`.\n\n- Machine-readable verification artifact at `artifacts/section_10_14/bd-v4l0/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_14/bd-v4l0/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.14] Enforce global remote bulkhead with configurable `remote_max_in_flight` and overload backpressure.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.14] Enforce global remote bulkhead with configurable `remote_max_in_flight` and overload backpressure.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.14] Enforce global remote bulkhead with configurable `remote_max_in_flight` and overload backpressure.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.14] Enforce global remote bulkhead with configurable `remote_max_in_flight` and overload backpressure.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.14] Enforce global remote bulkhead with configurable `remote_max_in_flight` and overload backpressure.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"In-flight remote operations never exceed cap; overload applies deterministic backpressure policy; p99 foreground latency remains within target under degradation.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:36:57.976072853Z","created_by":"ubuntu","updated_at":"2026-02-20T16:24:04.873268213Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-14","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-v4l0","depends_on_id":"bd-1nfu","type":"blocks","created_at":"2026-02-20T16:24:04.873186420Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-v4ps","title":"[12] Risk control: temporal concept drift","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 12\n\nTask Objective:\nImplement recalibration windows, cohort drift audits, and threshold-update regression gates.\n\nExecution Requirements:\n- Make this requirement machine-verifiable and release-gated where applicable.\n- Keep behavior self-documenting so future contributors do not need to re-open the master plan.\n\nTesting & Logging Requirements:\n- Unit tests for rule/contract logic and error conditions.\n- Integration/E2E tests for workflow-level enforcement.\n- Structured logs with stable codes and trace IDs.\n- Reproducible artifacts that prove contract satisfaction.\n\nAcceptance Criteria:\n- Success and failure invariants for [12] Risk control: temporal concept drift are explicitly quantified and machine-verifiable.\n- Determinism requirements for [12] Risk control: temporal concept drift are validated across repeated runs and adversarial perturbations.\n\n\nExpected Artifacts:\n- Machine-readable verification artifact with explicit canonical path under artifacts/section_12/bd-v4ps/verification_evidence.json, suitable for CI/release gating.\n- Human-readable verification summary with explicit canonical path under artifacts/section_12/bd-v4ps/verification_summary.md, linking implementation intent to measured validation outcomes.\n\nTask-Specific Clarification:\n- The implementation must deliver the exact capability named in the title, with no scope dilution and no silent feature omission.\n- Validation artifacts must include capability-specific thresholds, pass/fail criteria, and reproducible evidence bundles tied to this bead ID.\n- Program/section verification gates must be able to consume this bead's outputs without manual interpretation.\n\nWhy This Improves User Outcomes:\n- \"[12] Risk control: temporal concept drift\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[12] Risk control: temporal concept drift\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"RISK: Temporal concept drift — trust models, compatibility baselines, and threat profiles become stale as the ecosystem evolves.\nIMPACT: Outdated models make incorrect trust decisions, compatibility claims based on stale data, missed new threat patterns.\nCOUNTERMEASURES:\n  (a) Continuous recalibration: trust and compatibility models are retrained/revalidated on a defined cadence (at least monthly).\n  (b) Cohort-specific drift audits: performance metrics are broken down by time cohort; degradation in recent cohorts triggers recalibration.\n  (c) Staleness alerts: models older than their defined TTL trigger mandatory review.\nVERIFICATION:\n  1. Every model has a defined TTL (time-to-live) and last-calibration timestamp.\n  2. Models exceeding TTL are flagged; CI blocks deployment of decisions based on stale models.\n  3. Drift detection: accuracy on most-recent-30-day cohort vs all-time accuracy; if delta > 5%, recalibration is triggered.\n  4. Recalibration pipeline runs end-to-end in CI (on synthetic data) to verify it completes without errors.\nTEST SCENARIOS:\n  - Scenario A: Set a model TTL to 1 day; advance clock by 2 days; verify staleness alert fires and CI blocks deployment.\n  - Scenario B: Inject concept drift (change test distribution); verify drift detection catches accuracy degradation > 5%.\n  - Scenario C: Run recalibration pipeline; verify updated model has improved accuracy on recent cohort.\n  - Scenario D: Verify cohort breakdown: model accuracy reported separately for each monthly cohort.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:39:34.025097700Z","created_by":"ubuntu","updated_at":"2026-02-20T15:20:07.998850770Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-12","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-v4ps","depends_on_id":"bd-paui","type":"blocks","created_at":"2026-02-20T07:43:25.127813720Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-v97o","title":"[10.13] Implement authenticated control channel with per-direction sequence monotonicity and replay-window checks.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.13 — FCP Deep-Mined Expansion Execution Track (9I)\n\nWhy This Exists:\nDeep connector/provider contract track: lifecycle FSMs, conformance harnesses, fencing, sandboxing, revocation freshness, and protocol hardening.\n\nTask Objective:\nImplement authenticated control channel with per-direction sequence monotonicity and replay-window checks.\n\nAcceptance Criteria:\n- Channel rejects out-of-window and non-monotonic frames; per-direction sequence state survives restart safely; replay attack fixtures are blocked.\n\nExpected Artifacts:\n- `src/protocol/control_channel.rs`, `tests/security/control_channel_replay_window.rs`, `artifacts/10.13/control_channel_security_trace.jsonl`.\n\n- Machine-readable verification artifact at `artifacts/section_10_13/bd-v97o/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_13/bd-v97o/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.13] Implement authenticated control channel with per-direction sequence monotonicity and replay-window checks.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.13] Implement authenticated control channel with per-direction sequence monotonicity and replay-window checks.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.13] Implement authenticated control channel with per-direction sequence monotonicity and replay-window checks.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.13] Implement authenticated control channel with per-direction sequence monotonicity and replay-window checks.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.13] Implement authenticated control channel with per-direction sequence monotonicity and replay-window checks.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","status":"closed","priority":1,"issue_type":"task","assignee":"CrimsonCrane","created_at":"2026-02-20T07:36:54.416231153Z","created_by":"ubuntu","updated_at":"2026-02-20T13:05:56.544150103Z","closed_at":"2026-02-20T13:05:56.544126589Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-13","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-v97o","depends_on_id":"bd-12h8","type":"blocks","created_at":"2026-02-20T07:43:13.812042961Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-vjq","title":"[10.1] Add explicit product charter document aligned to `/dp/franken_engine/PLAN_TO_CREATE_FRANKEN_ENGINE.md`.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.1 — Charter + Split Governance\n\nWhy This Exists:\nProgram charter and governance baseline: split contracts, reproducibility, and anti-regression rules for strategic integrity.\n\nTask Objective:\nRatify and harden the bootstrap charter output from `bd-2nd` into the canonical 10.1 product charter artifact aligned with `/dp/franken_engine/PLAN_TO_CREATE_FRANKEN_ENGINE.md` and section-level governance controls.\n\nScope Clarification:\n- `bd-2nd` creates the initial explicit charter document for immediate operational alignment.\n- This bead finalizes/ratifies that document as the canonical, policy-enforced 10.1 contract to avoid duplicate charter divergence.\n\nAcceptance Criteria:\n- Canonical charter supersedes bootstrap draft with explicit governance ownership, decision rights, and escalation contracts.\n- Split-boundary language is consistent with engine/product ownership and no duplicate-implementation policy.\n- Ratified charter is linked into CI/policy references used by dependent governance checks.\n\nExpected Artifacts:\n- Canonical charter document revision with ratification change log.\n- Governance cross-reference matrix showing alignment to related 10.1 controls.\n- Machine-readable evidence artifact proving dependent checks reference canonical charter.\n\n- Machine-readable verification artifact at `artifacts/section_10_1/bd-vjq/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_1/bd-vjq/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit-style validation for charter schema/template completeness (if automated).\n- E2E doc-navigation validation from README -> charter -> split-governance references.\n- Structured logs/validation outputs with stable finding categories and trace IDs.\n\nTask-Specific Clarification:\n- For \"[10.1] Add explicit product charter document aligned to `/dp/franken_engine/PLAN_TO_CREATE_FRANKEN_ENGINE.md`.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.1] Add explicit product charter document aligned to `/dp/franken_engine/PLAN_TO_CREATE_FRANKEN_ENGINE.md`.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.1] Add explicit product charter document aligned to `/dp/franken_engine/PLAN_TO_CREATE_FRANKEN_ENGINE.md`.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.1] Add explicit product charter document aligned to `/dp/franken_engine/PLAN_TO_CREATE_FRANKEN_ENGINE.md`.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.1] Add explicit product charter document aligned to `/dp/franken_engine/PLAN_TO_CREATE_FRANKEN_ENGINE.md`.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","status":"closed","priority":1,"issue_type":"task","assignee":"CrimsonCrane","created_at":"2026-02-20T07:36:43.289046058Z","created_by":"ubuntu","updated_at":"2026-02-20T09:03:58.249864497Z","closed_at":"2026-02-20T09:03:58.249824643Z","close_reason":"Charter ratified as v1.1 canonical 10.1 artifact. Engine plan alignment verified across 10 dimensions. Governance cross-reference matrix added with 4 CI/enforcement points. All 6/6 verification checks pass.","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-1","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-vjq","depends_on_id":"bd-2nd","type":"blocks","created_at":"2026-02-20T08:03:10.014293129Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-vjq","depends_on_id":"bd-cda","type":"blocks","created_at":"2026-02-20T07:46:30.960158137Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-vll","title":"[10.5] Implement deterministic incident replay bundle generation.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.5 — Security + Policy Product Surfaces (Item 3 of 8)\n\nWhy This Exists:\nDeterministic incident replay is a core safety guarantee. When a security incident occurs, the system must be able to produce a self-contained bundle that captures all relevant state, decisions, and effects — and replay them deterministically to verify what happened and enable counterfactual analysis.\n\nTask Objective:\nImplement deterministic incident replay bundle generation that captures full incident context and enables exact replay of decision sequences.\n\nDetailed Acceptance Criteria:\n1. Incident bundle captures: triggering event, evidence ledger entries, policy state at decision time, actions taken (with decision receipts), affected artifacts/extensions, timeline.\n2. Bundle is deterministic: given identical inputs, produces bit-identical bundle.\n3. Bundle is self-contained: includes all data needed for replay without external dependencies.\n4. Replay harness can re-execute the decision sequence and verify identical outcomes.\n5. Bundle format supports versioning (v1 per config.rs replay.bundle_version).\n6. Bundle integrity verified with cryptographic signatures.\n7. CLI surface: franken-node incident bundle {incident_id} produces the bundle; franken-node incident replay {bundle_path} replays it.\n\nKey Dependencies:\n- Depends on 10.14 (FrankenSQLite Deep-Mined) for evidence ledger and replay validator.\n- Consumed by 10.5 counterfactual replay mode.\n- Consumed by 10.8 (Operational Readiness) for incident bundle retention policy.\n- Consumed by 13 (Success Criteria) for 100% replay artifact coverage target.\n\nExpected Artifacts:\n- src/replay/ module with bundle_generator.rs, replay_harness.rs, bundle_format.rs.\n- CLI integration for incident bundle/replay commands.\n- docs/specs/section_10_5/bd-vll_contract.md\n- artifacts/section_10_5/bd-vll/verification_evidence.json\n\nTesting and Logging Requirements:\n- Unit tests: bundle generation determinism, self-containment verification, signature integrity, version compatibility.\n- Integration tests: incident -> bundle generation -> replay -> outcome verification pipeline.\n- E2E tests: full CLI workflow incident bundle -> incident replay producing matching results.\n- Determinism tests: multiple bundle generations from same incident produce identical outputs.\n- Structured logs: BUNDLE_GENERATED, BUNDLE_SIGNED, REPLAY_STARTED, REPLAY_VERIFIED, DETERMINISM_CHECK_PASSED with trace IDs and bundle version.","acceptance_criteria":"1. Define a ReplayBundle struct containing: bundle_id (UUID v7), incident_id (string), created_at (RFC-3339), timeline (Vec<TimelineEvent>), initial_state_snapshot (serialized system state), policy_version (semver), and integrity_hash (SHA-256 over canonical serialization of all preceding fields).\n2. Each TimelineEvent includes: sequence_number (u64, monotonic), timestamp (RFC-3339, microsecond precision), event_type (enum: StateChange | PolicyEval | ExternalSignal | OperatorAction), payload (serde_json::Value), and causal_parent (Option<u64> referencing a prior sequence_number).\n3. Bundle generation must be deterministic: given the same incident log inputs, two independent calls produce byte-identical bundles (canonical JSON, sorted keys, no floating-point ambiguity).\n4. Implement generate_replay_bundle(incident_id: &str, event_log: &[RawEvent]) -> Result<ReplayBundle> that filters, orders, and packages events.\n5. Bundle size must include a manifest listing event count, time span, and compressed size; bundles over 10 MB must be split into numbered chunks with a shared bundle_id and chunk index.\n6. Provide validate_bundle_integrity(bundle: &ReplayBundle) -> Result<bool> that recomputes and checks the integrity_hash.\n7. Verification: scripts/check_replay_bundle.py --json generates a sample bundle from fixture data in fixtures/interop/, validates integrity, and checks determinism by generating twice and comparing; unit tests in tests/test_check_replay_bundle.py; evidence in artifacts/section_10_5/bd-vll/.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:36:46.218994247Z","created_by":"ubuntu","updated_at":"2026-02-20T16:59:01.773001683Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-5","self-contained","test-obligations"]}
{"id":"bd-w0jq","title":"[10.13] Emit explicit degraded-mode audit events whenever stale revocation frontier overrides are used.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.13 — FCP Deep-Mined Expansion Execution Track (9I)\n\nWhy This Exists:\nDeep connector/provider contract track: lifecycle FSMs, conformance harnesses, fencing, sandboxing, revocation freshness, and protocol hardening.\n\nTask Objective:\nEmit explicit degraded-mode audit events whenever stale revocation frontier overrides are used.\n\nAcceptance Criteria:\n- Every degraded-mode override emits required audit schema fields; missing event is a hard failure in conformance tests; events correlate to action IDs.\n\nExpected Artifacts:\n- `tests/conformance/degraded_mode_audit_events.rs`, `docs/specs/degraded_mode_audit_schema.md`, `artifacts/10.13/degraded_mode_events.jsonl`.\n\n- Machine-readable verification artifact at `artifacts/section_10_13/bd-w0jq/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_13/bd-w0jq/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.13] Emit explicit degraded-mode audit events whenever stale revocation frontier overrides are used.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.13] Emit explicit degraded-mode audit events whenever stale revocation frontier overrides are used.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.13] Emit explicit degraded-mode audit events whenever stale revocation frontier overrides are used.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.13] Emit explicit degraded-mode audit events whenever stale revocation frontier overrides are used.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.13] Emit explicit degraded-mode audit events whenever stale revocation frontier overrides are used.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","status":"closed","priority":1,"issue_type":"task","assignee":"CrimsonCrane","created_at":"2026-02-20T07:36:53.194210248Z","created_by":"ubuntu","updated_at":"2026-02-20T12:07:54.663148458Z","closed_at":"2026-02-20T12:07:54.663123642Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-13","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-w0jq","depends_on_id":"bd-1m8r","type":"blocks","created_at":"2026-02-20T07:43:13.184802996Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-whxp","title":"[13] Concrete target gate: >=2 independent replications","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 13\n\nTask Objective:\nBuild KPI gate for at least two independent external reproductions of headline claims.\n\nExecution Requirements:\n- Make this requirement machine-verifiable and release-gated where applicable.\n- Keep behavior self-documenting so future contributors do not need to re-open the master plan.\n\nTesting & Logging Requirements:\n- Unit tests for rule/contract logic and error conditions.\n- Integration/E2E tests for workflow-level enforcement.\n- Structured logs with stable codes and trace IDs.\n- Reproducible artifacts that prove contract satisfaction.\n\nAcceptance Criteria:\n- Success and failure invariants for [13] Concrete target gate: >=2 independent replications are explicitly quantified and machine-verifiable.\n- Determinism requirements for [13] Concrete target gate: >=2 independent replications are validated across repeated runs and adversarial perturbations.\n\n\nExpected Artifacts:\n- Machine-readable verification artifact with explicit canonical path under artifacts/section_13/bd-whxp/verification_evidence.json, suitable for CI/release gating.\n- Human-readable verification summary with explicit canonical path under artifacts/section_13/bd-whxp/verification_summary.md, linking implementation intent to measured validation outcomes.\n\nTask-Specific Clarification:\n- The implementation must deliver the exact capability named in the title, with no scope dilution and no silent feature omission.\n- Validation artifacts must include capability-specific thresholds, pass/fail criteria, and reproducible evidence bundles tied to this bead ID.\n- Program/section verification gates must be able to consume this bead's outputs without manual interpretation.\n\nWhy This Improves User Outcomes:\n- \"[13] Concrete target gate: >=2 independent replications\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[13] Concrete target gate: >=2 independent replications\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"1. At least 2 independent external parties (universities, security firms, or open-source organizations) have reproduced key claims.\n2. Reproduced claims must include at least: (a) one compatibility claim (corpus pass rate), (b) one security claim (compromise reduction), (c) one performance claim (latency/throughput under hardening).\n3. Each reproduction is documented with: reproducer identity, date, methodology, results, and delta from original claims.\n4. Reproduction results are within 10% of original claims (allowing for environment differences).\n5. Reproduction reports are published alongside project documentation.\n6. Reproduction kit (instructions, data, scripts) is available in the public repository.\n7. Evidence artifact: replication_registry.json listing each replication with party, claim, result, and date.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:39:35.213287458Z","created_by":"ubuntu","updated_at":"2026-02-20T15:22:03.385726778Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-13","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-whxp","depends_on_id":"bd-2l1k","type":"blocks","created_at":"2026-02-20T07:43:25.707874868Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-wpck","title":"[15] Pillar: migration kit ecosystem","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 15\n\nTask Objective:\nBuild migration kit ecosystem for major Node/Bun archetypes.\n\nExecution Requirements:\n- Make this requirement machine-verifiable and release-gated where applicable.\n- Keep behavior self-documenting so future contributors do not need to re-open the master plan.\n\nTesting & Logging Requirements:\n- Unit tests for rule/contract logic and error conditions.\n- Integration/E2E tests for workflow-level enforcement.\n- Structured logs with stable codes and trace IDs.\n- Reproducible artifacts that prove contract satisfaction.\n\nAcceptance Criteria:\n- Success and failure invariants for [15] Pillar: migration kit ecosystem are explicitly quantified and machine-verifiable.\n- Determinism requirements for [15] Pillar: migration kit ecosystem are validated across repeated runs and adversarial perturbations.\n\n\nExpected Artifacts:\n- Machine-readable verification artifact with explicit canonical path under artifacts/section_15/bd-wpck/verification_evidence.json, suitable for CI/release gating.\n- Human-readable verification summary with explicit canonical path under artifacts/section_15/bd-wpck/verification_summary.md, linking implementation intent to measured validation outcomes.\n\nTask-Specific Clarification:\n- The implementation must deliver the exact capability named in the title, with no scope dilution and no silent feature omission.\n- Validation artifacts must include capability-specific thresholds, pass/fail criteria, and reproducible evidence bundles tied to this bead ID.\n- Program/section verification gates must be able to consume this bead's outputs without manual interpretation.\n\nWhy This Improves User Outcomes:\n- \"[15] Pillar: migration kit ecosystem\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[15] Pillar: migration kit ecosystem\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"1. Migration kits exist for >= 5 major Node.js/Bun archetypes: (a) Express.js/Fastify web server, (b) Next.js/Remix SSR app, (c) CLI tool (Commander/Yargs), (d) npm library package, (e) background worker (Bull/BullMQ queue processor).\n2. Each kit includes: (a) automated analyzer that scores migration readiness, (b) migration script that handles >= 80% of steps automatically, (c) manual intervention guide for remaining steps, (d) post-migration validation suite, (e) rollback script.\n3. Kits are published as npm packages installable via 'npx franken-migrate --kit <archetype>'.\n4. Each kit has been validated on >= 3 real-world projects of its archetype (documented in test matrix).\n5. Kit documentation includes: estimated migration time, known limitations, and compatibility exceptions.\n6. Bun-specific kits exist for >= 2 archetypes with documented Bun-specific compatibility delta.\n7. Evidence: migration_kit_matrix.json with per-archetype kit status, validated projects, and success rates.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:39:36.246627380Z","created_by":"ubuntu","updated_at":"2026-02-20T15:26:11.404944424Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-15","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-wpck","depends_on_id":"bd-209w","type":"blocks","created_at":"2026-02-20T07:43:26.255430105Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-wzjl","title":"[14] Include security and trust co-metrics","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 14\n\nTask Objective:\nExpand benchmark suite beyond speed-only metrics to include security and operational trust dimensions.\n\nExecution Requirements:\n- Make this requirement machine-verifiable and release-gated where applicable.\n- Keep behavior self-documenting so future contributors do not need to re-open the master plan.\n\nTesting & Logging Requirements:\n- Unit tests for rule/contract logic and error conditions.\n- Integration/E2E tests for workflow-level enforcement.\n- Structured logs with stable codes and trace IDs.\n- Reproducible artifacts that prove contract satisfaction.\n\nAcceptance Criteria:\n- Success and failure invariants for [14] Include security and trust co-metrics are explicitly quantified and machine-verifiable.\n- Determinism requirements for [14] Include security and trust co-metrics are validated across repeated runs and adversarial perturbations.\n\n\nExpected Artifacts:\n- Machine-readable verification artifact with explicit canonical path under artifacts/section_14/bd-wzjl/verification_evidence.json, suitable for CI/release gating.\n- Human-readable verification summary with explicit canonical path under artifacts/section_14/bd-wzjl/verification_summary.md, linking implementation intent to measured validation outcomes.\n\nTask-Specific Clarification:\n- The implementation must deliver the exact capability named in the title, with no scope dilution and no silent feature omission.\n- Validation artifacts must include capability-specific thresholds, pass/fail criteria, and reproducible evidence bundles tied to this bead ID.\n- Program/section verification gates must be able to consume this bead's outputs without manual interpretation.\n\nWhy This Improves User Outcomes:\n- \"[14] Include security and trust co-metrics\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[14] Include security and trust co-metrics\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"1. Benchmark suite includes security co-metrics alongside performance metrics: (a) attack surface area (number of exposed APIs under test profile), (b) containment latency under attack, (c) trust decision accuracy under adversarial load.\n2. Trust co-metrics include: (a) false positive rate of trust decisions, (b) false negative rate (malicious extensions passing trust checks), (c) trust decision latency p50/p95/p99.\n3. Security and trust metrics are reported in the same benchmark output as performance metrics (unified report).\n4. Co-metric baselines are established: each security/trust metric has a defined acceptable range.\n5. Trade-off analysis: report explicitly shows performance vs security trade-offs (e.g., 'strict profile: +20ms p99 latency, -90% attack surface').\n6. Co-metrics are measured under at least 2 hardening profiles (balanced, strict).\n7. Evidence: security_trust_cometrics.json with per-profile metric values and trade-off summary.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:39:35.385320210Z","created_by":"ubuntu","updated_at":"2026-02-20T15:23:52.906712131Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-14","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-wzjl","depends_on_id":"bd-3h1g","type":"blocks","created_at":"2026-02-20T07:43:25.805877406Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-xwk5","title":"[10.14] Implement fork/divergence detection via marker-id prefix comparison and binary search.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.14 — FrankenSQLite Deep-Mined Expansion Execution Track (9J)\n\nWhy This Exists:\nFrankenSQLite deep-mined control integrity track: evidence ledgers, proofs, epochs, remote effects, markers/MMR, and deterministic fault labs.\n\nTask Objective:\nImplement fork/divergence detection via marker-id prefix comparison and binary search.\n\nAcceptance Criteria:\n- Divergence finder returns greatest common prefix deterministically; fork detection scales logarithmically; mismatch evidence includes exact divergence point.\n\nExpected Artifacts:\n- `tests/integration/marker_divergence_detection.rs`, `docs/specs/divergence_detection.md`, `artifacts/10.14/divergence_detection_examples.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_14/bd-xwk5/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_14/bd-xwk5/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.14] Implement fork/divergence detection via marker-id prefix comparison and binary search.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.14] Implement fork/divergence detection via marker-id prefix comparison and binary search.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.14] Implement fork/divergence detection via marker-id prefix comparison and binary search.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.14] Implement fork/divergence detection via marker-id prefix comparison and binary search.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.14] Implement fork/divergence detection via marker-id prefix comparison and binary search.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"Divergence finder returns greatest common prefix deterministically; fork detection scales logarithmically; mismatch evidence includes exact divergence point.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:36:58.710125669Z","created_by":"ubuntu","updated_at":"2026-02-20T16:24:18.661361392Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-14","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-xwk5","depends_on_id":"bd-126h","type":"blocks","created_at":"2026-02-20T16:24:18.661281023Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-y0v","title":"[10.12] Implement operator intelligence recommendation engine with rollback proofs.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md — Section 10.12 (Frontier Programs), cross-ref Section 9H.4 (Operator Intelligence Program), 9F.6 (Autonomous incident commander)\n\n## Why This Exists\nOperators of high-trust extension ecosystems face a combinatorial explosion of configuration, migration, and security decisions where wrong choices have compounding downstream costs. The Operator Intelligence Program (9H.4) addresses this by shipping expected-loss-aware control recommendations with deterministic action replays — meaning every recommendation the system makes comes with a quantified risk estimate, a concrete action plan, and a cryptographic rollback proof that guarantees the action can be undone to a known-good state. This bead implements the recommendation engine core, the expected-loss scoring model, and the rollback proof infrastructure. It directly advances the category-defining floor targets of \"100% deterministic replay artifact availability\" (every recommendation produces a replay artifact) and \">= 3x migration throughput and confidence quality\" (operator intelligence reduces decision latency and error rates during migration campaigns).\n\n## What This Must Do\n1. Implement a `RecommendationEngine` module (`crates/franken-node/src/connector/operator_intelligence.rs`) that accepts an operator context (current system state snapshot, pending operations queue, active alerts) and produces a ranked list of `Recommendation` structs, each containing: action description, expected-loss score (quantified downside in a normalized risk unit), confidence interval, prerequisite checks, and estimated execution time.\n2. Implement an expected-loss scoring model that computes risk estimates from: (a) historical outcome data (migration success/failure rates from 10.3), (b) current compatibility state (from 10.2 compatibility core), (c) trust posture (from 10.4/10.13 trust fabric — revocation state, certificate validity), and (d) operational telemetry (error rates, latency percentiles, resource utilization). The model must be deterministic: identical inputs produce identical scores.\n3. Implement a `RollbackProof` structure that captures: (a) pre-action state snapshot (content-addressed), (b) the action specification (deterministic command sequence), (c) post-action expected state, and (d) a rollback command sequence that restores pre-action state. The proof must be independently verifiable — an external party can check that the rollback sequence, applied to the post-action state, produces the pre-action state hash.\n4. Implement deterministic action replay: every recommendation that is executed produces a replay artifact containing the full input context, the recommendation chosen, the action sequence executed, the observed outcome, and the rollback proof. Replay artifacts must be machine-parseable and re-executable.\n5. Implement a recommendation audit trail that logs every recommendation generated (whether accepted or rejected by the operator), with timestamps, context fingerprints, and outcome tracking for accepted recommendations.\n6. Provide a CLI subcommand (`franken-node recommend`) that invokes the engine for the current system state and presents recommendations in priority order with expected-loss scores and rollback proof summaries.\n7. Implement degraded-mode behavior: when input data sources are unavailable (e.g., historical data missing), the engine must still produce recommendations but with wider confidence intervals and explicit data-quality warnings, never silently degrading accuracy.\n\n## Context from Enhancement Maps\n- 9H.4: \"Ship expected-loss-aware control recommendations with deterministic action replays.\" This bead is the direct implementation of that program's core engine.\n- 9F.6: \"Autonomous incident commander\" — the recommendation engine is the decision-making core that an autonomous incident commander would invoke; this bead establishes the deterministic, auditable recommendation infrastructure that future autonomous operation builds on.\n- Category-defining targets (Section 3.2): \"100% deterministic replay artifact availability\" — every executed recommendation produces a complete replay artifact with rollback proof. \">= 3x migration throughput and confidence quality\" — operator intelligence reduces decision errors and accelerates migration campaigns by quantifying expected loss before each action.\n\n## Dependencies\n- Upstream: bd-3c2 (verifier-economy SDK — provides independent validation workflows that can verify rollback proofs), Section 10.2 (compatibility core — compatibility state is an input to expected-loss scoring), Section 10.3 (migration system — historical migration outcome data feeds the scoring model), Section 10.4/10.13 (trust/security — trust posture and revocation state are scoring inputs).\n- Downstream: bd-2aj (ecosystem network-effect APIs — recommendation outcomes and rollback proofs are published as compliance evidence), bd-n1w (frontier demo gates — operator intelligence must register a demo gate), bd-1d6x (section-wide verification gate), bd-go4 (section 10.12 plan rollup).\n\n## Acceptance Criteria\n1. The `RecommendationEngine` accepts a well-defined operator context and produces a ranked list of recommendations, each with action description, expected-loss score, confidence interval, prerequisites, and estimated execution time.\n2. Expected-loss scores are deterministic: given identical operator context inputs, the engine produces byte-identical recommendation rankings and scores across 100 consecutive runs.\n3. Every executed recommendation produces a `RollbackProof` containing pre-action state hash, action specification, post-action expected state, and rollback command sequence.\n4. Rollback proofs are independently verifiable: a test harness can apply the rollback sequence to the post-action state and confirm the resulting state hash matches the pre-action state hash.\n5. Replay artifacts contain the full input-to-output trace and can be re-executed to reproduce the same outcome (deterministic replay).\n6. The recommendation audit trail records all generated recommendations (accepted and rejected) with context fingerprints and timestamps.\n7. Degraded-mode operation produces recommendations with explicit data-quality warnings and widened confidence intervals when input sources are unavailable, without crashing or producing silent errors.\n8. The `franken-node recommend` CLI subcommand produces human-readable output with expected-loss scores and rollback proof summaries for the current system state.\n\n## Testing & Logging Requirements\n- Unit tests: Test expected-loss score computation with fixed inputs for determinism; test recommendation ranking stability; test rollback proof generation and verification round-trip; test degraded-mode behavior with missing input sources; test confidence interval widening logic.\n- Integration tests: End-to-end recommendation generation from real system state snapshots; test action execution -> rollback proof generation -> rollback execution -> state verification cycle; test audit trail completeness (all recommendations logged).\n- E2E tests: Operator workflow — trigger a migration recommendation, review expected-loss scores, execute the recommended action, verify rollback proof is generated, execute rollback, confirm system returns to pre-action state.\n- External reproducibility tests: An independent verifier receives a replay artifact and re-executes it to confirm deterministic outcome; verifier checks rollback proof independently without access to the running system.\n- Structured logs: Emit `RECOMMENDATION_GENERATED`, `RECOMMENDATION_ACCEPTED`, `RECOMMENDATION_REJECTED`, `ACTION_EXECUTED`, `ROLLBACK_PROOF_CREATED`, `ROLLBACK_PROOF_VERIFIED`, `ROLLBACK_EXECUTED`, `REPLAY_ARTIFACT_CREATED`, `DEGRADED_MODE_ENTERED`, `DEGRADED_MODE_WARNING` events with trace correlation IDs, recommendation IDs, expected-loss scores, and action durations.\n\n## Expected Artifacts\n- docs/specs/section_10_12/bd-y0v_contract.md\n- artifacts/section_10_12/bd-y0v/verification_evidence.json\n- artifacts/section_10_12/bd-y0v/verification_summary.md","acceptance_criteria":"1. Define a Recommendation struct: (a) recommendation_id (unique), (b) action (enum: MIGRATE, ROLLBACK, UPGRADE, HOLD, DECOMMISSION), (c) target (system/component identifier), (d) confidence_score (f64 in [0.0, 1.0]), (e) expected_loss (f64, estimated cost/impact if the recommendation is wrong), (f) expected_gain (f64, estimated benefit if the recommendation succeeds), (g) evidence (list of {factor_name, weight, value} contributing to the score), (h) rollback_proof (Option<RollbackReceipt> from bd-3hm, required for MIGRATE and UPGRADE actions).\n2. Implement a RecommendationEngine with: (a) evaluate(target, context) -> Recommendation that computes a recommendation based on system state, historical outcomes, and policy constraints, (b) explain(recommendation_id) -> ExplanationReport that produces a human-readable breakdown of the scoring factors.\n3. Enforce rollback-proof requirement: recommendations for MIGRATE or UPGRADE actions MUST include a valid rollback_proof. The engine MUST NOT emit a recommendation without one. Return MissingRollbackProof error if rollback receipt generation fails.\n4. Implement expected-loss-aware ranking: when multiple recommendations are possible, rank by expected_gain - expected_loss (net expected value). Expose the ranking function for testing.\n5. Implement a confidence threshold gate: recommendations with confidence_score < configurable minimum (default 0.6) are emitted with a LOW_CONFIDENCE flag and MUST NOT be auto-executed. They require explicit operator approval.\n6. Implement recommendation audit trail: every recommendation emitted is logged as a structured event with all Recommendation fields plus the operator action taken (ACCEPTED, REJECTED, DEFERRED) and the actual outcome (SUCCESS, FAILURE, PENDING).\n7. Implement outcome feedback loop: after a recommendation is executed, record the actual outcome. Provide a function update_model(recommendation_id, actual_outcome) that adjusts future scoring (at minimum, track hit/miss ratio per action type).\n8. Unit tests: (a) MIGRATE recommendation includes rollback proof, (b) missing rollback proof error, (c) net expected value ranking, (d) low confidence flag, (e) explanation report contains all factors, (f) audit trail logging, (g) outcome feedback updates hit ratio.\n9. Verification: scripts/check_recommendation_engine.py --json, artifacts at artifacts/section_10_12/bd-y0v/.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:36:51.154948985Z","created_by":"ubuntu","updated_at":"2026-02-20T17:01:50.311189830Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-12","self-contained","test-obligations"]}
{"id":"bd-y4g","title":"[10.0] Implement trust cards for extensions and publishers.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.0 — Top 10 Initiative Tracking (Initiative #3)\nCross-references: 9A.3, 9B.3, 9C.3, 9D.3\n\nWhy This Exists:\nTrust cards are the #3 strategic initiative. They surface provenance, behavioral telemetry, revocation status, and policy constraints in a single explainable trust model consumable by both humans and automation. Trust cards are the primary interface through which operators and tools understand the trustworthiness of extensions and publishers.\n\nTask Objective:\nImplement the trust card system for extensions and publishers — a structured, queryable trust profile that aggregates provenance data, behavioral evidence, revocation status, policy constraints, and reputation signals into an explainable, API-accessible trust assessment.\n\nDetailed Acceptance Criteria:\n1. Trust card schema captures: provenance attestations, behavioral telemetry summary, revocation status, policy constraint set, certification level, publisher reputation score, and risk tier.\n2. CLI surface: franken-node trust card {extension_id} displays full trust card in human-readable format.\n3. API surface: programmatic trust-card query returns structured JSON suitable for automation.\n4. Trust deltas decomposed into posterior components with counterfactual action impacts exposed (9C.3).\n5. Authenticated data structures and transparency-style append-only proofs for trust evidence lineage (9B.3).\n6. Trust-card materialization optimized with incremental updates and bounded recomputation (9D.3).\n7. Trust card integrates with extension certification levels (10.4), policy controls (10.5), and fleet quarantine (10.8).\n8. Publisher trust cards aggregate across all published extensions with explainable reputation transitions.\n\nKey Dependencies:\n- Depends on 10.4 (Extension Ecosystem) for manifest schema and provenance attestation.\n- Consumed by 10.5 (Security) for policy-visible trust decisions.\n- Consumed by 10.17 (Radical Expansion) for Bayesian adversary graph integration.\n- Consumed by 10.21 (BPET) for behavioral phenotype evidence.\n\nExpected Artifacts:\n- src/security/trust_card.rs — trust card schema, builder, query API.\n- src/security/trust_card_publisher.rs — publisher-level aggregation.\n- CLI integration in cli.rs for trust card/list/revoke commands.\n- docs/specs/section_10_0/bd-y4g_contract.md\n- artifacts/section_10_0/bd-y4g/verification_evidence.json\n\nTesting and Logging Requirements:\n- Unit tests: trust card construction, field validation, incremental update correctness, posterior decomposition.\n- Integration tests: trust card generation from mock extension manifests + behavioral data + revocation feeds.\n- E2E tests: franken-node trust card {extension_id} CLI workflow producing correct output.\n- Adversarial tests: malformed provenance data, stale revocation, conflicting signals.\n- Structured logs: TRUST_CARD_MATERIALIZED, TRUST_CARD_QUERIED, TRUST_DELTA_COMPUTED, REPUTATION_UPDATED with trace IDs.","acceptance_criteria":"1. Trust card schema includes: publisher identity, provenance chain (build source -> artifact), behavioral telemetry summary, revocation status, policy constraint set, trust score, last-audit timestamp.\n2. Provenance chain is cryptographically verifiable: each link signed, tamper-evident, traceable to source commit.\n3. Behavioral telemetry captures: permission usage (fs/net/child_process), resource consumption bounds, anomaly flags; updated on each publish cycle.\n4. Revocation status propagates within <= 60 seconds to all connected fleet nodes (real-time revocation channel).\n5. Policy constraints expressed as declarative rules (e.g., \"no net access\", \"fs read-only /app/**\"); enforced at runtime with deny-by-default semantics.\n6. Trust model is explainable: CLI command (`franken-node trust explain <extension>`) outputs human-readable rationale for current trust score decomposition.\n7. Compromise reduction target: trust card system contributes to >= 10x compromise reduction (Section 3) by blocking extensions that fail provenance or behavioral checks.\n8. Trust cards queryable via API and CLI in JSON format; support filtering by publisher, score range, revocation status.\n9. Integration with secure extension distribution network (bd-2ac): trust card is mandatory for registry publish; missing card blocks distribution.\n10. Verification evidence includes: sample trust cards for test extensions, revocation propagation latency measurement, policy constraint enforcement test results.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:36:42.642636403Z","created_by":"ubuntu","updated_at":"2026-02-20T16:06:11.059508909Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-0","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-y4g","depends_on_id":"bd-2de","type":"blocks","created_at":"2026-02-20T07:43:10.181037921Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-y7lu","title":"[10.13] Implement revocation registry with monotonic revocation-head checkpoints.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.13 — FCP Deep-Mined Expansion Execution Track (9I)\n\nWhy This Exists:\nDeep connector/provider contract track: lifecycle FSMs, conformance harnesses, fencing, sandboxing, revocation freshness, and protocol hardening.\n\nTask Objective:\nImplement revocation registry with monotonic revocation-head checkpoints.\n\nAcceptance Criteria:\n- Revocation heads are monotonic per zone/tenant; stale head updates are rejected; head state is recoverable from canonical storage.\n\nExpected Artifacts:\n- `docs/specs/revocation_registry.md`, `tests/conformance/revocation_head_monotonicity.rs`, `artifacts/10.13/revocation_head_history.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_13/bd-y7lu/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_13/bd-y7lu/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.13] Implement revocation registry with monotonic revocation-head checkpoints.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.13] Implement revocation registry with monotonic revocation-head checkpoints.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.13] Implement revocation registry with monotonic revocation-head checkpoints.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.13] Implement revocation registry with monotonic revocation-head checkpoints.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.13] Implement revocation registry with monotonic revocation-head checkpoints.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","status":"closed","priority":1,"issue_type":"task","assignee":"CrimsonCrane","created_at":"2026-02-20T07:36:53.031084487Z","created_by":"ubuntu","updated_at":"2026-02-20T12:00:29.376707201Z","closed_at":"2026-02-20T12:00:29.376681092Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-13","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-y7lu","depends_on_id":"bd-2yc4","type":"blocks","created_at":"2026-02-20T07:43:13.096265779Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-ybe","title":"[PLAN 10.20] Dependency Graph Immune System Execution Track (9N)","description":"Section: 10.20 — Dependency Graph Immune System Execution Track (9N)\n\nStrategic Context:\nDGIS topological immune system track for dependency contagion modeling, choke-point immunization, and graph-aware containment economics.\n\nExecution Requirements:\n- Preserve all scoped capabilities from the canonical plan.\n- Keep contracts self-contained so future contributors can execute without reopening the master plan.\n- Require explicit testing strategy (unit + integration/e2e) and structured logging/telemetry for every child bead.\n- Require artifact-backed validation for performance, security, and correctness claims.\n\nDependency Semantics:\n- This epic is blocked by its child implementation beads.\n- Cross-epic dependencies encode strategic sequencing and canonical ownership boundaries.\n\n## Success Criteria\n- All child beads for this section are completed with acceptance artifacts attached and no unresolved blockers.\n- Section-level verification gate for comprehensive unit tests, integration/e2e workflows, and detailed structured logging evidence is green.\n- Scope-to-plan traceability is explicit, with no feature loss versus the canonical plan section.\n\n## Optimization Notes\n- User-Outcome Lens: \"[PLAN 10.20] Dependency Graph Immune System Execution Track (9N)\" must improve operator confidence, safety posture, and deterministic recovery behavior under both normal and adversarial conditions.\n- Cross-Section Coordination: child beads must encode integration assumptions explicitly to avoid local optimizations that degrade system-wide correctness.\n- Verification Non-Negotiable: completion requires reproducible unit/integration/E2E evidence and structured logs suitable for independent replay.\n- Scope Protection: any simplification must preserve canonical-plan feature intent and be justified with explicit tradeoff documentation.","status":"open","priority":2,"issue_type":"epic","created_at":"2026-02-20T07:36:41.868695628Z","created_by":"ubuntu","updated_at":"2026-02-20T08:16:44.582810918Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-20"],"dependencies":[{"issue_id":"bd-ybe","depends_on_id":"bd-19k2","type":"blocks","created_at":"2026-02-20T07:37:07.314030839Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-ybe","depends_on_id":"bd-1f8v","type":"blocks","created_at":"2026-02-20T07:37:07.399259833Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-ybe","depends_on_id":"bd-1q38","type":"blocks","created_at":"2026-02-20T07:37:06.786535764Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-ybe","depends_on_id":"bd-1tnu","type":"blocks","created_at":"2026-02-20T07:37:06.952108563Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-ybe","depends_on_id":"bd-1wz","type":"blocks","created_at":"2026-02-20T07:37:11.674423749Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-ybe","depends_on_id":"bd-2bj4","type":"blocks","created_at":"2026-02-20T07:37:06.539505946Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-ybe","depends_on_id":"bd-2d17","type":"blocks","created_at":"2026-02-20T07:37:07.482271307Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-ybe","depends_on_id":"bd-2fid","type":"blocks","created_at":"2026-02-20T07:37:06.870182601Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-ybe","depends_on_id":"bd-2jns","type":"blocks","created_at":"2026-02-20T07:37:06.704903769Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-ybe","depends_on_id":"bd-2wod","type":"blocks","created_at":"2026-02-20T07:37:07.116148597Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-ybe","depends_on_id":"bd-351r","type":"blocks","created_at":"2026-02-20T07:37:07.199226465Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-ybe","depends_on_id":"bd-38yt","type":"blocks","created_at":"2026-02-20T07:37:07.647037444Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-ybe","depends_on_id":"bd-39a","type":"blocks","created_at":"2026-02-20T07:37:11.713144824Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-ybe","depends_on_id":"bd-3po7","type":"blocks","created_at":"2026-02-20T07:48:21.598558852Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-ybe","depends_on_id":"bd-b541","type":"blocks","created_at":"2026-02-20T07:37:06.456439910Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-ybe","depends_on_id":"bd-c97l","type":"blocks","created_at":"2026-02-20T07:37:07.034435652Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-ybe","depends_on_id":"bd-cclm","type":"blocks","created_at":"2026-02-20T07:37:07.565014060Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-ybe","depends_on_id":"bd-cda","type":"blocks","created_at":"2026-02-20T07:37:09.817479825Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-ybe","depends_on_id":"bd-t89w","type":"blocks","created_at":"2026-02-20T07:37:06.621423352Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-ye4m","title":"[10.21] Implement adversarial evaluation suite for slow-roll mimicry, staged camouflage, and dormant-then-burst mutation campaigns.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.21 — Behavioral Phenotype Evolution Tracker Execution Track (9O)\n\nWhy This Exists:\nBPET longitudinal phenotype intelligence track for pre-compromise trajectory detection and integration with DGIS/ATC/migration/economic policy.\n\nTask Objective:\nImplement adversarial evaluation suite for slow-roll mimicry, staged camouflage, and dormant-then-burst mutation campaigns.\n\nAcceptance Criteria:\n- Simulated adversaries test resilience to trajectory-gaming tactics; bypasses emit typed failure classes and trigger policy hardening recommendations.\n\nExpected Artifacts:\n- `tests/security/bpet_adversarial_evolution_suite.rs`, `docs/security/bpet_adversarial_playbook.md`, `artifacts/10.21/bpet_adversarial_results.json`.\n\n- Machine-readable verification artifact at `artifacts/section_10_21/bd-ye4m/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_21/bd-ye4m/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests for core logic, invariants, and error handling.\n- Integration/E2E tests for end-to-end control-flow behavior.\n- Detailed structured logs/telemetry with stable codes and trace correlation IDs.\n- Deterministic replay fixture or reproducible artifact for failure analysis.\n\nTask-Specific Clarification:\n- For \"[10.21] Implement adversarial evaluation suite for slow-roll mimicry, staged camouflage, and dormant-then-burst mutation campaigns.\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.21] Implement adversarial evaluation suite for slow-roll mimicry, staged camouflage, and dormant-then-burst mutation campaigns.\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.21] Implement adversarial evaluation suite for slow-roll mimicry, staged camouflage, and dormant-then-burst mutation campaigns.\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.21] Implement adversarial evaluation suite for slow-roll mimicry, staged camouflage, and dormant-then-burst mutation campaigns.\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.21] Implement adversarial evaluation suite for slow-roll mimicry, staged camouflage, and dormant-then-burst mutation campaigns.\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"- Simulated adversaries test resilience to trajectory-gaming tactics; bypasses emit typed failure classes and trigger policy hardening recommendations.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:37:08.793889202Z","created_by":"ubuntu","updated_at":"2026-02-20T17:05:54.836671391Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-21","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-ye4m","depends_on_id":"bd-1jpc","type":"blocks","created_at":"2026-02-20T17:05:54.836620115Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-yqz","title":"[10.0] Implement fleet quarantine UX + control plane.","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.0 — Top 10 Initiative Tracking (Initiative #6)\nCross-references: 9A.6, 9B.6, 9C.6, 9D.6\n\nWhy This Exists:\nFleet quarantine UX + control plane is the #6 strategic initiative. It turns engine-level containment primitives into operator-grade workflows with global scope, blast-radius views, convergence indicators, and rollback controls. Without this, quarantine operations remain low-level and error-prone.\n\nTask Objective:\nBuild the fleet quarantine user experience and control plane that enables operators to manage quarantine/revocation operations across distributed deployments with clear visibility into scope, impact, convergence state, and rollback options.\n\nDetailed Acceptance Criteria:\n1. Fleet control API for quarantine/revocation operations with zone/tenant scoping.\n2. Blast-radius visualization showing affected extensions, publishers, and dependent workloads.\n3. Convergence indicators: real-time progress tracking of quarantine propagation across fleet.\n4. Rollback controls: deterministic rollback to pre-quarantine state with verification.\n5. Anti-entropy reconciliation and bounded degraded-mode semantics under network partition (9B.6).\n6. Probabilistic SLO proofs for containment latency and convergence quality (9C.6).\n7. Propagation path latency and conflict reconciliation fast paths optimized (9D.6).\n8. CLI surface: franken-node fleet status/release/reconcile commands.\n9. Convergence timeout configurable per profile (strict=60s, balanced=120s, legacy-risky=300s per config.rs).\n\nKey Dependencies:\n- Depends on engine-level containment primitives from franken_engine.\n- Consumed by 10.8 (Operational Readiness) for fleet control API.\n- Consumed by 10.5 (Security) for policy-driven quarantine decisions.\n- Integrates with 10.20 (DGIS) for topology-aware containment.\n\nExpected Artifacts:\n- src/fleet/ module with control_plane.rs, convergence.rs, blast_radius.rs, rollback.rs.\n- CLI integration for fleet commands in cli.rs.\n- docs/specs/section_10_0/bd-yqz_contract.md\n- artifacts/section_10_0/bd-yqz/verification_evidence.json\n\nTesting and Logging Requirements:\n- Unit tests: convergence calculation, blast radius computation, rollback state verification, anti-entropy reconciliation.\n- Integration tests: quarantine propagation across simulated multi-zone fleet.\n- E2E tests: franken-node fleet status/release/reconcile CLI workflows.\n- Fault injection tests: network partition during quarantine propagation, convergence under degraded mode.\n- Structured logs: QUARANTINE_INITIATED, PROPAGATION_PROGRESS, CONVERGENCE_REACHED, ROLLBACK_EXECUTED, DEGRADED_MODE_ENTERED with trace IDs and zone metadata.","acceptance_criteria":"1. Global scope: quarantine operations apply fleet-wide; single quarantine action propagates to all enrolled nodes within <= 120 seconds.\n2. Blast-radius views: CLI and API expose affected node count, affected workload count, estimated user impact percentage, and dependency graph of quarantined components.\n3. Convergence indicators: real-time dashboard data (JSON API) showing: quarantine propagation progress (% nodes confirmed), rollback progress, and time-to-convergence estimate.\n4. Rollback controls: deterministic single-command rollback (`franken-node quarantine rollback <id>`) that restores pre-quarantine state; rollback verified by post-rollback health check.\n5. Quarantine policies are declarative: trigger conditions (severity threshold, anomaly score, revocation event), scope (extension/version/publisher), and auto-escalation rules.\n6. Audit trail: every quarantine action (create/extend/rollback/expire) logged with timestamp, actor, justification, blast-radius snapshot; append-only and tamper-evident.\n7. Integration with trust cards (bd-y4g): revocation events automatically trigger quarantine evaluation; trust score drop below threshold triggers quarantine proposal.\n8. Compromise reduction contribution: quarantine system contributes to >= 10x compromise reduction target (Section 3) by containing blast radius within <= 60 seconds of detection.\n9. UX: quarantine status visible in `franken-node status` output; color-coded severity; actionable next-step suggestions.\n10. Verification evidence includes: propagation latency measurement, rollback success test, blast-radius calculation accuracy test, convergence indicator correctness test.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:36:42.883031172Z","created_by":"ubuntu","updated_at":"2026-02-20T15:36:06.178178085Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-0","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-yqz","depends_on_id":"bd-1vm","type":"blocks","created_at":"2026-02-20T15:01:23.957644953Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-yqz","depends_on_id":"bd-mwf","type":"blocks","created_at":"2026-02-20T07:43:10.310263860Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-yqz","depends_on_id":"bd-tg2","type":"blocks","created_at":"2026-02-20T15:01:24.143258180Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-yz3t","title":"[14] Publish verifier toolkit for independent validation","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 14\n\nTask Objective:\nDeliver verifier toolkit enabling independent claim validation.\n\nExecution Requirements:\n- Make this requirement machine-verifiable and release-gated where applicable.\n- Keep behavior self-documenting so future contributors do not need to re-open the master plan.\n\nTesting & Logging Requirements:\n- Unit tests for rule/contract logic and error conditions.\n- Integration/E2E tests for workflow-level enforcement.\n- Structured logs with stable codes and trace IDs.\n- Reproducible artifacts that prove contract satisfaction.\n\nAcceptance Criteria:\n- Success and failure invariants for [14] Publish verifier toolkit for independent validation are explicitly quantified and machine-verifiable.\n- Determinism requirements for [14] Publish verifier toolkit for independent validation are validated across repeated runs and adversarial perturbations.\n\n\nExpected Artifacts:\n- Machine-readable verification artifact with explicit canonical path under artifacts/section_14/bd-yz3t/verification_evidence.json, suitable for CI/release gating.\n- Human-readable verification summary with explicit canonical path under artifacts/section_14/bd-yz3t/verification_summary.md, linking implementation intent to measured validation outcomes.\n\nTask-Specific Clarification:\n- The implementation must deliver the exact capability named in the title, with no scope dilution and no silent feature omission.\n- Validation artifacts must include capability-specific thresholds, pass/fail criteria, and reproducible evidence bundles tied to this bead ID.\n- Program/section verification gates must be able to consume this bead's outputs without manual interpretation.\n\nWhy This Improves User Outcomes:\n- \"[14] Publish verifier toolkit for independent validation\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[14] Publish verifier toolkit for independent validation\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"1. Verifier toolkit is published as an independently installable package (npm, cargo crate, or standalone binary).\n2. Toolkit enables external parties to validate: (a) compatibility claims against the published corpus, (b) security claims via reproducible attack scenarios, (c) performance claims via standardized benchmark workloads.\n3. Toolkit requires no access to franken_node source code — operates on published artifacts and binary.\n4. Toolkit produces a validation report (JSON + Markdown) with per-claim verdict: confirmed/refuted/inconclusive.\n5. Documentation: getting-started guide enables first validation run in <= 15 minutes.\n6. Toolkit is versioned in lockstep with benchmark versions; version compatibility matrix is published.\n7. At least 1 external party has used the toolkit and provided feedback (tracked in feedback log).\n8. Evidence: verifier_toolkit_release.json with version, download URL, platform support, and external feedback summary.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-20T07:39:35.471414837Z","created_by":"ubuntu","updated_at":"2026-02-20T15:24:04.405317727Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-14","self-contained","test-obligations"],"dependencies":[{"issue_id":"bd-yz3t","depends_on_id":"bd-wzjl","type":"blocks","created_at":"2026-02-20T07:43:25.849846186Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-z7bt","title":"[13] Section-wide verification gate: comprehensive unit+e2e+logging","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 13\n\nTask Objective:\nCreate a hard section completion gate that verifies all implemented behavior in this section with comprehensive unit tests, integration/e2e scripts, and detailed structured logging evidence.\n\nAcceptance Criteria:\n- Section test matrix covers happy-path, edge-case, and adversarial/error-path scenarios for all section deliverables.\n- E2E scripts execute representative end-user workflows and policy/control-plane transitions for this section.\n- Structured logs include stable event/error codes, trace correlation IDs, and enough context for deterministic replay triage.\n- Verification report is deterministic and machine-readable for CI/release gating.\n\nExpected Artifacts:\n- Section-level test matrix and coverage summary artifact.\n- E2E script suite with pass/fail outputs and reproducible fixtures/seeds.\n- Structured log validation report and traceability bundle.\n- Gate verdict artifact consumable by release automation.\n\n- Machine-readable verification artifact at `artifacts/section_13/bd-z7bt/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_13/bd-z7bt/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests must validate contracts, invariants, and failure semantics.\n- E2E tests must validate cross-component behavior from user/operator entrypoints to persisted effects.\n- Logging must support root-cause analysis without hidden context requirements.\n\nTask-Specific Clarification:\n- For \"[13] Section-wide verification gate: comprehensive unit+e2e+logging\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[13] Section-wide verification gate: comprehensive unit+e2e+logging\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[13] Section-wide verification gate: comprehensive unit+e2e+logging\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[13] Section-wide verification gate: comprehensive unit+e2e+logging\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[13] Section-wide verification gate: comprehensive unit+e2e+logging\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"1. Section 13 verification gate runs all success-criteria check scripts and confirms 100% pass rate.\n2. Gate validates: (a) all quantitative targets have measurement infrastructure in place, (b) all success criteria have unit tests, (c) evidence artifacts exist for all measurable criteria.\n3. Quantitative summary: >= 95% compat pass, >= 3x migration velocity, >= 10x compromise reduction, 100% replay coverage, >= 2 replications — all measured and reported.\n4. Gate produces section_13_verification_summary.md with per-criterion measured value vs target.\n5. Any criterion below target is flagged with gap analysis and remediation plan.\n6. The gate itself has a unit test verifying correct aggregation and threshold comparison logic.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:48:29.034567608Z","created_by":"ubuntu","updated_at":"2026-02-20T15:23:28.617407767Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-13","test-obligations","verification-gate"],"dependencies":[{"issue_id":"bd-z7bt","depends_on_id":"bd-1dpd","type":"blocks","created_at":"2026-02-20T08:24:23.893583167Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-z7bt","depends_on_id":"bd-1w78","type":"blocks","created_at":"2026-02-20T07:48:29.583154761Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-z7bt","depends_on_id":"bd-1xao","type":"blocks","created_at":"2026-02-20T07:48:29.434802006Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-z7bt","depends_on_id":"bd-28sz","type":"blocks","created_at":"2026-02-20T07:48:29.336653099Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-z7bt","depends_on_id":"bd-2a4l","type":"blocks","created_at":"2026-02-20T07:48:29.534267146Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-z7bt","depends_on_id":"bd-2f43","type":"blocks","created_at":"2026-02-20T07:48:29.632150919Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-z7bt","depends_on_id":"bd-2l1k","type":"blocks","created_at":"2026-02-20T07:48:29.187313866Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-z7bt","depends_on_id":"bd-2twu","type":"blocks","created_at":"2026-02-20T08:20:49.302407217Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-z7bt","depends_on_id":"bd-34d5","type":"blocks","created_at":"2026-02-20T08:02:25.864726127Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-z7bt","depends_on_id":"bd-3agp","type":"blocks","created_at":"2026-02-20T07:48:29.287689802Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-z7bt","depends_on_id":"bd-3cpa","type":"blocks","created_at":"2026-02-20T07:48:29.238047871Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-z7bt","depends_on_id":"bd-3e74","type":"blocks","created_at":"2026-02-20T07:48:29.386053930Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-z7bt","depends_on_id":"bd-pga7","type":"blocks","created_at":"2026-02-20T07:48:29.484603013Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-z7bt","depends_on_id":"bd-whxp","type":"blocks","created_at":"2026-02-20T07:48:29.137567260Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-z7ix","title":"Epic: Extension Ecosystem + Registry [10.4]","description":"- type: epic","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-20T07:47:58.072163029Z","updated_at":"2026-02-20T07:49:21.107289257Z","closed_at":"2026-02-20T07:49:21.107270903Z","close_reason":"Superseded by canonical detailed master-plan graph (bd-33v) with full 10.N-10.21 and 11-16 coverage, explicit dependencies, and per-section verification gates.","source_repo":".","compaction_level":0,"original_size":0}
{"id":"bd-zm5b","title":"[10.21] Section-wide verification gate: comprehensive unit+e2e+logging","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.21\n\nTask Objective:\nCreate a hard section completion gate that verifies all implemented behavior in this section with comprehensive unit tests, integration/e2e scripts, and detailed structured logging evidence.\n\nAcceptance Criteria:\n- Section test matrix covers happy-path, edge-case, and adversarial/error-path scenarios for all section deliverables.\n- E2E scripts execute representative end-user workflows and policy/control-plane transitions for this section.\n- Structured logs include stable event/error codes, trace correlation IDs, and enough context for deterministic replay triage.\n- Verification report is deterministic and machine-readable for CI/release gating.\n\nExpected Artifacts:\n- Section-level test matrix and coverage summary artifact.\n- E2E script suite with pass/fail outputs and reproducible fixtures/seeds.\n- Structured log validation report and traceability bundle.\n- Gate verdict artifact consumable by release automation.\n\n- Machine-readable verification artifact at `artifacts/section_10_21/bd-zm5b/verification_evidence.json` for CI/release gating.\n- Human-readable verification summary at `artifacts/section_10_21/bd-zm5b/verification_summary.md` linking implementation intent to measured outcomes.\nTesting & Logging Requirements:\n- Unit tests must validate contracts, invariants, and failure semantics.\n- E2E tests must validate cross-component behavior from user/operator entrypoints to persisted effects.\n- Logging must support root-cause analysis without hidden context requirements.\n\nTask-Specific Clarification:\n- For \"[10.21] Section-wide verification gate: comprehensive unit+e2e+logging\", preserve full capability scope from the plan; no feature compression or silent omission is allowed.\n- Verification for \"[10.21] Section-wide verification gate: comprehensive unit+e2e+logging\" must include capability-specific thresholds and machine-readable pass/fail evidence artifacts.\n- Unit tests, integration/E2E scripts, and structured logs for \"[10.21] Section-wide verification gate: comprehensive unit+e2e+logging\" must support deterministic replay and root-cause triage without hidden context.\n\nWhy This Improves User Outcomes:\n- \"[10.21] Section-wide verification gate: comprehensive unit+e2e+logging\" is required to reduce operator error, increase deterministic safety guarantees, and improve time-to-diagnosis for failures in real-world workflows.\n- Explicit success/failure evidence for \"[10.21] Section-wide verification gate: comprehensive unit+e2e+logging\" prevents false confidence and improves trust in migration/compatibility/security outcomes.\n- Detailed unit + integration/E2E + structured logging requirements ensure reproducible quality and faster incident remediation for users.","acceptance_criteria":"- Section test matrix covers happy-path, edge-case, and adversarial/error-path scenarios for all section deliverables.\n- E2E scripts execute representative end-user workflows and policy/control-plane transitions for this section.\n- Structured logs include stable event/error codes, trace correlation IDs, and enough context for deterministic replay triage.\n- Verification report is deterministic and machine-readable for CI/release gating.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-20T07:48:21.762437314Z","created_by":"ubuntu","updated_at":"2026-02-20T15:45:28.124456449Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-21","test-obligations","verification-gate"],"dependencies":[{"issue_id":"bd-zm5b","depends_on_id":"bd-1b9x","type":"blocks","created_at":"2026-02-20T07:48:22.300226185Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-zm5b","depends_on_id":"bd-1dpd","type":"blocks","created_at":"2026-02-20T08:24:25.464208870Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-zm5b","depends_on_id":"bd-1ga5","type":"blocks","created_at":"2026-02-20T07:48:22.447989892Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-zm5b","depends_on_id":"bd-1jpc","type":"blocks","created_at":"2026-02-20T07:48:22.251792795Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-zm5b","depends_on_id":"bd-1naf","type":"blocks","created_at":"2026-02-20T07:48:21.911223176Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-zm5b","depends_on_id":"bd-232t","type":"blocks","created_at":"2026-02-20T07:48:22.202650104Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-zm5b","depends_on_id":"bd-2ao3","type":"blocks","created_at":"2026-02-20T07:48:22.399502893Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-zm5b","depends_on_id":"bd-2lll","type":"blocks","created_at":"2026-02-20T07:48:22.351386493Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-zm5b","depends_on_id":"bd-2twu","type":"blocks","created_at":"2026-02-20T08:20:51.133554194Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-zm5b","depends_on_id":"bd-2xgs","type":"blocks","created_at":"2026-02-20T07:48:22.545790912Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-zm5b","depends_on_id":"bd-2zo1","type":"blocks","created_at":"2026-02-20T07:48:22.103844453Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-zm5b","depends_on_id":"bd-39ga","type":"blocks","created_at":"2026-02-20T07:48:22.595406483Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-zm5b","depends_on_id":"bd-3cbi","type":"blocks","created_at":"2026-02-20T07:48:22.008292252Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-zm5b","depends_on_id":"bd-3rai","type":"blocks","created_at":"2026-02-20T07:48:22.495990636Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-zm5b","depends_on_id":"bd-3v9l","type":"blocks","created_at":"2026-02-20T07:48:21.859341734Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-zm5b","depends_on_id":"bd-aoq6","type":"blocks","created_at":"2026-02-20T07:48:22.056325276Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-zm5b","depends_on_id":"bd-kwwg","type":"blocks","created_at":"2026-02-20T07:48:22.152229884Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-zm5b","depends_on_id":"bd-ye4m","type":"blocks","created_at":"2026-02-20T07:48:21.959993523Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-zxk8","title":"[10.N] Publish canonical capability ownership registry","description":"Master Plan Source: PLAN_TO_CREATE_FRANKEN_NODE.md\nSection: 10.N — Execution Normalization Contract (No Duplicate Implementations)\n\nWhy This Exists:\nThe canonical ownership map prevents duplicate implementations of the same protocol semantics across parallel tracks. Without this, multiple tracks would independently implement the same capability, creating maintenance burden, divergence risk, and confusion about which version is authoritative.\n\nTask Objective:\nPublish and enforce a canonical ownership registry mapping each capability/protocol family to exactly one implementation owner track, with all other tracks constrained to integration/policy/adoption responsibilities.\n\nCANONICAL OWNERSHIP MAP (from Plan Section 10.N):\n- remote registry/idempotency/saga semantics: CANONICAL in 10.14, integrated and policy-gated in 10.15.\n- epoch validity + transition barriers: CANONICAL in 10.14, integrated into control workflows in 10.15.\n- evidence ledger + replay validator: CANONICAL in 10.14, mandatory adoption and release gating in 10.15.\n- fault harness/cancellation injection/DPOR exploration: CANONICAL harness in 10.14, control-plane enforcement gate in 10.15.\n- verifier SDK/replay capsules/claim compiler + trust scoreboard: CANONICAL in 10.17, ecosystem distribution/adoption in 10.9 + 10.12.\n- semantic oracle: L1 product oracle owned in 10.2, L2 engine-boundary oracle owned in 10.17.\n- authenticated control channel + anti-replay framing: CANONICAL protocol in 10.13, adoption and policy rollout in 10.10 + 10.15.\n- revocation freshness semantics: CANONICAL enforcement in 10.13, ecosystem/policy adoption in 10.4 + 10.10.\n- stable error taxonomy and recovery contract: CANONICAL definition in 10.13, operations/product-surface adoption in 10.8 + 10.10.\n- trust protocol vectors/golden fixtures: CANONICAL generation in 10.13 + 10.14, release and publication gates in 10.7 + 10.10.\n- verifiable execution fabric (policy-constraint compiler + receipt commitments + proof generation/verification): CANONICAL in 10.18, consumed by 10.17 verifier/claim surfaces and enforced through 10.15 control-plane gates.\n- adversarial trust commons federation (privacy-preserving signal sharing + global priors + incentive weighting): CANONICAL in 10.19, consumed by 10.17 adversary graph/reputation surfaces and enforced through 10.15 + 10.4 trust controls.\n- dependency graph immune system (topological risk model + contagion simulator + preemptive barrier planner): CANONICAL in 10.20, consumed by 10.17 adversary/economic scoring, 10.15 control-plane containment, and 10.19 federated threat-intelligence enrichment.\n- behavioral phenotype evolution tracker (longitudinal genome modeling + drift/regime-shift detection + hazard scoring): CANONICAL in 10.21, consumed by 10.17 adversary/trust scoring, 10.20 topological prioritization, 10.19 federated temporal intelligence, and 10.2/10.15 migration-control gating.\n- spec-first Node/Bun compatibility extraction and fixture-oracle baselining: CANONICAL in 10.2, consumed by 10.3 migration automation and 10.7 release verification.\n\nORACLE DELIVERY CLOSE CONDITION:\nDual-layer oracle is only complete when L1 (10.2) + L2 (10.17) + release policy linkage (10.2) are all green.\n\nAcceptance Criteria:\n- Ownership registry enumerates ALL capability domains listed above.\n- Each capability has one and only one canonical implementation owner.\n- Integration/adoption tracks reference owner capability IDs rather than re-defining implementation semantics.\n- Machine-readable registry format (JSON) queryable by CI gates and agents.\n- Registry is linked into all section epics as a dependency to ensure awareness.\n\nExpected Artifacts:\n- docs/capability_ownership_registry.json — machine-readable canonical map.\n- docs/CAPABILITY_OWNERSHIP_REGISTRY.md — human-readable reference.\n- CI integration for ownership validation.\n\nTesting and Logging Requirements:\n- Unit tests: ownership map schema validation, uniqueness constraints, no-orphan capability checks.\n- Integration tests: cross-track references resolve to canonical owners correctly.\n- E2E: CI gate blocks PRs that introduce duplicate implementations without waiver.\n- Structured logs: OWNERSHIP_VALIDATED, DUPLICATE_DETECTED, WAIVER_GRANTED with trace IDs.","status":"closed","priority":1,"issue_type":"task","assignee":"ubuntu","created_at":"2026-02-20T07:50:04.092016885Z","created_by":"ubuntu","updated_at":"2026-02-20T13:07:33.935321926Z","closed_at":"2026-02-20T08:13:12.040442327Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["plan","section-10-N","self-contained","test-obligations"]}
